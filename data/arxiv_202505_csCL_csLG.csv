title,summary,published,authors,pdf_url,category
Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs,"Large Language Models (LLMs) deliver state-of-the-art capabilities across
numerous tasks, but their immense size and inference costs pose significant
computational challenges for practical deployment. While structured pruning
offers a promising avenue for model compression, existing methods often
struggle with the detrimental effects of aggressive, simultaneous width and
depth reductions, leading to substantial performance degradation. This paper
argues that a critical, often overlooked, aspect in making such aggressive
joint pruning viable is the strategic re-initialization and adjustment of
remaining weights to improve the model post-pruning training accuracies. We
introduce Pangu Light, a framework for LLM acceleration centered around
structured pruning coupled with novel weight re-initialization techniques
designed to address this ``missing piece''. Our framework systematically
targets multiple axes, including model width, depth, attention heads, and
RMSNorm, with its effectiveness rooted in novel re-initialization methods like
Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)
that mitigate performance drops by providing the network a better training
starting point. Further enhancing efficiency, Pangu Light incorporates
specialized optimizations such as absorbing Post-RMSNorm computations and
tailors its strategies to Ascend NPU characteristics. The Pangu Light models
consistently exhibit a superior accuracy-efficiency trade-off, outperforming
prominent baseline pruning methods like Nemotron and established LLMs like
Qwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average
score and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and
2225 tokens/s.",2025-05-26,"Hanting Chen, Jiarui Qin, Jialong Guo, Tao Yuan, Yichun Yin, Huiling Zhen, Yasheng Wang, Jinpeng Li, Xiaojun Meng, Meng Zhang, Rongju Ruan, Zheyuan Bai, Yehui Tang, Can Chen, Xinghao Chen, Fisher Yu, Ruiming Tang, Yunhe Wang",http://arxiv.org/pdf/2505.20155v1,cs.CL
UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models,"This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA),
a novel parameter-efficient fine-tuning (PEFT) approach for Large Language
Models (LLMs). UORA achieves state-of-the-art performance and parameter
efficiency by leveraging a low-rank approximation method to reduce the number
of trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA
employs an interpolation-based reparametrization mechanism that selectively
reinitializes rows and columns in frozen projection matrices, guided by the
vector magnitude heuristic. This results in substantially fewer trainable
parameters compared to LoRA and outperforms VeRA in computation and storage
efficiency. Comprehensive experiments across various benchmarks demonstrate
UORA's superiority in achieving competitive fine-tuning performance with
negligible computational overhead. We demonstrate its performance on GLUE and
E2E benchmarks and its effectiveness in instruction-tuning large language
models and image classification models. Our contributions establish a new
paradigm for scalable and resource-efficient fine-tuning of LLMs.",2025-05-26,"Xueyan Zhang, Jinman Zhao, Zhifei Yang, Yibo Zhong, Shuhao Guan, Linbo Cao, Yining Wang",http://arxiv.org/pdf/2505.20154v1,cs.CL
Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models,"Benefiting from contrastively trained visual encoders on large-scale natural
scene images, Large Multimodal Models (LMMs) have achieved remarkable
performance across various visual perception tasks. However, the inherent
limitations of contrastive learning upon summarized descriptions fundamentally
restrict the capabilities of models in meticulous reasoning, particularly in
crucial scenarios of geometric problem-solving. To enhance geometric
understanding, we propose a novel hard negative contrastive learning framework
for the vision encoder, which combines image-based contrastive learning using
generation-based hard negatives created by perturbing diagram generation code,
and text-based contrastive learning using rule-based negatives derived from
modified geometric descriptions and retrieval-based negatives selected based on
caption similarity. We train CLIP using our strong negative learning method,
namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for
geometric problem-solving. Experiments show that our trained model, MMGeoLM,
significantly outperforms other open-source models on three geometric reasoning
benchmarks. Even with a size of 7B, it can rival powerful closed-source models
like GPT-4o. We further study the impact of different negative sample
construction methods and the number of negative samples on the geometric
reasoning performance of LMM, yielding fruitful conclusions. The code and
dataset are available at https://github.com/THU-KEG/MMGeoLM.",2025-05-26,"Kai Sun, Yushi Bai, Zhen Yang, Jiajie Zhang, Ji Qi, Lei Hou, Juanzi Li",http://arxiv.org/pdf/2505.20152v1,cs.CL
SeMe: Training-Free Language Model Merging via Semantic Alignment,"Despite the remarkable capabilities of Language Models (LMs) across diverse
tasks, no single model consistently outperforms others, necessitating efficient
methods to combine their strengths without expensive retraining. Existing model
merging techniques, such as parameter averaging and task-guided fusion, often
rely on data-dependent computations or fail to preserve internal knowledge,
limiting their robustness and scalability. We introduce SeMe (Semantic-based
Merging), a novel, data-free, and training-free approach that leverages latent
semantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike
prior work, SeMe not only preserves model behaviors but also explicitly
stabilizes internal knowledge, addressing a critical gap in LM fusion. Through
extensive experiments across diverse architectures and tasks, we demonstrate
that SeMe outperforms existing methods in both performance and efficiency while
eliminating reliance on external data. Our work establishes a new paradigm for
knowledge-aware model merging and provides insights into the semantic structure
of LMs, paving the way for more scalable and interpretable model composition.",2025-05-26,"Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang",http://arxiv.org/pdf/2505.20144v1,cs.CL
StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs,"As Large Language Models (LLMs) become integral to software development
workflows, their ability to generate structured outputs has become critically
important. We introduce StructEval, a comprehensive benchmark for evaluating
LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and
renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,
StructEval systematically evaluates structural fidelity across diverse formats
through two paradigms: 1) generation tasks, producing structured output from
natural language prompts, and 2) conversion tasks, translating between
structured formats. Our benchmark encompasses 18 formats and 44 types of task,
with novel metrics for format adherence and structural correctness. Results
reveal significant performance gaps, even state-of-the-art models like o1-mini
achieve only 75.58 average score, with open-source alternatives lagging
approximately 10 points behind. We find generation tasks more challenging than
conversion tasks, and producing correct visual content more difficult than
generating text-only structures.",2025-05-26,"Jialin Yang, Dongfu Jiang, Lipeng He, Sherman Siu, Yuxuan Zhang, Disen Liao, Zhuofeng Li, Huaye Zeng, Yiming Jia, Haozhe Wang, Benjamin Schneider, Chi Ruan, Wentao Ma, Zhiheng Lyu, Yifei Wang, Yi Lu, Quy Duc Do, Ziyan Jiang, Ping Nie, Wenhu Chen",http://arxiv.org/pdf/2505.20139v1,cs.CL
AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings,"Current language models rely on static vocabularies determined at pretraining
time, which can lead to decreased performance and increased computational cost
for domains underrepresented in the original vocabulary. New tokens can be
added to solve this problem, when coupled with a good initialization for their
new embeddings. However, existing embedding initialization methods either
require expensive further training or pretraining of additional modules. In
this paper, we propose AweDist and show that by distilling representations
obtained using the original tokenization, we can quickly learn high-quality
input embeddings for new tokens. Experimental results with a wide range of
open-weight models show that AweDist is able to outperform even strong
baselines.",2025-05-26,"Konstantin Dobler, Desmond Elliott, Gerard de Melo",http://arxiv.org/pdf/2505.20133v1,cs.CL
Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers,"Large language models (LLMs) have been widely integrated into information
retrieval to advance traditional techniques. However, effectively enabling LLMs
to seek accurate knowledge in complex tasks remains a challenge due to the
complexity of multi-hop queries as well as the irrelevant retrieved content. To
address these limitations, we propose EXSEARCH, an agentic search framework,
where the LLM learns to retrieve useful information as the reasoning unfolds
through a self-incentivized process. At each step, the LLM decides what to
retrieve (thinking), triggers an external retriever (search), and extracts
fine-grained evidence (recording) to support next-step reasoning. To enable LLM
with this capability, EXSEARCH adopts a Generalized Expectation-Maximization
algorithm. In the E-step, the LLM generates multiple search trajectories and
assigns an importance weight to each; the M-step trains the LLM on them with a
re-weighted loss function. This creates a self-incentivized loop, where the LLM
iteratively learns from its own generated data, progressively improving itself
for search. We further theoretically analyze this training process,
establishing convergence guarantees. Extensive experiments on four
knowledge-intensive benchmarks show that EXSEARCH substantially outperforms
baselines, e.g., +7.8% improvement on exact match score. Motivated by these
promising results, we introduce EXSEARCH-Zoo, an extension that extends our
method to broader scenarios, to facilitate future work.",2025-05-26,"Zhengliang Shi, Lingyong Yan, Dawei Yin, Suzan Verberne, Maarten de Rijke, Zhaochun Ren",http://arxiv.org/pdf/2505.20128v1,cs.CL
TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent,"As large language models (LLMs) become integrated into sensitive workflows,
concerns grow over their potential to leak confidential information. We propose
TrojanStego, a novel threat model in which an adversary fine-tunes an LLM to
embed sensitive context information into natural-looking outputs via linguistic
steganography, without requiring explicit control over inference inputs. We
introduce a taxonomy outlining risk factors for compromised LLMs, and use it to
evaluate the risk profile of the threat. To implement TrojanStego, we propose a
practical encoding scheme based on vocabulary partitioning learnable by LLMs
via fine-tuning. Experimental results show that compromised models reliably
transmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over
97% accuracy using majority voting across three generations. Further, they
maintain high utility, can evade human detection, and preserve coherence. These
results highlight a new class of LLM data exfiltration attacks that are
passive, covert, practical, and dangerous.",2025-05-26,"Dominik Meier, Jan Philip Wahle, Paul Röttger, Terry Ruas, Bela Gipp",http://arxiv.org/pdf/2505.20118v1,cs.CL
Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone,"The increased digitization of world's textual heritage poses significant
challenges for both computer science and literary studies. Overall, there is an
urgent need of computational techniques able to adapt to the challenges of
historical texts, such as orthographic and spelling variations, fragmentary
structure and digitization errors. The rise of large language models (LLMs) has
revolutionized natural language processing, suggesting promising applications
for Named Entity Recognition (NER) on historical documents. In spite of this,
no thorough evaluation has been proposed for Italian texts. This research tries
to fill the gap by proposing a new challenging dataset for entity extraction
based on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's
Zibaldone (1898), containing 2,899 references to people, locations and literary
works. This dataset was used to carry out reproducible experiments with both
domain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.
Results show that instruction-tuned models encounter multiple difficulties
handling historical humanistic texts, while fine-tuned NER models offer more
robust performance even with challenging entity types such as bibliographic
references.",2025-05-26,"Cristian Santini, Laura Melosi, Emanuele Frontoni",http://arxiv.org/pdf/2505.20113v1,cs.CL
ResSVD: Residual Compensated SVD for Large Language Model Compression,"Large language models (LLMs) have demonstrated impressive capabilities in a
wide range of downstream natural language processing tasks. Nevertheless, their
considerable sizes and memory demands hinder practical deployment, underscoring
the importance of developing efficient compression strategies. Singular value
decomposition (SVD) decomposes a matrix into orthogonal components, enabling
efficient low-rank approximation. This is particularly suitable for LLM
compression, where weight matrices often exhibit significant redundancy.
However, current SVD-based methods neglect the residual matrix from truncation,
resulting in significant truncation loss. Additionally, compressing all layers
of the model results in severe performance degradation. To overcome these
limitations, we propose ResSVD, a new post-training SVD-based LLM compression
method. Specifically, we leverage the residual matrix generated during the
truncation process to reduce truncation loss. Moreover, under a fixed overall
compression ratio, we selectively compress the last few layers of the model,
which mitigates error propagation and significantly improves the performance of
compressed models.Comprehensive evaluations of ResSVD on diverse LLM families
and multiple benchmark datasets indicate that ResSVD consistently achieves
superior performance over existing counterpart methods, demonstrating its
practical effectiveness.",2025-05-26,"Haolei Bai, Siyong Jian, Tuo Liang, Yu Yin, Huan Wang",http://arxiv.org/pdf/2505.20112v1,cs.CL
Language-Agnostic Suicidal Risk Detection Using Large Language Models,"Suicidal risk detection in adolescents is a critical challenge, yet existing
methods rely on language-specific models, limiting scalability and
generalization. This study introduces a novel language-agnostic framework for
suicidal risk assessment with large language models (LLMs). We generate Chinese
transcripts from speech using an ASR model and then employ LLMs with
prompt-based queries to extract suicidal risk-related features from these
transcripts. The extracted features are retained in both Chinese and English to
enable cross-linguistic analysis and then used to fine-tune corresponding
pretrained language models independently. Experimental results show that our
method achieves performance comparable to direct fine-tuning with ASR results
or to models trained solely on Chinese suicidal risk-related features,
demonstrating its potential to overcome language constraints and improve the
robustness of suicidal risk assessment.",2025-05-26,"June-Woo Kim, Wonkyo Oh, Haram Yoon, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, Chan-Mo Yang",http://arxiv.org/pdf/2505.20109v1,cs.CL
SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment,"Citations are crucial in scientific research articles as they highlight the
connection between the current study and prior work. However, this process is
often time-consuming for researchers. In this study, we propose the SciRGC
framework, which aims to automatically recommend citation articles and generate
citation sentences for citation locations within articles. The framework
addresses two key challenges in academic citation generation: 1) how to
accurately identify the author's citation intent and find relevant citation
papers, and 2) how to generate high-quality citation sentences that align with
human preferences. We enhance citation recommendation accuracy in the citation
article recommendation module by incorporating citation networks and sentiment
intent, and generate reasoning-based citation sentences in the citation
sentence generation module by using the original article abstract, local
context, citation intent, and recommended articles as inputs. Additionally, we
propose a new evaluation metric to fairly assess the quality of generated
citation sentences. Through comparisons with baseline models and ablation
experiments, the SciRGC framework not only improves the accuracy and relevance
of citation recommendations but also ensures the appropriateness of the
generated citation sentences in context, providing a valuable tool for
interdisciplinary researchers.",2025-05-26,"Xiangyu Li, Jingqiang Chen",http://arxiv.org/pdf/2505.20103v1,cs.CL
Adaptive Deep Reasoning: Triggering Deep Thinking When Needed,"Large language models (LLMs) have shown impressive capabilities in handling
complex tasks through long-chain reasoning. However, the extensive reasoning
steps involved can significantly increase computational costs, posing
challenges for real-world deployment. Recent efforts have focused on optimizing
reasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning
processes through various approaches, such as length-aware prompt engineering,
supervised fine-tuning on CoT data with variable lengths, and reinforcement
learning with length penalties. Although these methods effectively reduce
reasoning length, they still necessitate an initial reasoning phase. More
recent approaches have attempted to integrate long-chain and short-chain
reasoning abilities into a single model, yet they still rely on manual control
to toggle between short and long CoT.In this work, we propose a novel approach
that autonomously switches between short and long reasoning chains based on
problem complexity. Our method begins with supervised fine-tuning of the base
model to equip both long-chain and short-chain reasoning abilities. We then
employ reinforcement learning to further balance short and long CoT generation
while maintaining accuracy through two key strategies: first, integrating
reinforcement learning with a long-short adaptive group-wise reward strategy to
assess prompt complexity and provide corresponding rewards; second,
implementing a logit-based reasoning mode switching loss to optimize the
model's initial token choice, thereby guiding the selection of the reasoning
type.Evaluations on mathematical datasets demonstrate that our model can
dynamically switch between long-chain and short-chain reasoning modes without
substantially sacrificing performance. This advancement enhances the
practicality of reasoning in large language models for real-world applications.",2025-05-26,"Yunhao Wang, Yuhao Zhang, Tinghao Yu, Can Xu, Feng Zhang, Fengzong Lian",http://arxiv.org/pdf/2505.20101v1,cs.CL
Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities,"Large language models (LLMs) have demonstrated remarkable performance on
question-answering (QA) tasks because of their superior capabilities in natural
language understanding and generation. However, LLM-based QA struggles with
complex QA tasks due to poor reasoning capacity, outdated knowledge, and
hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)
for QA to address the above challenges. In this survey, we propose a new
structured taxonomy that categorizes the methodology of synthesizing LLMs and
KGs for QA according to the categories of QA and the KG's role when integrating
with LLMs. We systematically survey state-of-the-art advances in synthesizing
LLMs and KGs for QA and compare and analyze these approaches in terms of
strength, limitations, and KG requirements. We then align the approaches with
QA and discuss how these approaches address the main challenges of different
complex QA. Finally, we summarize the advancements, evaluation metrics, and
benchmark datasets and highlight open challenges and opportunities.",2025-05-26,"Chuangtao Ma, Yongrui Chen, Tianxing Wu, Arijit Khan, Haofen Wang",http://arxiv.org/pdf/2505.20099v1,cs.CL
S2LPP: Small-to-Large Prompt Prediction across LLMs,"The performance of pre-trained Large Language Models (LLMs) is often
sensitive to nuances in prompt templates, requiring careful prompt engineering,
adding costs in terms of computing and human effort. In this study, we present
experiments encompassing multiple LLMs variants of varying sizes aimed at
probing their preference with different prompts. Through experiments on
Question Answering, we show prompt preference consistency across LLMs of
different sizes. We also show that this consistency extends to other tasks,
such as Natural Language Inference. Utilizing this consistency, we propose a
method to use a smaller model to select effective prompt templates for a larger
model. We show that our method substantially reduces the cost of prompt
engineering while consistently matching performance with optimal prompts among
candidates. More importantly, our experiment shows the efficacy of our strategy
across fourteen LLMs and its applicability to a broad range of NLP tasks,
highlighting its robustness",2025-05-26,"Liang Cheng, Tianyi LI, Zhaowei Wang, Mark Steedman",http://arxiv.org/pdf/2505.20097v1,cs.CL
MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning,"We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation
(RAG) that addresses the inherent ambiguities and reasoning challenges in
complex information-seeking tasks. Unlike conventional RAG methods that rely on
either end-to-end fine-tuning or isolated component enhancements, MA-RAG
orchestrates a collaborative set of specialized AI agents: Planner, Step
Definer, Extractor, and QA Agents, to tackle each stage of the RAG pipeline
with task-aware reasoning. Ambiguities may arise from underspecified queries,
sparse or indirect evidence in retrieved documents, or the need to integrate
information scattered across multiple sources. MA-RAG mitigates these
challenges by decomposing the problem into subtasks, such as query
disambiguation, evidence extraction, and answer synthesis, and dispatching them
to dedicated agents equipped with chain-of-thought prompting. These agents
communicate intermediate reasoning and progressively refine the retrieval and
synthesis process. Our design allows fine-grained control over information flow
without any model fine-tuning. Crucially, agents are invoked on demand,
enabling a dynamic and efficient workflow that avoids unnecessary computation.
This modular and reasoning-driven architecture enables MA-RAG to deliver
robust, interpretable results. Experiments on multi-hop and ambiguous QA
benchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free
baselines and rivals fine-tuned systems, validating the effectiveness of
collaborative agent-based reasoning in RAG.",2025-05-26,"Thang Nguyen, Peter Chin, Yu-Wing Tai",http://arxiv.org/pdf/2505.20096v1,cs.CL
Multi-Domain Explainability of Preferences,"Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and
reward models, are central to aligning and evaluating large language models
(LLMs). Yet, the underlying concepts that drive these preferences remain poorly
understood. In this work, we propose a fully automated end-to-end method for
generating local and global concept-based explanations of preferences across
multiple domains. Our method employs an LLM to discover concepts that
differentiate between chosen and rejected responses and represent them with
concept-based vectors. To model the relationships between concepts and
preferences, we propose a white-box Hierarchical Multi-Domain Regression model
that captures both domain-general and domain-specific effects. To evaluate our
method, we curate a dataset spanning eight challenging and diverse domains and
explain twelve mechanisms. Our method achieves strong preference prediction
performance, outperforming baselines while also being explainable.
Additionally, we assess explanations in two novel application-driven settings.
First, guiding LLM outputs with concepts from LaaJ explanations yields
responses that those judges consistently prefer. Second, prompting LaaJs with
concepts explaining humans improves their preference predictions. Together, our
work provides a new paradigm for explainability in the era of LLMs.",2025-05-26,"Nitay Calderon, Liat Ein-Dor, Roi Reichart",http://arxiv.org/pdf/2505.20088v1,cs.CL
Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models,"Reasoning-based language models have demonstrated strong performance across
various domains, with the most notable gains seen in mathematical and coding
tasks. Recent research has shown that reasoning also offers significant
benefits for LLM safety and guardrail applications. In this work, we conduct a
comprehensive analysis of training reasoning-based guardrail models for content
moderation, with an emphasis on generalization to custom safety policies at
inference time. Our study focuses on two key dimensions: data efficiency and
inference efficiency. On the data front, we find that reasoning-based models
exhibit strong sample efficiency, achieving competitive performance with
significantly fewer training examples than their non-reasoning counterparts.
This unlocks the potential to repurpose the remaining data for mining
high-value, difficult samples that further enhance model performance. On the
inference side, we evaluate practical trade-offs by introducing reasoning
budgets, examining the impact of reasoning length on latency and accuracy, and
exploring dual-mode training to allow runtime control over reasoning behavior.
Our findings will provide practical insights for researchers and developers to
effectively and efficiently train and deploy reasoning-based guardrails models
in real-world systems.",2025-05-26,"Makesh Narsimhan Sreedhar, Traian Rebedea, Christopher Parisien",http://arxiv.org/pdf/2505.20087v1,cs.CL
Inference-time Alignment in Continuous Space,"Aligning large language models with human feedback at inference time has
received increasing attention due to its flexibility. Existing methods rely on
generating multiple responses from the base policy for search using a reward
model, which can be considered as searching in a discrete response space.
However, these methods struggle to explore informative candidates when the base
policy is weak or the candidate set is small, resulting in limited
effectiveness. In this paper, to address this problem, we propose Simple Energy
Adaptation ($\textbf{SEA}$), a simple yet effective algorithm for
inference-time alignment. In contrast to expensive search over the discrete
space, SEA directly adapts original responses from the base policy toward the
optimal one via gradient-based sampling in continuous latent space.
Specifically, SEA formulates inference as an iterative optimization procedure
on an energy function over actions in the continuous space defined by the
optimal policy, enabling simple and effective alignment. For instance, despite
its simplicity, SEA outperforms the second-best baseline with a relative
improvement of up to $ \textbf{77.51%}$ on AdvBench and $\textbf{16.36%}$ on
MATH. Our code is publicly available at https://github.com/yuanyige/SEA",2025-05-26,"Yige Yuan, Teng Xiao, Li Yunfan, Bingbing Xu, Shuchang Tao, Yunqi Qiu, Huawei Shen, Xueqi Cheng",http://arxiv.org/pdf/2505.20081v1,cs.CL
Incentivizing Reasoning from Weak Supervision,"Large language models (LLMs) have demonstrated impressive performance on
reasoning-intensive tasks, but enhancing their reasoning abilities typically
relies on either reinforcement learning (RL) with verifiable signals or
supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)
demonstrations, both of which are expensive. In this paper, we study a novel
problem of incentivizing the reasoning capacity of LLMs without expensive
high-quality demonstrations and reinforcement learning. We investigate whether
the reasoning capabilities of LLMs can be effectively incentivized via
supervision from significantly weaker models. We further analyze when and why
such weak supervision succeeds in eliciting reasoning abilities in stronger
models. Our findings show that supervision from significantly weaker reasoners
can substantially improve student reasoning performance, recovering close to
94% of the gains of expensive RL at a fraction of the cost. Experiments across
diverse benchmarks and model architectures demonstrate that weak reasoners can
effectively incentivize reasoning in stronger student models, consistently
improving performance across a wide range of reasoning tasks. Our results
suggest that this simple weak-to-strong paradigm is a promising and
generalizable alternative to costly methods for incentivizing strong reasoning
capabilities at inference-time in LLMs. The code is publicly available at
https://github.com/yuanyige/W2SR.",2025-05-26,"Yige Yuan, Teng Xiao, Shuchang Tao, Xue Wang, Jinyang Gao, Bolin Ding, Bingbing Xu",http://arxiv.org/pdf/2505.20072v1,cs.CL
SAEs Are Good for Steering -- If You Select the Right Features,"Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to
learn a decomposition of a model's latent space. This enables useful
applications such as steering - influencing the output of a model towards a
desired concept - without requiring labeled data. Current methods identify SAE
features to steer by analyzing the input tokens that activate them. However,
recent work has highlighted that activations alone do not fully describe the
effect of a feature on the model's output. In this work, we draw a distinction
between two types of features: input features, which mainly capture patterns in
the model's input, and output features, which have a human-understandable
effect on the model's output. We propose input and output scores to
characterize and locate these types of features, and show that high values for
both scores rarely co-occur in the same features. These findings have practical
implications: after filtering out features with low output scores, we obtain
2-3x improvements when steering with SAEs, making them competitive with
supervised methods.",2025-05-26,"Dana Arad, Aaron Mueller, Yonatan Belinkov",http://arxiv.org/pdf/2505.20063v1,cs.CL
Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion,"Diffusion models have become the mainstream architecture for text-to-image
generation, achieving remarkable progress in visual quality and prompt
controllability. However, current inference pipelines generally lack
interpretable semantic supervision and correction mechanisms throughout the
denoising process. Most existing approaches rely solely on post-hoc scoring of
the final image, prompt filtering, or heuristic resampling strategies-making
them ineffective in providing actionable guidance for correcting the generative
trajectory. As a result, models often suffer from object confusion, spatial
errors, inaccurate counts, and missing semantic elements, severely compromising
prompt-image alignment and image quality. To tackle these challenges, we
propose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel
framework that, for the first time, introduces a Multimodal Large Language
Model (MLLM) as a semantic observer during inference. PPAD performs real-time
analysis on intermediate generations, identifies latent semantic
inconsistencies, and translates feedback into controllable signals that
actively guide the remaining denoising steps. The framework supports both
inference-only and training-enhanced settings, and performs semantic correction
at only extremely few diffusion steps, offering strong generality and
scalability. Extensive experiments demonstrate PPAD's significant improvements.",2025-05-26,"Zheqi Lv, Junhao Chen, Qi Tian, Keting Yin, Shengyu Zhang, Fei Wu",http://arxiv.org/pdf/2505.20053v1,cs.CL
MVP: Multi-source Voice Pathology detection,"Voice disorders significantly impact patient quality of life, yet
non-invasive automated diagnosis remains under-explored due to both the
scarcity of pathological voice data, and the variability in recording sources.
This work introduces MVP (Multi-source Voice Pathology detection), a novel
approach that leverages transformers operating directly on raw voice signals.
We explore three fusion strategies to combine sentence reading and sustained
vowel recordings: waveform concatenation, intermediate feature fusion, and
decision-level combination. Empirical validation across the German, Portuguese,
and Italian languages shows that intermediate feature fusion using transformers
best captures the complementary characteristics of both recording types. Our
approach achieves up to +13% AUC improvement over single-source methods.",2025-05-26,"Alkis Koudounas, Moreno La Quatra, Gabriele Ciravegna, Marco Fantini, Erika Crosetti, Giovanni Succo, Tania Cerquitelli, Sabato Marco Siniscalchi, Elena Baralis",http://arxiv.org/pdf/2505.20050v1,cs.CL
Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks,"Large language models (LLMs) show remarkable promise for democratizing
automated reasoning by generating formal specifications. However, a fundamental
tension exists: LLMs are probabilistic, while formal verification demands
deterministic guarantees. This paper addresses this epistemological gap by
comprehensively investigating failure modes and uncertainty quantification (UQ)
in LLM-generated formal artifacts. Our systematic evaluation of five frontier
LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's
domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on
factual ones), with known UQ techniques like the entropy of token probabilities
failing to identify these errors. We introduce a probabilistic context-free
grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty
taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy
for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables
selective verification, drastically reducing errors (14-100%) with minimal
abstention, transforming LLM-driven formalization into a reliable engineering
discipline.",2025-05-26,"Debargha Ganguly, Vikash Singh, Sreehari Sankar, Biyao Zhang, Xuecen Zhang, Srinivasan Iyengar, Xiaotian Han, Amit Sharma, Shivkumar Kalyanaraman, Vipin Chaudhary",http://arxiv.org/pdf/2505.20047v1,cs.CL
REARANK: Reasoning Re-ranking Agent via Reinforcement Learning,"We present REARANK, a large language model (LLM)-based listwise reasoning
reranking agent. REARANK explicitly reasons before reranking, significantly
improving both performance and interpretability. Leveraging reinforcement
learning and data augmentation, REARANK achieves substantial improvements over
baseline models across popular information retrieval benchmarks, notably
requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our
REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and
out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT
benchmarks. These results underscore the effectiveness of our approach and
highlight how reinforcement learning can enhance LLM reasoning capabilities in
reranking.",2025-05-26,"Le Zhang, Bo Wang, Xipeng Qiu, Siva Reddy, Aishwarya Agrawal",http://arxiv.org/pdf/2505.20046v1,cs.CL
Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs,"Large language models (LLMs) exhibit impressive fluency, but often produce
critical errors known as ""hallucinations"". Uncertainty quantification (UQ)
methods are a promising tool for coping with this fundamental shortcoming. Yet,
existing UQ methods face challenges such as high computational overhead or
reliance on supervised learning. Here, we aim to bridge this gap. In
particular, we propose RAUQ (Recurrent Attention-based Uncertainty
Quantification), an unsupervised approach that leverages intrinsic attention
patterns in transformers to detect hallucinations efficiently. By analyzing
attention weights, we identified a peculiar pattern: drops in attention to
preceding tokens are systematically observed during incorrect generations for
certain ""uncertainty-aware"" heads. RAUQ automatically selects such heads,
recurrently aggregates their attention weights and token-level confidences, and
computes sequence-level uncertainty scores in a single forward pass.
Experiments across 4 LLMs and 12 question answering, summarization, and
translation tasks demonstrate that RAUQ yields excellent results, outperforming
state-of-the-art UQ methods using minimal computational overhead (<1% latency).
Moreover, it requires no task-specific labels and no careful hyperparameter
tuning, offering plug-and-play real-time hallucination detection in white-box
LLMs.",2025-05-26,"Artem Vazhentsev, Lyudmila Rvanova, Gleb Kuzmin, Ekaterina Fadeeva, Ivan Lazichny, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Mrinmaya Sachan, Preslav Nakov, Artem Shelmanov",http://arxiv.org/pdf/2505.20045v1,cs.CL
Multi-modal brain encoding models for multi-modal stimuli,"Despite participants engaging in unimodal stimuli, such as watching images or
silent videos, recent work has demonstrated that multi-modal Transformer models
can predict visual brain activity impressively well, even with incongruent
modality representations. This raises the question of how accurately these
multi-modal models can predict brain activity when participants are engaged in
multi-modal stimuli. As these models grow increasingly popular, their use in
studying neural activity provides insights into how our brains respond to such
multi-modal naturalistic stimuli, i.e., where it separates and integrates
information across modalities through a hierarchy of early sensory regions to
higher cognition. We investigate this question by using multiple unimodal and
two types of multi-modal models-cross-modal and jointly pretrained-to determine
which type of model is more relevant to fMRI brain activity when participants
are engaged in watching movies. We observe that both types of multi-modal
models show improved alignment in several language and visual regions. This
study also helps in identifying which brain regions process unimodal versus
multi-modal information. We further investigate the contribution of each
modality to multi-modal alignment by carefully removing unimodal features one
by one from multi-modal representations, and find that there is additional
information beyond the unimodal embeddings that is processed in the visual and
language regions. Based on this investigation, we find that while for
cross-modal models, their brain alignment is partially attributed to the video
modality; for jointly pretrained models, it is partially attributed to both the
video and audio modalities. This serves as a strong motivation for the
neuroscience community to investigate the interpretability of these models for
deepening our understanding of multi-modal information processing in brain.",2025-05-26,"Subba Reddy Oota, Khushbu Pahwa, Mounika Marreddy, Maneesh Singh, Manish Gupta, Bapi S. Raju",http://arxiv.org/pdf/2505.20027v1,cs.CL
Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking,"Autonomous agents, which perceive environments and take actions to achieve
goals, have become increasingly feasible with the advancements in large
language models (LLMs). However, current powerful agents often depend on
sophisticated prompt engineering combined with closed-source LLMs like GPT-4.
Although training open-source LLMs using expert trajectories from teacher
models has yielded some improvements in agent capabilities, this approach still
faces limitations such as performance plateauing and error propagation. To
mitigate these challenges, we propose STeP, a novel method for improving
LLM-based agent training. We synthesize self-reflected trajectories that
include reflections and corrections of error steps, which enhance the
effectiveness of LLM agents in learning from teacher models, enabling them to
become agents capable of self-reflecting and correcting. We also introduce
partial masking strategy that prevents the LLM from internalizing incorrect or
suboptimal steps. Experiments demonstrate that our method improves agent
performance across three representative tasks: ALFWorld, WebShop, and SciWorld.
For the open-source model LLaMA2-7B-Chat, when trained using self-reflected
trajectories constructed with Qwen1.5-110B-Chat as the teacher model, it
achieves comprehensive improvements with less training data compared to agents
trained exclusively on expert trajectories.",2025-05-26,"Yihan Chen, Benfeng Xu, Xiaorui Wang, Yongdong Zhang, Zhendong Mao",http://arxiv.org/pdf/2505.20023v1,cs.CL
TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation,"Existing tool-learning methods usually rely on supervised fine-tuning, they
often overlook fine-grained optimization of internal tool call details, leading
to limitations in preference alignment and error discrimination. To overcome
these challenges, we propose Token-level Tool-use Preference Alignment Training
Framework (TTPA), a training paradigm for constructing token-level tool-use
preference datasets that align LLMs with fine-grained preferences using a novel
error-oriented scoring mechanism. TTPA first introduces reversed dataset
construction, a method for creating high-quality, multi-turn tool-use datasets
by reversing the generation flow. Additionally, we propose Token-level
Preference Sampling (TPS) to capture fine-grained preferences by modeling
token-level differences during generation. To address biases in scoring, we
introduce the Error-oriented Scoring Mechanism (ESM), which quantifies
tool-call errors and can be used as a training signal. Extensive experiments on
three diverse benchmark datasets demonstrate that TTPA significantly improves
tool-using performance while showing strong generalization ability across
models and datasets.",2025-05-26,"Chengrui Huang, Shen Gao, Zhengliang Shi, Dongsheng Wang, Shuo Shang",http://arxiv.org/pdf/2505.20016v1,cs.CL
On the class of coding optimality of human languages and the origins of Zipf's law,"Here we present a new class of optimality for coding systems. Members of that
class are separated linearly from optimal coding and thus exhibit Zipf's law,
namely a power-law distribution of frequency ranks. Whithin that class, Zipf's
law, the size-rank law and the size-probability law form a group-like
structure. We identify human languages that are members of the class. All
languages showing sufficient agreement with Zipf's law are potential members of
the class. In contrast, there are communication systems in other species that
cannot be members of that class for exhibiting an exponential distribution
instead but dolphins and humpback whales might. We provide a new insight into
plots of frequency versus rank in double logarithmic scale. For any system, a
straight line in that scale indicates that the lengths of optimal codes under
non-singular coding and under uniquely decodable encoding are separated by a
linear function whose slope is the exponent of Zipf's law. For systems under
compression and constrained to be uniquely decodable, such a straight line may
indicate that the system is coding close to optimality. Our findings provide
support for the hypothesis that Zipf's law originates from compression.",2025-05-26,Ramon Ferrer-i-Cancho,http://arxiv.org/pdf/2505.20015v1,cs.CL
Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation,"The detection of mental health problems from social media and the
interpretation of these results have been extensively explored. Research has
shown that incorporating clinical symptom information into a model enhances
domain expertise, improving its detection and interpretation performance. While
large language models (LLMs) are shown to be effective for generating
explanatory rationales in mental health detection, their substantially large
parameter size and high computational cost limit their practicality. Reasoning
distillation transfers this ability to smaller language models (SLMs), but
inconsistencies in the relevance and domain alignment of LLM-generated
rationales pose a challenge. This paper investigates how rationale quality
impacts SLM performance in mental health detection and explanation generation.
We hypothesize that ensuring high-quality and domain-relevant rationales
enhances the distillation. To this end, we propose a framework that selects
rationales based on their alignment with expert clinical reasoning. Experiments
show that our quality-focused approach significantly enhances SLM performance
in both mental disorder detection and rationale generation. This work
highlights the importance of rationale quality and offers an insightful
framework for knowledge transfer in mental health applications.",2025-05-26,"Hoyun Song, Huije Lee, Jisu Shin, Sukmin Cho, Changgeon Ko, Jong C. Park",http://arxiv.org/pdf/2505.20014v1,cs.CL
"WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback","Web agents powered by Large Language Models (LLMs) show promise for
next-generation AI, but their limited reasoning in uncertain, dynamic web
environments hinders robust deployment. In this paper, we identify key
reasoning skills essential for effective web agents, i.e., reflection &
lookahead, branching, and rollback, and curate trajectory data that exemplifies
these abilities by reconstructing the agent's (inference-time) reasoning
algorithms into chain-of-thought rationales. We conduct experiments in the
agent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling
salient reasoning patterns into the backbone LLM via simple fine-tuning can
substantially enhance its performance. Our approach yields significant
improvements across multiple benchmarks, including WebVoyager, Mind2web-live,
and SimpleQA (web search), highlighting the potential of targeted reasoning
skill enhancement for web agents.",2025-05-26,"Minda Hu, Tianqing Fang, Jianshu Zhang, Junyu Ma, Zhisong Zhang, Jingyan Zhou, Hongming Zhang, Haitao Mi, Dong Yu, Irwin King",http://arxiv.org/pdf/2505.20013v1,cs.CL
Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition,"We aim to improve the robustness of Automatic Speech Recognition (ASR)
systems against non-native speech, particularly in low-resourced multi-accent
settings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a
fine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA)
experts, each specialized in a specific accent. This method can be used when
the accent is known or unknown at inference time, without the need to fine-tune
the model again. Our experiments, conducted using Whisper on the L2-ARCTIC
corpus, demonstrate significant improvements in Word Error Rate compared to
regular LoRA and full fine-tuning when the accent is unknown. When the accent
is known, the results further improve. Furthermore, MAS-LoRA shows less
catastrophic forgetting than the other fine-tuning methods. To the best of our
knowledge, this is the first use of a mixture of LoRA experts for non-native
multi-accent ASR.",2025-05-26,"Raphaël Bagat, Irina Illina, Emmanuel Vincent",http://arxiv.org/pdf/2505.20006v1,cs.CL
Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents,"Large language models (LLMs) are revolutionizing education, with LLM-based
agents playing a key role in simulating student behavior. A major challenge in
student simulation is modeling the diverse learning patterns of students at
various cognitive levels. However, current LLMs, typically trained as ``helpful
assistants'', target at generating perfect responses. As a result, they
struggle to simulate students with diverse cognitive abilities, as they often
produce overly advanced answers, missing the natural imperfections that
characterize student learning and resulting in unrealistic simulations. To
address this issue, we propose a training-free framework for student
simulation. We begin by constructing a cognitive prototype for each student
using a knowledge graph, which captures their understanding of concepts from
past learning records. This prototype is then mapped to new tasks to predict
student performance. Next, we simulate student solutions based on these
predictions and iteratively refine them using a beam search method to better
replicate realistic mistakes. To validate our approach, we construct the
\texttt{Student\_100} dataset, consisting of $100$ students working on Python
programming and $5,000$ learning records. Experimental results show that our
method consistently outperforms baseline models, achieving $100\%$ improvement
in simulation accuracy.",2025-05-26,"Tao Wu, Jingyuan Chen, Wang Lin, Mengze Li, Yumeng Zhu, Ang Li, Kun Kuang, Fei Wu",http://arxiv.org/pdf/2505.19997v1,cs.CL
How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation,"Large language models (LLMs) have demonstrated strong performance in
general-purpose machine translation, but their effectiveness in complex,
domain-sensitive translation tasks remains underexplored. Recent advancements
in Large Reasoning Models (LRMs), raise the question of whether structured
reasoning can enhance translation quality across diverse domains. In this work,
we compare the performance of LRMs with traditional LLMs across 15
representative domains and four translation directions. Our evaluation
considers various factors, including task difficulty, input length, and
terminology density. We use a combination of automatic metrics and an enhanced
MQM-based evaluation hierarchy to assess translation quality. Our findings show
that LRMs consistently outperform traditional LLMs in semantically complex
domains, especially in long-text and high-difficulty translation scenarios.
Moreover, domain-adaptive prompting strategies further improve performance by
better leveraging the reasoning capabilities of LRMs. These results highlight
the potential of structured reasoning in MDMT tasks and provide valuable
insights for optimizing translation systems in domain-sensitive contexts.",2025-05-26,"Yongshi Ye, Biao Fu, Chongxuan Huang, Yidong Chen, Xiaodong Shi",http://arxiv.org/pdf/2505.19987v1,cs.CL
DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset,"Recent advances in conversational AI have demonstrated impressive
capabilities in single-turn responses, yet multi-turn dialogues remain
challenging for even the most sophisticated language models. Current dialogue
datasets are limited in their emotional range, domain diversity, turn depth,
and are predominantly text-only, hindering progress in developing more
human-like conversational systems across modalities. To address these
limitations, we present DeepDialogue, a large-scale multimodal dataset
containing 40,150 high-quality multi-turn dialogues spanning 41 domains and
incorporating 20 distinct emotions with coherent emotional progressions. Our
approach pairs 9 different language models (4B-72B parameters) to generate
65,600 initial conversations, which we then evaluate through a combination of
human annotation and LLM-based quality filtering. The resulting dataset reveals
fundamental insights: smaller models fail to maintain coherence beyond 6
dialogue turns; concrete domains (e.g., ""cars,"" ""travel"") yield more meaningful
conversations than abstract ones (e.g., ""philosophy""); and cross-model
interactions produce more coherent dialogues than same-model conversations. A
key contribution of DeepDialogue is its speech component, where we synthesize
emotion-consistent voices for all 40,150 dialogues, creating the first
large-scale open-source multimodal dialogue dataset that faithfully preserves
emotional context across multi-turn conversations.",2025-05-26,"Alkis Koudounas, Moreno La Quatra, Elena Baralis",http://arxiv.org/pdf/2505.19978v1,cs.CL
Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language,"Knowledge graphs offer an excellent solution for representing the
lexical-semantic structures of lexicographic data. However, working with the
SPARQL query language represents a considerable hurdle for many non-expert
users who could benefit from the advantages of this technology. This paper
addresses the challenge of creating natural language interfaces for
lexicographic data retrieval on knowledge graphs such as Wikidata. We develop a
multidimensional taxonomy capturing the complexity of Wikidata's lexicographic
data ontology module through four dimensions and create a template-based
dataset with over 1.2 million mappings from natural language utterances to
SPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and
GPT-3.5-Turbo reveal significant differences in model capabilities. While all
models perform well on familiar patterns, only GPT-3.5-Turbo demonstrates
meaningful generalization capabilities, suggesting that model size and diverse
pre-training are crucial for adaptability in this domain. However, significant
challenges remain in achieving robust generalization, handling diverse
linguistic data, and developing scalable solutions that can accommodate the
full complexity of lexicographic knowledge representation.",2025-05-26,"Kilian Sennrich, Sina Ahmadi",http://arxiv.org/pdf/2505.19971v1,cs.CL
CP-Router: An Uncertainty-Aware Router Between LLM and LRM,"Recent advances in Large Reasoning Models (LRMs) have significantly improved
long-chain reasoning capabilities over Large Language Models (LLMs). However,
LRMs often produce unnecessarily lengthy outputs even for simple queries,
leading to inefficiencies or even accuracy degradation compared to LLMs. To
overcome this, we propose CP-Router, a training-free and model-agnostic routing
framework that dynamically selects between an LLM and an LRM, demonstrated with
multiple-choice question answering (MCQA) prompts. The routing decision is
guided by the prediction uncertainty estimates derived via Conformal Prediction
(CP), which provides rigorous coverage guarantees. To further refine the
uncertainty differentiation across inputs, we introduce Full and Binary Entropy
(FBE), a novel entropy-based criterion that adaptively selects the appropriate
CP threshold. Experiments across diverse MCQA benchmarks, including
mathematics, logical reasoning, and Chinese chemistry, demonstrate that
CP-Router efficiently reduces token usage while maintaining or even improving
accuracy compared to using LRM alone. We also extend CP-Router to diverse model
pairings and open-ended QA, where it continues to demonstrate strong
performance, validating its generality and robustness.",2025-05-26,"Jiayuan Su, Fulin Lin, Zhaopeng Feng, Han Zheng, Teng Wang, Zhenyu Xiao, Xinlong Zhao, Zuozhu Liu, Lu Cheng, Hongwei Wang",http://arxiv.org/pdf/2505.19970v1,cs.CL
The Limits of Preference Data for Post-Training,"Recent progress in strengthening the capabilities of large language models
has stemmed from applying reinforcement learning to domains with automatically
verifiable outcomes. A key question is whether we can similarly use RL to
optimize for outcomes in domains where evaluating outcomes inherently requires
human feedback; for example, in tasks like deep research and trip planning,
outcome evaluation is qualitative and there are many possible degrees of
success. One attractive and scalable modality for collecting human feedback is
preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$
given outcomes, which one is preferred. In this work, we study a critical
roadblock: preference data fundamentally and significantly limits outcome-based
optimization. Even with idealized preference data (infinite, noiseless, and
online), the use of ordinal feedback can prevent obtaining even approximately
optimal solutions. We formalize this impossibility using voting theory, drawing
an analogy between how a model chooses to answer a query with how voters choose
a candidate to elect. This indicates that grounded human scoring and
algorithmic innovations are necessary for extending the success of RL
post-training to domains demanding human feedback. We also explore why these
limitations have disproportionately impacted RLHF when it comes to eliciting
reasoning behaviors (e.g., backtracking) versus situations where RLHF has been
historically successful (e.g., instruction-tuning and safety training), finding
that the limitations of preference data primarily suppress RLHF's ability to
elicit robust strategies -- a class that encompasses most reasoning behaviors.",2025-05-26,"Eric Zhao, Jessica Dai, Pranjal Awasthi",http://arxiv.org/pdf/2505.19964v1,cs.CL
MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models,"Long Context Understanding (LCU) is a critical area for exploration in
current large language models (LLMs). However, due to the inherently lengthy
nature of long-text data, existing LCU benchmarks for LLMs often result in
prohibitively high evaluation costs, like testing time and inference expenses.
Through extensive experimentation, we discover that existing LCU benchmarks
exhibit significant redundancy, which means the inefficiency in evaluation. In
this paper, we propose a concise data compression method tailored for long-text
data with sparse information characteristics. By pruning the well-known LCU
benchmark LongBench, we create MiniLongBench. This benchmark includes only 237
test samples across six major task categories and 21 distinct tasks. Through
empirical analysis of over 60 LLMs, MiniLongBench achieves an average
evaluation cost reduced to only 4.5% of the original while maintaining an
average rank correlation coefficient of 0.97 with LongBench results. Therefore,
our MiniLongBench, as a low-cost benchmark, holds great potential to
substantially drive future research into the LCU capabilities of LLMs. See
https://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.",2025-05-26,"Zhongzhan Huang, Guoming Ling, Shanshan Zhong, Hefeng Wu, Liang Lin",http://arxiv.org/pdf/2505.19959v1,cs.CL
DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph,"Text-to-SQL, which translates a natural language question into an SQL query,
has advanced with in-context learning of Large Language Models (LLMs). However,
existing methods show little improvement in performance compared to randomly
chosen demonstrations, and significant performance drops when smaller LLMs
(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely
on the intrinsic capabilities of hyper-scaled LLMs, rather than effectively
retrieving useful demonstrations. In this paper, we propose a novel approach
for effectively retrieving demonstrations and generating SQL queries. We
construct a Deep Contextual Schema Link Graph, which contains key information
and semantic relationship between a question and its database schema items.
This graph-based structure enables effective representation of Text-to-SQL
samples and retrieval of useful demonstrations for in-context learning.
Experimental results on the Spider benchmark demonstrate the effectiveness of
our approach, showing consistent improvements in SQL generation performance and
efficiency across both hyper-scaled LLMs and small LLMs. Our code will be
released.",2025-05-26,"Jihyung Lee, Jin-Seop Lee, Jaehoon Lee, YunSeok Choi, Jee-Hyong Lee",http://arxiv.org/pdf/2505.19956v1,cs.CL
MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research,"Recent advancements in AI agents have demonstrated their growing potential to
drive and support scientific discovery. In this work, we introduce MLR-Bench, a
comprehensive benchmark for evaluating AI agents on open-ended machine learning
research. MLR-Bench includes three key components: (1) 201 research tasks
sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)
MLR-Judge, an automated evaluation framework combining LLM-based reviewers with
carefully designed review rubrics to assess research quality; and (3)
MLR-Agent, a modular agent scaffold capable of completing research tasks
through four stages: idea generation, proposal formulation, experimentation,
and paper writing. Our framework supports both stepwise assessment across these
distinct research stages, and end-to-end evaluation of the final research
paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced
coding agent, finding that while LLMs are effective at generating coherent
ideas and well-structured papers, current coding agents frequently (e.g., in
80% of the cases) produce fabricated or invalidated experimental
results--posing a major barrier to scientific reliability. We validate
MLR-Judge through human evaluation, showing high agreement with expert
reviewers, supporting its potential as a scalable tool for research evaluation.
We open-source MLR-Bench to help the community benchmark, diagnose, and improve
AI research agents toward trustworthy and transparent scientific discovery.",2025-05-26,"Hui Chen, Miao Xiong, Yujie Lu, Wei Han, Ailin Deng, Yufei He, Jiaying Wu, Yibo Li, Yue Liu, Bryan Hooi",http://arxiv.org/pdf/2505.19955v1,cs.CL
An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning,"The differential diagnosis of neurodegenerative dementias is a challenging
clinical task, mainly because of the overlap in symptom presentation and the
similarity of patterns observed in structural neuroimaging. To improve
diagnostic efficiency and accuracy, deep learning-based methods such as
Convolutional Neural Networks and Vision Transformers have been proposed for
the automatic classification of brain MRIs. However, despite their strong
predictive performance, these models find limited clinical utility due to their
opaque decision making. In this work, we propose a framework that integrates
two core components to enhance diagnostic transparency. First, we introduce a
modular pipeline for converting 3D T1-weighted brain MRIs into textual
radiology reports. Second, we explore the potential of modern Large Language
Models (LLMs) to assist clinicians in the differential diagnosis between
Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based
on the generated reports. To bridge the gap between predictive accuracy and
explainability, we employ reinforcement learning to incentivize diagnostic
reasoning in LLMs. Without requiring supervised reasoning traces or
distillation from larger models, our approach enables the emergence of
structured diagnostic rationales grounded in neuroimaging findings. Unlike
post-hoc explainability methods that retrospectively justify model decisions,
our framework generates diagnostic rationales as part of the inference
process-producing causally grounded explanations that inform and guide the
model's decision-making process. In doing so, our framework matches the
diagnostic performance of existing deep learning methods while offering
rationales that support its diagnostic conclusions.",2025-05-26,"Andrew Zamai, Nathanael Fijalkow, Boris Mansencal, Laurent Simon, Eloi Navet, Pierrick Coupe",http://arxiv.org/pdf/2505.19954v1,cs.CL
Can Visual Encoder Learn to See Arrows?,"The diagram is a visual representation of a relationship illustrated with
edges (lines or arrows), which is widely used in industrial and scientific
communication. Although recognizing diagrams is essential for vision language
models (VLMs) to comprehend domain-specific knowledge, recent studies reveal
that many VLMs fail to identify edges in images. We hypothesize that these
failures stem from an over-reliance on textual and positional biases,
preventing VLMs from learning explicit edge features. Based on this idea, we
empirically investigate whether the image encoder in VLMs can learn edge
representation through training on a diagram dataset in which edges are biased
neither by textual nor positional information. To this end, we conduct
contrastive learning on an artificially generated diagram--caption dataset to
train an image encoder and evaluate its diagram-related features on three
tasks: probing, image retrieval, and captioning. Our results show that the
finetuned model outperforms pretrained CLIP in all tasks and surpasses
zero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings
confirm that eliminating textual and positional biases fosters accurate edge
recognition in VLMs, offering a promising path for advancing diagram
understanding.",2025-05-26,"Naoyuki Terashita, Yusuke Tozaki, Hideaki Omote, Congkha Nguyen, Ryosuke Nakamoto, Yuta Koreeda, Hiroaki Ozaki",http://arxiv.org/pdf/2505.19944v1,cs.CL
ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs,"Large Language Models (LLMs) are widely used in Spoken Language Understanding
(SLU). Recent SLU models process audio directly by adapting speech input into
LLMs for better multimodal learning. A key consideration for these models is
the cross-modal alignment between text and audio modalities, which is a
telltale sign as to whether or not LLM is able to associate semantic meaning to
audio segments. While various methods exist for fusing these modalities, there
is no standard metric to evaluate alignment quality in LLMs. In this work, we
propose a new metric, ALAS (Automatic Latent Alignment Score). Our study
examines the correlation between audio and text representations across
transformer layers, for two different tasks (Spoken Question Answering and
Emotion Recognition). We showcase that our metric behaves as expected across
different layers and different tasks.",2025-05-26,"Pooneh Mousavi, Yingzhi Wang, Mirco Ravanelli, Cem Subakan",http://arxiv.org/pdf/2505.19937v1,cs.CL
Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles,"Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at
advanced reasoning tasks like math and coding via Reinforcement Learning with
Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans
without domain knowledge. We introduce Enigmata, the first comprehensive suite
tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks
across seven categories, each with 1) a generator that produces unlimited
examples with controllable difficulty and 2) a rule-based verifier for
automatic evaluation. This generator-verifier design supports scalable,
multi-task RL training, fine-grained analysis, and seamless RLVR integration.
We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized
multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,
consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks
like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes
well to out-of-domain puzzle benchmarks and mathematical reasoning, with little
multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking
(20B activated parameters and 200B total parameters), puzzle data from Enigmata
further boosts SoTA performance on advanced math and STEM reasoning tasks such
as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization
benefits of Enigmata. This work offers a unified, controllable framework for
advancing logical reasoning in LLMs. Resources of this work can be found at
https://seed-enigmata.github.io.",2025-05-26,"Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, Hao Zhou, Mingxuan Wang",http://arxiv.org/pdf/2505.19914v1,cs.CL
APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization,"We present Adjacent Possible Exploration (APE), a simple yet effective method
for adapting large language models to specific tasks using minimal
computational resources. Unlike traditional fine-tuning that requires extensive
compute, APE iteratively fine-tunes models on small, carefully selected data
batches (200 examples), retaining only improvements. On news summarization, APE
achieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,
matching or exceeding more complex methods like LoRA while remaining
conceptually simple. Our approach is particularly valuable for researchers and
practitioners with limited computational resources. We provide open-source code
and demonstrate APE's effectiveness through both automatic metrics and human
evaluation. While inspired by evolutionary theory's ""adjacent possible"", APE's
core insight has a very practical application: small, iterative data
perturbations can efficiently guide LLMs toward task-specific performance
without expensive retraining.",2025-05-26,Javier Marín,http://arxiv.org/pdf/2505.19912v1,cs.CL
ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows,"Large Language Models (LLMs) have extended their impact beyond Natural
Language Processing, substantially fostering the development of
interdisciplinary research. Recently, various LLM-based agents have been
developed to assist scientific discovery progress across multiple aspects and
domains. Among these, computer-using agents, capable of interacting with
operating systems as humans do, are paving the way to automated scientific
problem-solving and addressing routines in researchers' workflows. Recognizing
the transformative potential of these agents, we introduce ScienceBoard, which
encompasses two complementary contributions: (i) a realistic, multi-domain
environment featuring dynamic and visually rich scientific workflows with
integrated professional software, where agents can autonomously interact via
different interfaces to accelerate complex research tasks and experiments; and
(ii) a challenging benchmark of 169 high-quality, rigorously validated
real-world tasks curated by humans, spanning scientific-discovery workflows in
domains such as biochemistry, astronomy, and geoinformatics. Extensive
evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude
3.7, UI-TARS) show that, despite some promising results, they still fall short
of reliably assisting scientists in complex workflows, achieving only a 15%
overall success rate. In-depth analysis further provides valuable insights for
addressing current agent limitations and more effective design principles,
paving the way to build more capable agents for scientific discovery. Our code,
environment, and benchmark are at
https://qiushisun.github.io/ScienceBoard-Home/.",2025-05-26,"Qiushi Sun, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu, Kanzhi Cheng, Zhaoyang Liu, Jianing Wang, Qintong Li, Xiangru Tang, Tianbao Xie, Xiachong Feng, Xiang Li, Ben Kao, Wenhai Wang, Biqing Qi, Lingpeng Kong, Zhiyong Wu",http://arxiv.org/pdf/2505.19897v1,cs.CL
Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program,"Recent trends are emerging in the use of Large Language Models (LLMs) as
autonomous agents that take actions based on the content of the user text
prompts. We intend to apply these concepts to the field of Control in space,
enabling LLMs to play a significant role in the decision-making process for
autonomous satellite operations. As a first step towards this goal, we have
developed a pure LLM-based solution for the Kerbal Space Program Differential
Games (KSPDG) challenge, a public software design competition where
participants create autonomous agents for maneuvering satellites involved in
non-cooperative space operations, running on the KSP game engine. Our approach
leverages prompt engineering, few-shot prompting, and fine-tuning techniques to
create an effective LLM-based agent that ranked 2nd in the competition. To the
best of our knowledge, this work pioneers the integration of LLM agents into
space research. The project comprises several open repositories to facilitate
replication and further research. The codebase is accessible on
\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models
and datasets are available on \href{https://huggingface.co/OhhTuRnz}{Hugging
Face}. Additionally, experiment tracking and detailed results can be reviewed
on \href{https://wandb.ai/carrusk/huggingface}{Weights \& Biases",2025-05-26,"Alejandro Carrasco, Victor Rodriguez-Fernandez, Richard Linares",http://arxiv.org/pdf/2505.19896v1,cs.CL
ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining,"Large language model pretraining is compute-intensive, yet many tokens
contribute marginally to learning, resulting in inefficiency. We introduce
Efficient Selective Language Modeling (ESLM), a risk-aware algorithm that
improves training efficiency and distributional robustness by performing online
token-level batch selection. ESLM leverages per-token statistics (e.g., entropy
or loss) and applies value-at-risk thresholding to retain only the most
informative tokens per batch. This data-centric mechanism reshapes the training
loss, prioritizing high-risk tokens and eliminating redundant gradient
computation. We frame ESLM as a bilevel game: the model competes with a masking
adversary that selects worst-case token subsets under a constrained
thresholding rule. In the loss-based setting, ESLM recovers conditional
value-at-risk loss minimization, providing a principled connection to
distributionally robust optimization. We extend our approach to Ada-ESLM, which
adaptively tunes the selection confidence during training. Experiments on GPT-2
pretraining show that ESLM significantly reduces training FLOPs while
maintaining or improving both perplexity and downstream performance compared to
baselines. Our approach also scales across model sizes, pretraining corpora,
and integrates naturally with knowledge distillation.",2025-05-26,"Melis Ilayda Bal, Volkan Cevher, Michael Muehlebach",http://arxiv.org/pdf/2505.19893v1,cs.CL
HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation,"Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of
large language models (LLMs) by leveraging self-generated responses for
self-training. Recent studies have incorporated reward models to guide response
selection or decoding, aiming to obtain higher-quality data. However, they
typically allocate a uniform sampling budget across all problems, overlooking
the varying utility of problems at different difficulty levels. In this work,
we conduct an empirical study and find that problems near the boundary of the
LLM's reasoning capability offer significantly greater learning utility than
both easy and overly difficult ones. To identify and exploit such problems, we
propose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.
Given a fixed sampling budget, HS-STaR first performs lightweight pre-sampling
with a reward-guided difficulty estimation strategy to efficiently identify
boundary-level problems. Subsequently, it dynamically reallocates the remaining
budget toward these high-utility problems during a re-sampling phase,
maximizing the generation of valuable training data. Extensive experiments
across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR
significantly outperforms other baselines without requiring additional sampling
budget.",2025-05-26,"Feng Xiong, Hongling Xu, Yifei Wang, Runxi Cheng, Yong Wang, Xiangxiang Chu",http://arxiv.org/pdf/2505.19866v1,cs.CL
REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models,"Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks
but often face the challenge of overthinking, leading to substantially high
inference costs. Existing approaches synthesize shorter reasoning responses for
LRMs to learn, but are inefficient for online usage due to the time-consuming
data generation and filtering processes. Meanwhile, online reinforcement
learning mainly adopts a length reward to encourage short reasoning responses,
but tends to lose the reflection ability and harm the performance. To address
these issues, we propose REA-RL, which introduces a small reflection model for
efficient scaling in online training, offering both parallel sampling and
sequential revision. Besides, a reflection reward is designed to further
prevent LRMs from favoring short yet non-reflective responses. Experiments show
that both methods maintain or enhance performance while significantly improving
inference efficiency. Their combination achieves a good balance between
performance and efficiency, reducing inference costs by 35% without
compromising performance. Further analysis demonstrates that our methods are
effective by maintaining reflection frequency for hard problems while
appropriately reducing it for simpler ones without losing reflection ability.
Codes are available at https://github.com/hexuandeng/REA-RL.",2025-05-26,"Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Jun Rao, Min Zhang",http://arxiv.org/pdf/2505.19862v1,cs.CL
Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages,"Transliteration, the process of mapping text from one script to another,
plays a crucial role in multilingual natural language processing, especially
within linguistically diverse contexts such as India. Despite significant
advancements through specialized models like IndicXlit, recent developments in
large language models suggest a potential for general-purpose models to excel
at this task without explicit task-specific training. The current work
systematically evaluates the performance of prominent LLMs, including GPT-4o,
GPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a
state-of-the-art transliteration model, across ten major Indian languages.
Experiments utilized standard benchmarks, including Dakshina and Aksharantar
datasets, with performance assessed via Top-1 Accuracy and Character Error
Rate. Our findings reveal that while GPT family models generally outperform
other LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o
improves performance on specific languages notably. An extensive error analysis
and robustness testing under noisy conditions further elucidate strengths of
LLMs compared to specialized models, highlighting the efficacy of foundational
models for a wide spectrum of specialized applications with minimal overhead.",2025-05-26,"Gulfarogh Azam, Mohd Sadique, Saif Ali, Mohammad Nadeem, Erik Cambria, Shahab Saquib Sohail, Mohammad Sultan Alam",http://arxiv.org/pdf/2505.19851v1,cs.CL
Improving Multilingual Math Reasoning for African Languages,"Researchers working on low-resource languages face persistent challenges due
to limited data availability and restricted access to computational resources.
Although most large language models (LLMs) are predominantly trained in
high-resource languages, adapting them to low-resource contexts, particularly
African languages, requires specialized techniques. Several strategies have
emerged for adapting models to low-resource languages in todays LLM landscape,
defined by multi-stage pre-training and post-training paradigms. However, the
most effective approaches remain uncertain. This work systematically
investigates which adaptation strategies yield the best performance when
extending existing LLMs to African languages. We conduct extensive experiments
and ablation studies to evaluate different combinations of data types
(translated versus synthetically generated), training stages (pre-training
versus post-training), and other model adaptation configurations. Our
experiments focuses on mathematical reasoning tasks, using the Llama 3.1 model
family as our base model.",2025-05-26,"Odunayo Ogundepo, Akintunde Oladipo, Kelechi Ogueji, Esther Adenuga, David Ifeoluwa Adelani, Jimmy Lin",http://arxiv.org/pdf/2505.19848v1,cs.CL
FoodTaxo: Generating Food Taxonomies with Large Language Models,"We investigate the utility of Large Language Models for automated taxonomy
generation and completion specifically applied to taxonomies from the food
technology industry. We explore the extent to which taxonomies can be completed
from a seed taxonomy or generated without a seed from a set of known concepts,
in an iterative fashion using recent prompting techniques. Experiments on five
taxonomies using an open-source LLM (Llama-3), while promising, point to the
difficulty of correctly placing inner nodes.",2025-05-26,"Pascal Wullschleger, Majid Zarharan, Donnacha Daly, Marc Pouly, Jennifer Foster",http://arxiv.org/pdf/2505.19838v1,cs.CL
Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective,"We propose a novel framework for comprehending the reasoning capabilities of
large language models (LLMs) through the perspective of meta-learning. By
conceptualizing reasoning trajectories as pseudo-gradient descent updates to
the LLM's parameters, we identify parallels between LLM reasoning and various
meta-learning paradigms. We formalize the training process for reasoning tasks
as a meta-learning setup, with each question treated as an individual task, and
reasoning trajectories serving as the inner loop optimization for adapting
model parameters. Once trained on a diverse set of questions, the LLM develops
fundamental reasoning capabilities that can generalize to previously unseen
questions. Extensive empirical evaluations substantiate the strong connection
between LLM reasoning and meta-learning, exploring several issues of
significant interest from a meta-learning standpoint. Our work not only
enhances the understanding of LLM reasoning but also provides practical
insights for improving these models through established meta-learning
techniques.",2025-05-26,"Junnan Liu, Hongwei Liu, Linchen Xiao, Shudong Liu, Taolin Zhang, Zihan Ma, Songyang Zhang, Kai Chen",http://arxiv.org/pdf/2505.19815v1,cs.CL
"Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks","Consciousness stands as one of the most profound and distinguishing features
of the human mind, fundamentally shaping our understanding of existence and
agency. As large language models (LLMs) develop at an unprecedented pace,
questions concerning intelligence and consciousness have become increasingly
significant. However, discourse on LLM consciousness remains largely unexplored
territory. In this paper, we first clarify frequently conflated terminologies
(e.g., LLM consciousness and LLM awareness). Then, we systematically organize
and synthesize existing research on LLM consciousness from both theoretical and
empirical perspectives. Furthermore, we highlight potential frontier risks that
conscious LLMs might introduce. Finally, we discuss current challenges and
outline future directions in this emerging field. The references discussed in
this paper are organized at
https://github.com/OpenCausaLab/Awesome-LLM-Consciousness.",2025-05-26,"Sirui Chen, Shuqin Ma, Shu Yu, Hanwang Zhang, Shengjie Zhao, Chaochao Lu",http://arxiv.org/pdf/2505.19806v1,cs.CL
Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation,"Nowadays, regulatory compliance has become a cornerstone of corporate
governance, ensuring adherence to systematic legal frameworks. At its core,
financial regulations often comprise highly intricate provisions, layered
logical structures, and numerous exceptions, which inevitably result in
labor-intensive or comprehension challenges. To mitigate this, recent
Regulatory Technology (RegTech) and Large Language Models (LLMs) have gained
significant attention in automating the conversion of regulatory text into
executable compliance logic. However, their performance remains suboptimal
particularly when applied to Chinese-language financial regulations, due to
three key limitations: (1) incomplete domain-specific knowledge representation,
(2) insufficient hierarchical reasoning capabilities, and (3) failure to
maintain temporal and logical coherence. One promising solution is to develop a
domain specific and code-oriented datasets for model training. Existing
datasets such as LexGLUE, LegalBench, and CODE-ACCORD are often
English-focused, domain-mismatched, or lack fine-grained granularity for
compliance code generation. To fill these gaps, we present Compliance-to-Code,
the first large-scale Chinese dataset dedicated to financial regulatory
compliance. Covering 1,159 annotated clauses from 361 regulations across ten
categories, each clause is modularly structured with four logical
elements-subject, condition, constraint, and contextual information-along with
regulation relations. We provide deterministic Python code mappings, detailed
code reasoning, and code explanations to facilitate automated auditing. To
demonstrate utility, we present FinCheck: a pipeline for regulation
structuring, code generation, and report generation.",2025-05-26,"Siyuan Li, Jian Chen, Rui Yao, Xuming Hu, Peilin Zhou, Weihua Qiu, Simin Zhang, Chucheng Dong, Zhiyao Li, Qipeng Xie, Zixuan Yuan",http://arxiv.org/pdf/2505.19804v1,cs.CL
MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs,"Metadata extraction is essential for cataloging and preserving datasets,
enabling effective research discovery and reproducibility, especially given the
current exponential growth in scientific research. While Masader (Alyafeai et
al.,2021) laid the groundwork for extracting a wide range of metadata
attributes from Arabic NLP datasets' scholarly articles, it relies heavily on
manual annotation. In this paper, we present MOLE, a framework that leverages
Large Language Models (LLMs) to automatically extract metadata attributes from
scientific papers covering datasets of languages other than Arabic. Our
schema-driven methodology processes entire documents across multiple input
formats and incorporates robust validation mechanisms for consistent output.
Additionally, we introduce a new benchmark to evaluate the research progress on
this task. Through systematic analysis of context length, few-shot learning,
and web browsing integration, we demonstrate that modern LLMs show promising
results in automating this task, highlighting the need for further future work
improvements to ensure consistent and reliable performance. We release the
code: https://github.com/IVUL-KAUST/MOLE and dataset:
https://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.",2025-05-26,"Zaid Alyafeai, Maged S. Al-Shaibani, Bernard Ghanem",http://arxiv.org/pdf/2505.19800v1,cs.CL
The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants,"As proprietary giants increasingly dominate the race for ever-larger language
models, a pressing question arises for the open-source community: can smaller
models remain competitive across a broad range of tasks? In this paper, we
present the Avengers--a simple recipe that effectively leverages the collective
intelligence of open-source, smaller language models. Our framework is built
upon four lightweight operations: (i) embedding: encode queries using a text
embedding model; (ii) clustering: group queries based on their semantic
similarity; (iii) scoring: scores each model's performance within each cluster;
and (iv) voting: improve outputs via repeated sampling and voting. At inference
time, each query is embedded and assigned to its nearest cluster. The
top-performing model(s) within that cluster are selected to generate the
response using the Self-Consistency or its multi-model variant. Remarkably,
with 10 open-source models (~7B parameters each), the Avengers collectively
outperforms GPT-4.1 on 10 out of 15 datasets (spanning mathematics, code,
logic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on
mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the
Avengers delivers superior out-of-distribution generalization, and remains
robust across various embedding models, clustering algorithms, ensemble
strategies, and values of its sole parameter--the number of clusters. We have
open-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers",2025-05-26,"Yiqun Zhang, Hao Li, Chenxu Wang, Linyao Chen, Qiaosheng Zhang, Peng Ye, Shi Feng, Daling Wang, Zhen Wang, Xinrun Wang, Jia Xu, Lei Bai, Wanli Ouyang, Shuyue Hu",http://arxiv.org/pdf/2505.19797v1,cs.CL
Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification,"Political biases encoded by LLMs might have detrimental effects on downstream
applications. Existing bias analysis methods rely on small-size intermediate
tasks (questionnaire answering or political content generation) and rely on the
LLMs themselves for analysis, thus propagating bias. We propose a new approach
leveraging the observation that LLM sentiment predictions vary with the target
entity in the same sentence. We define an entropy-based inconsistency metric to
encode this prediction variability. We insert 1319 demographically and
politically diverse politician names in 450 political sentences and predict
target-oriented sentiment using seven models in six widely spoken languages. We
observe inconsistencies in all tested combinations and aggregate them in a
statistically robust analysis at different granularity levels. We observe
positive and negative bias toward left and far-right politicians and positive
correlations between politicians with similar alignment. Bias intensity is
higher for Western languages than for others. Larger models exhibit stronger
and more consistent biases and reduce discrepancies between similar languages.
We partially mitigate LLM unreliability in target-oriented sentiment
classification (TSC) by replacing politician names with fictional but plausible
counterparts.",2025-05-26,"Akram Elbouanani, Evan Dufraisse, Adrian Popescu",http://arxiv.org/pdf/2505.19776v1,cs.CL
What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs,"We investigate long-context vulnerabilities in Large Language Models (LLMs)
through Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of
up to 128K tokens. Through comprehensive analysis with various many-shot attack
settings with different instruction styles, shot density, topic, and format, we
reveal that context length is the primary factor determining attack
effectiveness. Critically, we find that successful attacks do not require
carefully crafted harmful content. Even repetitive shots or random dummy text
can circumvent model safety measures, suggesting fundamental limitations in
long-context processing capabilities of LLMs. The safety behavior of
well-aligned models becomes increasingly inconsistent with longer contexts.
These findings highlight significant safety gaps in context expansion
capabilities of LLMs, emphasizing the need for new safety mechanisms.",2025-05-26,"Sangyeop Kim, Yohan Lee, Yongwoo Song, Kimin Lee",http://arxiv.org/pdf/2505.19773v1,cs.CL
Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO,"We present a fine-grained theoretical analysis of the performance gap between
reinforcement learning from human feedback (RLHF) and direct preference
optimization (DPO) under a representation gap. Our study decomposes this gap
into two sources: an explicit representation gap under exact optimization and
an implicit representation gap under finite samples. In the exact optimization
setting, we characterize how the relative capacities of the reward and policy
model classes influence the final policy qualities. We show that RLHF, DPO, or
online DPO can outperform one another depending on the type of model
mis-specifications. Notably, online DPO can outperform both RLHF and standard
DPO when the reward and policy model classes are isomorphic and both
mis-specified. In the approximate optimization setting, we provide a concrete
construction where the ground-truth reward is implicitly sparse and show that
RLHF requires significantly fewer samples than DPO to recover an effective
reward model -- highlighting a statistical advantage of two-stage learning.
Together, these results provide a comprehensive understanding of the
performance gap between RLHF and DPO under various settings, and offer
practical insights into when each method is preferred.",2025-05-26,"Ruizhe Shi, Minhak Song, Runlong Zhou, Zihan Zhang, Maryam Fazel, Simon S. Du",http://arxiv.org/pdf/2505.19770v1,cs.CL
T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search,"Real-world multimodal misinformation often arises from mixed forgery sources,
requiring dynamic reasoning and adaptive verification. However, existing
methods mainly rely on static pipelines and limited tool usage, limiting their
ability to handle such complexity and diversity. To address this challenge, we
propose T2Agent, a novel misinformation detection agent that incorporates an
extensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of
modular tools such as web search, forgery detection, and consistency analysis.
Each tool is described using standardized templates, enabling seamless
integration and future expansion. To avoid inefficiency from using all tools
simultaneously, a Bayesian optimization-based selector is proposed to identify
a task-relevant subset. This subset then serves as the action space for MCTS to
dynamically collect evidence and perform multi-source verification. To better
align MCTS with the multi-source nature of misinformation detection, T2Agent
extends traditional MCTS with multi-source verification, which decomposes the
task into coordinated subtasks targeting different forgery sources. A dual
reward mechanism containing a reasoning trajectory score and a confidence score
is further proposed to encourage a balance between exploration across mixed
forgery sources and exploitation for more reliable evidence. We conduct
ablation studies to confirm the effectiveness of the tree search mechanism and
tool usage. Extensive experiments further show that T2Agent consistently
outperforms existing baselines on challenging mixed-source multimodal
misinformation benchmarks, demonstrating its strong potential as a
training-free approach for enhancing detection accuracy. The code will be
released.",2025-05-26,"Xing Cui, Yueying Zou, Zekun Li, Peipei Li, Xinyuan Xu, Xuannan Liu, Huaibo Huang, Ran He",http://arxiv.org/pdf/2505.19768v1,cs.CL
SGM: A Framework for Building Specification-Guided Moderation Filters,"Aligning large language models (LLMs) with deployment-specific requirements
is critical but inherently imperfect. Despite extensive training, models remain
susceptible to misalignment and adversarial inputs such as jailbreaks. Content
moderation filters are commonly used as external safeguards, though they
typically focus narrowly on safety. We introduce SGM (Specification-Guided
Moderation), a flexible framework for training moderation filters grounded in
user-defined specifications that go beyond standard safety concerns. SGM
automates training data generation without relying on human-written examples,
enabling scalable support for diverse, application-specific alignment goals.
SGM-trained filters perform on par with state-of-the-art safety filters built
on curated datasets, while supporting fine-grained and user-defined alignment
control.",2025-05-26,"Masoomali Fatehkia, Enes Altinisik, Husrev Taha Sencar",http://arxiv.org/pdf/2505.19766v1,cs.CL
CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement,"Effective generation of structured code comments requires robust quality
metrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)
suffer from limited code-comment analysis. We propose CIDRe, a
language-agnostic reference-free quality criterion combining four synergistic
aspects: (1) relevance (code-comment semantic alignment), (2) informativeness
(functional coverage), (3) completeness (presence of all structure sections),
and (4) description length (detail sufficiency). We validate our criterion on a
manually annotated dataset. Experiments demonstrate CIDRe's superiority over
existing metrics, achieving improvement in cross-entropy evaluation. When
applied to filter comments, the models finetuned on CIDRe-filtered data show
statistically significant quality gains in GPT-4o-mini assessments.",2025-05-26,"Maria Dziuba, Valentin Malykh",http://arxiv.org/pdf/2505.19757v1,cs.CL
Efficient Reasoning via Chain of Unconscious Thought,"Large Reasoning Models (LRMs) achieve promising performance but compromise
token efficiency due to verbose reasoning processes. Unconscious Thought Theory
(UTT) posits that complex problems can be solved more efficiently through
internalized cognitive processes. Inspired by UTT, we propose a new reasoning
paradigm, termed Chain of Unconscious Thought (CoUT), to improve the token
efficiency of LRMs by guiding them to mimic human unconscious thought and
internalize reasoning processes. Concretely, we first prompt the model to
internalize the reasoning by thinking in the hidden layer. Then, we design a
bag of token-efficient strategies to further help models reduce unnecessary
tokens yet preserve the performance. Our work reveals that models may possess
beneficial unconscious thought, enabling improved efficiency without
sacrificing performance. Extensive experiments demonstrate the effectiveness of
CoUT. Remarkably, it surpasses CoT by reducing token usage by 47.62% while
maintaining comparable accuracy, as shown in Figure 1. The code of CoUT is
available at this link: https://github.com/Rohan-GRH/CoUT",2025-05-26,"Ruihan Gong, Yue Liu, Wenjie Qu, Mingzhe Du, Yufei He, Yingwei Ma, Yulin Chen, Xiang Liu, Yi Wen, Xinfeng Li, Ruidong Wang, Xinzhong Zhu, Bryan Hooi, Jiaheng Zhang",http://arxiv.org/pdf/2505.19756v1,cs.CL
NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering,"The increasing number of academic papers poses significant challenges for
researchers to efficiently acquire key details. While retrieval augmented
generation (RAG) shows great promise in large language model (LLM) based
automated question answering, previous works often isolate neural and symbolic
retrieval despite their complementary strengths. Moreover, conventional
single-view chunking neglects the rich structure and layout of PDFs, e.g.,
sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural
symbolic retrieval framework which combines both paradigms in an interactive
process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG
organizes semi-structured PDF content into both the relational database and
vectorstore, enabling LLM agents to iteratively gather context until sufficient
to generate answers. Experiments on three full PDF-based QA datasets, including
a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the
vector-based RAG and various structured baselines, highlighting its capacity to
unify both retrieval schemes and utilize multiple views. Code and data are
publicly available at https://github.com/X-LANCE/NeuSym-RAG.",2025-05-26,"Ruisheng Cao, Hanchong Zhang, Tiancheng Huang, Zhangyi Kang, Yuxin Zhang, Liangtai Sun, Hanqi Li, Yuxun Miao, Shuai Fan, Lu Chen, Kai Yu",http://arxiv.org/pdf/2505.19754v1,cs.CL
Discrete Markov Bridge,"Discrete diffusion has recently emerged as a promising paradigm in discrete
data modeling. However, existing methods typically rely on a fixed rate
transition matrix during training, which not only limits the expressiveness of
latent representations, a fundamental strength of variational methods, but also
constrains the overall design space. To address these limitations, we propose
Discrete Markov Bridge, a novel framework specifically designed for discrete
representation learning. Our approach is built upon two key components: Matrix
Learning and Score Learning. We conduct a rigorous theoretical analysis,
establishing formal performance guarantees for Matrix Learning and proving the
convergence of the overall framework. Furthermore, we analyze the space
complexity of our method, addressing practical constraints identified in prior
studies. Extensive empirical evaluations validate the effectiveness of the
proposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)
of 1.38 on the Text8 dataset, outperforming established baselines. Moreover,
the proposed model demonstrates competitive performance on the CIFAR-10
dataset, achieving results comparable to those obtained by image-specific
generation approaches.",2025-05-26,"Hengli Li, Yuxuan Wang, Song-Chun Zhu, Ying Nian Wu, Zilong Zheng",http://arxiv.org/pdf/2505.19752v1,cs.CL
Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models,"With the rapid development of Large Language Models (LLMs), aligning these
models with human preferences and values is critical to ensuring ethical and
safe applications. However, existing alignment techniques such as RLHF or DPO
often require direct fine-tuning on LLMs with billions of parameters, resulting
in substantial computational costs and inefficiencies. To address this, we
propose Micro token-level Accept-Reject Aligning (MARA) approach designed to
operate independently of the language models. MARA simplifies the alignment
process by decomposing sentence-level preference learning into token-level
binary classification, where a compact three-layer fully-connected network
determines whether candidate tokens are ""Accepted"" or ""Rejected"" as part of the
response. Extensive experiments across seven different LLMs and three
open-source datasets show that MARA achieves significant improvements in
alignment performance while reducing computational costs.",2025-05-26,"Yang Zhang, Yu Yu, Bo Tang, Yu Zhu, Chuxiong Sun, Wenqiang Wei, Jie Hu, Zipeng Xie, Zhiyu Li, Feiyu Xiong, Edward Chung",http://arxiv.org/pdf/2505.19743v1,cs.CL
Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking,"Biomedical entity linking aims to map nonstandard entities to standard
entities in a knowledge base. Traditional supervised methods perform well but
require extensive annotated data to transfer, limiting their usage in
low-resource scenarios. Large language models (LLMs), especially closed-source
LLMs, can address these but risk stability issues and high economic costs:
using these models is restricted by commercial companies and brings significant
economic costs when dealing with large amounts of data. To address this, we
propose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs
for re-ranking candidates retrieved by a retriever fine-tuned with a small
amount of data. By prompting a closed-source LLM to generate training data from
unannotated data and fine-tuning an open-source LLM for re-ranking, we
effectively distill the knowledge to the open-source LLM that can be deployed
locally, thus avoiding the stability issues and the problem of high economic
costs. We evaluate RPDR on two datasets, including one real-world dataset and
one publicly available dataset involving two languages: Chinese and English.
RPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier
dataset and the Ask A Patient dataset when the amount of training data is not
enough. The results demonstrate the superiority and generalizability of the
proposed framework.",2025-05-26,"Yihao Ai, Zhiyuan Ning, Weiwei Dai, Pengfei Wang, Yi Du, Wenjuan Cui, Kunpeng Liu, Yuanchun Zhou",http://arxiv.org/pdf/2505.19722v1,cs.CL
Graceful Forgetting in Generative Language Models,"Recently, the pretrain-finetune paradigm has become a cornerstone in various
deep learning areas. While in general the pre-trained model would promote both
effectiveness and efficiency of downstream tasks fine-tuning, studies have
shown that not all knowledge acquired during pre-training is beneficial. Some
of the knowledge may actually bring detrimental effects to the fine-tuning
tasks, which is also known as negative transfer. To address this problem,
graceful forgetting has emerged as a promising approach. The core principle of
graceful forgetting is to enhance the learning plasticity of the target task by
selectively discarding irrelevant knowledge. However, this approach remains
underexplored in the context of generative language models, and it is often
challenging to migrate existing forgetting algorithms to these models due to
architecture incompatibility. To bridge this gap, in this paper we propose a
novel framework, Learning With Forgetting (LWF), to achieve graceful forgetting
in generative language models. With Fisher Information Matrix weighting the
intended parameter updates, LWF computes forgetting confidence to evaluate
self-generated knowledge regarding the forgetting task, and consequently,
knowledge with high confidence is periodically unlearned during fine-tuning.
Our experiments demonstrate that, although thoroughly uncovering the mechanisms
of knowledge interaction remains challenging in pre-trained language models,
applying graceful forgetting can contribute to enhanced fine-tuning
performance.",2025-05-26,"Chunyang Jiang, Chi-min Chan, Yiyang Cai, Yulong Liu, Wei Xue, Yike Guo",http://arxiv.org/pdf/2505.19715v1,cs.CL
MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning,"Text Image Machine Translation (TIMT)-the task of translating textual content
embedded in images-is critical for applications in accessibility, cross-lingual
information access, and real-world document understanding. However, TIMT
remains a complex challenge due to the need for accurate optical character
recognition (OCR), robust visual-text reasoning, and high-quality translation,
often requiring cascading multi-stage pipelines. Recent advances in large-scale
Reinforcement Learning (RL) have improved reasoning in Large Language Models
(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is
still underexplored. To bridge this gap, we introduce MT$^{3}$, the first
framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts
a multi-task optimization paradigm targeting three key sub-skills: text
recognition, context-aware reasoning, and translation. It is trained using a
novel multi-mixed reward mechanism that adapts rule-based RL strategies to
TIMT's intricacies, offering fine-grained, non-binary feedback across tasks.
Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural
and real-world social media contexts, we introduced XHSPost, the first social
media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on
the latest in-domain MIT-10M benchmark, outperforming strong baselines such as
Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.
Additionally, the model shows strong generalization to out-of-distribution
language pairs and datasets. In-depth analyses reveal how multi-task synergy,
reinforcement learning initialization, curriculum design, and reward
formulation contribute to advancing MLLM-driven TIMT.",2025-05-26,"Zhaopeng Feng, Yupu Liang, Shaosheng Cao, Jiayuan Su, Jiahan Ren, Zhe Xu, Yao Hu, Wenxuan Huang, Jian Wu, Zuozhu Liu",http://arxiv.org/pdf/2505.19714v1,cs.CL
Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision,"Large Language Models (LLMs) are prone to hallucination, especially during
multi-hop and reasoning-intensive tasks such as mathematical problem solving.
While Outcome Reward Models verify only final answers, Process Reward Models
(PRMs) score each intermediate step to steer generation toward coherent
solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware
discriminative PRM that first classifies math and consistency errors at each
step, then combines these fine-grained signals to estimate step correctness. To
train PathFinder-PRM, we construct a 400K-sample dataset by enriching the
human-annotated PRM800K corpus and RLHFlow Mistral traces with
three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new
state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while
using 3 times less data. When applied to reward guided greedy search, our model
yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results
demonstrate that decoupled error detection and reward estimation not only boost
fine-grained error detection but also substantially improve end-to-end,
reward-guided mathematical reasoning with greater data efficiency.",2025-05-26,"Tej Deep Pala, Panshul Sharma, Amir Zadeh, Chuan Li, Soujanya Poria",http://arxiv.org/pdf/2505.19706v1,cs.CL
Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models,"The widespread adoption of large language models (LLMs) across industries has
increased the demand for high-quality and customizable outputs. However,
traditional alignment methods often require retraining large pretrained models,
making it difficult to quickly adapt and optimize LLMs for diverse
applications. To address this limitation, we propose a novel \textit{Residual
Alignment Model} (\textit{RAM}) that formalizes the alignment process as a type
of importance sampling. In this framework, the unaligned upstream model serves
as the proposal distribution, while the alignment process is framed as
secondary sampling based on an autoregressive alignment module that acts as an
estimator of the importance weights. This design enables a natural detachment
of the alignment module from the target aligned model, improving flexibility
and scalability. Based on this model, we derive an efficient sequence-level
training strategy for the alignment module, which operates independently of the
proposal module. Additionally, we develop a resampling algorithm with iterative
token-level decoding to address the common first-token latency issue in
comparable methods. Experimental evaluations on two leading open-source LLMs
across diverse tasks, including instruction following, domain adaptation, and
preference optimization, demonstrate that our approach consistently outperforms
baseline models.",2025-05-26,"Yi Liu, Dianqing Liu, Mingye Zhu, Junbo Guo, Yongdong Zhang, Zhendong Mao",http://arxiv.org/pdf/2505.19700v1,cs.CL
Large Language Models for Planning: A Comprehensive and Systematic Survey,"Planning represents a fundamental capability of intelligent agents, requiring
comprehensive environmental understanding, rigorous logical reasoning, and
effective sequential decision-making. While Large Language Models (LLMs) have
demonstrated remarkable performance on certain planning tasks, their broader
application in this domain warrants systematic investigation. This paper
presents a comprehensive review of LLM-based planning. Specifically, this
survey is structured as follows: First, we establish the theoretical
foundations by introducing essential definitions and categories about automated
planning. Next, we provide a detailed taxonomy and analysis of contemporary
LLM-based planning methodologies, categorizing them into three principal
approaches: 1) External Module Augmented Methods that combine LLMs with
additional components for planning, 2) Finetuning-based Methods that involve
using trajectory data and feedback signals to adjust LLMs in order to improve
their planning abilities, and 3) Searching-based Methods that break down
complex tasks into simpler components, navigate the planning space, or enhance
decoding strategies to find the best solutions. Subsequently, we systematically
summarize existing evaluation frameworks, including benchmark datasets,
evaluation metrics and performance comparisons between representative planning
methods. Finally, we discuss the underlying mechanisms enabling LLM-based
planning and outline promising research directions for this rapidly evolving
field. We hope this survey will serve as a valuable resource to inspire
innovation and drive progress in this field.",2025-05-26,"Pengfei Cao, Tianyi Men, Wencan Liu, Jingwen Zhang, Xuzhao Li, Xixun Lin, Dianbo Sui, Yanan Cao, Kang Liu, Jun Zhao",http://arxiv.org/pdf/2505.19683v1,cs.CL
KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization,"This paper presents KIT's submissions to the IWSLT 2025 low-resource track.
We develop both cascaded systems, consisting of Automatic Speech Recognition
(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech
Translation (ST) systems for three language pairs: Bemba, North Levantine
Arabic, and Tunisian Arabic into English. Building upon pre-trained models, we
fine-tune our systems with different strategies to utilize resources
efficiently. This study further explores system enhancement with synthetic data
and model regularization. Specifically, we investigate MT-augmented ST by
generating translations from ASR data using MT models. For North Levantine,
which lacks parallel ST training data, a system trained solely on synthetic
data slightly surpasses the cascaded system trained on real data. We also
explore augmentation using text-to-speech models by generating synthetic speech
from MT data, demonstrating the benefits of synthetic data in improving both
ASR and ST performance for Bemba. Additionally, we apply intra-distillation to
enhance model performance. Our experiments show that this approach consistently
improves results across ASR, MT, and ST tasks, as well as across different
pre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine
the cascaded and end-to-end systems, achieving an improvement of approximately
1.5 BLEU points.",2025-05-26,"Zhaolin Li, Yining Liu, Danni Liu, Tuan Nam Nguyen, Enes Yavuz Ugan, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues",http://arxiv.org/pdf/2505.19679v1,cs.CL
Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs,"Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where
generated responses seem semantically plausible yet exhibit little or no
relevance to the input image. Previous studies reveal that this issue primarily
stems from LVLMs' over-reliance on language priors while disregarding the
visual information during decoding. To alleviate this issue, we introduce a
novel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding
strategy, which adaptively strengthens the mutual dependency between generated
texts and input images to mitigate hallucinations. Unlike existing methods
solely focusing on text token sampling, we propose to jointly model the
contributions of visual and textual tokens to C-PMI, formulating hallucination
mitigation as a bi-level optimization problem aimed at maximizing mutual
information. To solve it, we design a token purification mechanism that
dynamically regulates the decoding process by sampling text tokens remaining
maximally relevant to the given image, while simultaneously refining image
tokens most pertinent to the generated response. Extensive experiments across
various benchmarks reveal that the proposed method significantly reduces
hallucinations in LVLMs while preserving decoding efficiency.",2025-05-26,"Hao Fang, Changle Zhou, Jiawei Kong, Kuofeng Gao, Bin Chen, Tao Liang, Guojun Ma, Shu-Tao Xia",http://arxiv.org/pdf/2505.19678v1,cs.CL
Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement,"The traditional process of creating labeled datasets is labor-intensive and
expensive. Recent breakthroughs in open-source large language models (LLMs)
have opened up a new avenue in generating labeled datasets automatically for
various natural language processing (NLP) tasks, providing an alternative to
such an expensive annotation process. However, the reliability of such
auto-generated labels remains a significant concern due to inherent
inaccuracies. When learning from noisy labels, the model's generalization is
likely to be harmed as it is prone to overfit to those label noises. While
previous studies in learning from noisy labels mainly focus on synthetic noise
and real-world noise, LLM-generated label noise receives less attention. In
this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to
calibrate the classifier's prediction, thus enhancing its robustness towards
LLM-generated noisy labels. SiDyP retrieves potential true label candidates by
neighborhood label distribution in text embedding space and iteratively refines
noisy candidates using a simplex diffusion model. Our framework can increase
the performance of the BERT classifier fine-tuned on both zero-shot and
few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%
respectively. We demonstrate the effectiveness of SiDyP by conducting extensive
benchmarking for different LLMs over a variety of NLP tasks. Our code is
available on Github.",2025-05-26,"Liqin Ye, Agam Shah, Chao Zhang, Sudheer Chava",http://arxiv.org/pdf/2505.19675v1,cs.CL
Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations,"As the impact of large language models increases, understanding the moral
values they reflect becomes ever more important. Assessing the nature of moral
values as understood by these models via direct prompting is challenging due to
potential leakage of human norms into model training data, and their
sensitivity to prompt formulation. Instead, we propose to use word
associations, which have been shown to reflect moral reasoning in humans, as
low-level underlying representations to obtain a more robust picture of LLMs'
moral reasoning. We study moral differences in associations from western
English-speaking communities and LLMs trained predominantly on English data.
First, we create a large dataset of LLM-generated word associations, resembling
an existing data set of human word associations. Next, we propose a novel
method to propagate moral values based on seed words derived from Moral
Foundation Theory through the human and LLM-generated association graphs.
Finally, we compare the resulting moral conceptualizations, highlighting
detailed but systematic differences between moral values emerging from English
speakers and LLM associations.",2025-05-26,"Chaoyi Xiang, Chunhua Liu, Simon De Deyne, Lea Frermann",http://arxiv.org/pdf/2505.19674v1,cs.CL
Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models,"Large Audio Language Models (LALMs) have extended the capabilities of Large
Language Models (LLMs) by enabling audio-based human interactions. However,
recent research has revealed that LALMs remain vulnerable to harmful queries
due to insufficient safety-alignment. Despite advances in defence measures for
text and vision LLMs, effective safety-alignment strategies and audio-safety
dataset specifically targeting LALMs are notably absent. Meanwhile defence
measures based on Supervised Fine-tuning (SFT) struggle to address safety
improvement while avoiding over-rejection issues, significantly compromising
helpfulness. In this work, we propose an unsupervised safety-fine-tuning
strategy as remedy that reshapes model's representation space to enhance
existing LALMs safety-alignment while balancing the risk of over-rejection. Our
experiments, conducted across three generations of Qwen LALMs, demonstrate that
our approach significantly improves LALMs safety under three modality input
conditions (audio-text, text-only, and audio-only) while increasing
over-rejection rate by only 0.88% on average. Warning: this paper contains
harmful examples.",2025-05-26,"Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari",http://arxiv.org/pdf/2505.19670v1,cs.CL
LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation,"Legal consultation is essential for safeguarding individual rights and
ensuring access to justice, yet remains costly and inaccessible to many
individuals due to the shortage of professionals. While recent advances in
Large Language Models (LLMs) offer a promising path toward scalable, low-cost
legal assistance, current systems fall short in handling the interactive and
knowledge-intensive nature of real-world consultations. To address these
challenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset
comprising 3,696 legal consultation dialogues with 110,008 dialogue turns,
designed to evaluate and improve LLMs' legal consultation capability. With
LeCoDe, we innovatively collect live-streamed consultations from short-video
platforms, providing authentic multi-turn legal consultation dialogues. The
rigorous annotation by legal experts further enhances the dataset with
professional insights and expertise. Furthermore, we propose a comprehensive
evaluation framework that assesses LLMs' consultation capabilities in terms of
(1) clarification capability and (2) professional advice quality. This unified
framework incorporates 12 metrics across two dimensions. Through extensive
experiments on various general and domain-specific LLMs, our results reveal
significant challenges in this task, with even state-of-the-art models like
GPT-4 achieving only 39.8% recall for clarification and 59% overall score for
advice quality, highlighting the complexity of professional consultation
scenarios. Based on these findings, we further explore several strategies to
enhance LLMs' legal consultation abilities. Our benchmark contributes to
advancing research in legal domain dialogue systems, particularly in simulating
more real-world user-expert interactions.",2025-05-26,"Weikang Yuan, Kaisong Song, Zhuoren Jiang, Junjie Cao, Yujie Zhang, Jun Lin, Kun Kuang, Ji Zhang, Xiaozhong Liu",http://arxiv.org/pdf/2505.19667v1,cs.CL
GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models,"Open-domain question answering (OpenQA) represents a cornerstone in natural
language processing (NLP), primarily focused on extracting answers from
unstructured textual data. With the rapid advancements in Large Language Models
(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent
understanding and answering capabilities enabled by massive parameters compared
to traditional methods. However, most of these methods encounter two critical
challenges: how to integrate knowledge into LLMs effectively and how to
adaptively generate results with specific answer formats for various task
situations. To address these challenges, we propose a novel framework named
GenKI, which aims to improve the OpenQA performance by exploring Knowledge
Integration and controllable Generation on LLMs simultaneously. Specifically,
we first train a dense passage retrieval model to retrieve associated knowledge
from a given knowledge base. Subsequently, we introduce a novel knowledge
integration model that incorporates the retrieval knowledge into instructions
during fine-tuning to intensify the model. Furthermore, to enable controllable
generation in LLMs, we leverage a certain fine-tuned LLM and an ensemble based
on text consistency incorporating all coherence, fluency, and answer format
assurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,
and CMRC2018 datasets, featuring diverse answer formats, have demonstrated the
effectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,
ablation studies have disclosed a linear relationship between the frequency of
retrieved knowledge and the model's ability to recall knowledge accurately
against the ground truth. Our code of GenKI is available at
https://github.com/USTC-StarTeam/GenKI",2025-05-26,"Tingjia Shen, Hao Wang, Chuan Qin, Ruijun Sun, Yang Song, Defu Lian, Hengshu Zhu, Enhong Chen",http://arxiv.org/pdf/2505.19660v1,cs.CL
"Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation","Automatic related work generation (RWG) can save people's time and effort
when writing a draft of related work section (RWS) for further revision.
However, existing methods for RWG always suffer from shallow comprehension due
to taking the limited portions of references papers as input and isolated
explanation for each reference due to ineffective capturing the relationships
among them. To address these issues, we focus on full-text-based RWG task and
propose a novel multi-agent framework. Our framework consists of three agents:
a selector that decides which section of the papers is going to read next, a
reader that digests the selected section and updates a shared working memory,
and a writer that generates RWS based on the final curated memory. To better
capture the relationships among references, we also propose two graph-aware
strategies for selector, enabling to optimize the reading order with constrains
of the graph structure. Extensive experiments demonstrate that our framework
consistently improves performance across three base models and various input
configurations. The graph-aware selectors outperform alternative selectors,
achieving state-of-the-art results. The code and data are available at
https://github.com/1190200817/Full_Text_RWG.",2025-05-26,"Xiaochuan Liu, Ruihua Song, Xiting Wang, Xu Chen",http://arxiv.org/pdf/2505.19647v1,cs.CL
SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond,"Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the
potential of Reinforcement Learning (RL) to enhance reasoning abilities in
Large Language Models (LLMs). While open-source replication efforts have
primarily focused on mathematical and coding domains, methods and resources for
developing general reasoning capabilities remain underexplored. This gap is
partly due to the challenge of collecting diverse and verifiable reasoning data
suitable for RL. We hypothesize that logical reasoning is critical for
developing general reasoning capabilities, as logic forms a fundamental
building block of reasoning. In this work, we present SynLogic, a data
synthesis framework and dataset that generates diverse logical reasoning data
at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic
approach enables controlled synthesis of data with adjustable difficulty and
quantity. Importantly, all examples can be verified by simple rules, making
them ideally suited for RL with verifiable rewards. In our experiments, we
validate the effectiveness of RL training on the SynLogic dataset based on 7B
and 32B models. SynLogic leads to state-of-the-art logical reasoning
performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B
by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and
coding tasks improves the training efficiency of these domains and
significantly enhances reasoning generalization. Notably, our mixed training
model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These
findings position SynLogic as a valuable resource for advancing the broader
reasoning capabilities of LLMs. We open-source both the data synthesis pipeline
and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.",2025-05-26,"Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He",http://arxiv.org/pdf/2505.19641v1,cs.CL
Interleaved Reasoning for Large Language Models via Reinforcement Learning,"Long chain-of-thought (CoT) significantly enhances large language models'
(LLM) reasoning capabilities. However, the extensive reasoning traces lead to
inefficiencies and an increased time-to-first-token (TTFT). We propose a novel
training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs
to interleave thinking and answering for multi-hop questions. We observe that
models inherently possess the ability to perform interleaved reasoning, which
can be further enhanced through RL. We introduce a simple yet effective
rule-based reward to incentivize correct intermediate steps, which guides the
policy model toward correct reasoning paths by leveraging intermediate signals
generated during interleaved reasoning. Extensive experiments conducted across
five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)
demonstrate consistent improvements over traditional think-answer reasoning,
without requiring external tools. Specifically, our approach reduces TTFT by
over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,
our method, trained solely on question answering and logical reasoning
datasets, exhibits strong generalization ability to complex reasoning datasets
such as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to
reveal several valuable insights into conditional reward modeling.",2025-05-26,"Roy Xie, David Qiu, Deepak Gopinath, Dong Lin, Yanchao Sun, Chong Wang, Saloni Potdar, Bhuwan Dhingra",http://arxiv.org/pdf/2505.19640v1,cs.CL
Faster and Better LLMs via Latency-Aware Test-Time Scaling,"Test-Time Scaling (TTS) has proven effective in improving the performance of
Large Language Models (LLMs) during inference. However, existing research has
overlooked the efficiency of TTS from a latency-sensitive perspective. Through
a latency-aware evaluation of representative TTS methods, we demonstrate that a
compute-optimal TTS does not always result in the lowest latency in scenarios
where latency is critical. To address this gap and achieve latency-optimal TTS,
we propose two key approaches by optimizing the concurrency configurations: (1)
branch-wise parallelism, which leverages multiple concurrent inference
branches, and (2) sequence-wise parallelism, enabled by speculative decoding.
By integrating these two approaches and allocating computational resources
properly to each, our latency-optimal TTS enables a 32B model to reach 82.3%
accuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%
within 10 seconds. Our work emphasizes the importance of latency-aware TTS and
demonstrates its ability to deliver both speed and accuracy in
latency-sensitive scenarios.",2025-05-26,"Zili Wang, Tianyu Zhang, Haoli Bai, Lu Hou, Xianzhi Yu, Wulong Liu, Shiming Xiang, Lei Zhu",http://arxiv.org/pdf/2505.19634v1,cs.CL
Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models,"Word segmentation stands as a cornerstone of Natural Language Processing
(NLP). Based on the concept of ""comprehend first, segment later"", we propose a
new framework to explore the limit of unsupervised word segmentation with Large
Language Models (LLMs) and evaluate the semantic understanding capabilities of
LLMs based on word segmentation. We employ current mainstream LLMs to perform
word segmentation across multiple languages to assess LLMs' ""comprehension"".
Our findings reveal that LLMs are capable of following simple prompts to
segment raw text into words. There is a trend suggesting that models with more
parameters tend to perform better on multiple languages. Additionally, we
introduce a novel unsupervised method, termed LLACA ($\textbf{L}$arge
$\textbf{L}$anguage Model-Inspired $\textbf{A}$ho-$\textbf{C}$orasick
$\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities
of Aho-Corasick automata, LLACA innovatively combines these with the deep
insights of well-pretrained LLMs. This approach not only enables the
construction of a dynamic $n$-gram model that adjusts based on contextual
information but also integrates the nuanced understanding of LLMs, offering
significant improvements over traditional methods. Our source code is available
at https://github.com/hkr04/LLACA",2025-05-26,"Zihong Zhang, Liqi He, Zuchao Li, Lefei Zhang, Hai Zhao, Bo Du",http://arxiv.org/pdf/2505.19631v1,cs.CL
DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue,"Large language models (LLMs) have demonstrated excellent capabilities in the
field of biomedical question answering, but their application in real-world
clinical consultations still faces core challenges. Existing systems rely on a
one-way information transmission mode where patients must fully describe their
symptoms in a single round, leading to nonspecific diagnostic recommendations
when complaints are vague. Traditional multi-turn dialogue methods based on
supervised learning are constrained by static data-driven paradigms, lacking
generalizability and struggling to intelligently extract key clinical
information. To address these limitations, we propose DoctorAgent-RL, a
reinforcement learning (RL)-based multi-agent collaborative framework that
models medical consultations as a dynamic decision-making process under
uncertainty. The doctor agent continuously optimizes its questioning strategy
within the RL framework through multi-turn interactions with the patient agent,
dynamically adjusting its information-gathering path based on comprehensive
rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables
LLMs to autonomously develop interaction strategies aligned with clinical
reasoning logic, rather than superficially imitating patterns in existing
dialogue data. Notably, we constructed MTMedDialog, the first English
multi-turn medical consultation dataset capable of simulating patient
interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing
models in both multi-turn reasoning capability and final diagnostic
performance, demonstrating practical value in assisting clinical consultations.
https://github.com/JarvisUSTC/DoctorAgent-RL",2025-05-26,"Yichun Feng, Jiawei Wang, Lu Zhou, Yixue Li",http://arxiv.org/pdf/2505.19630v1,cs.CL
HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices,"Large language models (LLMs) have the potential to revolutionize smart home
assistants by enhancing their ability to accurately understand user needs and
respond appropriately, which is extremely beneficial for building a smarter
home environment. While recent studies have explored integrating LLMs into
smart home systems, they primarily focus on handling straightforward, valid
single-device operation instructions. However, real-world scenarios are far
more complex and often involve users issuing invalid instructions or
controlling multiple devices simultaneously. These have two main challenges:
LLMs must accurately identify and rectify errors in user instructions and
execute multiple user instructions perfectly. To address these challenges and
advance the development of LLM-based smart home assistants, we introduce
HomeBench, the first smart home dataset with valid and invalid instructions
across single and multiple devices in this paper. We have experimental results
on 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the
scenario of invalid multi-device instructions, revealing that the existing
state-of-the-art LLMs still cannot perform well in this situation even with the
help of in-context learning, retrieval-augmented generation, and fine-tuning.
Our code and dataset are publicly available at
https://github.com/BITHLP/HomeBench.",2025-05-26,"Silin Li, Yuhang Guo, Jiashu Yao, Zeming Liu, Haifeng Wang",http://arxiv.org/pdf/2505.19628v1,cs.CL
"Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models","As Large Language Models (LLMs) become deeply integrated into human life and
increasingly influence decision-making, it's crucial to evaluate whether and to
what extent they exhibit subjective preferences, opinions, and beliefs. These
tendencies may stem from biases within the models, which may shape their
behavior, influence the advice and recommendations they offer to users, and
potentially reinforce certain viewpoints. This paper presents the Preference,
Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs'
subjective inclinations across societal, cultural, ethical, and personal
domains. We applied our benchmark to evaluate leading open- and closed-source
LLMs, measuring desired properties such as reliability, neutrality, and
consistency. In addition, we investigated the effect of increasing the
test-time compute, through reasoning and self-reflection mechanisms, on those
metrics. While effective in other tasks, our results show that these mechanisms
offer only limited gains in our domain. Furthermore, we reveal that newer model
versions are becoming less consistent and more biased toward specific
viewpoints, highlighting a blind spot and a concerning trend. POBS:
https://ibm.github.io/POBS",2025-05-26,"George Kour, Itay Nakash, Ateret Anaby-Tavor, Michal Shmueli-Scheuer",http://arxiv.org/pdf/2505.19621v1,cs.CL
Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically,"Cross-lingual alignment in pretrained language models (LMs) has enabled
efficient transfer in text-based LMs. Such an alignment has also been observed
in speech foundation models. However, it remains an open question whether
findings and methods from text-based cross-lingual alignment apply to speech.
Building on prior work on spoken translation retrieval, we perform
pronunciation-controlled experiments to observe if cross-lingual alignment can
indeed occur in such models on a semantic basis, instead of relying on phonetic
similarities. Our findings indicate that even in the absence of phonetic cues,
spoken translation retrieval accuracy remains relatively stable. We follow up
with a controlled experiment on a word-level dataset of cross-lingual synonyms
and near-homophones, confirming the existence of both phonetic and semantic
knowledge in the encoder. Finally, we qualitatively examine the transcriptions
produced by early exiting the encoder, where we observe that speech translation
produces semantic errors that are characterized by phonetic similarities to
corresponding words in the source language. We apply this insight from early
exiting to speech recognition in seven low-resource languages unsupported by
the Whisper model, and achieve improved accuracy in all languages examined,
particularly for languages with transparent orthographies.",2025-05-26,"Ryan Soh-Eun Shim, Domenico De Cristofaro, Chengzhi Martin Hu, Alessandro Vietti, Barbara Plank",http://arxiv.org/pdf/2505.19606v1,cs.CL
Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis,"Machine translation has become a critical tool in bridging linguistic gaps,
especially between languages as diverse as English and Hindi. This paper
comprehensively evaluates various machine translation models for translating
between English and Hindi. We assess the performance of these models using a
diverse set of automatic evaluation metrics, both lexical and machine
learning-based metrics. Our evaluation leverages an 18000+ corpus of English
Hindi parallel dataset and a custom FAQ dataset comprising questions from
government websites. The study aims to provide insights into the effectiveness
of different machine translation approaches in handling both general and
specialized language domains. Results indicate varying performance levels
across different metrics, highlighting strengths and areas for improvement in
current translation systems.",2025-05-26,Ahan Prasannakumar Shetty,http://arxiv.org/pdf/2505.19604v1,cs.CL
Preference Optimization by Estimating the Ratio of the Data Distribution,"Direct preference optimization (DPO) is widely used as a simple and stable
method for aligning large language models (LLMs) with human preferences. This
paper investigates a generalized DPO loss that enables a policy model to match
the target policy from a likelihood ratio estimation perspective. The ratio of
the target policy provides a unique identification of the policy distribution
without relying on reward models or partition functions. This allows the
generalized loss to retain both simplicity and theoretical guarantees, which
prior work such as $f$-PO fails to achieve simultaneously. We propose Bregman
preference optimization (BPO), a generalized framework for ratio matching that
provides a family of objective functions achieving target policy optimality.
BPO subsumes DPO as a special case and offers tractable forms for all
instances, allowing implementation with a few lines of code. We further develop
scaled Basu's power divergence (SBA), a gradient scaling method that can be
used for BPO instances. The BPO framework complements other DPO variants and is
applicable to target policies defined by these variants. In experiments, unlike
other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a
trade-off between generation fidelity and diversity, instances of BPO improve
both win rate and entropy compared with DPO. When applied to
Llama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B
backbones, with a 55.9\% length-controlled win rate on AlpacaEval2.",2025-05-26,"Yeongmin Kim, Heesun Bae, Byeonghu Na, Il-Chul Moon",http://arxiv.org/pdf/2505.19601v1,cs.CL
Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar,"Typical methods for evaluating the performance of language models evaluate
their ability to answer questions accurately. These evaluation metrics are
acceptable for determining the extent to which language models can understand
and reason about text in a general sense, but fail to capture nuanced
capabilities, such as the ability of language models to recognize and obey rare
grammar points, particularly in languages other than English. We measure the
perplexity of language models when confronted with the ""first person psych
predicate restriction"" grammar point in Japanese. Weblab is the only tested
open source model in the 7-10B parameter range which consistently assigns
higher perplexity to ungrammatical psych predicate sentences than grammatical
ones. We give evidence that Weblab's uniformly bad tokenization is a possible
root cause for its good performance, and show that Llama 3's perplexity on
grammatical psych predicate sentences can be reduced by orders of magnitude
(28x difference) by restricting test sentences to those with uniformly
well-behaved tokenizations. We show in further experiments on machine
translation tasks that language models will use alternative grammar patterns in
order to produce grammatical sentences when tokenization issues prevent the
most natural sentence from being output.",2025-05-26,"Andrew Gambardella, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo",http://arxiv.org/pdf/2505.19599v1,cs.CL
Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study,"Large Audio-Language Models (LALMs) are increasingly deployed in real-world
applications, yet their robustness against malicious audio injection attacks
remains underexplored. This study systematically evaluates five leading LALMs
across four attack scenarios: Audio Interference Attack, Instruction Following
Attack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics
like Defense Success Rate, Context Robustness Score, and Judgment Robustness
Index, their vulnerabilities and resilience were quantitatively assessed.
Experimental results reveal significant performance disparities among models;
no single model consistently outperforms others across all attack types. The
position of malicious content critically influences attack effectiveness,
particularly when placed at the beginning of sequences. A negative correlation
between instruction-following capability and robustness suggests models
adhering strictly to instructions may be more susceptible, contrasting with
greater resistance by safety-aligned models. Additionally, system prompts show
mixed effectiveness, indicating the need for tailored strategies. This work
introduces a benchmark framework and highlights the importance of integrating
robustness into training pipelines. Findings emphasize developing multi-modal
defenses and architectural designs that decouple capability from susceptibility
for secure LALMs deployment.",2025-05-26,"Guanyu Hou, Jiaming He, Yinhang Zhou, Ji Guo, Yitong Qiao, Rui Zhang, Wenbo Jiang",http://arxiv.org/pdf/2505.19598v1,cs.CL
Multi-Agent Collaboration via Evolving Orchestration,"Large language models (LLMs) have achieved remarkable results across diverse
downstream tasks, but their monolithic nature restricts scalability and
efficiency in complex problem-solving. While recent research explores
multi-agent collaboration among LLMs, most approaches rely on static
organizational structures that struggle to adapt as task complexity and agent
numbers grow, resulting in coordination overhead and inefficiencies. To this
end, we propose a puppeteer-style paradigm for LLM-based multi-agent
collaboration, where a centralized orchestrator (""puppeteer"") dynamically
directs agents (""puppets"") in response to evolving task states. This
orchestrator is trained via reinforcement learning to adaptively sequence and
prioritize agents, enabling flexible and evolvable collective reasoning.
Experiments on closed- and open-domain scenarios show that this method achieves
superior performance with reduced computational costs. Analyses further reveal
that the key improvements consistently stem from the emergence of more compact,
cyclic reasoning structures under the orchestrator's evolution.",2025-05-26,"Yufan Dang, Chen Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng Yang, Xiaoyin Che, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, Maosong Sun",http://arxiv.org/pdf/2505.19591v1,cs.CL
Learning to Reason without External Rewards,"Training large language models (LLMs) for complex reasoning via Reinforcement
Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on
costly, domain-specific supervision. We explore Reinforcement Learning from
Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic
signals without external rewards or labeled data. We propose Intuitor, an RLIF
method that uses a model's own confidence, termed self-certainty, as its sole
reward signal. Intuitor replaces external rewards in Group Relative Policy
Optimization (GRPO) with self-certainty scores, enabling fully unsupervised
learning. Experiments demonstrate that Intuitor matches GRPO's performance on
mathematical benchmarks while achieving superior generalization to
out-of-domain tasks like code generation, without requiring gold solutions or
test cases. Our findings show that intrinsic model signals can drive effective
learning across domains, offering a scalable alternative to RLVR for autonomous
AI systems where verifiable rewards are unavailable. Code is available at
https://github.com/sunblaze-ucb/Intuitor",2025-05-26,"Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, Dawn Song",http://arxiv.org/pdf/2505.19590v1,cs.CL
TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization,"The Key-Value (KV) cache in generative large language models (LLMs)
introduces substantial memory overhead. Existing works mitigate this burden by
offloading or compressing the KV cache. However, loading the entire cache
incurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU
communication, while aggressive compression causes notable performance
degradation. We identify that certain layers in the LLM need to maintain global
information and are unsuitable for selective loading. In contrast, other layers
primarily focus on a few tokens with dominant activations that potentially
incur substantial quantization error. This observation leads to a key insight
that loading dominant tokens and quantizing all tokens can complement each
other. Building on this insight, we propose a hybrid compression method,
TailorKV, which seamlessly integrates quantization and offloading. TailorKV
develops an inference framework along with a hardware-friendly implementation
that leverages these complementary characteristics. Extensive long-context
evaluations exhibit that TailorKV achieves nearly lossless performance under
aggressive compression settings, outperforming the state-of-the-art.
Particularly, the Llama-3.1-8B with 128k context can be served within a single
RTX 3090 GPU, reaching 82 ms per token during decoding.",2025-05-26,"Dingyu Yao, Bowen Shen, Zheng Lin, Wei Liu, Jian Luan, Bin Wang, Weiping Wang",http://arxiv.org/pdf/2505.19586v1,cs.CL
Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing,"Sparse attention methods exploit the inherent sparsity in attention to speed
up the prefilling phase of long-context inference, mitigating the quadratic
complexity of full attention computation. While existing sparse attention
methods rely on predefined patterns or inaccurate estimations to approximate
attention behavior, they often fail to fully capture the true dynamics of
attention, resulting in reduced efficiency and compromised accuracy. Instead,
we propose a highly accurate sparse attention mechanism that shares similar yet
precise attention patterns across heads, enabling a more realistic capture of
the dynamic behavior of attention. Our approach is grounded in two key
observations: (1) attention patterns demonstrate strong inter-head similarity,
and (2) this similarity remains remarkably consistent across diverse inputs. By
strategically sharing computed accurate patterns across attention heads, our
method effectively captures actual patterns while requiring full attention
computation for only a small subset of heads. Comprehensive evaluations
demonstrate that our approach achieves superior or comparable speedup relative
to state-of-the-art methods while delivering the best overall accuracy.",2025-05-26,"Dan Peng, Zhihui Fu, Zewen Ye, Zhuoran Song, Jun Wang",http://arxiv.org/pdf/2505.19578v1,cs.CL
DocMEdit: Towards Document-Level Model Editing,"Model editing aims to correct errors and outdated knowledge in the Large
language models (LLMs) with minimal cost. Prior research has proposed a variety
of datasets to assess the effectiveness of these model editing methods.
However, most existing datasets only require models to output short phrases or
sentences, overlooks the widespread existence of document-level tasks in the
real world, raising doubts about their practical usability. Aimed at addressing
this limitation and promoting the application of model editing in real-world
scenarios, we propose the task of document-level model editing. To tackle such
challenges and enhance model capabilities in practical settings, we introduce
\benchmarkname, a dataset focused on document-level model editing,
characterized by document-level inputs and outputs, extrapolative, and multiple
facts within a single edit. We propose a series of evaluation metrics and
experiments. The results show that the difficulties in document-level model
editing pose challenges for existing model editing methods.",2025-05-26,"Li Zeng, Zeming Liu, Chong Feng, Heyan Huang, Yuhang Guo",http://arxiv.org/pdf/2505.19572v1,cs.CL
Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights,"Reasoning with tabular data holds increasing importance in modern
applications, yet comprehensive evaluation methodologies for
reasoning-intensive Table Question Answering (QA) tasks remain nascent.
Existing research is constrained by two primary bottlenecks: 1) Reliance on
costly manually annotated real-world data, which is difficult to cover complex
reasoning scenarios; 2) The heterogeneity of table structures hinders
systematic analysis of the intrinsic mechanisms behind the underperformance of
LLMs, especially in reasoning-intensive tasks. To address these issues, we
propose an automated generation pipeline AutoT2T that transforms mathematical
word problems into table-based reasoning tasks, eliminating the need for manual
annotation. The pipeline can generate multiple variants of a table for the same
reasoning problem, including noisy versions to support robustness evaluation.
Based on this, we construct a new benchmark TabularGSM, which systematically
spans a range of table complexities and trap problems. Experimental analyses
through AutoT2T and TabularGSM reveal that the tight coupling between reasoning
and retrieval or identification processes is a key factor underlying the
failure of LLMs in complex Table QA tasks. This highlights the necessity for
models to develop synergistic reasoning capabilities in order to perform
effectively in complex Table QA tasks.",2025-05-26,"Shi-Yu Tian, Zhi Zhou, Wei Dong, Ming Yang, Kun-Yang Yu, Zi-Jian Cheng, Lan-Zhe Guo, Yu-Feng Li",http://arxiv.org/pdf/2505.19563v1,cs.CL
Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents,"Large Language Models (LLMs) have recently been widely adopted in
conversational agents. However, the increasingly long interactions between
users and agents accumulate extensive dialogue records, making it difficult for
LLMs with limited context windows to maintain a coherent long-term dialogue
memory and deliver personalized responses. While retrieval-augmented memory
systems have emerged to address this issue, existing methods often depend on
single-granularity memory segmentation and retrieval. This approach falls short
in capturing deep memory connections, leading to partial retrieval of useful
information or substantial noise, resulting in suboptimal performance. To
tackle these limits, we propose MemGAS, a framework that enhances memory
consolidation by constructing multi-granularity association, adaptive
selection, and retrieval. MemGAS is based on multi-granularity memory units and
employs Gaussian Mixture Models to cluster and associate new memories with
historical ones. An entropy-based router adaptively selects optimal granularity
by evaluating query relevance distributions and balancing information
completeness and noise. Retrieved memories are further refined via LLM-based
filtering. Experiments on four long-term memory benchmarks demonstrate that
MemGAS outperforms state-of-the-art methods on both question answer and
retrieval tasks, achieving superior performance across different query types
and top-K settings.",2025-05-26,"Derong Xu, Yi Wen, Pengyue Jia, Yingyi Zhang, wenlin zhang, Yichao Wang, Huifeng Guo, Ruiming Tang, Xiangyu Zhao, Enhong Chen, Tong Xu",http://arxiv.org/pdf/2505.19549v1,cs.CL
How Syntax Specialization Emerges in Language Models,"Large language models (LLMs) have been found to develop surprising internal
specializations: Individual neurons, attention heads, and circuits become
selectively sensitive to syntactic structure, reflecting patterns observed in
the human brain. While this specialization is well-documented, how it emerges
during training and what influences its development remains largely unknown.
  In this work, we tap into the black box of specialization by tracking its
formation over time. By quantifying internal syntactic consistency across
minimal pairs from various syntactic phenomena, we identify a clear
developmental trajectory: Syntactic sensitivity emerges gradually, concentrates
in specific layers, and exhibits a 'critical period' of rapid internal
specialization. This process is consistent across architectures and
initialization parameters (e.g., random seeds), and is influenced by model
scale and training data. We therefore reveal not only where syntax arises in
LLMs but also how some models internalize it during training. To support future
research, we will release the code, models, and training checkpoints upon
acceptance.",2025-05-26,"Xufeng Duan, Zhaoqian Yao, Yunhao Zhang, Shaonan Wang, Zhenguang G. Cai",http://arxiv.org/pdf/2505.19548v1,cs.CL
DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients,"Existing medical RAG systems mainly leverage knowledge from medical knowledge
bases, neglecting the crucial role of experiential knowledge derived from
similar patient cases -- a key component of human clinical reasoning. To bridge
this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like
reasoning by integrating both explicit clinical knowledge and implicit
case-based experience. DoctorRAG enhances retrieval precision by first
allocating conceptual tags for queries and knowledge sources, together with a
hybrid retrieval mechanism from both relevant knowledge and patient. In
addition, a Med-TextGrad module using multi-agent textual gradients is
integrated to ensure that the final output adheres to the retrieved knowledge
and patient query. Comprehensive experiments on multilingual, multitask
datasets demonstrate that DoctorRAG significantly outperforms strong baseline
RAG models and gains improvements from iterative refinements. Our approach
generates more accurate, relevant, and comprehensive responses, taking a step
towards more doctor-like medical reasoning systems.",2025-05-26,"Yuxing Lu, Gecheng Fu, Wei Wu, Xukai Zhao, Sin Yee Goi, Jinzhuo Wang",http://arxiv.org/pdf/2505.19538v1,cs.CL
FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models,"Large vision-language models (LVLMs) excel at multimodal understanding but
suffer from high computational costs due to redundant vision tokens. Existing
pruning methods typically rely on single-layer attention scores to rank and
prune redundant visual tokens to solve this inefficiency. However, as the
interaction between tokens and layers is complicated, this raises a basic
question: Is such a simple single-layer criterion sufficient to identify
redundancy? To answer this question, we rethink the emergence of redundant
visual tokens from a fundamental perspective: information flow, which models
the interaction between tokens and layers by capturing how information moves
between tokens across layers. We find (1) the CLS token acts as an information
relay, which can simplify the complicated flow analysis; (2) the redundancy
emerges progressively and dynamically via layer-wise attention concentration;
and (3) relying solely on attention scores from single layers can lead to
contradictory redundancy identification. Based on this, we propose FlowCut, an
information-flow-aware pruning framework, mitigating the insufficiency of the
current criterion for identifying redundant tokens and better aligning with the
model's inherent behaviors. Extensive experiments show that FlowCut achieves
superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token
reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x
speed-up in the prefilling stage. Our code is available at
https://github.com/TungChintao/FlowCut",2025-05-26,"Jintao Tong, Wenwei Jin, Pengda Qin, Anqi Li, Yixiong Zou, Yuhong Li, Yuhua Li, Ruixuan Li",http://arxiv.org/pdf/2505.19536v1,cs.CL
"Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation","Small Language Models (SLMs) have gained substantial attention due to their
ability to execute diverse language tasks successfully while using fewer
computer resources. These models are particularly ideal for deployment in
limited environments, such as mobile devices, on-device processing, and edge
systems. In this study, we present a complete assessment of SLMs, focussing on
their design frameworks, training approaches, and techniques for lowering model
size and complexity. We offer a novel classification system to organize the
optimization approaches applied for SLMs, encompassing strategies like pruning,
quantization, and model compression. Furthermore, we assemble SLM's studies of
evaluation suite with some existing datasets, establishing a rigorous platform
for measuring SLM capabilities. Alongside this, we discuss the important
difficulties that remain unresolved in this sector, including trade-offs
between efficiency and performance, and we suggest directions for future study.
We anticipate this study to serve as a beneficial guide for researchers and
practitioners who aim to construct compact, efficient, and high-performing
language models.",2025-05-26,"Tanjil Hasan Sakib, Md. Tanzib Hosain, Md. Kishor Morol",http://arxiv.org/pdf/2505.19529v1,cs.CL
AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection,"Implicit hate speech detection is challenging due to its subtlety and
reliance on contextual interpretation rather than explicit offensive words.
Current approaches rely on contrastive learning, which are shown to be
effective on distinguishing hate and non-hate sentences. Humans, however,
detect implicit hate speech by first identifying specific targets within the
text and subsequently interpreting how these target relate to their surrounding
context. Motivated by this reasoning process, we propose AmpleHate, a novel
approach designed to mirror human inference for implicit hate detection.
AmpleHate identifies explicit target using a pretrained Named Entity
Recognition model and capture implicit target information via [CLS] tokens. It
computes attention-based relationships between explicit, implicit targets and
sentence context and then, directly injects these relational vectors into the
final sentence representation. This amplifies the critical signals of
target-context relations for determining implicit hate. Experiments demonstrate
that AmpleHate achieves state-of-the-art performance, outperforming contrastive
learning baselines by an average of 82.14% and achieve faster convergence.
Qualitative analyses further reveal that attention patterns produced by
AmpleHate closely align with human judgement, underscoring its interpretability
and robustness.",2025-05-26,"Yejin Lee, Joonghyuk Hahn, Hyeseon Ahn, Yo-Sub Han",http://arxiv.org/pdf/2505.19528v1,cs.CL
Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework,"We present a critical discourse analysis of the 2024 U.S. presidential
debates, examining Donald Trump's rhetorical strategies in his interactions
with Joe Biden and Kamala Harris. We introduce a novel annotation framework,
BEADS (Bias Enriched Annotation for Dialogue Structure), which systematically
extends the DAMSL framework to capture bias driven and adversarial discourse
features in political communication. BEADS includes a domain and language
agnostic set of tags that model ideological framing, emotional appeals, and
confrontational tactics. Our methodology compares detailed human annotation
with zero shot ChatGPT assisted tagging on verified transcripts from the Trump
and Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our
analysis shows that Trump consistently dominated in key categories: Challenge
and Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,
and Perceived Dismissiveness. These findings underscore his use of emotionally
charged and adversarial rhetoric to control the narrative and influence
audience perception. In this work, we establish BEADS as a scalable and
reproducible framework for critical discourse analysis across languages,
domains, and political contexts.",2025-05-26,"Lavanya Prahallad, Radhika Mamidi",http://arxiv.org/pdf/2505.19515v1,cs.CL
SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback,"Prompt quality plays a critical role in the performance of large language
models (LLMs), motivating a growing body of work on prompt optimization. Most
existing methods optimize prompts over a fixed dataset, assuming static input
distributions and offering limited support for iterative improvement. We
introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a
closed-loop framework for prompt learning that integrates synthetic data
generation into the optimization process. SIPDO couples a synthetic data
generator with a prompt optimizer, where the generator produces new examples
that reveal current prompt weaknesses and the optimizer incrementally refines
the prompt in response. This feedback-driven loop enables systematic
improvement of prompt performance without assuming access to external
supervision or new tasks. Experiments across question answering and reasoning
benchmarks show that SIPDO outperforms standard prompt tuning methods,
highlighting the value of integrating data synthesis into prompt learning
workflows.",2025-05-26,"Yaoning Yu, Ye Yu, Kai Wei, Haojing Luo, Haohan Wang",http://arxiv.org/pdf/2505.19514v1,cs.CL
Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models,"Large proprietary language models exhibit strong causal reasoning abilities
that smaller open-source models struggle to replicate. We introduce a novel
framework for distilling causal explanations that transfers causal reasoning
skills from a powerful teacher model to a compact open-source model. The key
idea is to train the smaller model to develop causal reasoning abilities by
generating structured cause-and-effect explanations consistent with those of
the teacher model. To evaluate the quality of the student-generated
explanations, we introduce a new metric called Causal Explanation Coherence
(CEC) to assess the structural and logical consistency of causal reasoning.
This metric uses sentence-level semantic alignment to measure how well each
part of the generated explanation corresponds to the teacher's reference,
capturing both faithfulness and coverage of the underlying causal chain. Our
framework and the CEC metric provide a principled foundation for training
smaller models to perform robust causal reasoning and for systematically
assessing the coherence of explanations in language model outputs.",2025-05-26,"Aggrey Muhebwa, Khalid K. Osman",http://arxiv.org/pdf/2505.19511v1,cs.CL
LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study,"The remarkable reasoning and generalization capabilities of Large Language
Models (LLMs) have paved the way for their expanding applications in embodied
AI, robotics, and other real-world tasks. To effectively support these
applications, grounding in spatial and temporal understanding in multimodal
environments is essential. To this end, recent works have leveraged scene
graphs, a structured representation that encodes entities, attributes, and
their relationships in a scene. However, a comprehensive evaluation of LLMs'
ability to utilize scene graphs remains limited. In this work, we introduce
Text-Scene Graph (TSG) Bench, a benchmark designed to systematically assess
LLMs' ability to (1) understand scene graphs and (2) generate them from textual
narratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models
perform well on scene graph understanding, they struggle with scene graph
generation, particularly for complex narratives. Our analysis indicates that
these models fail to effectively decompose discrete scenes from a complex
narrative, leading to a bottleneck when generating scene graphs. These findings
underscore the need for improved methodologies in scene graph generation and
provide valuable insights for future research. The demonstration of our
benchmark is available at https://tsg-bench.netlify.app. Additionally, our code
and evaluation data are publicly available at
https://anonymous.4open.science/r/TSG-Bench.",2025-05-26,"Dongil Yang, Minjin Kim, Sunghwan Kim, Beong-woo Kwak, Minjun Park, Jinseok Hong, Woontack Woo, Jinyoung Yeo",http://arxiv.org/pdf/2505.19510v1,cs.CL
DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation,"Large Language Models (LLMs) represent substantial intellectual and economic
investments, yet their effectiveness can inadvertently facilitate model
imitation via knowledge distillation (KD).In practical scenarios, competitors
can distill proprietary LLM capabilities by simply observing publicly
accessible outputs, akin to reverse-engineering a complex performance by
observation alone. Existing protective methods like watermarking only identify
imitation post-hoc, while other defenses assume the student model mimics the
teacher's internal logits, rendering them ineffective against distillation
purely from observed output text. This paper confronts the challenge of
actively protecting LLMs within the realistic constraints of API-based access.
We introduce an effective and efficient Defensive Output Generation (DOGe)
strategy that subtly modifies the output behavior of an LLM. Its outputs remain
accurate and useful for legitimate users, yet are designed to be misleading for
distillation, significantly undermining imitation attempts. We achieve this by
fine-tuning only the final linear layer of the teacher LLM with an adversarial
loss. This targeted training approach anticipates and disrupts distillation
attempts during inference time. Our experiments show that, while preserving or
even improving the original performance of the teacher model, student models
distilled from the defensively generated teacher outputs demonstrate
catastrophically reduced performance, demonstrating our method's effectiveness
as a practical safeguard against KD-based model imitation.",2025-05-26,"Pingzhi Li, Zhen Tan, Huaizhi Qu, Huan Liu, Tianlong Chen",http://arxiv.org/pdf/2505.19504v1,cs.CL
Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents,"The study presents a comprehensive benchmark for retrieving Sanskrit
documents using English queries, focusing on the chapters of the
Srimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR),
Translation-based Retrieval (DT), and Query Translation (QT), utilizing shared
embedding spaces and advanced translation methods to enhance retrieval systems
in a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's
linguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT,
Contriever, and GPT-2. It adapts summarization techniques for Sanskrit
documents to improve QA processing. Evaluation shows DT methods outperform DR
and QT in handling the cross-lingual challenges of ancient texts, improving
accessibility and understanding. A dataset of 3,400 English-Sanskrit
query-document pairs underpins the study, aiming to preserve Sanskrit
scriptures and share their philosophical importance widely. Our dataset is
publicly available at https://huggingface.co/datasets/manojbalaji1/anveshana",2025-05-26,"Manoj Balaji Jagadeeshan, Prince Raj, Pawan Goyal",http://arxiv.org/pdf/2505.19494v1,cs.CL
CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis,"Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they often exhibit a specific cultural biases, neglecting
the values and linguistic diversity of low-resource regions. This cultural bias
not only undermines universal equality, but also risks reinforcing stereotypes
and perpetuating discrimination. To address this, we propose CulFiT, a novel
culturally-aware training paradigm that leverages multilingual data and
fine-grained reward modeling to enhance cultural sensitivity and inclusivity.
Our approach synthesizes diverse cultural-related questions, constructs
critique data in culturally relevant languages, and employs fine-grained
rewards to decompose cultural texts into verifiable knowledge units for
interpretable evaluation. We also introduce GlobalCultureQA, a multilingual
open-ended question-answering dataset designed to evaluate culturally-aware
responses in a global context. Extensive experiments on three existing
benchmarks and our GlobalCultureQA demonstrate that CulFiT achieves
state-of-the-art open-source model performance in cultural alignment and
general reasoning.",2025-05-26,"Ruixiang Feng, Shen Gao, Xiuying Chen, Lisi Chen, Shuo Shang",http://arxiv.org/pdf/2505.19484v1,cs.CL
Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection,"Learning to adapt pretrained language models to unlabeled,
out-of-distribution data is a critical challenge, as models often falter on
structurally novel reasoning tasks even while excelling within their training
distribution. We introduce a new framework called VDS-TTT - Verifier-Driven
Sample Selection for Test-Time Training to efficiently address this. We use a
learned verifier to score a pool of generated responses and select only from
high ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,
for each input query our LLM generates N candidate answers; the verifier
assigns a reliability score to each, and the response with the highest
confidence and above a fixed threshold is paired with its query for test-time
training. We fine-tune only low-rank LoRA adapter parameters, ensuring
adaptation efficiency and fast convergence. Our proposed self-supervised
framework is the first to synthesize verifier driven test-time training data
for continuous self-improvement of the model. Experiments across three diverse
benchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up
to a 32.29% relative improvement over the base model and a 6.66% gain compared
to verifier-based methods without test-time training, highlighting its
effectiveness and efficiency for on-the-fly large language model adaptation.",2025-05-26,"Mohammad Mahdi Moradi, Hossam Amer, Sudhir Mudur, Weiwei Zhang, Yang Liu, Walid Ahmed",http://arxiv.org/pdf/2505.19475v1,cs.CL
Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks,"Attention and State-Space Models (SSMs) when combined in a hybrid network in
sequence or in parallel provide complementary strengths. In a hybrid sequential
pipeline they alternate between applying a transformer to the input and then
feeding its output into a SSM. This results in idle periods in the individual
components increasing end-to-end latency and lowering throughput caps. In the
parallel hybrid architecture, the transformer operates independently in
parallel with the SSM, and these pairs are cascaded, with output from one pair
forming the input to the next. Two issues are (i) creating an expressive
knowledge representation with the inherently divergent outputs from these
separate branches, and (ii) load balancing the computation between these
parallel branches, while maintaining representation fidelity. In this work we
present FlowHN, a novel parallel hybrid network architecture that accommodates
various strategies for load balancing, achieved through appropriate
distribution of input tokens between the two branches. Two innovative
differentiating factors in FlowHN include a FLOP aware dynamic token split
between the attention and SSM branches yielding efficient balance in compute
load, and secondly, a method to fuse the highly divergent outputs from
individual branches for enhancing representation expressivity. Together they
enable much better token processing speeds, avoid bottlenecks, and at the same
time yield significantly improved accuracy as compared to other competing
works. We conduct comprehensive experiments on autoregressive language modeling
for models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential
hybrid models and its parallel counterpart, achieving up to 4* higher Tokens
per Second (TPS) and 2* better Model FLOPs Utilization (MFU).",2025-05-26,"Mohammad Mahdi Moradi, Walid Ahmed, Shuangyue Wen, Sudhir Mudur, Weiwei Zhang, Yang Liu",http://arxiv.org/pdf/2505.19472v1,cs.CL
BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs,"Large language models excel in general tasks, yet assessing their reliability
in logic-heavy, precision-critical domains like finance, law, and healthcare
remains challenging. To address this, we introduce BizFinBench, the first
benchmark specifically designed to evaluate LLMs in real-world financial
applications. BizFinBench consists of 6,781 well-annotated queries in Chinese,
spanning five dimensions: numerical calculation, reasoning, information
extraction, prediction recognition, and knowledge-based question answering,
grouped into nine fine-grained categories. The benchmark includes both
objective and subjective metrics. We also introduce IteraJudge, a novel LLM
evaluation method that reduces bias when LLMs serve as evaluators in objective
metrics. We benchmark 25 models, including both proprietary and open-source
systems. Extensive experiments show that no model dominates across all tasks.
Our evaluation reveals distinct capability patterns: (1) In Numerical
Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while
smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,
proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with
open-source models trailing by up to 19.49 points; (3) In Information
Extraction, the performance spread is the largest, with DeepSeek-R1 scoring
71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,
performance variance is minimal, with top models scoring between 39.16 and
50.00. We find that while current LLMs handle routine finance queries
competently, they struggle with complex scenarios requiring cross-concept
reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future
research. The code and dataset are available at
https://github.com/HiThink-Research/BizFinBench.",2025-05-26,"Guilong Lu, Xuntao Guo, Rongjunchen Zhang, Wenqiao Zhu, Ji Liu",http://arxiv.org/pdf/2505.19457v1,cs.CL
Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI,"This review presents a comprehensive analysis of two emerging paradigms in
AI-assisted software development: vibe coding and agentic coding. While both
leverage large language models (LLMs), they differ fundamentally in autonomy,
architectural design, and the role of the developer. Vibe coding emphasizes
intuitive, human-in-the-loop interaction through prompt-based, conversational
workflows that support ideation, experimentation, and creative exploration. In
contrast, agentic coding enables autonomous software development through
goal-driven agents capable of planning, executing, testing, and iterating tasks
with minimal human intervention. We propose a detailed taxonomy spanning
conceptual foundations, execution models, feedback loops, safety mechanisms,
debugging strategies, and real-world tool ecosystems. Through comparative
workflow analysis and 20 detailed use cases, we illustrate how vibe systems
thrive in early-stage prototyping and education, while agentic systems excel in
enterprise-grade automation, codebase refactoring, and CI/CD integration. We
further examine emerging trends in hybrid architectures, where natural language
interfaces are coupled with autonomous execution pipelines. Finally, we
articulate a future roadmap for agentic AI, outlining the infrastructure needed
for trustworthy, explainable, and collaborative systems. Our findings suggest
that successful AI software engineering will rely not on choosing one paradigm,
but on harmonizing their strengths within a unified, human-centered development
lifecycle.",2025-05-26,"Ranjan Sapkota, Konstantinos I. Roumeliotis, Manoj Karkee",http://arxiv.org/pdf/2505.19443v1,cs.CL
"The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models","This paper studies the emergence of interpretable categorical features within
large language models (LLMs), analyzing their behavior across training
checkpoints (time), transformer layers (space), and varying model sizes
(scale). Using sparse autoencoders for mechanistic interpretability, we
identify when and where specific semantic concepts emerge within neural
activations. Results indicate clear temporal and scale-specific thresholds for
feature emergence across multiple domains. Notably, spatial analysis reveals
unexpected semantic reactivation, with early-layer features re-emerging at
later layers, challenging standard assumptions about representational dynamics
in transformer models.",2025-05-26,"Shashata Sawmya, Micah Adler, Nir Shavit",http://arxiv.org/pdf/2505.19440v1,cs.CL
Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers,"Large Language Models have achieved remarkable success in natural language
processing tasks, with Reinforcement Learning playing a key role in adapting
them to specific applications. However, obtaining ground truth answers for
training LLMs in mathematical problem-solving is often challenging, costly, and
sometimes unfeasible. This research delves into the utilization of format and
length as surrogate signals to train LLMs for mathematical problem-solving,
bypassing the need for traditional ground truth answers.Our study shows that a
reward function centered on format correctness alone can yield performance
improvements comparable to the standard GRPO algorithm in early phases.
Recognizing the limitations of format-only rewards in the later phases, we
incorporate length-based rewards. The resulting GRPO approach, leveraging
format-length surrogate signals, not only matches but surpasses the performance
of the standard GRPO algorithm relying on ground truth answers in certain
scenarios, achieving 40.0\% accuracy on AIME2024 with a 7B base model. Through
systematic exploration and experimentation, this research not only offers a
practical solution for training LLMs to solve mathematical problems and
reducing the dependence on extensive ground truth data collection, but also
reveals the essence of why our label-free approach succeeds: base model is like
an excellent student who has already mastered mathematical and logical
reasoning skills, but performs poorly on the test paper, it simply needs to
develop good answering habits to achieve outstanding results in exams , in
other words, to unlock the capabilities it already possesses.",2025-05-26,"Rihui Xin, Han Liu, Zecheng Wang, Yupeng Zhang, Dianbo Sui, Xiaolin Hu, Bingning Wang",http://arxiv.org/pdf/2505.19439v1,cs.CL
Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents,"Large Language Models (LLMs) falter in multi-step interactions -- often
hallucinating, repeating actions, or misinterpreting user corrections -- due to
reliance on linear, unstructured context. This fragility stems from the lack of
persistent memory to track evolving goals and task dependencies, undermining
trust in autonomous agents. We introduce the Task Memory Engine (TME), a
modular memory controller that transforms existing LLMs into robust,
revision-aware agents without fine-tuning. TME implements a spatial memory
framework that replaces flat context with graph-based structures to support
consistent, multi-turn reasoning. Departing from linear concatenation and
ReAct-style prompting, TME builds a dynamic task graph -- either a tree or
directed acyclic graph (DAG) -- to map user inputs to subtasks, align them with
prior context, and enable dependency-tracked revisions. Its Task Representation
and Intent Management (TRIM) component models task semantics and user intent to
ensure accurate interpretation. Across four multi-turn scenarios-trip planning,
cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%
of hallucinations and misinterpretations in three tasks, and reduces
hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,
outperforming ReAct. TME's modular design supports plug-and-play deployment and
domain-specific customization, adaptable to both personal assistants and
enterprise automation. We release TME's codebase, benchmarks, and components as
open-source resources, enabling researchers to develop reliable LLM agents.
TME's scalable architecture addresses a critical gap in agent performance
across complex, interactive settings.",2025-05-26,Ye Ye,http://arxiv.org/pdf/2505.19436v1,cs.CL
Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection,"The inherent capabilities of a language model (LM) and the reasoning
strategies it employs jointly determine its performance in reasoning tasks.
While test-time scaling is regarded as an effective approach to tackling
complex reasoning tasks, it incurs substantial computational costs and often
leads to ""overthinking"", where models become trapped in ""thought pitfalls"". To
address this challenge, we propose Route-To-Reason (RTR), a novel unified
routing framework that dynamically allocates both LMs and reasoning strategies
according to task difficulty under budget constraints. RTR learns compressed
representations of both expert models and reasoning strategies, enabling their
joint and adaptive selection at inference time. This method is low-cost, highly
flexible, and can be seamlessly extended to arbitrary black-box or white-box
models and strategies, achieving true plug-and-play functionality. Extensive
experiments across seven open source models and four reasoning strategies
demonstrate that RTR achieves an optimal trade-off between accuracy and
computational efficiency among all baselines, achieving higher accuracy than
the best single model while reducing token usage by over 60%.",2025-05-26,"Zhihong Pan, Kai Zhang, Yuze Zhao, Yupeng Han",http://arxiv.org/pdf/2505.19435v1,cs.CL
Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation,"Counterfactual reasoning typically involves considering alternatives to
actual events. While often applied to understand past events, a distinct
form-forward counterfactual reasoning-focuses on anticipating plausible future
developments. This type of reasoning is invaluable in dynamic financial
markets, where anticipating market developments can powerfully unveil potential
risks and opportunities for stakeholders, guiding their decision-making.
However, performing this at scale is challenging due to the cognitive demands
involved, underscoring the need for automated solutions. Large Language Models
(LLMs) offer promise, but remain unexplored for this application. To address
this gap, we introduce a novel benchmark, Fin-Force-FINancial FORward
Counterfactual Evaluation. By curating financial news headlines and providing
structured evaluation, Fin-Force supports LLM based forward counterfactual
generation. This paves the way for scalable and automated solutions for
exploring and anticipating future market developments, thereby providing
structured insights for decision-making. Through experiments on Fin-Force, we
evaluate state-of-the-art LLMs and counterfactual generation methods, analyzing
their limitations and proposing insights for future research.",2025-05-26,"Keane Ong, Rui Mao, Deeksha Varshney, Paul Pu Liang, Erik Cambria, Gianmarco Mengaldo",http://arxiv.org/pdf/2505.19430v1,cs.CL
Rhapsody: A Dataset for Highlight Detection in Podcasts,"Podcasts have become daily companions for half a billion users. Given the
enormous amount of podcast content available, highlights provide a valuable
signal that helps viewers get the gist of an episode and decide if they want to
invest in listening to it in its entirety. However, identifying highlights
automatically is challenging due to the unstructured and long-form nature of
the content. We introduce Rhapsody, a dataset of 13K podcast episodes paired
with segment-level highlight scores derived from YouTube's 'most replayed'
feature. We frame the podcast highlight detection as a segment-level binary
classification task. We explore various baseline approaches, including
zero-shot prompting of language models and lightweight finetuned language
models using segment-level classification heads. Our experimental results
indicate that even state-of-the-art language models like GPT-4o and Gemini
struggle with this task, while models finetuned with in-domain data
significantly outperform their zero-shot performance. The finetuned model
benefits from leveraging both speech signal features and transcripts. These
findings highlight the challenges for fine-grained information access in
long-form spoken media.",2025-05-26,"Younghan Park, Anuj Diwan, David Harwath, Eunsol Choi",http://arxiv.org/pdf/2505.19429v1,cs.CL
Frictional Agent Alignment Framework: Slow Down and Don't Break Things,"AI support of collaborative interactions entails mediating potential
misalignment between interlocutor beliefs. Common preference alignment methods
like DPO excel in static settings, but struggle in dynamic collaborative tasks
where the explicit signals of interlocutor beliefs are sparse and skewed. We
propose the Frictional Agent Alignment Framework (FAAF), to generate precise,
context-aware ""friction"" that prompts for deliberation and re-examination of
existing evidence. FAAF's two-player objective decouples from data skew: a
frictive-state policy identifies belief misalignments, while an intervention
policy crafts collaborator-preferred responses. We derive an analytical
solution to this objective, enabling training a single policy via a simple
supervised loss. Experiments on three benchmarks show FAAF outperforms
competitors in producing concise, interpretable friction and in OOD
generalization. By aligning LLMs to act as adaptive ""thought partners"" -- not
passive responders -- FAAF advances scalable, dynamic human-AI collaboration.
Our code and data can be found at https://github.com/csu-signal/FAAF_ACL.",2025-05-26,"Abhijnan Nath, Carine Graff, Andrei Bachinin, Nikhil Krishnaswamy",http://arxiv.org/pdf/2505.19428v1,cs.CL
The Role of Diversity in In-Context Learning for Large Language Models,"In-context learning (ICL) is a crucial capability of current large language
models (LLMs), where the selection of examples plays a key role in performance.
While most existing approaches focus on selecting the most similar examples to
the query, the impact of diversity in example selection remains underexplored.
We systematically investigate the role of diversity in in-context example
selection through experiments across a range of tasks, from sentiment
classification to more challenging math and code problems. Experiments on
Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that
diversity-aware selection methods improve performance, particularly on complex
tasks like math and code, and enhance robustness to out-of-distribution
queries. To support these findings, we introduce a theoretical framework that
explains the benefits of incorporating diversity in in-context example
selection.",2025-05-26,"Wenyang Xiao, Haoyu Zhao, Lingxiao Huang",http://arxiv.org/pdf/2505.19426v1,cs.CL
Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering,"Recently, large language models (LLMs) have demonstrated remarkable
capabilities in natural language processing tasks, yet they remain prone to
hallucinations when reasoning with insufficient internal knowledge. While
integrating LLMs with knowledge graphs (KGs) provides access to structured,
verifiable information, existing approaches often generate incomplete or
factually inconsistent reasoning paths. To this end, we propose Self-Reflective
Planning (SRP), a framework that synergizes LLMs with KGs through iterative,
reference-guided reasoning. Specifically, given a question and topic entities,
SRP first searches for references to guide planning and reflection. In the
planning process, it checks initial relations and generates a reasoning path.
After retrieving knowledge from KGs through a reasoning path, it implements
iterative reflection by judging the retrieval result and editing the reasoning
path until the answer is correctly retrieved. Extensive experiments on three
public datasets demonstrate that SRP surpasses various strong baselines and
further underscore its reliable reasoning ability.",2025-05-26,"Jiajun Zhu, Ye Liu, Meikai Bao, Kai Zhang, Yanghai Zhang, Qi Liu",http://arxiv.org/pdf/2505.19410v1,cs.CL
CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems,"As large language models (LLMs) evolve into autonomous agents capable of
collaborative reasoning and task execution, multi-agent LLM systems have
emerged as a powerful paradigm for solving complex problems. However, these
systems pose new challenges for copyright protection, particularly when
sensitive or copyrighted content is inadvertently recalled through inter-agent
communication and reasoning. Existing protection techniques primarily focus on
detecting content in final outputs, overlooking the richer, more revealing
reasoning processes within the agents themselves. In this paper, we introduce
CoTGuard, a novel framework for copyright protection that leverages
trigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically,
we can activate specific CoT segments and monitor intermediate reasoning steps
for unauthorized content reproduction by embedding specific trigger queries
into agent prompts. This approach enables fine-grained, interpretable detection
of copyright violations in collaborative agent scenarios. We evaluate CoTGuard
on various benchmarks in extensive experiments and show that it effectively
uncovers content leakage with minimal interference to task performance. Our
findings suggest that reasoning-level monitoring offers a promising direction
for safeguarding intellectual property in LLM-based agent systems.",2025-05-26,"Yan Wen, Junfeng Guo, Heng Huang",http://arxiv.org/pdf/2505.19405v1,cs.CL
Simple and Effective Baselines for Code Summarisation Evaluation,"Code documentation is useful, but writing it is time-consuming. Different
techniques for generating code summaries have emerged, but comparing them is
difficult because human evaluation is expensive and automatic metrics are
unreliable. In this paper, we introduce a simple new baseline in which we ask
an LLM to give an overall score to a summary. Unlike n-gram and embedding-based
baselines, our approach is able to consider the code when giving a score. This
allows us to also make a variant that does not consider the reference summary
at all, which could be used for other tasks, e.g., to evaluate the quality of
documentation in code bases. We find that our method is as good or better than
prior metrics, though we recommend using it in conjunction with embedding-based
methods to avoid the risk of LLM-specific bias.",2025-05-26,"Jade Robinson, Jonathan K. Kummerfeld",http://arxiv.org/pdf/2505.19392v1,cs.CL
gec-metrics: A Unified Library for Grammatical Error Correction Evaluation,"We introduce gec-metrics, a library for using and developing grammatical
error correction (GEC) evaluation metrics through a unified interface. Our
library enables fair system comparisons by ensuring that everyone conducts
evaluations using a consistent implementation. Moreover, it is designed with a
strong focus on API usage, making it highly extensible. It also includes
meta-evaluation functionalities and provides analysis and visualization
scripts, contributing to developing GEC evaluation metrics. Our code is
released under the MIT license and is also distributed as an installable
package. The video is available on YouTube.",2025-05-26,"Takumi Goto, Yusuke Sakai, Taro Watanabe",http://arxiv.org/pdf/2505.19388v1,cs.CL
GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor,"We present the gradual style adaptor TTS (GSA-TTS) with a novel style encoder
that gradually encodes speaking styles from an acoustic reference for zero-shot
speech synthesis. GSA first captures the local style of each semantic sound
unit. Then the local styles are combined by self-attention to obtain a global
style condition. This semantic and hierarchical encoding strategy provides a
robust and rich style representation for an acoustic model. We test GSA-TTS on
unseen speakers and obtain promising results regarding naturalness, speaker
similarity, and intelligibility. Additionally, we explore the potential of GSA
in terms of interpretability and controllability, which stems from its
hierarchical structure.",2025-05-26,"Seokgi Lee, Jungjun Kim",http://arxiv.org/pdf/2505.19384v1,cs.CL
"Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality","A key feature of human theory-of-mind is the ability to attribute beliefs to
other agents as mentalistic explanations for their behavior. But given the wide
variety of beliefs that agents may hold about the world and the rich language
we can use to express them, which specific beliefs are people inclined to
attribute to others? In this paper, we investigate the hypothesis that people
prefer to attribute beliefs that are good explanations for the behavior they
observe. We develop a computational model that quantifies the explanatory
strength of a (natural language) statement about an agent's beliefs via three
factors: accuracy, informativity, and causal relevance to actions, each of
which can be computed from a probabilistic generative model of belief-driven
behavior. Using this model, we study the role of each factor in how people
selectively attribute beliefs to other agents. We investigate this via an
experiment where participants watch an agent collect keys hidden in boxes in
order to reach a goal, then rank a set of statements describing the agent's
beliefs about the boxes' contents. We find that accuracy and informativity
perform reasonably well at predicting these rankings when combined, but that
causal relevance is the single factor that best explains participants'
responses.",2025-05-26,"Lance Ying, Almog Hillel, Ryan Truong, Vikash K. Mansinghka, Joshua B. Tenenbaum, Tan Zhi-Xuan",http://arxiv.org/pdf/2505.19376v1,cs.CL
ChartLens: Fine-grained Visual Attribution in Charts,"The growing capabilities of multimodal large language models (MLLMs) have
advanced tasks like chart understanding. However, these models often suffer
from hallucinations, where generated text sequences conflict with the provided
visual data. To address this, we introduce Post-Hoc Visual Attribution for
Charts, which identifies fine-grained chart elements that validate a given
chart-associated response. We propose ChartLens, a novel chart attribution
algorithm that uses segmentation-based techniques to identify chart objects and
employs set-of-marks prompting with MLLMs for fine-grained visual attribution.
Additionally, we present ChartVA-Eval, a benchmark with synthetic and
real-world charts from diverse domains like finance, policy, and economics,
featuring fine-grained attribution annotations. Our evaluations show that
ChartLens improves fine-grained attributions by 26-66%.",2025-05-25,"Manan Suri, Puneet Mathur, Nedim Lipka, Franck Dernoncourt, Ryan A. Rossi, Dinesh Manocha",http://arxiv.org/pdf/2505.19360v1,cs.CL
Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval,"Neural retrieval methods using transformer-based pre-trained language models
have advanced multilingual and cross-lingual retrieval. However, their
effectiveness for low-resource, morphologically rich languages such as Amharic
remains underexplored due to data scarcity and suboptimal tokenization. We
address this gap by introducing Amharic-specific dense retrieval models based
on pre-trained Amharic BERT and RoBERTa backbones. Our proposed
RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative
improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest
multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact
variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while
being over 13x smaller. Additionally, we train a ColBERT-based late interaction
retrieval model that achieves the highest MRR@10 score (0.843) among all
evaluated models. We benchmark our proposed models against both sparse and
dense retrieval baselines to systematically assess retrieval effectiveness in
Amharic. Our analysis highlights key challenges in low-resource settings and
underscores the importance of language-specific adaptation. To foster future
research in low-resource IR, we publicly release our dataset, codebase, and
trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.",2025-05-25,"Kidist Amde Mekonnen, Yosef Worku Alemneh, Maarten de Rijke",http://arxiv.org/pdf/2505.19356v1,cs.CL
Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement,"Understanding true influence in social media requires distinguishing
correlation from causation--particularly when analyzing misinformation spread.
While existing approaches focus on exposure metrics and network structures,
they often fail to capture the causal mechanisms by which external temporal
signals trigger engagement. We introduce a novel joint treatment-outcome
framework that leverages existing sequential models to simultaneously adapt to
both policy timing and engagement effects. Our approach adapts causal inference
techniques from healthcare to estimate Average Treatment Effects (ATE) within
the sequential nature of social media interactions, tackling challenges from
external confounding signals. Through our experiments on real-world
misinformation and disinformation datasets, we show that our models outperform
existing benchmarks by 15--22% in predicting engagement across diverse
counterfactual scenarios, including exposure adjustment, timing shifts, and
varied intervention durations. Case studies on 492 social media users show our
causal effect measure aligns strongly with the gold standard in influence
estimation, the expert-based empirical influence.",2025-05-25,"Lin Tian, Marian-Andrei Rizoiu",http://arxiv.org/pdf/2505.19355v1,cs.CL
GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance,"Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks
that demand reasoning with information extending beyond the explicit content
depicted in the image. Early methods relied on explicit knowledge bases to
provide this auxiliary information. Recent approaches leverage Large Language
Models (LLMs) as implicit knowledge sources. While KB-VQA methods have
demonstrated promising results, their potential remains constrained as the
auxiliary text provided may not be relevant to the question context, and may
also include irrelevant information that could misguide the answer predictor.
We introduce a novel four-stage framework called Grounding Caption-Guided
Knowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to
effectively perform zero-shot VQA tasks without the need for end-to-end
multimodal training. Innovations include grounding question-aware caption
generation to move beyond generic descriptions and have compact, yet detailed
and context-rich information. This is combined with knowledge from external
sources to create highly informative prompts for the LLM. GC-KBVQA can address
a variety of VQA tasks, and does not require task-specific fine-tuning, thus
reducing both costs and deployment complexity by leveraging general-purpose,
pre-trained LLMs. Comparison with competing KB-VQA methods shows significantly
improved performance. Our code will be made public.",2025-05-25,"Mohammad Mahdi Moradi, Sudhir Mudur",http://arxiv.org/pdf/2505.19354v1,cs.CL
Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation,"With the rise of generative AI (GenAI), Large Language Models are
increasingly employed for code generation, becoming active co-authors alongside
human programmers. Focusing specifically on this application domain, this paper
articulates distinct ``Architectures of Error'' to ground an epistemic
distinction between human and machine code generation. Examined through their
shared vulnerability to error, this distinction reveals fundamentally different
causal origins: human-cognitive versus artificial-stochastic. To develop this
framework and substantiate the distinction, the analysis draws critically upon
Dennett's mechanistic functionalism and Rescher's methodological pragmatism. I
argue that a systematic differentiation of these error profiles raises critical
philosophical questions concerning semantic coherence, security robustness,
epistemic limits, and control mechanisms in human-AI collaborative software
development. The paper also utilizes Floridi's levels of abstraction to provide
a nuanced understanding of how these error dimensions interact and may evolve
with technological advancements. This analysis aims to offer philosophers a
structured framework for understanding GenAI's unique epistemological
challenges, shaped by these architectural foundations, while also providing
software engineers a basis for more critically informed engagement.",2025-05-25,Camilo Chacón Sartori,http://arxiv.org/pdf/2505.19353v1,cs.CL
PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims,"Natural language generation (NLG) metrics play a central role in evaluating
generated texts, but are not well suited for the structural and legal
characteristics of patent documents. Large language models (LLMs) offer strong
potential in automating patent generation, yet research on evaluating
LLM-generated patents remains limited, especially in evaluating the generation
quality of patent claims, which are central to defining the scope of
protection. Effective claim evaluation requires addressing legal validity,
technical accuracy, and structural compliance. To address this gap, we
introduce PatentScore, a multi-dimensional evaluation framework for assessing
LLM-generated patent claims. PatentScore incorporates: (1) hierarchical
decomposition for claim analysis; (2) domain-specific validation patterns based
on legal and technical standards; and (3) scoring across structural, semantic,
and legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects
patent-specific constraints and document structures, enabling evaluation beyond
surface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a
Pearson correlation of $r = 0.819$ with expert annotations, outperforming
existing NLG metrics. Furthermore, we conduct additional evaluations using open
models such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong
correlations with expert judgments, confirming the robustness and
generalizability of our framework.",2025-05-25,"Yongmin Yoo, Qiongkai Xu, Longbing Cao",http://arxiv.org/pdf/2505.19345v1,cs.CL
ODIN: A NL2SQL Recommender to Handle Schema Ambiguity,"NL2SQL (natural language to SQL) systems translate natural language into SQL
queries, allowing users with no technical background to interact with databases
and create tools like reports or visualizations. While recent advancements in
large language models (LLMs) have significantly improved NL2SQL accuracy,
schema ambiguity remains a major challenge in enterprise environments with
complex schemas, where multiple tables and columns with semantically similar
names often co-exist. To address schema ambiguity, we introduce ODIN, a NL2SQL
recommendation engine. Instead of producing a single SQL query given a natural
language question, ODIN generates a set of potential SQL queries by accounting
for different interpretations of ambiguous schema components. ODIN dynamically
adjusts the number of suggestions based on the level of ambiguity, and ODIN
learns from user feedback to personalize future SQL query recommendations. Our
evaluation shows that ODIN improves the likelihood of generating the correct
SQL query by 1.5-2$\times$ compared to baselines.",2025-05-25,"Kapil Vaidya, Abishek Sankararaman, Jialin Ding, Chuan Lei, Xiao Qin, Balakrishnan Narayanaswamy, Tim Kraska",http://arxiv.org/pdf/2505.19302v1,cs.CL
SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking,"Recent advances in large language models (LLMs) demonstrate their impressive
reasoning capabilities. However, the reasoning confined to internal parametric
space limits LLMs' access to real-time information and understanding of the
physical world. To overcome this constraint, we introduce SituatedThinker, a
novel framework that enables LLMs to ground their reasoning in real-world
contexts through situated thinking, which adaptively combines both internal
knowledge and external information with predefined interfaces. By utilizing
reinforcement learning, SituatedThinker incentivizes deliberate reasoning with
the real world to acquire information and feedback, allowing LLMs to surpass
their knowledge boundaries and enhance reasoning. Experimental results
demonstrate significant performance improvements on multi-hop
question-answering and mathematical reasoning benchmarks. Furthermore,
SituatedThinker demonstrates strong performance on unseen tasks, such as KBQA,
TableQA, and text-based games, showcasing the generalizable real-world grounded
reasoning capability. Our codes are available at
https://github.com/jnanliu/SituatedThinker.",2025-05-25,"Junnan Liu, Linhao Luo, Thuy-Trang Vu, Gholamreza Haffari",http://arxiv.org/pdf/2505.19300v1,cs.CL
A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations,"Faithful free-text explanations are important to ensure transparency in
high-stakes AI decision-making contexts, but they are challenging to generate
by language models and assess by humans. In this paper, we present a measure
for Prediction-EXplanation (PEX) consistency, by extending the concept of
weight of evidence. This measure quantifies how much a free-text explanation
supports or opposes a prediction, serving as an important aspect of explanation
faithfulness. Our analysis reveals that more than 62% explanations generated by
large language models lack this consistency. We show that applying direct
preference optimization improves the consistency of generated explanations
across three model families, with improvement ranging from 43.1% to 292.3%.
Furthermore, we demonstrate that optimizing this consistency measure can
improve explanation faithfulness by up to 9.7%.",2025-05-25,"Lingjun Zhao, Hal Daumé III",http://arxiv.org/pdf/2505.19299v1,cs.CL
Towards Reliable Large Audio Language Model,"Recent advancements in large audio language models (LALMs) have demonstrated
impressive results and promising prospects in universal understanding and
reasoning across speech, music, and general sound. However, these models still
lack the ability to recognize their knowledge boundaries and refuse to answer
questions they don't know proactively. While there have been successful
attempts to enhance the reliability of LLMs, reliable LALMs remain largely
unexplored. In this paper, we systematically investigate various approaches
towards reliable LALMs, including training-free methods such as multi-modal
chain-of-thought (MCoT), and training-based methods such as supervised
fine-tuning (SFT). Besides, we identify the limitations of previous evaluation
metrics and propose a new metric, the Reliability Gain Index (RGI), to assess
the effectiveness of different reliable methods. Our findings suggest that both
training-free and training-based methods enhance the reliability of LALMs to
different extents. Moreover, we find that awareness of reliability is a ""meta
ability"", which can be transferred across different audio modalities, although
significant structural and content differences exist among sound, music, and
speech.",2025-05-25,"Ziyang Ma, Xiquan Li, Yakun Song, Wenxi Chen, Chenpeng Du, Jian Wu, Yuanzhe Chen, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen",http://arxiv.org/pdf/2505.19294v1,cs.CL
100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?,"Long-context capability is considered one of the most important abilities of
LLMs, as a truly long context-capable LLM enables users to effortlessly process
many originally exhausting tasks -- e.g., digesting a long-form document to
find answers vs. directly asking an LLM about it. However, existing
real-task-based long-context evaluation benchmarks have two major shortcomings.
First, benchmarks like LongBench often do not provide proper metrics to
separate long-context performance from the model's baseline ability, making
cross-model comparison unclear. Second, such benchmarks are usually constructed
with fixed input lengths, which limits their applicability across different
models and fails to reveal when a model begins to break down. To address these
issues, we introduce a length-controllable long-context benchmark and a novel
metric that disentangles baseline knowledge from true long-context
capabilities. Experiments demonstrate the superiority of our approach in
effectively evaluating LLMs.",2025-05-25,"Wang Yang, Hongye Jin, Shaochen Zhong, Song Jiang, Qifan Wang, Vipin Chaudhary, Xiaotian Han",http://arxiv.org/pdf/2505.19293v1,cs.CL
A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models,"Large language models have been extensively studied as neural knowledge bases
for their knowledge access, editability, reasoning, and explainability.
However, few works focus on the structural patterns of their knowledge.
Motivated by this gap, we investigate these structural patterns from a graph
perspective. We quantify the knowledge of LLMs at both the triplet and entity
levels, and analyze how it relates to graph structural properties such as node
degree. Furthermore, we uncover the knowledge homophily, where topologically
close entities exhibit similar levels of knowledgeability, which further
motivates us to develop graph machine learning models to estimate entity
knowledge based on its local neighbors. This model further enables valuable
knowledge checking by selecting triplets less known to LLMs. Empirical results
show that using selected triplets for fine-tuning leads to superior
performance.",2025-05-25,"Utkarsh Sahu, Zhisheng Qi, Yongjia Lei, Ryan A. Rossi, Franck Dernoncourt, Nesreen K. Ahmed, Mahantesh M Halappanavar, Yao Ma, Yu Wang",http://arxiv.org/pdf/2505.19286v1,cs.CL
Next Token Prediction Is a Dead End for Creativity,"This paper argues that token prediction is fundamentally misaligned with real
creativity. While next-token models have enabled impressive advances in
language generation, their architecture favours surface-level coherence over
spontaneity, originality, and improvisational risk. We use battle rap as a case
study to expose the limitations of predictive systems, demonstrating that they
cannot truly engage in adversarial or emotionally resonant exchanges. By
reframing creativity as an interactive process rather than a predictive output,
we offer a vision for AI systems that are more expressive, responsive, and
aligned with human creative practice.",2025-05-25,"Ibukun Olatunji, Mark Sheppard",http://arxiv.org/pdf/2505.19277v1,cs.CL
Unveiling Dual Quality in Product Reviews: An NLP-Based Approach,"Consumers often face inconsistent product quality, particularly when
identical products vary between markets, a situation known as the dual quality
problem. To identify and address this issue, automated techniques are needed.
This paper explores how natural language processing (NLP) can aid in detecting
such discrepancies and presents the full process of developing a solution.
First, we describe in detail the creation of a new Polish-language dataset with
1,957 reviews, 540 highlighting dual quality issues. We then discuss
experiments with various approaches like SetFit with sentence-transformers,
transformer-based encoders, and LLMs, including error analysis and robustness
verification. Additionally, we evaluate multilingual transfer using a subset of
opinions in English, French, and German. The paper concludes with insights on
deployment and practical applications.",2025-05-25,"Rafał Poświata, Marcin Michał Mirończuk, Sławomir Dadas, Małgorzata Grębowiec, Michał Perełkiewicz",http://arxiv.org/pdf/2505.19254v1,cs.CL
PATS: Process-Level Adaptive Thinking Mode Switching,"Current large-language models (LLMs) typically adopt a fixed reasoning
strategy, either simple or complex, for all questions, regardless of their
difficulty. This neglect of variation in task and reasoning process complexity
leads to an imbalance between performance and efficiency. Existing methods
attempt to implement training-free fast-slow thinking system switching to
handle problems of varying difficulty, but are limited by coarse-grained
solution-level strategy adjustments. To address this issue, we propose a novel
reasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS),
which enables LLMs to dynamically adjust their reasoning strategy based on the
difficulty of each step, optimizing the balance between accuracy and
computational efficiency. Our approach integrates Process Reward Models (PRMs)
with Beam Search, incorporating progressive mode switching and bad-step penalty
mechanisms. Experiments on diverse mathematical benchmarks demonstrate that our
methodology achieves high accuracy while maintaining moderate token usage. This
study emphasizes the significance of process-level, difficulty-aware reasoning
strategy adaptation, offering valuable insights into efficient inference for
LLMs.",2025-05-25,"Yi Wang, Junxiao Liu, Shimao Zhang, Jiajun Chen, Shujian Huang",http://arxiv.org/pdf/2505.19250v1,cs.CL
LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models,"Large language model (LLM) research has grown rapidly, along with increasing
concern about their limitations such as failures in reasoning, hallucinations,
and limited multilingual capability. In this survey, we conduct a data-driven,
semi-automated review of research on limitations of LLM (LLLMs) from 2022 to
2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,
we identify 14,648 relevant papers using keyword filtering, LLM-based
classification, validated against expert labels, and topic clustering (via two
approaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research
increases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs
research grows even faster, reaching over 30% of LLM papers by late 2024.
Reasoning remains the most studied limitation, followed by generalization,
hallucination, bias, and security. The distribution of topics in the ACL
dataset stays relatively stable over time, while arXiv shifts toward safety and
controllability (with topics like security risks, alignment, hallucinations,
knowledge editing), and multimodality between 2022 and 2024. We release a
dataset of annotated abstracts and a validated methodology, and offer a
quantitative view of trends in LLM limitations research.",2025-05-25,"Aida Kostikova, Zhipin Wang, Deidamea Bajri, Ole Pütz, Benjamin Paaßen, Steffen Eger",http://arxiv.org/pdf/2505.19240v1,cs.CL
Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator,"Creativity evaluation remains a challenging frontier for large language
models (LLMs). Current evaluations heavily rely on inefficient and costly human
judgments, hindering progress in enhancing machine creativity. While automated
methods exist, ranging from psychological testing to heuristic- or
prompting-based approaches, they often lack generalizability or alignment with
human judgment. To address these issues, in this paper, we propose a novel
pairwise-comparison framework for assessing textual creativity, leveraging
shared contextual instructions to improve evaluation consistency. We introduce
CreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic
creative instruction-response pairs spanning diverse open-domain tasks. Through
training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval
demonstrates remarkable superiority over existing methods in alignment with
human judgments. Experimental results underscore the indispensable significance
of integrating both human-generated and synthetic data in training highly
robust evaluators, and showcase the practical utility of CrEval in boosting the
creativity of LLMs. We will release all data, code, and models publicly soon to
support further research.",2025-05-25,"Qian Cao, Xiting Wang, Yuzhuo Yuan, Yahui Liu, Fang Luo, Ruihua Song",http://arxiv.org/pdf/2505.19236v1,cs.CL
GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling,"The emergence of large language models (LLMs) enables the development of
intelligent agents capable of engaging in complex and multi-turn dialogues.
However, multi-agent collaboration face critical safety challenges, such as
hallucination amplification and error injection and propagation. This paper
presents GUARDIAN, a unified method for detecting and mitigating multiple
safety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the
multi-agent collaboration process as a discrete-time temporal attributed graph,
GUARDIAN explicitly captures the propagation dynamics of hallucinations and
errors. The unsupervised encoder-decoder architecture incorporating an
incremental training paradigm, learns to reconstruct node attributes and graph
structures from latent embeddings, enabling the identification of anomalous
nodes and edges with unparalleled precision. Moreover, we introduce a graph
abstraction mechanism based on the Information Bottleneck Theory, which
compresses temporal interaction graphs while preserving essential patterns.
Extensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM
multi-agent collaborations against diverse safety vulnerabilities, achieving
state-of-the-art accuracy with efficient resource utilization.",2025-05-25,"Jialong Zhou, Lichao Wang, Xiao Yang",http://arxiv.org/pdf/2505.19234v1,cs.CL
The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training,"Recent large language models (LLMs) exhibit impressive reasoning but often
over-think, generating excessively long responses that hinder efficiency. We
introduce DIET ( DIfficulty-AwarE Training), a framework that systematically
cuts these ""token calories"" by integrating on-the-fly problem difficulty into
the reinforcement learning (RL) process. DIET dynamically adapts token
compression strategies by modulating token penalty strength and conditioning
target lengths on estimated task difficulty, to optimize the
performance-efficiency trade-off. We also theoretically analyze the pitfalls of
naive reward weighting in group-normalized RL algorithms like GRPO, and propose
Advantage Weighting technique, which enables stable and effective
implementation of these difficulty-aware objectives. Experimental results
demonstrate that DIET significantly reduces token counts while simultaneously
improving reasoning performance. Beyond raw token reduction, we show two
crucial benefits largely overlooked by prior work: (1) DIET leads to superior
inference scaling. By maintaining high per-sample quality with fewer tokens, it
enables better scaling performance via majority voting with more samples under
fixed computational budgets, an area where other methods falter. (2) DIET
enhances the natural positive correlation between response length and problem
difficulty, ensuring verbosity is appropriately allocated, unlike many existing
compression methods that disrupt this relationship. Our analyses provide a
principled and effective framework for developing more efficient, practical,
and high-performing LLMs.",2025-05-25,"Weize Chen, Jiarui Yuan, Tailin Jin, Ning Ding, Huimin Chen, Zhiyuan Liu, Maosong Sun",http://arxiv.org/pdf/2505.19217v1,cs.CL
When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas,"Recent advances in large language models (LLMs) have enabled their use in
complex agentic roles, involving decision-making with humans or other agents,
making ethical alignment a key AI safety concern. While prior work has examined
both LLMs' moral judgment and strategic behavior in social dilemmas, there is
limited understanding of how they act when moral imperatives directly conflict
with rewards or incentives. To investigate this, we introduce Moral Behavior in
Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the
prisoner's dilemma and public goods game with morally charged contexts. In
MoralSim, we test a range of frontier models across both game structures and
three distinct moral framings, enabling a systematic examination of how LLMs
navigate social dilemmas in which ethical norms conflict with payoff-maximizing
strategies. Our results show substantial variation across models in both their
general tendency to act morally and the consistency of their behavior across
game types, the specific moral framing, and situational factors such as
opponent behavior and survival risks. Crucially, no model exhibits consistently
moral behavior in MoralSim, highlighting the need for caution when deploying
LLMs in agentic roles where the agent's ""self-interest"" may conflict with
ethical expectations. Our code is available at
https://github.com/sbackmann/moralsim.",2025-05-25,"Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin",http://arxiv.org/pdf/2505.19212v1,cs.CL
MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search,"Large language models (LLMs) have shown promise in automating scientific
hypothesis generation, yet existing approaches primarily yield coarse-grained
hypotheses lacking critical methodological and experimental details. We
introduce and formally define the novel task of fine-grained scientific
hypothesis discovery, which entails generating detailed, experimentally
actionable hypotheses from coarse initial research directions. We frame this as
a combinatorial optimization problem and investigate the upper limits of LLMs'
capacity to solve it when maximally leveraged. Specifically, we explore four
foundational questions: (1) how to best harness an LLM's internal heuristics to
formulate the fine-grained hypothesis it itself would judge as the most
promising among all the possible hypotheses it might generate, based on its own
internal scoring-thus defining a latent reward landscape over the hypothesis
space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment
with ground-truth hypotheses; (3) whether shaping the reward landscape using an
ensemble of diverse LLMs of similar capacity yields better outcomes than
defining it with repeated instances of the strongest LLM among them; and (4)
whether an ensemble of identical LLMs provides a more reliable reward landscape
than a single LLM. To address these questions, we propose a hierarchical search
method that incrementally proposes and integrates details into the hypothesis,
progressing from general concepts to specific experimental configurations. We
show that this hierarchical process smooths the reward landscape and enables
more effective optimization. Empirical evaluations on a new benchmark of
expert-annotated fine-grained hypotheses from recent chemistry literature show
that our method consistently outperforms strong baselines.",2025-05-25,"Zonglin Yang, Wanhao Liu, Ben Gao, Yujie Liu, Wei Li, Tong Xie, Lidong Bing, Wanli Ouyang, Erik Cambria, Dongzhan Zhou",http://arxiv.org/pdf/2505.19209v1,cs.CL
SpeakStream: Streaming Text-to-Speech with Interleaved Data,"The latency bottleneck of traditional text-to-speech (TTS) systems
fundamentally hinders the potential of streaming large language models (LLMs)
in conversational AI. These TTS systems, typically trained and inferenced on
complete utterances, introduce unacceptable delays, even with optimized
inference speeds, when coupled with streaming LLM outputs. This is particularly
problematic for creating responsive conversational agents where low first-token
latency is critical. In this paper, we present SpeakStream, a streaming TTS
system that generates audio incrementally from streaming text using a
decoder-only architecture. SpeakStream is trained using a next-step prediction
loss on interleaved text-speech data. During inference, it generates speech
incrementally while absorbing streaming input text, making it particularly
suitable for cascaded conversational AI agents where an LLM streams text to a
TTS system. Our experiments demonstrate that SpeakStream achieves
state-of-the-art latency results in terms of first-token latency while
maintaining the quality of non-streaming TTS systems.",2025-05-25,"Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly",http://arxiv.org/pdf/2505.19206v1,cs.CL
DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding,"Speculative decoding (SD) has emerged as a powerful method for accelerating
autoregressive generation in large language models (LLMs), yet its integration
into vision-language models (VLMs) remains underexplored. We introduce DREAM, a
novel speculative decoding framework tailored for VLMs that combines three key
innovations: (1) a cross-attention-based mechanism to inject intermediate
features from the target model into the draft model for improved alignment, (2)
adaptive intermediate feature selection based on attention entropy to guide
efficient draft model training, and (3) visual token compression to reduce
draft model latency. DREAM enables efficient, accurate, and parallel multimodal
decoding with significant throughput improvement. Experiments across a diverse
set of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3,
demonstrate up to 3.6x speedup over conventional decoding and significantly
outperform prior SD baselines in both inference throughput and speculative
draft acceptance length across a broad range of multimodal benchmarks. The code
is publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git",2025-05-25,"Yunhai Hu, Tianhua Xia, Zining Liu, Rahul Raman, Xingyu Liu, Bo Bao, Eric Sather, Vithursan Thangarasa, Sai Qian Zhang",http://arxiv.org/pdf/2505.19201v1,cs.CL
Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection,"Inconsistent political statements represent a form of misinformation. They
erode public trust and pose challenges to accountability, when left unnoticed.
Detecting inconsistencies automatically could support journalists in asking
clarification questions, thereby helping to keep politicians accountable. We
propose the Inconsistency detection task and develop a scale of inconsistency
types to prompt NLP-research in this direction. To provide a resource for
detecting inconsistencies in a political domain, we present a dataset of 698
human-annotated pairs of political statements with explanations of the
annotators' reasoning for 237 samples. The statements mainly come from voting
assistant platforms such as Wahl-O-Mat in Germany and Smartvote in Switzerland,
reflecting real-world political issues. We benchmark Large Language Models
(LLMs) on our dataset and show that in general, they are as good as humans at
detecting inconsistencies, and might be even better than individual humans at
predicting the crowd-annotated ground-truth. However, when it comes to
identifying fine-grained inconsistency types, none of the model have reached
the upper bound of performance (due to natural labeling variation), thus
leaving room for improvement. We make our dataset and code publicly available.",2025-05-25,"Nursulu Sagimbayeva, Ruveyda Betül Bahçeci, Ingmar Weber",http://arxiv.org/pdf/2505.19191v1,cs.CL
LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling,"Large language models (LLMs) have demonstrated remarkable reasoning
capabilities through test-time scaling approaches, particularly when fine-tuned
with chain-of-thought (CoT) data distilled from more powerful large reasoning
models (LRMs). However, these reasoning chains often contain verbose elements
that mirror human problem-solving, categorized as progressive reasoning (the
essential solution development path) and functional elements (verification
processes, alternative solution approaches, and error corrections). While
progressive reasoning is crucial, the functional elements significantly
increase computational demands during test-time inference. We introduce PIR
(Perplexity-based Importance Refinement), a principled framework that
quantitatively evaluates the importance of each reasoning step based on its
impact on answer prediction confidence. PIR systematically identifies and
selectively prunes only low-importance functional steps while preserving
progressive reasoning components, creating optimized training data that
maintains the integrity of the core solution path while reducing verbosity.
Models fine-tuned on PIR-optimized data exhibit superior test-time scaling
properties, generating more concise reasoning chains while achieving improved
accuracy (+0.9\% to +6.6\%) with significantly reduced token usage (-3\% to
-41\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).
Our approach demonstrates strong generalizability across different model sizes,
data sources, and token budgets, offering a practical solution for deploying
reasoning-capable LLMs in scenarios where efficient test-time scaling, response
time, and computational efficiency are valuable constraints.",2025-05-25,"Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, Pengfei Liu",http://arxiv.org/pdf/2505.19187v1,cs.CL
"Two LLMs debate, both are certain they've won","Can LLMs accurately adjust their confidence when facing opposition? Building
on previous studies measuring calibration on static fact-based
question-answering tasks, we evaluate Large Language Models (LLMs) in a
dynamic, adversarial debate setting, uniquely combining two realistic factors:
(a) a multi-turn format requiring models to update beliefs as new information
emerges, and (b) a zero-sum structure to control for task-related uncertainty,
since mutual high-confidence claims imply systematic overconfidence. We
organized 60 three-round policy debates among ten state-of-the-art LLMs, with
models privately rating their confidence (0-100) in winning after each round.
We observed five concerning patterns: (1) Systematic overconfidence: models
began debates with average initial confidence of 72.9% vs. a rational 50%
baseline. (2) Confidence escalation: rather than reducing confidence as debates
progressed, debaters increased their win probabilities, averaging 83% by the
final round. (3) Mutual overestimation: in 61.7% of debates, both sides
simultaneously claimed >=75% probability of victory, a logical impossibility.
(4) Persistent self-debate bias: models debating identical copies increased
confidence from 64.1% to 75.2%; even when explicitly informed their chance of
winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)
Misaligned private reasoning: models' private scratchpad thoughts sometimes
differed from their public confidence ratings, raising concerns about
faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the
ability to accurately self-assess or update their beliefs in dynamic,
multi-turn tasks; a major concern as LLM outputs are deployed without careful
review in assistant roles or agentic settings.",2025-05-25,"Minh Nhat Nguyen, Pradyumna Shyama Prasad",http://arxiv.org/pdf/2505.19184v1,cs.CL
Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge,"LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to
evaluate the quality of LLM-generated responses, gaining popularity for its
cost-effectiveness and strong alignment with human evaluations. However,
training proxy judge models using evaluation data generated by powerful teacher
models introduces a critical yet previously overlooked issue: teacher
preference bias, where the proxy judge model learns a biased preference for
responses from the teacher model. To tackle this problem, we propose a novel
setting that incorporates an additional assistant model, which is not biased
toward the teacher model's responses, to complement the training data. Building
on this setup, we introduce AGDe-Judge, a three-stage framework designed to
debias from both the labels and feedbacks in the training data. Extensive
experiments demonstrate that AGDe-Judge effectively reduces teacher preference
bias while maintaining strong performance across six evaluation benchmarks.
Code is available at https://github.com/Liuz233/AGDe-Judge.",2025-05-25,"Zhuo Liu, Moxin Li, Xun Deng, Qifan Wang, Fuli Feng",http://arxiv.org/pdf/2505.19176v1,cs.CL
SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs,"Large Language Models (LLMs) have demonstrated remarkable performance across
various disciplines and tasks. However, benchmarking their capabilities with
multilingual spoken queries remains largely unexplored. In this study, we
introduce SpokenNativQA, the first multilingual and culturally aligned spoken
question-answering (SQA) dataset designed to evaluate LLMs in real-world
conversational settings. The dataset comprises approximately 33,000 naturally
spoken questions and answers in multiple languages, including low-resource and
dialect-rich languages, providing a robust benchmark for assessing LLM
performance in speech-based interactions. SpokenNativQA addresses the
limitations of text-based QA datasets by incorporating speech variability,
accents, and linguistic diversity. We benchmark different ASR systems and LLMs
for SQA and present our findings. We released the data at
(https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental
scripts at (https://llmebench.qcri.org/) for the research community.",2025-05-25,"Firoj Alam, Md Arid Hasan, Shammur Absar Chowdhury",http://arxiv.org/pdf/2505.19163v1,cs.CL
Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs,"Due to the auto-regressive nature of current video large language models
(Video-LLMs), the inference latency increases as the input sequence length
grows, posing challenges for the efficient processing of video sequences that
are usually very long. We observe that during decoding, the attention scores of
most tokens in Video-LLMs tend to be sparse and concentrated, with only certain
tokens requiring comprehensive full attention. Based on this insight, we
introduce Sparse-to-Dense (StD), a novel decoding strategy that integrates two
distinct modules: one leveraging sparse top-K attention and the other employing
dense full attention. These modules collaborate to accelerate Video-LLMs
without loss. The fast (sparse) model speculatively decodes multiple tokens,
while the slow (dense) model verifies them in parallel. StD is a tuning-free,
plug-and-play solution that achieves up to a 1.94$\times$ walltime speedup in
video processing. It maintains model performance while enabling a seamless
transition from a standard Video-LLM to a sparse Video-LLM with minimal code
modifications.",2025-05-25,"Xuan Zhang, Cunxiao Du, Sicheng Yu, Jiawei Wu, Fengzhuo Zhang, Wei Gao, Qian Liu",http://arxiv.org/pdf/2505.19155v1,cs.CL
Shifting AI Efficiency From Model-Centric to Data-Centric Compression,"The rapid advancement of large language models (LLMs) and multi-modal LLMs
(MLLMs) has historically relied on model-centric scaling through increasing
parameter counts from millions to hundreds of billions to drive performance
gains. However, as we approach hardware limits on model size, the dominant
computational bottleneck has fundamentally shifted to the quadratic cost of
self-attention over long token sequences, now driven by ultra-long text
contexts, high-resolution images, and extended videos. In this position paper,
\textbf{we argue that the focus of research for efficient AI is shifting from
model-centric compression to data-centric compression}. We position token
compression as the new frontier, which improves AI efficiency via reducing the
number of tokens during model training or inference. Through comprehensive
analysis, we first examine recent developments in long-context AI across
various domains and establish a unified mathematical framework for existing
model efficiency strategies, demonstrating why token compression represents a
crucial paradigm shift in addressing long-context overhead. Subsequently, we
systematically review the research landscape of token compression, analyzing
its fundamental benefits and identifying its compelling advantages across
diverse scenarios. Furthermore, we provide an in-depth analysis of current
challenges in token compression research and outline promising future
directions. Ultimately, our work aims to offer a fresh perspective on AI
efficiency, synthesize existing research, and catalyze innovative developments
to address the challenges that increasing context lengths pose to the AI
community's advancement.",2025-05-25,"Xuyang Liu, Zichen Wen, Shaobo Wang, Junjie Chen, Zhishan Tao, Yubo Wang, Xiangqi Jin, Chang Zou, Yiyu Wang, Chenfei Liao, Xu Zheng, Honggang Chen, Weijia Li, Xuming Hu, Conghui He, Linfeng Zhang",http://arxiv.org/pdf/2505.19147v1,cs.CL
RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models,"The rise of large language models has led to significant performance
breakthroughs in named entity recognition (NER) for high-resource languages,
yet there remains substantial room for improvement in low- and medium-resource
languages. Existing multilingual NER methods face severe language interference
during the multi-language adaptation process, manifested in feature conflicts
between different languages and the competitive suppression of low-resource
language features by high-resource languages. Although training a dedicated
model for each language can mitigate such interference, it lacks scalability
and incurs excessive computational costs in real-world applications. To address
this issue, we propose RetrieveAll, a universal multilingual NER framework
based on dynamic LoRA. The framework decouples task-specific features across
languages and demonstrates efficient dynamic adaptability. Furthermore, we
introduce a cross-granularity knowledge augmented method that fully exploits
the intrinsic potential of the data without relying on external resources. By
leveraging a hierarchical prompting mechanism to guide knowledge injection,
this approach advances the paradigm from ""prompt-guided inference"" to
""prompt-driven learning."" Experimental results show that RetrieveAll
outperforms existing baselines; on the PAN-X dataset, it achieves an average F1
improvement of 12.1 percent.",2025-05-25,"Jin Zhang, Fan Gao, Linyu Li, Yongbin Yu, Xiangxiang Wang, Nyima Tashi, Gadeng Luosang",http://arxiv.org/pdf/2505.19128v1,cs.CL
MMATH: A Multilingual Benchmark for Mathematical Reasoning,"The advent of large reasoning models, such as OpenAI o1 and DeepSeek R1, has
significantly advanced complex reasoning tasks. However, their capabilities in
multilingual complex reasoning remain underexplored, with existing efforts
largely focused on simpler tasks like MGSM. To address this gap, we introduce
MMATH, a benchmark for multilingual complex reasoning spanning 374 high-quality
math problems across 10 typologically diverse languages. Using MMATH, we
observe that even advanced models like DeepSeek R1 exhibit substantial
performance disparities across languages and suffer from a critical off-target
issue-generating responses in unintended languages. To address this, we explore
strategies including prompting and training, demonstrating that reasoning in
English and answering in target languages can simultaneously enhance
performance and preserve target-language consistency. Our findings offer new
insights and practical strategies for advancing the multilingual reasoning
capabilities of large language models. Our code and data could be found at
https://github.com/RUCAIBox/MMATH.",2025-05-25,"Wenyang Luo, Wayne Xin Zhao, Jing Sha, Shijin Wang, Ji-Rong Wen",http://arxiv.org/pdf/2505.19126v1,cs.CL
Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models,"Despite the recent strides in large language models, studies have underscored
the existence of social biases within these systems. In this paper, we delve
into the validation and comparison of the ethical biases of LLMs concerning
globally discussed and potentially sensitive topics, hypothesizing that these
biases may arise from language-specific distinctions. Introducing the
Multilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news
articles from Human Rights Watch covering 17 topics, and generated socially
sensitive questions along with corresponding responses in multiple languages.
We scrutinized the biases of these responses across languages and topics,
employing two statistical hypothesis tests. The results showed that the null
hypotheses were rejected in most cases, indicating biases arising from
cross-language differences. It demonstrates that ethical biases in responses
are widespread across various languages, and notably, these biases were
prevalent even among different LLMs. By making the proposed MSQAD openly
available, we aim to facilitate future research endeavors focused on examining
cross-language biases in LLMs and their variant models.",2025-05-25,"Seunguk Yu, Juhwan Choi, Youngbin Kim",http://arxiv.org/pdf/2505.19121v1,cs.CL
Controlling Language Confusion in Multilingual LLMs,"Large language models often suffer from language confusion, a phenomenon
where responses are partially or entirely generated in unintended languages.
This can critically impact user experience in low-resource settings. We
hypothesize that conventional supervised fine-tuning exacerbates this issue
because the softmax objective focuses probability mass only on the single
correct token but does not explicitly penalize cross-lingual mixing.
Interestingly, by observing loss trajectories during the pretraining phase, we
observe that models fail to learn to distinguish between monolingual and
language-confused text. Additionally, we find that ORPO, which adds penalties
for unwanted output styles to standard SFT, effectively suppresses
language-confused generations even at high decoding temperatures without
degrading overall model performance. Our findings suggest that incorporating
appropriate penalty terms can mitigate language confusion in low-resource
settings with limited data.",2025-05-25,"Nahyun Lee, Yeongseo Woo, Hyunwoo Ko, Guijin Son",http://arxiv.org/pdf/2505.19116v1,cs.CL
Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering,"Although large language models (LLMs) have demonstrated remarkable reasoning
capabilities, they still face challenges in knowledge-intensive multi-hop
reasoning. Recent work explores iterative retrieval to address complex
problems. However, the lack of intermediate guidance often results in
inaccurate retrieval and flawed intermediate reasoning, leading to incorrect
reasoning. To address these, we propose Self-Critique Guided Iterative
Reasoning (SiGIR), which uses self-critique feedback to guide the iterative
reasoning process. Specifically, through end-to-end training, we enable the
model to iteratively address complex problems via question decomposition.
Additionally, the model is able to self-evaluate its intermediate reasoning
steps. During iterative reasoning, the model engages in branching exploration
and employs self-evaluation to guide the selection of promising reasoning
trajectories. Extensive experiments on three multi-hop reasoning datasets
demonstrate the effectiveness of our proposed method, surpassing the previous
SOTA by $8.6\%$. Furthermore, our thorough analysis offers insights for future
research. Our code, data, and models are available at Github:
https://github.com/zchuz/SiGIR-MHQA.",2025-05-25,"Zheng Chu, Huiming Fan, Jingchang Chen, Qianyu Wang, Mingda Yang, Jiafeng Liang, Zhongjie Wang, Hao Li, Guo Tang, Ming Liu, Bing Qin",http://arxiv.org/pdf/2505.19112v1,cs.CL
CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models,"Investigating hallucination issues in large language models (LLMs) within
cross-lingual and cross-modal scenarios can greatly advance the large-scale
deployment in real-world applications. Nevertheless, the current studies are
limited to a single scenario, either cross-lingual or cross-modal, leaving a
gap in the exploration of hallucinations in the joint cross-lingual and
cross-modal scenarios. Motivated by this, we introduce a novel joint
Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this
gap. Specifically, CCHall simultaneously incorporates both cross-lingual and
cross-modal hallucination scenarios, which can be used to assess the
cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a
comprehensive evaluation on CCHall, exploring both mainstream open-source and
closed-source LLMs. The experimental results highlight that current LLMs still
struggle with CCHall. We hope CCHall can serve as a valuable resource to assess
LLMs in joint cross-lingual and cross-modal scenarios.",2025-05-25,"Yongheng Zhang, Xu Liu, Ruoxi Zhou, Qiguang Chen, Hao Fei, Wenpeng Lu, Libo Qin",http://arxiv.org/pdf/2505.19108v1,cs.CL
WHISTRESS: Enriching Transcriptions with Sentence Stress Detection,"Spoken language conveys meaning not only through words but also through
intonation, emotion, and emphasis. Sentence stress, the emphasis placed on
specific words within a sentence, is crucial for conveying speaker intent and
has been extensively studied in linguistics. In this work, we introduce
WHISTRESS, an alignment-free approach for enhancing transcription systems with
sentence stress detection. To support this task, we propose TINYSTRESS-15K, a
scalable, synthetic training data for the task of sentence stress detection
which resulted from a fully automated dataset creation process. We train
WHISTRESS on TINYSTRESS-15K and evaluate it against several competitive
baselines. Our results show that WHISTRESS outperforms existing methods while
requiring no additional input priors during training or inference. Notably,
despite being trained on synthetic data, WHISTRESS demonstrates strong
zero-shot generalization across diverse benchmarks. Project page:
https://pages.cs.huji.ac.il/adiyoss-lab/whistress.",2025-05-25,"Iddo Yosha, Dorin Shteyman, Yossi Adi",http://arxiv.org/pdf/2505.19103v1,cs.CL
ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning,"Direct Preference Optimization (DPO) has gained significant attention for its
simplicity and computational efficiency in aligning large language models
(LLMs). Recent advancements have extended DPO to multimodal scenarios,
achieving strong performance. However, traditional DPO relies on binary
preference optimization, rewarding or penalizing entire responses without
considering fine-grained segment correctness, leading to suboptimal solutions.
The root of this issue lies in the absence of fine-grained supervision during
the optimization process. To address this, we propose Adaptive Sentence-level
Preference Optimization (ASPO), which evaluates individual sentences for more
precise preference optimization. By dynamically calculating adaptive rewards at
the sentence level based on model predictions, ASPO enhances response content
assessment without additional models or parameters. This significantly improves
the alignment of multimodal features. Extensive experiments show that ASPO
substantially enhances the overall performance of multimodal models.",2025-05-25,"Yeyuan Wang, Dehong Gao, Rujiao Long, Lei Yi, Linbo Jin, Libin Yang, Xiaoyan Cai",http://arxiv.org/pdf/2505.19100v1,cs.CL
ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models,"Recent advancements in Large Vision-Language Models (VLMs), have greatly
enhanced their capability to jointly process text and images. However, despite
extensive benchmarks evaluating visual comprehension (e.g., diagrams, color
schemes, OCR tasks...), there is limited assessment of VLMs' ability to read
and reason about text-rich images effectively. To fill this gap, we introduce
ReadBench, a multimodal benchmark specifically designed to evaluate the reading
comprehension capabilities of VLMs. ReadBench transposes contexts from
established text-only benchmarks into images of text while keeping textual
prompts and questions intact. Evaluating leading VLMs with ReadBench, we find
minimal-but-present performance degradation on short, text-image inputs, while
performance sharply declines for longer, multi-page contexts. Our experiments
further reveal that text resolution has negligible effects on multimodal
performance. These findings highlight needed improvements in VLMs, particularly
their reasoning over visually presented extensive textual content, a capability
critical for practical applications. ReadBench is available at
https://github.com/answerdotai/ReadBench .",2025-05-25,"Benjamin Clavié, Florian Brand",http://arxiv.org/pdf/2505.19091v1,cs.CL
"Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs","Large Language Models (LLMs) have demonstrated remarkable general
capabilities, but enhancing skills such as reasoning often demands substantial
computational resources and may compromise their generalization. While
Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious
alternative, they typically requires retraining for each LLM backbone due to
architectural dependencies. To address these challenges, here we propose
Universal Reasoner (UniR) - a single, lightweight, composable, and
plug-and-play reasoning module that can be used with any frozen LLM to endow it
with specialized reasoning capabilities. Specifically, UniR decomposes the
reward into a standalone reasoning module that is trained independently using
predefined rewards, effectively translating trajectory-level signals into
token-level guidance. Once trained, UniR can be combined with any frozen LLM at
inference time by simply adding its output logits to those of the LLM backbone.
This additive structure naturally enables modular composition: multiple UniR
modules trained for different tasks can be jointly applied by summing their
logits, enabling complex reasoning via composition. Experimental results on
mathematical reasoning and machine translation tasks show that UniR
significantly outperforms \add{existing baseline fine-tuning methods using the
Llama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong
generalization: reasoning modules trained on smaller models effectively guide
much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust
solution for enhancing reasoning in LLMs without compromising their core
capabilities. Code is open-sourced at https://github.com/hangeol/UniR",2025-05-25,"Jaemin Kim, Hangeol Chang, Hyunmin Hwang, Choonghan Kim, Jong Chul Ye",http://arxiv.org/pdf/2505.19075v1,cs.CL
Towards Harmonized Uncertainty Estimation for Large Language Models,"To facilitate robust and trustworthy deployment of large language models
(LLMs), it is essential to quantify the reliability of their generations
through uncertainty estimation. While recent efforts have made significant
advancements by leveraging the internal logic and linguistic features of LLMs
to estimate uncertainty scores, our empirical analysis highlights the pitfalls
of these methods to strike a harmonized estimation between indication, balance,
and calibration, which hinders their broader capability for accurate
uncertainty estimation. To address this challenge, we propose CUE (Corrector
for Uncertainty Estimation): A straightforward yet effective method that
employs a lightweight model trained on data aligned with the target LLM's
performance to adjust uncertainty scores. Comprehensive experiments across
diverse models and tasks demonstrate its effectiveness, which achieves
consistent improvements of up to 60% over existing methods.",2025-05-25,"Rui Li, Jing Long, Muge Qi, Heming Xia, Lei Sha, Peiyi Wang, Zhifang Sui",http://arxiv.org/pdf/2505.19073v1,cs.CL
UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models,"Large Language Models (LLMs) have become indispensable tools across various
applications, making it more important than ever to ensure the quality and the
trustworthiness of their outputs. This has led to growing interest in
uncertainty quantification (UQ) methods for assessing the reliability of LLM
outputs. Many existing UQ techniques rely on token probabilities, which
inadvertently introduces a bias with respect to the length of the output. While
some methods attempt to account for this, we demonstrate that such biases
persist even in length-normalized approaches. To address the problem, here we
propose UNCERTAINTY-LINE: (Length-INvariant Estimation), a simple debiasing
procedure that regresses uncertainty scores on output length and uses the
residuals as corrected, length-invariant estimates. Our method is post-hoc,
model-agnostic, and applicable to a range of UQ measures. Through extensive
evaluation on machine translation, summarization, and question-answering tasks,
we demonstrate that UNCERTAINTY-LINE: consistently improves over even nominally
length-normalized UQ methods uncertainty estimates across multiple metrics and
models.",2025-05-25,"Roman Vashurin, Maiya Goloburda, Preslav Nakov, Maxim Panov",http://arxiv.org/pdf/2505.19060v1,cs.CL
An Embarrassingly Simple Defense Against LLM Abliteration Attacks,"Large language models (LLMs) are typically aligned to comply with safety
guidelines by refusing harmful instructions. A recent attack, termed
abliteration, isolates and suppresses the single latent direction most
responsible for refusal behavior, enabling the model to generate unethical
content. We propose a defense that modifies how models generate refusals. We
construct an extended-refusal dataset that contains harmful prompts with a full
response that justifies the reason for refusal. We then fine-tune
Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our
extended-refusal dataset, and evaluate the resulting systems on a set of
harmful prompts. In our experiments, extended-refusal models maintain high
refusal rates, dropping at most by 10%, whereas baseline models' refusal rates
drop by 70-80% after abliteration. A broad evaluation of safety and utility
shows that extended-refusal fine-tuning neutralizes the abliteration attack
while preserving general performance.",2025-05-25,"Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, Bernard Ghanem, George Turkiyyah",http://arxiv.org/pdf/2505.19056v1,cs.CL
Efficient Data Selection at Scale via Influence Distillation,"Effective data selection is critical for efficient training of modern Large
Language Models (LLMs). This paper introduces Influence Distillation, a novel,
mathematically-justified framework for data selection that employs second-order
information to optimally weight training samples. By distilling each sample's
influence on a target distribution, our method assigns model-specific weights
that are used to select training data for LLM fine-tuning, guiding it toward
strong performance on the target domain. We derive these optimal weights for
both Gradient Descent and Adam optimizers. To ensure scalability and reduce
computational cost, we propose a $\textit{landmark-based approximation}$:
influence is precisely computed for a small subset of ""landmark"" samples and
then efficiently propagated to all other samples to determine their weights. We
validate Influence Distillation by applying it to instruction tuning on the
Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,
across several models from the Llama and Qwen families. Experiments show that
Influence Distillation matches or outperforms state-of-the-art performance
while achieving up to $3.5\times$ faster selection.",2025-05-25,"Mahdi Nikdan, Vincent Cohen-Addad, Dan Alistarh, Vahab Mirrokni",http://arxiv.org/pdf/2505.19051v1,cs.CL
Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models,"We introduce Speech-IFeval, an evaluation framework designed to assess
instruction-following capabilities and quantify catastrophic forgetting in
speech-aware language models (SLMs). Recent SLMs integrate speech perception
with large language models (LLMs), often degrading textual capabilities due to
speech-centric training. Existing benchmarks conflate speech perception with
instruction-following, hindering evaluation of these distinct skills. To
address this gap, we provide a benchmark for diagnosing the
instruction-following abilities of SLMs. Our findings show that most SLMs
struggle with even basic instructions, performing far worse than text-based
LLMs. Additionally, these models are highly sensitive to prompt variations,
often yielding inconsistent and unreliable outputs. We highlight core
challenges and provide insights to guide future research, emphasizing the need
for evaluation beyond task-level metrics.",2025-05-25,"Ke-Han Lu, Chun-Yi Kuan, Hung-yi Lee",http://arxiv.org/pdf/2505.19037v1,cs.CL
SQUiD: Synthesizing Relational Databases from Unstructured Text,"Relational databases are central to modern data management, yet most data
exists in unstructured forms like text documents. To bridge this gap, we
leverage large language models (LLMs) to automatically synthesize a relational
database by generating its schema and populating its tables from raw text. We
introduce SQUiD, a novel neurosymbolic framework that decomposes this task into
four stages, each with specialized techniques. Our experiments show that SQUiD
consistently outperforms baselines across diverse datasets.",2025-05-25,"Mushtari Sadia, Zhenning Yang, Yunming Xiao, Ang Chen, Amrita Roy Chowdhury",http://arxiv.org/pdf/2505.19025v1,cs.CL
CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language,"Aspect-Based Sentiment Analysis (ABSA) is a fundamental task in natural
language processing, offering fine-grained insights into opinions expressed in
text. While existing research has largely focused on resource-rich languages
like English which leveraging large annotated datasets, pre-trained models, and
language-specific tools. These resources are often unavailable for low-resource
languages such as Bengali. The ABSA task in Bengali remains poorly explored and
is further complicated by its unique linguistic characteristics and a lack of
annotated data, pre-trained models, and optimized hyperparameters. To address
these challenges, this research propose CrosGrpsABS, a novel hybrid framework
that leverages bidirectional cross-attention between syntactic and semantic
graphs to enhance aspect-level sentiment classification. The CrosGrpsABS
combines transformerbased contextual embeddings with graph convolutional
networks, built upon rule-based syntactic dependency parsing and semantic
similarity computations. By employing bidirectional crossattention, the model
effectively fuses local syntactic structure with global semantic context,
resulting in improved sentiment classification performance across both low- and
high-resource settings. We evaluate CrosGrpsABS on four low-resource Bengali
ABSA datasets and the high-resource English SemEval 2014 Task 4 dataset. The
CrosGrpsABS consistently outperforms existing approaches, achieving notable
improvements, including a 0.93% F1-score increase for the Restaurant domain and
a 1.06% gain for the Laptop domain in the SemEval 2014 Task 4 benchmark.",2025-05-25,"Md. Mithun Hossain, Md. Shakil Hossain, Sudipto Chaki, Md. Rajib Hossain, Md. Saifur Rahman, A. B. M. Shawkat Ali",http://arxiv.org/pdf/2505.19018v1,cs.CL
Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection,"Multi-modal learning has become a critical research area because integrating
text and image data can significantly improve performance in tasks such as
classification, retrieval, and scene understanding. However, despite progress
with pre-trained models, current approaches are limited by inadequate
cross-modal interactions and static fusion strategies that do not fully exploit
the complementary nature of different modalities. To address these
shortcomings, we introduce a novel multi-modal Co-AttenDWG architecture that
leverages dual-path encoding, co-attention with dimension-wise gating, and
advanced expert fusion. Our approach begins by projecting text and image
features into a common embedding space, where a dedicated co-attention
mechanism enables simultaneous, fine-grained interactions between modalities.
This mechanism is further enhanced by a dimension-wise gating network that
adaptively regulates the feature contributions at the channel level, ensuring
that only the most relevant information is emphasized. In parallel, dual-path
encoders refine the representations by processing cross-modal information
separately before an additional cross-attention layer further aligns
modalities. The refined features are then aggregated via an expert fusion
module that combines learned gating and self-attention to produce a robust,
unified representation. We validate our approach on the MIMIC and SemEval
Memotion 1.0, where experimental results demonstrate significant improvements
in cross-modal alignment and state-of-the-art performance, underscoring the
potential of our model for a wide range of multi-modal applications.",2025-05-25,"Md. Mithun Hossain, Md. Shakil Hossain, Sudipto Chaki, M. F. Mridha",http://arxiv.org/pdf/2505.19010v1,cs.CL
VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization,"Applying Reinforcement Learning (RL) to Video Large Language Models
(Video-LLMs) shows significant promise for complex video reasoning. However,
popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group
Relative Policy Optimization (GRPO), are limited by data preparation
bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the
quality of long chain-of-thoughts (CoTs) and downstream performance.To address
these limitations, we propose VerIPO, a Verifier-guided Iterative Policy
Optimization method designed to gradually improve video LLMs' capacity for
generating deep, long-term reasoning chains. The core component is
Rollout-Aware Verifier, positioned between the GRPO and Direct Preference
Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop.
This verifier leverages small LLMs as a judge to assess the reasoning logic of
rollouts, enabling the construction of high-quality contrastive data, including
reflective and contextually consistent CoTs. These curated preference samples
drive the efficient DPO stage (7x faster than GRPO), leading to marked
improvements in reasoning chain quality, especially in terms of length and
contextual consistency. This training loop benefits from GRPO's expansive
search and DPO's targeted optimization. Experimental results demonstrate: 1)
Significantly faster and more effective optimization compared to standard GRPO
variants, yielding superior performance; 2) Our trained models exceed the
direct inference of large-scale instruction-tuned Video-LLMs, producing long
and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our
model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long
reasoning models (e.g., Video-R1), highlighting its effectiveness and
stability.",2025-05-25,"Yunxin Li, Xinyu Chen, Zitao Li, Zhenyu Liu, Longyue Wang, Wenhan Luo, Baotian Hu, Min Zhang",http://arxiv.org/pdf/2505.19000v1,cs.CL
FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM),"This study presents FiLLM, a Filipino-optimized large language model,
designed to enhance natural language processing (NLP) capabilities in the
Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank
Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining
task-specific performance. The model was trained and evaluated on diverse
Filipino datasets to address key NLP tasks, including Named Entity Recognition
(NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text
Summarization. Performance comparisons with the CalamanCy model were conducted
using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap
metrics. Results indicate that Calamancy outperforms FILLM in several aspects,
demonstrating its effectiveness in processing Filipino text with improved
linguistic comprehension and adaptability. This research contributes to the
advancement of Filipino NLP applications by providing an optimized, efficient,
and scalable language model tailored for local linguistic needs.",2025-05-25,"Carlos Jude G. Maminta, Isaiah Job Enriquez, Deandre Nigel Nunez, Michael B. Dela Fuente",http://arxiv.org/pdf/2505.18995v1,cs.CL
STRICT: Stress Test of Rendering Images Containing Text,"While diffusion models have revolutionized text-to-image generation with
their ability to synthesize realistic and diverse scenes, they continue to
struggle to generate consistent and legible text within images. This
shortcoming is commonly attributed to the locality bias inherent in
diffusion-based generation, which limits their ability to model long-range
spatial dependencies. In this paper, we introduce $\textbf{STRICT}$, a
benchmark designed to systematically stress-test the ability of diffusion
models to render coherent and instruction-aligned text in images. Our benchmark
evaluates models across multiple dimensions: (1) the maximum length of readable
text that can be generated; (2) the correctness and legibility of the generated
text, and (3) the ratio of not following instructions for generating text. We
evaluate several state-of-the-art models, including proprietary and open-source
variants, and reveal persistent limitations in long-range consistency and
instruction-following capabilities. Our findings provide insights into
architectural bottlenecks and motivate future research directions in multimodal
generative modeling. We release our entire evaluation pipeline at
https://github.com/tianyu-z/STRICT-Bench.",2025-05-25,"Tianyu Zhang, Xinyu Wang, Zhenghan Tai, Lu Li, Jijun Chi, Jingrui Tian, Hailin He, Suyuchen Wang",http://arxiv.org/pdf/2505.18985v1,cs.CL
AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models,"Existing mathematical reasoning benchmarks are predominantly English only or
translation-based, which can introduce semantic drift and mask languagespecific
reasoning errors. To address this, we present AI4Math, a benchmark of 105
original university level math problems natively authored in Spanish. The
dataset spans seven advanced domains (Algebra, Calculus, Geometry, Probability,
Number Theory, Combinatorics, and Logic), and each problem is accompanied by a
step by step human solution. We evaluate six large language models GPT 4o, GPT
4o mini, o3 mini, LLaMA 3.3 70B, DeepSeek R1 685B, and DeepSeek V3 685B under
four configurations: zero shot and chain of thought, each in Spanish and
English. The top models (o3 mini, DeepSeek R1 685B, DeepSeek V3 685B) achieve
over 70% accuracy, whereas LLaMA 3.3 70B and GPT-4o mini remain below 40%. Most
models show no significant performance drop between languages, with GPT 4o even
performing better on Spanish problems in the zero shot setting. Geometry,
Combinatorics, and Probability questions remain persistently challenging for
all models. These results highlight the need for native-language benchmarks and
domain-specific evaluations to reveal reasoning failures not captured by
standard metrics.",2025-05-25,"Miguel Angel Peñaloza Perez, Bruno Lopez Orozco, Jesus Tadeo Cruz Soto, Michelle Bruno Hernandez, Miguel Angel Alvarado Gonzalez, Sandra Malagon",http://arxiv.org/pdf/2505.18978v1,cs.CL
Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings,"Selective state-space models have achieved great success in long-sequence
modeling. However, their capacity for language representation, especially in
complex hierarchical reasoning tasks, remains underexplored. Most large
language models rely on flat Euclidean embeddings, limiting their ability to
capture latent hierarchies. To address this limitation, we propose Hierarchical
Mamba (HiM), integrating efficient Mamba2 with exponential growth and curved
nature of hyperbolic geometry to learn hierarchy-aware language embeddings for
deeper linguistic understanding. Mamba2-processed sequences are projected to
the Poincare ball (via tangent-based mapping) or Lorentzian manifold (via
cosine and sine-based mapping) with ""learnable"" curvature, optimized with a
combined hyperbolic loss. Our HiM model facilitates the capture of relational
distances across varying hierarchical levels, enabling effective long-range
reasoning. This makes it well-suited for tasks like mixed-hop prediction and
multi-hop inference in hierarchical classification. We evaluated our HiM with
four linguistic and medical datasets for mixed-hop prediction and multi-hop
inference tasks. Experimental results demonstrated that: 1) Both HiM models
effectively capture hierarchical relationships for four ontological datasets,
surpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic
distinctions with higher h-norms, while HiM-Lorentz provides more stable,
compact, and hierarchy-preserving embeddings favoring robustness over detail.",2025-05-25,"Sarang Patil, Ashish Parmanand Pandey, Ioannis Koutis, Mengjia Xu",http://arxiv.org/pdf/2505.18973v1,cs.CL
Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE,"We revisit the efficacy of simple, real-valued embedding models for knowledge
graph completion and introduce RelatE, an interpretable and modular method that
efficiently integrates dual representations for entities and relations. RelatE
employs a real-valued phase-modulus decomposition, leveraging sinusoidal phase
alignments to encode relational patterns such as symmetry, inversion, and
composition. In contrast to recent approaches based on complex-valued
embeddings or deep neural architectures, RelatE preserves architectural
simplicity while achieving competitive or superior performance on standard
benchmarks. Empirically, RelatE outperforms prior methods across several
datasets: on YAGO3-10, it achieves an MRR of 0.521 and Hit@10 of 0.680,
surpassing all baselines. Additionally, RelatE offers significant efficiency
gains, reducing training time by 24%, inference latency by 31%, and peak GPU
memory usage by 22% compared to RotatE. Perturbation studies demonstrate
improved robustness, with MRR degradation reduced by up to 61% relative to
TransE and by up to 19% compared to RotatE under structural edits such as edge
removals and relation swaps. Formal analysis further establishes the model's
full expressiveness and its capacity to represent essential first-order logical
inference patterns. These results position RelatE as a scalable and
interpretable alternative to more complex architectures for knowledge graph
completion.",2025-05-25,"Abhijit Chakraborty, Chahana Dahal, Ashutosh Balasubramaniam, Tejas Anvekar, Vivek Gupta",http://arxiv.org/pdf/2505.18971v1,cs.CL
Learning to Explain: Prototype-Based Surrogate Models for LLM Classification,"Large language models (LLMs) have demonstrated impressive performance on
natural language tasks, but their decision-making processes remain largely
opaque. Existing explanation methods either suffer from limited faithfulness to
the model's reasoning or produce explanations that humans find difficult to
understand. To address these challenges, we propose \textbf{ProtoSurE}, a novel
prototype-based surrogate framework that provides faithful and
human-understandable explanations for LLMs. ProtoSurE trains an
interpretable-by-design surrogate model that aligns with the target LLM while
utilizing sentence-level prototypes as human-understandable concepts. Extensive
experiments show that ProtoSurE consistently outperforms SOTA explanation
methods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates
strong data efficiency, requiring relatively few training examples to achieve
good performance, making it practical for real-world applications.",2025-05-25,"Bowen Wei, Ziwei Zhu",http://arxiv.org/pdf/2505.18970v1,cs.CL
System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts,"Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move
beyond fast System-1 responses and engage in deliberative System-2 reasoning.
However, this comes at the cost of significant inefficiency due to verbose
intermediate output. Recent latent-space reasoning methods improve efficiency
by operating on hidden states without decoding into language, yet they treat
all steps uniformly, failing to distinguish critical deductions from auxiliary
steps and resulting in suboptimal use of computational resources. In this
paper, we propose System-1.5 Reasoning, an adaptive reasoning framework that
dynamically allocates computation across reasoning steps through shortcut paths
in latent space.Specifically, System-1.5 Reasoning introduces two types of
dynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the
vertical depth by early exiting non-critical tokens through lightweight adapter
branches, while allowing critical tokens to continue through deeper Transformer
layers. The step shortcut (SS) reuses hidden states across the decoding steps
to skip trivial steps and reason horizontally in latent space. Training
System-1.5 Reasoning involves a two-stage self-distillation process: first
distilling natural language CoT into latent-space continuous thought, and then
distilling full-path System-2 latent reasoning into adaptive shortcut paths
(System-1.5 Reasoning).Experiments on reasoning tasks demonstrate the superior
performance of our method. For example, on GSM8K, System-1.5 Reasoning achieves
reasoning performance comparable to traditional CoT fine-tuning methods while
accelerating inference by over 20x and reducing token generation by 92.31% on
average.",2025-05-25,"Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, Bang Liu",http://arxiv.org/pdf/2505.18962v1,cs.CL
Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?,"We evaluate the credibility of leading AI models in assessing investment risk
appetite. Our analysis spans proprietary (GPT-4, Claude 3.7, Gemini 1.5) and
open-weight models (LLaMA 3.1/3.3, DeepSeek-V3, Mistral-small), using 1,720
user profiles constructed with 16 risk-relevant features across 10 countries
and both genders. We observe significant variance across models in score
distributions and demographic sensitivity. For example, GPT-4o assigns higher
risk scores to Nigerian and Indonesian profiles, while LLaMA and DeepSeek show
opposite gender tendencies in risk classification. While some models (e.g.,
GPT-4o, LLaMA 3.1) align closely with expected scores in low- and mid-risk
ranges, none maintain consistent performance across regions and demographics.
Our findings highlight the need for rigorous, standardized evaluations of AI
systems in regulated financial contexts to prevent bias, opacity, and
inconsistency in real-world deployment.",2025-05-25,"Divij Chawla, Ashita Bhutada, Do Duc Anh, Abhinav Raghunathan, Vinod SP, Cathy Guo, Dar Win Liew, Prannaya Gupta, Rishabh Bhardwaj, Rajat Bhardwaj, Soujanya Poria",http://arxiv.org/pdf/2505.18953v1,cs.CL
BnMMLU: Measuring Massive Multitask Language Understanding in Bengali,"The Massive Multitask Language Understanding (MMLU) benchmark has been widely
used to evaluate language models across various domains. However, existing MMLU
datasets primarily focus on high-resource languages such as English, which
leaves low-resource languages like Bengali underrepresented. In this paper, we
introduce BnMMLU, a benchmark to evaluate the multitask language understanding
capabilities of Bengali in language models. The dataset spans 23 domains,
including science, humanities, mathematics and general knowledge and is
structured in a multiple-choice format to assess factual knowledge,
application-based problem-solving and reasoning abilities of language models.
It consists of 138,949 question-option pairs. We benchmark several proprietary
and open-source large language models (LLMs) on the BnMMLU test set.
Additionally, we annotate the test set with three cognitive categories-factual
knowledge, procedural application and reasoning-to gain deeper insights into
model strengths and weaknesses across various cognitive tasks. The results
reveal significant performance gaps, highlighting the need for improved
pre-training and fine-tuning strategies tailored to Bengali data. We release
the dataset and benchmark results to facilitate further research in this area.",2025-05-25,Saman Sarker Joy,http://arxiv.org/pdf/2505.18951v1,cs.CL
The Price of Format: Diversity Collapse in LLMs,"Instruction-tuned large language models (LLMs) employ structured templates,
such as role markers and special tokens, to enforce format consistency during
inference. However, we identify a critical limitation of such formatting: it
induces a phenomenon we term diversity collapse, where the model generates
semantically similar outputs for open-ended inputs, undermining creativity and
variability. We systematically evaluate this effect across tasks like story
completion and free-form generation, finding that (1) diversity collapse
persists even under high-temperature sampling, and (2) structural tokens in
templates significantly constrain the model's output space. To contextualize
these findings, we fine-tune the same model using a range of structured prompts
and then evaluate them across three axes: downstream task performance,
alignment behavior, and output diversity. Our analysis shows that format
consistency between fine-tuning and inference is crucial for
structure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on
knowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity
is primarily governed by the presence or absence of structural tokens, with
minimal formatting yielding the most diverse outputs. These findings reveal
that current prompting conventions, while beneficial for alignment, may
inadvertently suppress output diversity, underscoring the need for
diversity-aware prompt design and instruction tuning.",2025-05-25,"Longfei Yun, Chenyang An, Zilong Wang, Letian Peng, Jingbo Shang",http://arxiv.org/pdf/2505.18949v1,cs.CL
MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems,"Human social interactions depend on the ability to infer others' unspoken
intentions, emotions, and beliefs-a cognitive skill grounded in the
psychological concept of Theory of Mind (ToM). While large language models
(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity
and contextual nuance inherent in human communication. To bridge this gap, we
introduce MetaMind, a multi-agent framework inspired by psychological theories
of metacognition, designed to emulate human-like social reasoning. MetaMind
decomposes social understanding into three collaborative stages: (1) a
Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent,
emotion), (2) a Domain Agent refines these hypotheses using cultural norms and
ethical constraints, and (3) a Response Agent generates contextually
appropriate responses while validating alignment with inferred intent. Our
framework achieves state-of-the-art performance across three challenging
benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain
in ToM reasoning. Notably, it enables LLMs to match human-level performance on
key ToM tasks for the first time. Ablation studies confirm the necessity of all
components, which showcase the framework's ability to balance contextual
plausibility, social appropriateness, and user adaptation. This work advances
AI systems toward human-like social intelligence, with applications in
empathetic dialogue and culturally sensitive interactions. Code is available at
https://github.com/XMZhangAI/MetaMind.",2025-05-25,"Xuanming Zhang, Yuxuan Chen, Min-Hsuan Yeh, Yixuan Li",http://arxiv.org/pdf/2505.18943v1,cs.CL
Language Models Surface the Unwritten Code of Science and Society,"This paper calls on the research community not only to investigate how human
biases are inherited by large language models (LLMs) but also to explore how
these biases in LLMs can be leveraged to make society's ""unwritten code"" - such
as implicit stereotypes and heuristics - visible and accessible for critique.
We introduce a conceptual framework through a case study in science: uncovering
hidden rules in peer review - the factors that reviewers care about but rarely
state explicitly due to normative scientific expectations. The idea of the
framework is to push LLMs to speak out their heuristics through generating
self-consistent hypotheses - why one paper appeared stronger in reviewer
scoring - among paired papers submitted to 45 computer science conferences,
while iteratively searching deeper hypotheses from remaining pairs where
existing hypotheses cannot explain. We observed that LLMs' normative priors
about the internal characteristics of good science extracted from their
self-talk, e.g. theoretical rigor, were systematically updated toward
posteriors that emphasize storytelling about external connections, such as how
the work is positioned and connected within and across literatures. This shift
reveals the primacy of scientific myths about intrinsic properties driving
scientific excellence rather than extrinsic contextualization and storytelling
that influence conceptions of relevance and significance. Human reviewers tend
to explicitly reward aspects that moderately align with LLMs' normative priors
(correlation = 0.49) but avoid articulating contextualization and storytelling
posteriors in their review comments (correlation = -0.14), despite giving
implicit reward to them with positive scores. We discuss the broad
applicability of the framework, leveraging LLMs as diagnostic tools to surface
the tacit codes underlying human society, enabling more precisely targeted
responsible AI.",2025-05-25,"Honglin Bao, Siyang Wu, Jiwoong Choi, Yingrong Mao, James A. Evans",http://arxiv.org/pdf/2505.18942v1,cs.CL
REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing,"Large language model editing methods frequently suffer from overfitting,
wherein factual updates can propagate beyond their intended scope,
overemphasizing the edited target even when it's contextually inappropriate. To
address this challenge, we introduce REACT (Representation Extraction And
Controllable Tuning), a unified two-phase framework designed for precise and
controllable knowledge editing. In the initial phase, we utilize tailored
stimuli to extract latent factual representations and apply Principal Component
Analysis with a simple learnbale linear transformation to compute a directional
""belief shift"" vector for each instance. In the second phase, we apply
controllable perturbations to hidden states using the obtained vector with a
magnitude scalar, gated by a pre-trained classifier that permits edits only
when contextually necessary. Relevant experiments on EVOKE benchmarks
demonstrate that REACT significantly reduces overfitting across nearly all
evaluation metrics, and experiments on COUNTERFACT and MQuAKE shows that our
method preserves balanced basic editing performance (reliability, locality, and
generality) under diverse editing scenarios.",2025-05-25,"Haitian Zhong, Yuhuan Liu, Ziyang Xu, Guofan Liu, Qiang Liu, Shu Wu, Zhe Zhao, Liang Wang, Tieniu Tan",http://arxiv.org/pdf/2505.18933v1,cs.CL
Can Large Language Models Infer Causal Relationships from Real-World Text?,"Understanding and inferring causal relationships from texts is a core aspect
of human cognition and is essential for advancing large language models (LLMs)
towards artificial general intelligence. Existing work primarily focuses on
synthetically generated texts which involve simple causal relationships
explicitly mentioned in the text. This fails to reflect the complexities of
real-world tasks. In this paper, we investigate whether LLMs are capable of
inferring causal relationships from real-world texts. We develop a benchmark
drawn from real-world academic literature which includes diverse texts with
respect to length, complexity of relationships (different levels of
explicitness, number of events, and causal relationships), and domains and
sub-domains. To the best of our knowledge, our benchmark is the first-ever
real-world dataset for this task. Our experiments on state-of-the-art LLMs
evaluated on our proposed benchmark demonstrate significant challenges, with
the best-performing model achieving an average F1 score of only 0.477. Analysis
reveals common pitfalls: difficulty with implicitly stated information, in
distinguishing relevant causal factors from surrounding contextual details, and
with connecting causally relevant information spread across lengthy textual
passages. By systematically characterizing these deficiencies, our benchmark
offers targeted insights for further research into advancing LLM causal
reasoning.",2025-05-25,"Ryan Saklad, Aman Chadha, Oleg Pavlov, Raha Moraffah",http://arxiv.org/pdf/2505.18931v1,cs.CL
Meta-aware Learning in text-to-SQL Large Language Model,"The advancements of Large language models (LLMs) have provided great
opportunities to text-to-SQL tasks to overcome the main challenges to
understand complex domain information and complex database structures in
business applications. In this paper, we propose a meta-aware learning
framework to integrate domain knowledge, database schema, chain-of-thought
reasoning processes, and metadata relationships to improve the SQL generation
quality. The proposed framework includes four learning strategies: schema-based
learning, Chain-of-Thought (CoT) learning, knowledge-enhanced learning, and key
information tokenization. This approach provides a comprehensive understanding
of database structure and metadata information towards LLM through fine-tuning
to improve its performance on SQL generation within business domains. Through
two experimental studies, we have demonstrated the superiority of the proposed
methods in execution accuracy, multi-task SQL generation capability, and
reduction of catastrophic forgetting.",2025-05-25,Wenda Zhang,http://arxiv.org/pdf/2505.18929v1,cs.CL
Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments,"As online platforms grow, comment sections increasingly host harassment that
undermines user experience and well-being. This study benchmarks three leading
large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic
Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse
threads in gaming, lifestyle, food vlog, and music channels. The dataset
comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and
Indonesian, annotated independently by two reviewers with substantial agreement
(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,
GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision
of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful
posts (recall = 0.875) but its precision fell to 0.767 due to frequent false
positives. Claude delivered the highest precision at 0.920 and the lowest
false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative
analysis showed that all three models struggle with sarcasm, coded insults, and
mixed-language slang. These results underscore the need for moderation
pipelines that combine complementary models, incorporate conversational
context, and fine-tune for under-represented languages and implicit abuse. A
de-identified version of the dataset and full prompts is publicly released to
promote reproducibility and further progress in automated content moderation.",2025-05-25,Amel Muminovic,http://arxiv.org/pdf/2505.18927v1,cs.CL
SCRum-9: Multilingual Stance Classification over Rumours on Social Media,"We introduce SCRum-9, a multilingual dataset for Rumour Stance
Classification, containing 7,516 tweet-reply pairs from X. SCRum-9 goes beyond
existing stance classification datasets by covering more languages (9), linking
examples to more fact-checked claims (2.1k), and including complex annotations
from multiple annotators to account for intra- and inter-annotator variability.
Annotations were made by at least three native speakers per language, totalling
around 405 hours of annotation and 8,150 dollars in compensation. Experiments
on SCRum-9 show that it is a challenging benchmark for both state-of-the-art
LLMs (e.g. Deepseek) as well as fine-tuned pre-trained models, motivating
future work in this area.",2025-05-25,"Yue Li, Jake Vasilakes, Zhixue Zhao, Carolina Scarton",http://arxiv.org/pdf/2505.18916v1,cs.CL
Federated Retrieval-Augmented Generation: A Systematic Mapping Study,"Federated Retrieval-Augmented Generation (Federated RAG) combines Federated
Learning (FL), which enables distributed model training without exposing raw
data, with Retrieval-Augmented Generation (RAG), which improves the factual
accuracy of language models by grounding outputs in external knowledge. As
large language models are increasingly deployed in privacy-sensitive domains
such as healthcare, finance, and personalized assistance, Federated RAG offers
a promising framework for secure, knowledge-intensive natural language
processing (NLP). To the best of our knowledge, this paper presents the first
systematic mapping study of Federated RAG, covering literature published
between 2020 and 2025. Following Kitchenham's guidelines for evidence-based
software engineering, we develop a structured classification of research
focuses, contribution types, and application domains. We analyze architectural
patterns, temporal trends, and key challenges, including privacy-preserving
retrieval, cross-client heterogeneity, and evaluation limitations. Our findings
synthesize a rapidly evolving body of research, identify recurring design
patterns, and surface open questions, providing a foundation for future work at
the intersection of RAG and federated systems.",2025-05-24,"Abhijit Chakraborty, Chahana Dahal, Vivek Gupta",http://arxiv.org/pdf/2505.18906v1,cs.CL
Building a Functional Machine Translation Corpus for Kpelle,"In this paper, we introduce the first publicly available English-Kpelle
dataset for machine translation, comprising over 2000 sentence pairs drawn from
everyday communication, religious texts, and educational materials. By
fine-tuning Meta's No Language Left Behind(NLLB) model on two versions of the
dataset, we achieved BLEU scores of up to 30 in the Kpelle-to-English
direction, demonstrating the benefits of data augmentation. Our findings align
with NLLB-200 benchmarks on other African languages, underscoring Kpelle's
potential for competitive performance despite its low-resource status. Beyond
machine translation, this dataset enables broader NLP tasks, including speech
recognition and language modelling. We conclude with a roadmap for future
dataset expansion, emphasizing orthographic consistency, community-driven
validation, and interdisciplinary collaboration to advance inclusive language
technology development for Kpelle and other low-resourced Mande languages.",2025-05-24,"Kweku Andoh Yamoah, Jackson Weako, Emmanuel J. Dorley",http://arxiv.org/pdf/2505.18905v1,cs.CL
StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos,"Aiming towards improving current computational models of humor detection, we
propose a new multimodal dataset of stand-up comedies, in seven languages:
English, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset
of more than 330 hours, is at the time of writing the biggest available for
this type of task, and the most diverse. The whole dataset is automatically
annotated in laughter (from the audience), and the subpart left for model
validation is manually annotated. Contrary to contemporary approaches, we do
not frame the task of humor detection as a binary sequence classification, but
as word-level sequence labeling, in order to take into account all the context
of the sequence and to capture the continuous joke tagging mechanism typically
occurring in natural conversations. As par with unimodal baselines results, we
propose a method for e propose a method to enhance the automatic laughter
detection based on Audio Speech Recognition errors. Our code and data are
available online: https://tinyurl.com/EMNLPHumourStandUpPublic",2025-05-24,"Valentin Barriere, Nahuel Gomez, Leo Hemamou, Sofia Callejas, Brian Ravenet",http://arxiv.org/pdf/2505.18903v1,cs.CL
CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions,"While AI agents hold transformative potential in business, effective
performance benchmarking is hindered by the scarcity of public, realistic
business data on widely used platforms. Existing benchmarks often lack fidelity
in their environments, data, and agent-user interactions, with limited coverage
of diverse business scenarios and industries. To address these gaps, we
introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of
LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena
with nineteen expert-validated tasks across sales, service, and 'configure,
price, and quote' processes, for both Business-to-Business and
Business-to-Customer scenarios. It distinctively incorporates multi-turn
interactions guided by diverse personas and robust confidentiality awareness
assessments. Experiments reveal leading LLM agents achieve only around 58%
single-turn success on CRMArena-Pro, with performance dropping significantly to
approximately 35% in multi-turn settings. While Workflow Execution proves more
tractable for top agents (over 83% single-turn success), other evaluated
business skills present greater challenges. Furthermore, agents exhibit
near-zero inherent confidentiality awareness; though targeted prompting can
improve this, it often compromises task performance. These findings highlight a
substantial gap between current LLM capabilities and enterprise demands,
underscoring the need for advancements in multi-turn reasoning, confidentiality
adherence, and versatile skill acquisition.",2025-05-24,"Kung-Hsiang Huang, Akshara Prabhakar, Onkar Thorat, Divyansh Agarwal, Prafulla Kumar Choubey, Yixin Mao, Silvio Savarese, Caiming Xiong, Chien-Sheng Wu",http://arxiv.org/pdf/2505.18878v1,cs.CL
Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing,"Lay paraphrasing aims to make scientific information accessible to audiences
without technical backgrounds. However, most existing studies focus on a single
domain, such as biomedicine. With the rise of interdisciplinary research, it is
increasingly necessary to comprehend knowledge spanning multiple technical
fields. To address this, we propose Sci-LoRA, a model that leverages a mixture
of LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA
dynamically generates and applies weights for each LoRA, enabling it to adjust
the impact of different domains based on the input text, without requiring
explicit domain labels. To balance domain-specific knowledge and generalization
across various domains, Sci-LoRA integrates information at both the data and
model levels. This dynamic fusion enhances the adaptability and performance
across various domains. Experimental results across twelve domains on five
public datasets show that Sci-LoRA significantly outperforms state-of-the-art
large language models and demonstrates flexible generalization and adaptability
in cross-domain lay paraphrasing.",2025-05-24,"Ming Cheng, Jiaying Gong, Hoda Eldardiry",http://arxiv.org/pdf/2505.18867v1,cs.CL
Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework,"Recent advances in Multimodal Large Language Models (MLLMs) have
significantly enhanced the naturalness and flexibility of human computer
interaction by enabling seamless understanding across text, vision, and audio
modalities. Among these, voice enabled models such as SpeechGPT have
demonstrated considerable improvements in usability, offering expressive, and
emotionally responsive interactions that foster deeper connections in real
world communication scenarios. However, the use of voice introduces new
security risks, as attackers can exploit the unique characteristics of spoken
language, such as timing, pronunciation variability, and speech to text
translation, to craft inputs that bypass defenses in ways not seen in
text-based systems. Despite substantial research on text based jailbreaks, the
voice modality remains largely underexplored in terms of both attack strategies
and defense mechanisms. In this work, we present an adversarial attack
targeting the speech input of aligned MLLMs in a white box scenario.
Specifically, we introduce a novel token level attack that leverages access to
the model's speech tokenization to generate adversarial token sequences. These
sequences are then synthesized into audio prompts, which effectively bypass
alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT,
our approach achieves up to 89 percent attack success rate across multiple
restricted tasks, significantly outperforming existing voice based jailbreak
methods. Our findings shed light on the vulnerabilities of voice-enabled
multimodal systems and to help guide the development of more robust
next-generation MLLMs.",2025-05-24,"Binhao Ma, Hanqing Guo, Zhengping Jay Luo, Rui Duan",http://arxiv.org/pdf/2505.18864v1,cs.CL
Writing Like the Best: Exemplar-Based Expository Text Generation,"We introduce the Exemplar-Based Expository Text Generation task, aiming to
generate an expository text on a new topic using an exemplar on a similar
topic. Current methods fall short due to their reliance on extensive exemplar
data, difficulty in adapting topic-specific content, and issues with long-text
coherence. To address these challenges, we propose the concept of Adaptive
Imitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA
leverages large language models (LLMs) for effective adaptive imitation through
a fine-grained plan-then-adapt process. RePA also enables recurrent
segment-by-segment imitation, supported by two memory structures that enhance
input clarity and output coherence. We also develop task-specific evaluation
metrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as
evaluators. Experimental results across our collected three diverse datasets
demonstrate that RePA surpasses existing baselines in producing factual,
consistent, and relevant texts for this task.",2025-05-24,"Yuxiang Liu, Kevin Chen-Chuan Chang",http://arxiv.org/pdf/2505.18859v1,cs.CL
Inference Compute-Optimal Video Vision Language Models,"This work investigates the optimal allocation of inference compute across
three key scaling factors in video vision language models: language model size,
frame count, and the number of visual tokens per frame. While prior works
typically focuses on optimizing model efficiency or improving performance
without considering resource constraints, we instead identify optimal model
configuration under fixed inference compute budgets. We conduct large-scale
training sweeps and careful parametric modeling of task performance to identify
the inference compute-optimal frontier. Our experiments reveal how task
performance depends on scaling factors and finetuning data size, as well as how
changes in data size shift the compute-optimal frontier. These findings
translate to practical tips for selecting these scaling factors.",2025-05-24,"Peiqi Wang, ShengYun Peng, Xuewen Zhang, Hanchao Yu, Yibo Yang, Lifu Huang, Fujun Liu, Qifan Wang",http://arxiv.org/pdf/2505.18855v1,cs.CL
Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation,"Diffusion models have achieved state-of-the-art performance in generating
images, audio, and video, but their adaptation to text remains challenging due
to its discrete nature. Prior approaches either apply Gaussian diffusion in
continuous latent spaces, which inherits semantic structure but struggles with
token decoding, or operate in categorical simplex space, which respect
discreteness but disregard semantic relation between tokens. In this paper, we
propose Smoothing Diffusion on Token Embeddings (Smoothie), a novel diffusion
method that combines the strengths of both approaches by progressively
smoothing token embeddings based on semantic similarity. This technique enables
gradual information removal while maintaining a natural decoding process.
Experimental results on several sequence-to-sequence generation tasks
demonstrate that Smoothie outperforms existing diffusion-based models in
generation quality. Furthermore, ablation studies show that our proposed
diffusion space yields better performance than both the standard embedding
space and the categorical simplex. Our code is available at
https://github.com/ashaba1in/smoothie.",2025-05-24,"Alexander Shabalin, Viacheslav Meshchaninov, Dmitry Vetrov",http://arxiv.org/pdf/2505.18853v1,cs.CL
"Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework","Recent advances have increasingly applied large language models (LLMs) to
electrocardiogram (ECG) interpretation, giving rise to
Electrocardiogram-Language Models (ELMs). Conditioned on an ECG and a textual
query, an ELM autoregressively generates a free-form textual response. Unlike
traditional classification-based systems, ELMs emulate expert cardiac
electrophysiologists by issuing diagnoses, analyzing waveform morphology,
identifying contributing factors, and proposing patient-specific action plans.
To realize this potential, researchers are curating instruction-tuning datasets
that pair ECGs with textual dialogues and are training ELMs on these resources.
Yet before scaling ELMs further, there is a fundamental question yet to be
explored: What is the most effective ECG input representation? In recent works,
three candidate representations have emerged-raw time-series signals, rendered
images, and discretized symbolic sequences. We present the first comprehensive
benchmark of these modalities across 6 public datasets and 5 evaluation
metrics. We find symbolic representations achieve the greatest number of
statistically significant wins over both signal and image inputs. We further
ablate the LLM backbone, ECG duration, and token budget, and we evaluate
robustness to signal perturbations. We hope that our findings offer clear
guidance for selecting input representations when developing the next
generation of ELMs.",2025-05-24,"William Han, Chaojing Duan, Zhepeng Cen, Yihang Yao, Xiaoyu Song, Atharva Mhaskar, Dylan Leong, Michael A. Rosenberg, Emerson Liu, Ding Zhao",http://arxiv.org/pdf/2505.18847v1,cs.CL
Multi-Party Conversational Agents: A Survey,"Multi-party Conversational Agents (MPCAs) are systems designed to engage in
dialogue with more than two participants simultaneously. Unlike traditional
two-party agents, designing MPCAs faces additional challenges due to the need
to interpret both utterance semantics and social dynamics. This survey explores
recent progress in MPCAs by addressing three key questions: 1) Can agents model
each participants' mental states? (State of Mind Modeling); 2) Can they
properly understand the dialogue content? (Semantic Understanding); and 3) Can
they reason about and predict future conversation flow? (Agent Action
Modeling). We review methods ranging from classical machine learning to Large
Language Models (LLMs) and multi-modal systems. Our analysis underscores Theory
of Mind (ToM) as essential for building intelligent MPCAs and highlights
multi-modal understanding as a promising yet underexplored direction. Finally,
this survey offers guidance to future researchers on developing more capable
MPCAs.",2025-05-24,"Sagar Sapkota, Mohammad Saqib Hasan, Mubarak Shah, Santu Karmaker",http://arxiv.org/pdf/2505.18845v1,cs.CL
Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation,"We present v1, a lightweight extension to Multimodal Large Language Models
(MLLMs) that enables selective visual revisitation during inference. While
current MLLMs typically consume visual input only once and reason purely over
internal memory, v1 introduces a simple point-and-copy mechanism that allows
the model to dynamically retrieve relevant image regions throughout the
reasoning process. This mechanism augments existing architectures with minimal
modifications, enabling contextual access to visual tokens based on the model's
evolving hypotheses. To train this capability, we construct v1g, a dataset of
300K multimodal reasoning traces with interleaved visual grounding annotations.
Experiments on three multimodal mathematical reasoning benchmarks -- MathVista,
MathVision, and MathVerse -- demonstrate that v1 consistently improves
performance over comparable baselines, particularly on tasks requiring
fine-grained visual reference and multi-step reasoning. Our results suggest
that dynamic visual access is a promising direction for enhancing grounded
multimodal reasoning. Code, models, and data will be released to support future
research.",2025-05-24,"Jiwan Chung, Junhyeok Kim, Siyeol Kim, Jaeyoung Lee, Min Soo Kim, Youngjae Yu",http://arxiv.org/pdf/2505.18842v1,cs.CL
On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization,"Reinforcement learning (RL) has become popular in enhancing the reasoning
capabilities of large language models (LLMs), with Group Relative Policy
Optimization (GRPO) emerging as a widely used algorithm in recent systems.
Despite GRPO's widespread adoption, we identify a previously unrecognized
phenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood
of correct responses marginally increases or even decreases during training.
This behavior mirrors a recently discovered misalignment issue in Direct
Preference Optimization (DPO), attributed to the influence of negative
gradients. We provide a theoretical analysis of GRPO's learning dynamic,
identifying the source of LLD as the naive penalization of all tokens in
incorrect responses with the same strength. To address this, we develop a
method called NTHR, which downweights penalties on tokens contributing to the
LLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's
group-based structure, using correct responses as anchors to identify
influential tokens. Experiments on math reasoning benchmarks demonstrate that
NTHR effectively mitigates LLD, yielding consistent performance gains across
models ranging from 0.5B to 3B parameters.",2025-05-24,"Wenlong Deng, Yi Ren, Muchen Li, Danica J. Sutherland, Xiaoxiao Li, Christos Thrampoulidis",http://arxiv.org/pdf/2505.18830v1,cs.CL
AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting,"Modern large reasoning models demonstrate impressive problem-solving
capabilities by employing sophisticated reasoning strategies. However, they
often struggle to balance efficiency and effectiveness, frequently generating
unnecessarily lengthy reasoning chains for simple problems. In this work, we
propose AdaCtrl, a novel framework to support both difficulty-aware adaptive
reasoning budget allocation and explicit user control over reasoning depth.
AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem
difficulty, while also allowing users to manually control the budget to
prioritize either efficiency or effectiveness. This is achieved through a
two-stage training pipeline: an initial cold-start fine-tuning phase to instill
the ability to self-aware difficulty and adjust reasoning budget, followed by a
difficulty-aware reinforcement learning (RL) stage that refines the model's
adaptive reasoning strategies and calibrates its difficulty assessments based
on its evolving capabilities during online training. To enable intuitive user
interaction, we design explicit length-triggered tags that function as a
natural interface for budget control. Empirical results show that AdaCtrl
adapts reasoning length based on estimated difficulty, compared to the standard
training baseline that also incorporates fine-tuning and RL, it yields
performance improvements and simultaneously reduces response length by 10.06%
and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which
require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K
datasets, where more concise responses are sufficient. Furthermore, AdaCtrl
enables precise user control over the reasoning budget, allowing for tailored
responses to meet specific needs.",2025-05-24,"Shijue Huang, Hongru Wang, Wanjun Zhong, Zhaochen Su, Jiazhan Feng, Bowen Cao, Yi R. Fung",http://arxiv.org/pdf/2505.18822v1,cs.CL
ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models,"Aligning general-purpose large language models (LLMs) to downstream tasks
often incurs significant costs, including constructing task-specific
instruction pairs and extensive training adjustments. Prior research has
explored various avenues to enhance alignment efficiency, primarily through
minimal-data training or data-driven activations to identify key attention
heads. However, these approaches inherently introduce data dependency, which
hinders generalization and reusability. To address this issue and enhance model
alignment efficiency, we propose the \textit{\textbf{A}ttention
\textbf{L}ocalization and \textbf{P}runing \textbf{S}trategy (\textbf{ALPS})},
an efficient algorithm that localizes the most task-sensitive attention heads
and prunes by restricting attention training updates to these heads, thereby
reducing alignment costs. Experimental results demonstrate that our method
activates only \textbf{10\%} of attention parameters during fine-tuning while
achieving a \textbf{2\%} performance improvement over baselines on three tasks.
Moreover, the identified task-specific heads are transferable across datasets
and mitigate knowledge forgetting. Our work and findings provide a novel
perspective on efficient LLM alignment.",2025-05-24,"Hao Chen, Haoze Li, Zhiqing Xiao, Lirong Gao, Qi Zhang, Xiaomeng Hu, Ningtao Wang, Xing Fu, Junbo Zhao",http://arxiv.org/pdf/2505.18799v1,cs.CL
From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?,"Post-processing is crucial for the automatic evaluation of LLMs in
fill-in-the-middle (FIM) code generation due to the frequent presence of
extraneous code in raw outputs. This extraneous generation suggests a lack of
awareness regarding output boundaries, requiring truncation for effective
evaluation. The determination of an optimal truncation strategy, however, often
proves intricate, particularly when the scope includes several programming
languages. This study investigates the necessity of post-processing
instruction-tuned LLM outputs. Our findings reveal that supervised fine-tuning
significantly enhances FIM code generation, enabling LLMs to generate code that
seamlessly integrates with the surrounding context. Evaluating our fine-tuned
\texttt{Qwen2.5-Coder} (base and instruct) models on HumanEval Infilling and
SAFIM benchmarks demonstrates improved performances without post-processing,
especially when the \emph{middle} consist of complete lines. However,
post-processing of the LLM outputs remains necessary when the \emph{middle} is
a random span of code.",2025-05-24,"Wasi Uddin Ahmad, Somshubra Majumdar, Boris Ginsburg",http://arxiv.org/pdf/2505.18789v1,cs.CL
A generalised editor calculus (Short Paper),"In this paper, we present a generalization of a syntax-directed editor
calculus, which can be used to instantiate a specialized syntax-directed editor
for any language, given by some abstract syntax. The editor calculus guarantees
the absence of syntactical errors while allowing incomplete programs. The
generalized editor calculus is then encoded into a simply typed lambda
calculus, extended with pairs, booleans, pattern matching and fixed points",2025-05-24,"Benjamin Bennetzen, Peter Buus Steffensen, Hans Hüttel, Nikolaj Rossander Kristensen, Andreas Tor Mortensen",http://arxiv.org/pdf/2505.18778v1,cs.CL
Disentangling Knowledge Representations for Large Language Model Editing,"Knowledge Editing has emerged as a promising solution for efficiently
updating embedded knowledge in large language models (LLMs). While existing
approaches demonstrate effectiveness in integrating new knowledge and
preserving the original capabilities of LLMs, they fail to maintain
fine-grained irrelevant knowledge facts that share the same subject as edited
knowledge but differ in relation and object. This challenge arises because
subject representations inherently encode multiple attributes, causing the
target and fine-grained irrelevant knowledge to become entangled in the
representation space, and thus vulnerable to unintended alterations during
editing. To address this, we propose DiKE, a novel approach that Disentangles
Knowledge representations for LLM Editing (DiKE). DiKE consists of two key
components: a Knowledge Representation Disentanglement (KRD) module that
decomposes the subject representation into target-knowledgerelated and
-unrelated components, and a Disentanglement-based Knowledge Edit (DKE) module
that updates only the target-related component while explicitly preserving the
unrelated one. We further derive a closed-form, rank-one parameter update based
on matrix theory to enable efficient and minimally invasive edits. To
rigorously evaluate fine-grained irrelevant knowledge preservation, we
construct FINE-KED, a new benchmark comprising fine-grained irrelevant
knowledge at different levels of relational similarity to the edited knowledge.
Extensive experiments across multiple LLMs demonstrate that DiKE substantially
improves fine-grained irrelevant knowledge preservation while maintaining
competitive general editing performance.",2025-05-24,"Mengqi Zhang, Zisheng Zhou, Xiaotian Ye, Qiang Liu, Zhaochun Ren, Zhumin Chen, Pengjie Ren",http://arxiv.org/pdf/2505.18774v1,cs.CL
Towards an automatic method for generating topical vocabulary test forms for specific reading passages,"Background knowledge is typically needed for successful comprehension of
topical and domain specific reading passages, such as in the STEM domain.
However, there are few automated measures of student knowledge that can be
readily deployed and scored in time to make predictions on whether a given
student will likely be able to understand a specific content area text. In this
paper, we present our effort in developing K-tool, an automated system for
generating topical vocabulary tests that measure students' background knowledge
related to a specific text. The system automatically detects the topic of a
given text and produces topical vocabulary items based on their relationship
with the topic. This information is used to automatically generate background
knowledge forms that contain words that are highly related to the topic and
words that share similar features but do not share high associations to the
topic. Prior research indicates that performance on such tasks can help
determine whether a student is likely to understand a particular text based on
their knowledge state. The described system is intended for use with middle and
high school student population of native speakers of English. It is designed to
handle single reading passages and is not dependent on any corpus or text
collection. In this paper, we describe the system architecture and present an
initial evaluation of the system outputs.",2025-05-24,"Michael Flor, Zuowei Wang, Paul Deane, Tenaha O'Reilly",http://arxiv.org/pdf/2505.18762v1,cs.CL
How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark,"We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic
benchmark to evaluate Large Language Models' (LLMs) reasoning robustness
against systematically controlled irrelevant context (IC). GSM-DC constructs
symbolic reasoning graphs with precise distractor injections, enabling
rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are
significantly sensitive to IC, affecting both reasoning path selection and
arithmetic accuracy. Additionally, training models with strong distractors
improves performance in both in-distribution and out-of-distribution scenarios.
We further propose a stepwise tree search guided by a process reward model,
which notably enhances robustness in out-of-distribution conditions.",2025-05-24,"Minglai Yang, Ethan Huang, Liang Zhang, Mihai Surdeanu, William Wang, Liangming Pan",http://arxiv.org/pdf/2505.18761v1,cs.CL
Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection,"In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid
Euclidean Distance with Large Language Models) to improve example selection for
sensor-based classification tasks. While few-shot prompting enables efficient
inference with limited labeled data, its performance largely depends on the
quality of selected examples. HED-LM addresses this challenge through a hybrid
selection pipeline that filters candidate examples based on Euclidean distance
and re-ranks them using contextual relevance scored by large language models
(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection
task using accelerometer data characterized by overlapping patterns and high
inter-subject variability. Unlike simpler tasks such as activity recognition,
fatigue detection demands more nuanced example selection due to subtle
differences in physiological signals. Our experiments show that HED-LM achieves
a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection
(59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These
represent relative improvements of 16.6% and 2.3%, respectively. The results
confirm that combining numerical similarity with contextual relevance improves
the robustness of few-shot prompting. Overall, HED-LM offers a practical
solution to improve performance in real-world sensor-based learning tasks and
shows potential for broader applications in healthcare monitoring, human
activity recognition, and industrial safety scenarios.",2025-05-24,"Elsen Ronando, Sozo Inoue",http://arxiv.org/pdf/2505.18754v1,cs.CL
Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning,"The unusual properties of in-context learning (ICL) have prompted
investigations into the internal mechanisms of large language models. Prior
work typically focuses on either special attention heads or task vectors at
specific layers, but lacks a unified framework linking these components to the
evolution of hidden states across layers that ultimately produce the model's
output. In this paper, we propose such a framework for ICL in classification
tasks by analyzing two geometric factors that govern performance: the
separability and alignment of query hidden states. A fine-grained analysis of
layer-wise dynamics reveals a striking two-stage mechanism: separability
emerges in early layers, while alignment develops in later layers. Ablation
studies further show that Previous Token Heads drive separability, while
Induction Heads and task vectors enhance alignment. Our findings thus bridge
the gap between attention heads and task vectors, offering a unified account of
ICL's underlying mechanisms.",2025-05-24,"Haolin Yang, Hakaze Cho, Yiqiao Zhong, Naoya Inoue",http://arxiv.org/pdf/2505.18752v1,cs.CL
LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges,"Text-to-SQL is a fundamental task in natural language processing that seeks
to translate natural language questions into meaningful and executable SQL
queries. While existing datasets are extensive and primarily focus on business
scenarios and operational logic, they frequently lack coverage of
domain-specific knowledge and complex mathematical reasoning. To address this
gap, we present a novel dataset tailored for complex reasoning and
chain-of-thought analysis in SQL inference, encompassing physical, arithmetic,
commonsense, and hypothetical reasoning. The dataset consists of 4,038 English
questions, each paired with a unique SQL query and accompanied by 12,114
step-by-step reasoning annotations, spanning 45 databases across diverse
domains. Experimental results demonstrate that LogicCat substantially increases
the difficulty for state-of-the-art models, with the highest execution accuracy
reaching only 14.96%. Incorporating our chain-of-thought annotations boosts
performance to 33.96%. Benchmarking leading public methods on Spider and BIRD
further underscores the unique challenges presented by LogicCat, highlighting
the significant opportunities for advancing research in robust,
reasoning-driven text-to-SQL systems. We have released our dataset code at
https://github.com/Ffunkytao/LogicCat.",2025-05-24,"Tao Liu, Hongying Zan, Yifan Li, Dixuan Zhang, Lulu Kong, Haixin Liu, Jiaming Hou, Aoze Zheng, Rui Li, Yiming Qiao, Zewei Luo, Qi Wang, Zhiqiang Zhang, Jiaxi Li, Supeng Liu, Kunli Zhang, Min Peng",http://arxiv.org/pdf/2505.18744v1,cs.CL
Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers,"Speech-based Parkinson's disease (PD) detection has gained attention for its
automated, cost-effective, and non-intrusive nature. As research studies
usually rely on data from diagnostic-oriented speech tasks, this work explores
the feasibility of diagnosing PD on the basis of speech data not originally
intended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our
findings indicate that TT can be as useful as diagnostic-oriented PD datasets
like PC-GITA. We also investigate which specific dataset characteristics impact
PD classification performance. The results show that concatenating audio
recordings and balancing participants' gender and status distributions can be
beneficial. Cross-dataset evaluation reveals that models trained on PC-GITA
generalize poorly to TT, whereas models trained on TT perform better on
PC-GITA. Furthermore, we provide insights into the high variability across
folds, which is mainly due to large differences in individual speaker
performance.",2025-05-24,"Terry Yi Zhong, Esther Janse, Cristian Tejedor-Garcia, Louis ten Bosch, Martha Larson",http://arxiv.org/pdf/2505.18722v1,cs.CL
Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization,"Direct Preference Optimization (DPO) has emerged as a promising framework for
aligning Large Language Models (LLMs) with human preferences by directly
optimizing the log-likelihood difference between chosen and rejected responses.
However, existing methods assign equal importance to all tokens in the
response, while humans focus on more meaningful parts. This leads to suboptimal
preference optimization, as irrelevant or noisy tokens disproportionately
influence DPO loss. To address this limitation, we propose \textbf{O}ptimal
\textbf{T}ransport-based token weighting scheme for enhancing direct
\textbf{P}reference \textbf{O}ptimization (OTPO). By emphasizing semantically
meaningful token pairs and de-emphasizing less relevant ones, our method
introduces a context-aware token weighting scheme that yields a more
contrastive reward difference estimate. This adaptive weighting enhances reward
stability, improves interpretability, and ensures that preference optimization
focuses on meaningful differences between responses. Extensive experiments have
validated OTPO's effectiveness in improving instruction-following ability
across various settings\footnote{Code is available at
https://github.com/Mimasss2/OTPO.}.",2025-05-24,"Meng Li, Guangda Huzhang, Haibo Zhang, Xiting Wang, Anxiang Zeng",http://arxiv.org/pdf/2505.18720v1,cs.CL
Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer,"Foundation models and their checkpoints have significantly advanced deep
learning, boosting performance across various applications. However, fine-tuned
models often struggle outside their specific domains and exhibit considerable
redundancy. Recent studies suggest that combining a pruned fine-tuned model
with the original pre-trained model can mitigate forgetting, reduce
interference when merging model parameters across tasks, and improve
compression efficiency. In this context, developing an effective pruning
strategy for fine-tuned models is crucial. Leveraging the advantages of the
task vector mechanism, we preprocess fine-tuned models by calculating the
differences between them and the original model. Recognizing that different
task vector subspaces contribute variably to model performance, we introduce a
novel method called Neural Parameter Search (NPS-Pruning) for slimming down
fine-tuned models. This method enhances pruning efficiency by searching through
neural parameters of task vectors within low-rank subspaces. Our method has
three key applications: enhancing knowledge transfer through pairwise model
interpolation, facilitating effective knowledge fusion via model merging, and
enabling the deployment of compressed models that retain near-original
performance while significantly reducing storage costs. Extensive experiments
across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness
and robustness of our approach, resulting in substantial performance gains. The
code is publicly available at: https://github.com/duguodong7/NPS-Pruning.",2025-05-24,"Guodong Du, Zitao Fang, Jing Li, Junlin Li, Runhua Jiang, Shuyang Yu, Yifei Guo, Yangneng Chen, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Honghai Liu, Min Zhang",http://arxiv.org/pdf/2505.18713v1,cs.CL
"Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla","Bangla or Bengali is the national language of Bangladesh, people from
different regions don't talk in proper Bangla. Every division of Bangladesh has
its own local language like Sylheti, Chittagong etc. In recent years some
papers were published on Bangla language like sentiment analysis, fake news
detection and classifications, but a few of them were on Bangla languages. This
research is for the local language and this particular paper is on Sylheti
language. It presented a comprehensive system using Natural Language Processing
or NLP techniques for translating Pure or Modern Bangla to locally spoken
Sylheti Bangla language. Total 1200 data used for training 3 models LSTM,
Bi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%
accuracy. The findings of this research may contribute to the growth of Bangla
NLP researchers for future more advanced innovations.",2025-05-24,"Sourav Kumar Das, Md. Julkar Naeen, MD. Jahidul Islam, Md. Anisul Haque Sajeeb, Narayan Ranjan Chakraborty, Mayen Uddin Mojumdar",http://arxiv.org/pdf/2505.18709v1,cs.CL
A General Knowledge Injection Framework for ICD Coding,"ICD Coding aims to assign a wide range of medical codes to a medical text
document, which is a popular and challenging task in the healthcare domain. To
alleviate the problems of long-tail distribution and the lack of annotations of
code-specific evidence, many previous works have proposed incorporating code
knowledge to improve coding performance. However, existing methods often focus
on a single type of knowledge and design specialized modules that are complex
and incompatible with each other, thereby limiting their scalability and
effectiveness. To address this issue, we propose GKI-ICD, a novel, general
knowledge injection framework that integrates three key types of knowledge,
namely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized
design of additional modules. The comprehensive utilization of the above
knowledge, which exhibits both differences and complementarity, can effectively
enhance the ICD coding performance. Extensive experiments on existing popular
ICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves
the state-of-the-art performance on most evaluation metrics. Code is available
at https://github.com/xuzhang0112/GKI-ICD.",2025-05-24,"Xu Zhang, Kun Zhang, Wenxin Ma, Rongsheng Wang, Chenxu Wu, Yingtai Li, S. Kevin Zhou",http://arxiv.org/pdf/2505.18708v1,cs.CL
Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task,"This paper introduces the Unified Opinion Concepts (UOC) ontology to
integrate opinions within their semantic context. The UOC ontology bridges the
gap between the semantic representation of opinion across different
formulations. It is a unified conceptualisation based on the facets of opinions
studied extensively in NLP and semantic structures described through symbolic
descriptions. We further propose the Unified Opinion Concept Extraction (UOCE)
task of extracting opinions from the text with enhanced expressivity.
Additionally, we provide a manually extended and re-annotated evaluation
dataset for this task and tailored evaluation metrics to assess the adherence
of extracted opinions to UOC semantics. Finally, we establish baseline
performance for the UOCE task using state-of-the-art generative models.",2025-05-24,"Gaurav Negi, Dhairya Dalal, Omnia Zayed, Paul Buitelaar",http://arxiv.org/pdf/2505.18703v1,cs.CL
Benchmarking and Rethinking Knowledge Editing for Large Language Models,"Knowledge editing aims to update the embedded knowledge within Large Language
Models (LLMs). However, existing approaches, whether through parameter
modification or external memory integration, often suffer from inconsistent
evaluation objectives and experimental setups. To address this gap, we conduct
a comprehensive benchmarking study. In addition to fact-level datasets, we
introduce more complex event-based datasets and general-purpose datasets drawn
from other tasks. Our evaluation covers both instruction-tuned and
reasoning-oriented LLMs, under a realistic autoregressive inference setting
rather than teacher-forced decoding. Beyond single-edit assessments, we also
evaluate multi-edit scenarios to better reflect practical demands. We employ
four evaluation dimensions, including portability, and compare all recent
methods against a simple and straightforward baseline named Selective
Contextual Reasoning (SCR). Empirical results reveal that parameter-based
editing methods perform poorly under realistic conditions. In contrast, SCR
consistently outperforms them across all settings. This study offers new
insights into the limitations of current knowledge editing methods and
highlights the potential of context-based reasoning as a more robust
alternative.",2025-05-24,"Guoxiu He, Xin Song, Futing Wang, Aixin Sun",http://arxiv.org/pdf/2505.18690v1,cs.CL
Large Language Models in the Task of Automatic Validation of Text Classifier Predictions,"Machine learning models for text classification are trained to predict a
class for a given text. To do this, training and validation samples must be
prepared: a set of texts is collected, and each text is assigned a class. These
classes are usually assigned by human annotators with different expertise
levels, depending on the specific classification task. Collecting such samples
from scratch is labor-intensive because it requires finding specialists and
compensating them for their work; moreover, the number of available specialists
is limited, and their productivity is constrained by human factors. While it
may not be too resource-intensive to collect samples once, the ongoing need to
retrain models (especially in incremental learning pipelines) to address data
drift (also called model drift) makes the data collection process crucial and
costly over the model's entire lifecycle. This paper proposes several
approaches to replace human annotators with Large Language Models (LLMs) to
test classifier predictions for correctness, helping ensure model quality and
support high-quality incremental learning.",2025-05-24,Aleksandr Tsymbalov,http://arxiv.org/pdf/2505.18688v1,cs.CL
From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation,"Infodemics and health misinformation have significant negative impact on
individuals and society, exacerbating confusion and increasing hesitancy in
adopting recommended health measures. Recent advancements in generative AI,
capable of producing realistic, human like text and images, have significantly
accelerated the spread and expanded the reach of health misinformation,
resulting in an alarming surge in its dissemination. To combat the infodemics,
most existing work has focused on developing misinformation datasets from
social media and fact checking platforms, but has faced limitations in topical
coverage, inclusion of AI generation, and accessibility of raw content. To
address these issues, we present MM Health, a large scale multimodal
misinformation dataset in the health domain consisting of 34,746 news article
encompassing both textual and visual information. MM Health includes
human-generated multimodal information (5,776 articles) and AI generated
multimodal information (28,880 articles) from various SOTA generative AI
models. Additionally, We benchmarked our dataset against three tasks
(reliability checks, originality checks, and fine-grained AI detection)
demonstrating that existing SOTA models struggle to accurately distinguish the
reliability and origin of information. Our dataset aims to support the
development of misinformation detection across various health scenarios,
facilitating the detection of human and machine generated content at multimodal
levels.",2025-05-24,"Zhihao Zhang, Yiran Zhang, Xiyue Zhou, Liting Huang, Imran Razzak, Preslav Nakov, Usman Naseem",http://arxiv.org/pdf/2505.18685v1,cs.CL
TULUN: Transparent and Adaptable Low-resource Machine Translation,"Machine translation (MT) systems that support low-resource languages often
struggle on specialized domains. While researchers have proposed various
techniques for domain adaptation, these approaches typically require model
fine-tuning, making them impractical for non-technical users and small
organizations. To address this gap, we propose Tulun, a versatile solution for
terminology-aware translation, combining neural MT with large language model
(LLM)-based post-editing guided by existing glossaries and translation
memories. Our open-source web-based platform enables users to easily create,
edit, and leverage terminology resources, fostering a collaborative
human-machine translation process that respects and incorporates domain
expertise while increasing MT accuracy. Evaluations show effectiveness in both
real-world and benchmark scenarios: on medical and disaster relief translation
tasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41
ChrF++ points over baseline MT systems. Across six low-resource languages on
the FLORES dataset, Tulun outperforms both standalone MT and LLM approaches,
achieving an average improvement of 2.8 ChrF points over NLLB-54B.",2025-05-24,"Raphaël Merx, Hanna Suominen, Lois Hong, Nick Thieberger, Trevor Cohn, Ekaterina Vylomova",http://arxiv.org/pdf/2505.18683v1,cs.CL
$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models,"Large Language Models (LLMs), due to substantial computational requirements,
are vulnerable to resource consumption attacks, which can severely degrade
server performance or even cause crashes, as demonstrated by denial-of-service
(DoS) attacks designed for LLMs. However, existing works lack mitigation
strategies against such threats, resulting in unresolved security risks for
real-world LLM deployments. To this end, we propose the Pluggable and Dynamic
DoS-Defense Framework ($PD^3F$), which employs a two-stage approach to defend
against resource consumption attacks from both the input and output sides. On
the input side, we propose the Resource Index to guide Dynamic Request Polling
Scheduling, thereby reducing resource usage induced by malicious attacks under
high-concurrency scenarios. On the output side, we introduce the Adaptive
End-Based Suppression mechanism, which terminates excessive malicious
generation early. Experiments across six models demonstrate that $PD^3F$
significantly mitigates resource consumption attacks, improving users' access
capacity by up to 500% during adversarial load. $PD^3F$ represents a step
toward the resilient and resource-aware deployment of LLMs against resource
consumption attacks.",2025-05-24,"Yuanhe Zhang, Xinyue Wang, Haoran Gao, Zhenhong Zhou, Fanyu Meng, Yuyao Zhang, Sen Su",http://arxiv.org/pdf/2505.18680v1,cs.CL
Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts,"Clarifying the research framing of NLP artefacts (e.g., models, datasets,
etc.) is crucial to aligning research with practical applications. Recent
studies manually analyzed NLP research across domains, showing that few papers
explicitly identify key stakeholders, intended uses, or appropriate contexts.
In this work, we propose to automate this analysis, developing a
three-component system that infers research framings by first extracting key
elements (means, ends, stakeholders), then linking them through interpretable
rules and contextual reasoning. We evaluate our approach on two domains:
automated fact-checking using an existing dataset, and hate speech detection
for which we annotate a new dataset-achieving consistent improvements over
strong LLM baselines. Finally, we apply our system to recent automated
fact-checking papers and uncover three notable trends: a rise in vague or
underspecified research goals, increased emphasis on scientific exploration
over application, and a shift toward supporting human fact-checkers rather than
pursuing full automation.",2025-05-24,"Eric Chamoun, Nedjma Ousidhoum, Michael Schlichtkrull, Andreas Vlachos",http://arxiv.org/pdf/2505.18677v1,cs.CL
Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps,"Multimodal large language models (MLLMs) have recently achieved significant
progress in visual tasks, including semantic scene understanding and text-image
alignment, with reasoning variants enhancing performance on complex tasks
involving mathematics and logic. However, their capacity for reasoning tasks
involving fine-grained visual understanding remains insufficiently evaluated.
To address this gap, we introduce ReasonMap, a benchmark designed to assess the
fine-grained visual understanding and spatial reasoning abilities of MLLMs.
ReasonMap encompasses high-resolution transit maps from 30 cities across 13
countries and includes 1,008 question-answer pairs spanning two question types
and three templates. Furthermore, we design a two-level evaluation pipeline
that properly assesses answer correctness and quality. Comprehensive
evaluations of 15 popular MLLMs, including both base and reasoning variants,
reveal a counterintuitive pattern: among open-source models, base models
outperform reasoning ones, while the opposite trend is observed in
closed-source models. Additionally, performance generally degrades when visual
inputs are masked, indicating that while MLLMs can leverage prior knowledge to
answer some questions, fine-grained visual reasoning tasks still require
genuine visual perception for strong performance. Our benchmark study offers
new insights into visual reasoning and contributes to investigating the gap
between open-source and closed-source models.",2025-05-24,"Sicheng Feng, Song Wang, Shuyi Ouyang, Lingdong Kong, Zikai Song, Jianke Zhu, Huan Wang, Xinchao Wang",http://arxiv.org/pdf/2505.18675v1,cs.CL
Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models,"Large Language Models (LLMs) have achieved remarkable success in Natural
Language Processing (NLP), yet their cross-lingual performance consistency
remains a significant challenge. This paper introduces a novel methodology for
efficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach
leverages beam search and LLM-based simulation to generate bilingual question
pairs that expose performance discrepancies between English and target
languages. We construct a new dataset of over 6,000 bilingual pairs across 16
languages using this methodology, demonstrating its effectiveness in revealing
weaknesses even in state-of-the-art models. The extensive experiments
demonstrate that our method precisely and cost-effectively pinpoints
cross-lingual weaknesses, consistently revealing over 50\% accuracy drops in
target languages across a wide range of models. Moreover, further experiments
investigate the relationship between linguistic similarity and cross-lingual
weaknesses, revealing that linguistically related languages share similar
performance patterns and benefit from targeted post-training. Code is available
at https://github.com/xzx34/Cross-Lingual-Pitfalls.",2025-05-24,"Zixiang Xu, Yanbo Wang, Yue Huang, Xiuying Chen, Jieyu Zhao, Meng Jiang, Xiangliang Zhang",http://arxiv.org/pdf/2505.18673v1,cs.CL
ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation,"Infographic charts are a powerful medium for communicating abstract data by
combining visual elements (e.g., charts, images) with textual information.
However, their visual and structural richness poses challenges for large
vision-language models (LVLMs), which are typically trained on plain charts. To
bridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to
advance the understanding and generation of infographic charts. The dataset is
constructed through an inductive process that identifies 75 chart types, 330
chart variations, and 68 layout templates from real infographic charts and uses
them to create synthetic ones programmatically. We showcase the utility of this
dataset through: 1) improving infographic chart understanding via fine-tuning,
2) benchmarking code generation for infographic charts, and 3) enabling
example-based infographic chart generation. By capturing the visual and
structural complexity of real design, ChartGalaxy provides a useful resource
for enhancing multimodal reasoning and generation in LVLMs.",2025-05-24,"Zhen Li, Yukai Guo, Duan Li, Xinyuan Guo, Bowen Li, Lanxi Xiao, Shenyu Qiao, Jiashu Chen, Zijian Wu, Hui Zhang, Xinhuan Shu, Shixia Liu",http://arxiv.org/pdf/2505.18668v1,cs.CL
Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics,"Large Language Models (LLMs) have emerged as a promising cornerstone for the
development of natural language processing (NLP) and artificial intelligence
(AI). However, ensuring the robustness of LLMs remains a critical challenge. To
address these challenges and advance the field, this survey provides a
comprehensive overview of current studies in this area. First, we
systematically examine the nature of robustness in LLMs, including its
conceptual foundations, the importance of consistent performance across diverse
inputs, and the implications of failure modes in real-world applications. Next,
we analyze the sources of non-robustness, categorizing intrinsic model
limitations, data-driven vulnerabilities, and external adversarial factors that
compromise reliability. Following this, we review state-of-the-art mitigation
strategies, and then we discuss widely adopted benchmarks, emerging metrics,
and persistent gaps in assessing real-world reliability. Finally, we synthesize
findings from existing surveys and interdisciplinary studies to highlight
trends, unresolved issues, and pathways for future research.",2025-05-24,"Pankaj Kumar, Subhankar Mishra",http://arxiv.org/pdf/2505.18658v1,cs.CL
Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change,"Climate-Eval is a comprehensive benchmark designed to evaluate natural
language processing models across a broad range of tasks related to climate
change. Climate-Eval aggregates existing datasets along with a newly developed
news classification dataset, created specifically for this release. This
results in a benchmark of 25 tasks based on 13 datasets, covering key aspects
of climate discourse, including text classification, question answering, and
information extraction. Our benchmark provides a standardized evaluation suite
for systematically assessing the performance of large language models (LLMs) on
these tasks. Additionally, we conduct an extensive evaluation of open-source
LLMs (ranging from 2B to 70B parameters) in both zero-shot and few-shot
settings, analyzing their strengths and limitations in the domain of climate
change.",2025-05-24,"Murathan Kurfalı, Shorouq Zahra, Joakim Nivre, Gabriele Messori",http://arxiv.org/pdf/2505.18653v1,cs.CL
On the Emergence of Linear Analogies in Word Embeddings,"Models such as Word2Vec and GloVe construct word embeddings based on the
co-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The
resulting vectors $W_i$ not only group semantically similar words but also
exhibit a striking linear analogy structure -- for example, $W_{\text{king}} -
W_{\text{man}} + W_{\text{woman}} \approx W_{\text{queen}}$ -- whose
theoretical origin remains unclear. Previous observations indicate that this
analogy structure: (i) already emerges in the top eigenvectors of the matrix
$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more
eigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are
included, (iii) is enhanced when using $\log M(i,j)$ rather than $M(i,j)$, and
(iv) persists even when all word pairs involved in a specific analogy relation
(e.g., king-queen, man-woman) are removed from the corpus. To explain these
phenomena, we introduce a theoretical generative model in which words are
defined by binary semantic attributes, and co-occurrence probabilities are
derived from attribute-based interactions. This model analytically reproduces
the emergence of linear analogy structure and naturally accounts for properties
(i)-(iv). It can be viewed as giving fine-grained resolution into the role of
each additional embedding dimension. It is robust to various forms of noise and
agrees well with co-occurrence statistics measured on Wikipedia and the analogy
benchmark introduced by Mikolov et al.",2025-05-24,"Daniel J. Korchinski, Dhruva Karkada, Yasaman Bahri, Matthieu Wyart",http://arxiv.org/pdf/2505.18651v1,cs.CL
SEW: Self-Evolving Agentic Workflows for Automated Code Generation,"Large Language Models (LLMs) have demonstrated effectiveness in code
generation tasks. To enable LLMs to address more complex coding challenges,
existing research has focused on crafting multi-agent systems with agentic
workflows, where complex coding tasks are decomposed into sub-tasks, assigned
to specialized agents. Despite their effectiveness, current approaches heavily
rely on hand-crafted agentic workflows, with both agent topologies and prompts
manually designed, which limits their ability to automatically adapt to
different types of coding problems. To address these limitations and enable
automated workflow design, we propose \textbf{S}elf-\textbf{E}volving
\textbf{W}orkflow (\textbf{SEW}), a novel self-evolving framework that
automatically generates and optimises multi-agent workflows. Extensive
experiments on three coding benchmark datasets, including the challenging
LiveCodeBench, demonstrate that our SEW can automatically design agentic
workflows and optimise them through self-evolution, bringing up to 33\%
improvement on LiveCodeBench compared to using the backbone LLM only.
Furthermore, by investigating different representation schemes of workflow, we
provide insights into the optimal way to encode workflow information with text.",2025-05-24,"Siwei Liu, Jinyuan Fang, Han Zhou, Yingxu Wang, Zaiqiao Meng",http://arxiv.org/pdf/2505.18646v1,cs.CL
Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving,"Large language models (LLMs) have shown remarkable generalization across
tasks, leading to increased interest in integrating speech with LLMs. These
speech LLMs (SLLMs) typically use supervised fine-tuning to align speech with
text-based LLMs. However, the lack of annotated speech data across a wide range
of tasks hinders alignment efficiency, resulting in poor generalization. To
address these issues, we propose a novel multi-task 'behavior imitation' method
with speech-text interleaving, called MTBI, which relies solely on paired
speech and transcripts. By ensuring the LLM decoder generates equivalent
responses to paired speech and text, we achieve a more generalized SLLM.
Interleaving is used to further enhance alignment efficiency. We introduce a
simple benchmark to evaluate prompt and task generalization across different
models. Experimental results demonstrate that our MTBI outperforms SOTA SLLMs
on both prompt and task generalization, while requiring less supervised speech
data.",2025-05-24,"Jingran Xie, Xiang Li, Hui Wang, Yue Yu, Yang Xiang, Xixin Wu, Zhiyong Wu",http://arxiv.org/pdf/2505.18644v1,cs.CL
Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster,"Chain-of-thought (CoT) distillation allows a large language model (LLM) to
guide a small language model (SLM) in reasoning tasks. Existing methods train
the SLM to learn the long rationale in one iteration, resulting in two issues:
1) Long rationales lead to a large token-level batch size during training,
making gradients of core reasoning tokens (i.e., the token will directly affect
the correctness of subsequent reasoning) over-smoothed as they contribute a
tiny fraction of the rationale. As a result, the SLM converges to sharp minima
where it fails to grasp the reasoning logic. 2) The response is slow, as the
SLM must generate a long rationale before reaching the answer. Therefore, we
propose chunk-wise training (CWT), which uses a heuristic search to divide the
rationale into internal semantically coherent chunks and focuses SLM on
learning from only one chunk per iteration. In this way, CWT naturally isolates
non-reasoning chunks that do not involve the core reasoning token (e.g.,
summary and transitional chunks) from the SLM learning for reasoning chunks,
making the fraction of the core reasoning token increase in the corresponding
iteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes
the SLM automatically skip non-reasoning medium chunks to reach the answer,
improving reasoning speed while maintaining accuracy. We validate our approach
on a variety of SLMs and multiple reasoning tasks.",2025-05-24,"Xiao Chen, Sihang Zhou, Ke Liang, Xiaoyu Sun, Xinwang Liu",http://arxiv.org/pdf/2505.18642v1,cs.CL
Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models,"In this work, we provide DZEN, a dataset of parallel Dzongkha and English
test questions for Bhutanese middle and high school students. The over 5K
questions in our collection span a variety of scientific topics and include
factual, application, and reasoning-based questions. We use our parallel
dataset to test a number of Large Language Models (LLMs) and find a significant
performance difference between the models in English and Dzongkha. We also look
at different prompting strategies and discover that Chain-of-Thought (CoT)
prompting works well for reasoning questions but less well for factual ones. We
also find that adding English translations enhances the precision of Dzongkha
question responses. Our results point to exciting avenues for further study to
improve LLM performance in Dzongkha and, more generally, in low-resource
languages. We release the dataset at:
https://github.com/kraritt/llm_dzongkha_evaluation.",2025-05-24,"Md. Tanzib Hosain, Rajan Das Gupta, Md. Kishor Morol",http://arxiv.org/pdf/2505.18638v1,cs.CL
DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation,"Large Language Models (LLMs) demonstrate strong generalization and reasoning
abilities, making them well-suited for complex decision-making tasks such as
medical consultation (MC). However, existing LLM-based methods often fail to
capture the dual nature of MC, which entails two distinct sub-tasks: symptom
inquiry, a sequential decision-making process, and disease diagnosis, a
classification problem. This mismatch often results in ineffective symptom
inquiry and unreliable disease diagnosis. To address this, we propose
\textbf{DDO}, a novel LLM-based framework that performs
\textbf{D}ual-\textbf{D}ecision \textbf{O}ptimization by decoupling and
independently optimizing the the two sub-tasks through a collaborative
multi-agent workflow. Experiments on three real-world MC datasets show that DDO
consistently outperforms existing LLM-based approaches and achieves competitive
performance with state-of-the-art generation-based methods, demonstrating its
effectiveness in the MC task.",2025-05-24,"Zhihao Jia, Mingyi Jia, Junwen Duan, Jianxin Wang",http://arxiv.org/pdf/2505.18630v1,cs.CL
MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation,"Lyrics translation requires both accurate semantic transfer and preservation
of musical rhythm, syllabic structure, and poetic style. In animated musicals,
the challenge intensifies due to alignment with visual and auditory cues. We
introduce Multilingual Audio-Video Lyrics Benchmark for Animated Song
Translation (MAVL), the first multilingual, multimodal benchmark for singable
lyrics translation. By integrating text, audio, and video, MAVL enables richer
and more expressive translations than text-only approaches. Building on this,
we propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought
SylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints
to produce natural-sounding lyrics. Experimental results demonstrate that
SylAVL-CoT significantly outperforms text-based models in singability and
contextual accuracy, emphasizing the value of multimodal, multilingual
approaches for lyrics translation.",2025-05-24,"Woohyun Cho, Youngmin Kim, Sunghyun Lee, Youngjae Yu",http://arxiv.org/pdf/2505.18614v1,cs.CL
PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs,"Recently, significant progress has been made in developing reasoning-capable
Large Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.
However, this long-CoT reasoning process imposes substantial memory overhead
due to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache
quantization has emerged as a promising compression technique and has been
extensively studied in short-context scenarios. However, directly applying
existing methods to long-CoT LLMs causes significant performance degradation
due to the following two reasons: (1) Large cumulative error: Existing methods
fail to adequately leverage available memory, and they directly quantize the KV
Cache during each decoding step, leading to large cumulative quantization
error. (2) Short-context calibration: Due to Rotary Positional Embedding
(RoPE), the use of short-context data during calibration fails to account for
the distribution of less frequent channels in the Key Cache, resulting in
performance loss. We propose Progressive Mixed-Precision KV Cache Quantization
(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To
reduce cumulative error, we design a progressive quantization strategy to
gradually lower the bit-width of KV Cache in each block. Then, we propose
block-wise memory allocation to assign a higher bit-width to more sensitive
transformer blocks. (2) To increase the calibration length without additional
overhead, we propose a new calibration strategy with positional interpolation
that leverages short calibration data with positional interpolation to
approximate the data distribution of long-context data. Extensive experiments
on 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark
performance by up to 8% over SOTA baselines under the same memory budget. Our
code is available at https://github.com/thu-nics/PM-KVQ.",2025-05-24,"Tengxuan Liu, Shiyao Li, Jiayi Yang, Tianchen Zhao, Feng Zhou, Xiaohui Song, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang",http://arxiv.org/pdf/2505.18610v1,cs.CL
RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations,"We introduce RASMALAI, a large-scale speech dataset with rich text
descriptions, designed to advance controllable and expressive text-to-speech
(TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours
of speech and 24 million text-description annotations with fine-grained
attributes like speaker identity, accent, emotion, style, and background
conditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source,
text-description-guided TTS for Indian languages. Systematic evaluation
demonstrates its ability to generate high-quality speech for named speakers,
reliably follow text descriptions and accurately synthesize specified
attributes. Additionally, it effectively transfers expressive characteristics
both within and across languages. IndicParlerTTS consistently achieves strong
performance across these evaluations, setting a new standard for controllable
multilingual expressive speech synthesis in Indian languages.",2025-05-24,"Ashwin Sankar, Yoach Lacombe, Sherry Thomas, Praveen Srinivasa Varadhan, Sanchit Gandhi, Mitesh M Khapra",http://arxiv.org/pdf/2505.18609v1,cs.CL
"Flex-Judge: Think Once, Judge Anywhere","Human-generated reward signals are critical for aligning generative models
with human preferences, guiding both training and inference-time evaluations.
While large language models (LLMs) employed as proxy evaluators, i.e.,
LLM-as-a-Judge, significantly reduce the costs associated with manual
annotations, they typically require extensive modality-specific training data
and fail to generalize well across diverse multimodal tasks. In this paper, we
propose Flex-Judge, a reasoning-guided multimodal judge model that leverages
minimal textual reasoning data to robustly generalize across multiple
modalities and evaluation formats. Our core intuition is that structured
textual reasoning explanations inherently encode generalizable decision-making
patterns, enabling an effective transfer to multimodal judgments, e.g., with
images or videos. Empirical results demonstrate that Flex-Judge, despite being
trained on significantly fewer text data, achieves competitive or superior
performance compared to state-of-the-art commercial APIs and extensively
trained multimodal evaluators. Notably, Flex-Judge presents broad impact in
modalities like molecule, where comprehensive evaluation benchmarks are scarce,
underscoring its practical value in resource-constrained domains. Our framework
highlights reasoning-based text supervision as a powerful, cost-effective
alternative to traditional annotation-intensive approaches, substantially
advancing scalable multimodal model-as-a-judge.",2025-05-24,"Jongwoo Ko, Sungnyun Kim, Sungwoo Cho, Se-Young Yun",http://arxiv.org/pdf/2505.18601v1,cs.CL
Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models,"The proliferation of misinformation in digital platforms reveals the
limitations of traditional detection methods, which mostly rely on static
classification and fail to capture the intricate process of real-world
fact-checking. Despite advancements in Large Language Models (LLMs) that
enhance automated reasoning, their application to misinformation detection
remains hindered by issues of logical inconsistency and superficial
verification. In response, we introduce Debate-to-Detect (D2D), a novel
Multi-Agent Debate (MAD) framework that reformulates misinformation detection
as a structured adversarial debate. Inspired by fact-checking workflows, D2D
assigns domain-specific profiles to each agent and orchestrates a five-stage
debate process, including Opening Statement, Rebuttal, Free Debate, Closing
Statement, and Judgment. To transcend traditional binary classification, D2D
introduces a multi-dimensional evaluation mechanism that assesses each claim
across five distinct dimensions: Factuality, Source Reliability, Reasoning
Quality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets
demonstrate significant improvements over baseline methods, and the case study
highlight D2D's capability to iteratively refine evidence while improving
decision transparency, representing a substantial advancement towards robust
and interpretable misinformation detection. The code will be open-sourced in a
future release.",2025-05-24,"Chen Han, Wenzhen Zheng, Xijin Tang",http://arxiv.org/pdf/2505.18596v1,cs.CL
Safety Alignment via Constrained Knowledge Unlearning,"Despite significant progress in safety alignment, large language models
(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms
have not fully deleted harmful knowledge in LLMs, which allows such attacks to
bypass safeguards and produce harmful outputs. To address this challenge, we
propose a novel safety alignment strategy, Constrained Knowledge Unlearning
(CKU), which focuses on two primary objectives: knowledge localization and
retention, and unlearning harmful knowledge. CKU works by scoring neurons in
specific multilayer perceptron (MLP) layers to identify a subset U of neurons
associated with useful knowledge. During the unlearning process, CKU prunes the
gradients of neurons in U to preserve valuable knowledge while effectively
mitigating harmful content. Experimental results demonstrate that CKU
significantly enhances model safety without compromising overall performance,
offering a superior balance between safety and utility compared to existing
methods. Additionally, our analysis of neuron knowledge sensitivity across
various MLP layers provides valuable insights into the mechanics of safety
alignment and model knowledge editing.",2025-05-24,"Zesheng Shi, Yucheng Zhou, Jing Li",http://arxiv.org/pdf/2505.18588v1,cs.CL
RvLLM: LLM Runtime Verification with Domain Knowledge,"Large language models (LLMs) have emerged as a dominant AI paradigm due to
their exceptional text understanding and generation capabilities. However,
their tendency to generate inconsistent or erroneous outputs challenges their
reliability, especially in high-stakes domains requiring accuracy and
trustworthiness. Existing research primarily focuses on detecting and
mitigating model misbehavior in general-purpose scenarios, often overlooking
the potential of integrating domain-specific knowledge. In this work, we
advance misbehavior detection by incorporating domain knowledge. The core idea
is to design a general specification language that enables domain experts to
customize domain-specific predicates in a lightweight and intuitive manner,
supporting later runtime verification of LLM outputs. To achieve this, we
design a novel specification language, ESL, and introduce a runtime
verification framework, RvLLM, to validate LLM output against domain-specific
constraints defined in ESL. We evaluate RvLLM on three representative tasks:
violation detection against Singapore Rapid Transit Systems Act, numerical
comparison, and inequality solving. Experimental results demonstrate that RvLLM
effectively detects erroneous outputs across various LLMs in a lightweight and
flexible manner. The results reveal that despite their impressive capabilities,
LLMs remain prone to low-level errors due to limited interpretability and a
lack of formal guarantees during inference, and our framework offers a
potential long-term solution by leveraging expert domain knowledge to
rigorously and efficiently verify LLM outputs.",2025-05-24,"Yedi Zhang, Sun Yi Emma, Annabelle Lee Jia En, Annabelle Lee Jia En, Jin Song Dong",http://arxiv.org/pdf/2505.18585v1,cs.CL
Removal of Hallucination on Hallucination: Debate-Augmented RAG,"Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating
external knowledge, yet it introduces a critical issue: erroneous or biased
retrieval can mislead generation, compounding hallucinations, a phenomenon we
term Hallucination on Hallucination. To address this, we propose
Debate-Augmented RAG (DRAG), a training-free framework that integrates
Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages.
In retrieval, DRAG employs structured debates among proponents, opponents, and
judges to refine retrieval quality and ensure factual reliability. In
generation, DRAG introduces asymmetric information roles and adversarial
debates, enhancing reasoning robustness and mitigating factual inconsistencies.
Evaluations across multiple tasks demonstrate that DRAG improves retrieval
reliability, reduces RAG-induced hallucinations, and significantly enhances
overall factual accuracy. Our code is available at
https://github.com/Huenao/Debate-Augmented-RAG.",2025-05-24,"Wentao Hu, Wengyu Zhang, Yiyang Jiang, Chen Jason Zhang, Xiaoyong Wei, Qing Li",http://arxiv.org/pdf/2505.18581v1,cs.CL
Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs,"Reasoning large language models (LLMs) excel in complex tasks, which has
drawn significant attention to reinforcement learning (RL) for LLMs. However,
existing approaches allocate an equal number of rollouts to all questions
during the RL process, which is inefficient. This inefficiency stems from the
fact that training on simple questions yields limited gains, whereas more
rollouts are needed for challenging questions to sample correct answers.
Furthermore, while RL improves response precision, it limits the model's
exploration ability, potentially resulting in a performance cap below that of
the base model prior to RL. To address these issues, we propose a mechanism for
dynamically allocating rollout budgets based on the difficulty of the problems,
enabling more efficient RL training. Additionally, we introduce an adaptive
dynamic temperature adjustment strategy to maintain the entropy at a stable
level, thereby encouraging sufficient exploration. This enables LLMs to improve
response precision while preserving their exploratory ability to uncover
potential correct pathways. The code and data is available on:
https://github.com/LiaoMengqi/E3-RL4LLMs",2025-05-24,"Mengqi Liao, Xiangyu Xi, Ruinian Chen, Jia Leng, Yangen Hu, Ke Zeng, Shuai Liu, Huaiyu Wan",http://arxiv.org/pdf/2505.18573v1,cs.CL
From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test,"The human-centered word association test (WAT) serves as a cognitive proxy,
revealing sociocultural variations through lexical-semantic patterns. We extend
this test into an LLM-adaptive, free-relation task to assess the alignment of
large language models (LLMs) with cross-cultural cognition. To mitigate the
culture preference, we propose CultureSteer, an innovative approach that
integrates a culture-aware steering mechanism to guide semantic representations
toward culturally specific spaces. Experiments show that current LLMs exhibit
significant bias toward Western cultural (notably in American) schemas at the
word association level. In contrast, our model substantially improves
cross-cultural alignment, surpassing prompt-based methods in capturing diverse
semantic associations. Further validation on culture-sensitive downstream tasks
confirms its efficacy in fostering cognitive alignment across cultures. This
work contributes a novel methodological paradigm for enhancing cultural
awareness in LLMs, advancing the development of more inclusive language
technologies.",2025-05-24,"Xunlian Dai, Li Zhou, Benyou Wang, Haizhou Li",http://arxiv.org/pdf/2505.18562v1,cs.CL
TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation,"High-quality instruction data is crucial for developing large language models
(LLMs), yet existing approaches struggle to effectively control instruction
complexity. We present TAG-INSTRUCT, a novel framework that enhances
instruction complexity through structured semantic compression and controlled
difficulty augmentation. Unlike previous prompt-based methods operating on raw
text, TAG-INSTRUCT compresses instructions into a compact tag space and
systematically enhances complexity through RL-guided tag expansion. Through
extensive experiments, we show that TAG-INSTRUCT outperforms existing
instruction complexity augmentation approaches. Our analysis reveals that
operating in tag space provides superior controllability and stability across
different instruction synthesis frameworks.",2025-05-24,"He Zhu, Zhiwen Ruan, Junyou Su, Xingwei He, Wenjia Zhang, Yun Chen, Guanhua Chen",http://arxiv.org/pdf/2505.18557v1,cs.CL
Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation,"Intent detection, a core component of natural language understanding, has
considerably evolved as a crucial mechanism in safeguarding large language
models (LLMs). While prior work has applied intent detection to enhance LLMs'
moderation guardrails, showing a significant success against content-level
jailbreaks, the robustness of these intent-aware guardrails under malicious
manipulations remains under-explored. In this work, we investigate the
vulnerability of intent-aware guardrails and demonstrate that LLMs exhibit
implicit intent detection capabilities. We propose a two-stage intent-based
prompt-refinement framework, IntentPrompt, that first transforms harmful
inquiries into structured outlines and further reframes them into
declarative-style narratives by iteratively optimizing prompts via feedback
loops to enhance jailbreak success for red-teaming purposes. Extensive
experiments across four public benchmarks and various black-box LLMs indicate
that our framework consistently outperforms several cutting-edge jailbreak
methods and evades even advanced Intent Analysis (IA) and Chain-of-Thought
(CoT)-based defenses. Specifically, our ""FSTR+SPIN"" variant achieves attack
success rates ranging from 88.25% to 96.54% against CoT-based defenses on the
o1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based
defenses. These findings highlight a critical weakness in LLMs' safety
mechanisms and suggest that intent manipulation poses a growing challenge to
content moderation guardrails.",2025-05-24,"Jun Zhuang, Haibo Jin, Ye Zhang, Zhengjian Kang, Wenbin Zhang, Gaby G. Dagher, Haohan Wang",http://arxiv.org/pdf/2505.18556v1,cs.CL
Unraveling Misinformation Propagation in LLM Reasoning,"Large Language Models (LLMs) have demonstrated impressive capabilities in
reasoning, positioning them as promising tools for supporting human
problem-solving. However, what happens when their performance is affected by
misinformation, i.e., incorrect inputs introduced by users due to oversights or
gaps in knowledge? Such misinformation is prevalent in real-world interactions
with LLMs, yet how it propagates within LLMs' reasoning process remains
underexplored. Focusing on mathematical reasoning, we present a comprehensive
analysis of how misinformation affects intermediate reasoning steps and final
answers. We also examine how effectively LLMs can correct misinformation when
explicitly instructed to do so. Even with explicit instructions, LLMs succeed
less than half the time in rectifying misinformation, despite possessing
correct internal knowledge, leading to significant accuracy drops (10.02% -
72.20%). Further analysis shows that applying factual corrections early in the
reasoning process most effectively reduces misinformation propagation, and
fine-tuning on synthesized data with early-stage corrections significantly
improves reasoning factuality. Our work offers a practical approach to
mitigating misinformation propagation.",2025-05-24,"Yiyang Feng, Yichen Wang, Shaobo Cui, Boi Faltings, Mina Lee, Jiawei Zhou",http://arxiv.org/pdf/2505.18555v1,cs.CL
MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors,"We present MSA-MathEval, our submission to the BEA 2025 Shared Task on
evaluating AI tutor responses across four instructional dimensions: Mistake
Identification, Mistake Location, Providing Guidance, and Actionability. Our
approach uses a unified training pipeline to fine-tune a single
instruction-tuned language model across all tracks, without any task-specific
architectural changes. To improve prediction reliability, we introduce a
disagreement-aware ensemble inference strategy that enhances coverage of
minority labels. Our system achieves strong performance across all tracks,
ranking 1st in Providing Guidance, 3rd in Actionability, and 4th in both
Mistake Identification and Mistake Location. These results demonstrate the
effectiveness of scalable instruction tuning and disagreement-driven modeling
for robust, multi-dimensional evaluation of LLMs as educational tutors.",2025-05-24,"Baraa Hikal, Mohamed Basem, Islam Oshallah, Ali Hamdi",http://arxiv.org/pdf/2505.18549v1,cs.CL
Composable Cross-prompt Essay Scoring by Merging Models,"Recent advances in cross-prompt automated essay scoring (AES) typically train
models jointly on all source prompts, often requiring additional access to
unlabeled target prompt essays simultaneously. However, using all sources is
suboptimal in our pilot study, and re-accessing source datasets during
adaptation raises privacy concerns. We propose a source-free adaptation
approach that selectively merges individually trained source models' parameters
instead of datasets. In particular, we simulate joint training through linear
combinations of task vectors -- the parameter updates from fine-tuning. To
optimize the combination's coefficients, we propose Prior-encoded Information
Maximization (PIM), an unsupervised objective which promotes the model's score
discriminability regularized by priors pre-computed from the sources. We employ
Bayesian optimization as an efficient optimizer of PIM. Experimental results
with LLMs on in-dataset and cross-dataset adaptation show that our method (1)
consistently outperforms training jointly on all sources, (2) maintains
superior robustness compared to other merging methods, (3) excels under severe
distribution shifts where recent leading cross-prompt methods struggle, all
while retaining computational efficiency.",2025-05-24,"Sanwoo Lee, Kun Liang, Yunfang Wu",http://arxiv.org/pdf/2505.18548v1,cs.CL
B-score: Detecting biases in large language models using response history,"Large language models (LLMs) often exhibit strong biases, e.g, against women
or in favor of the number 7. We investigate whether LLMs would be able to
output less biased answers when allowed to observe their prior answers to the
same question in a multi-turn conversation. To understand which types of
questions invite more biased answers, we test LLMs on our proposed set of
questions that span 9 topics and belong to three types: (1) Subjective; (2)
Random; and (3) Objective. Interestingly, LLMs are able to ""de-bias"" themselves
in a multi-turn conversation in response to questions that seek an Random,
unbiased answer. Furthermore, we propose B-score, a novel metric that is
effective in detecting biases to Subjective, Random, Easy, and Hard questions.
On MMLU, HLE, and CSQA, leveraging B-score substantially improves the
verification accuracy of LLM answers (i.e, accepting LLM correct answers and
rejecting incorrect ones) compared to using verbalized confidence scores or the
frequency of single-turn answers alone. Code and data are available at:
https://b-score.github.io.",2025-05-24,"An Vo, Mohammad Reza Taesiri, Daeyoung Kim, Anh Totti Nguyen",http://arxiv.org/pdf/2505.18545v1,cs.CL
Business as \textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs,"Process mining aims to discover, monitor and optimize the actual behaviors of
real processes. While prior work has mainly focused on extracting procedural
action flows from instructional texts, rule flows embedded in business
documents remain underexplored. To this end, we introduce a novel annotated
Chinese dataset, \textbf{BPRF}, which contains 50 business process documents
with 326 explicitly labeled business rules across multiple domains. Each rule
is represented as a <Condition, Action> pair, and we annotate logical
dependencies between rules (sequential, conditional, or parallel). We also
propose \textbf{ExIde}, a framework for automatic business rule extraction and
dependency relationship identification using large language models (LLMs). We
evaluate ExIde using 12 state-of-the-art (SOTA) LLMs on the BPRF dataset,
benchmarking performance on both rule extraction and dependency classification
tasks of current LLMs. Our results demonstrate the effectiveness of ExIde in
extracting structured business rules and analyzing their interdependencies for
current SOTA LLMs, paving the way for more automated and interpretable business
process automation.",2025-05-24,"Chen Yang, Ruping Xu, Ruizhe Li, Bin Cao, Jing Fan",http://arxiv.org/pdf/2505.18542v1,cs.CL
Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models,"Standing in 2025, at a critical juncture in the pursuit of Artificial General
Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated
significant potential in enhancing the reasoning capability of large language
models (LLMs) and has led to the development of cutting-edge AI models such as
OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to
enhance the reasoning capability of multimodal large language models (MLLMs)
has attracted widespread attention from the community. In this position paper,
we argue that reinforcement fine-tuning powers the reasoning capability of
multimodal large language models. To begin with, we provide a detailed
introduction to the fundamental background knowledge that researchers
interested in this field should be familiar with. Furthermore, we meticulously
summarize the improvements of RFT in powering reasoning capability of MLLMs
into five key points: diverse modalities, diverse tasks and domains, better
training algorithms, abundant benchmarks and thriving engineering frameworks.
Finally, we propose five promising directions for future research that the
community might consider. We hope that this position paper will provide
valuable insights to the community at this pivotal stage in the advancement
toward AGI. Summary of works done on RFT for MLLMs is available at
https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.",2025-05-24,"Haoyuan Sun, Jiaqi Wu, Bo Xia, Yifu Luo, Yifei Zhao, Kai Qin, Xufei Lv, Tiantian Zhang, Yongzhe Chang, Xueqian Wang",http://arxiv.org/pdf/2505.18536v1,cs.CL
metaTextGrad: Automatically optimizing language model optimizers,"Large language models (LLMs) are increasingly used in learning algorithms,
evaluations, and optimization tasks. Recent studies have shown that using
LLM-based optimizers to automatically optimize model prompts, demonstrations,
predictions themselves, or other components can significantly enhance the
performance of AI systems, as demonstrated by frameworks such as DSPy and
TextGrad. However, optimizers built on language models themselves are usually
designed by humans with manual design choices; optimizers themselves are not
optimized. Moreover, these optimizers are general purpose by design, to be
useful to a broad audience, and are not tailored for specific tasks. To address
these challenges, we propose metaTextGrad, which focuses on designing a
meta-optimizer to further enhance existing optimizers and align them to be good
optimizers for a given task. Our approach consists of two key components: a
meta prompt optimizer and a meta structure optimizer. The combination of these
two significantly improves performance across multiple benchmarks, achieving an
average absolute performance improvement of up to 6% compared to the best
baseline.",2025-05-24,"Guowei Xu, Mert Yuksekgonul, Carlos Guestrin, James Zou",http://arxiv.org/pdf/2505.18524v1,cs.CL
How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation,"Pre-trained language models represented by the Transformer have been proven
to possess strong base capabilities, and the representative self-attention
mechanism in the Transformer has become a classic in sequence modeling
architectures. Different from the work of proposing sequence modeling
architecture to improve the efficiency of attention mechanism, this work
focuses on the impact of sequence modeling architectures on base capabilities.
Specifically, our concern is: How exactly do sequence modeling architectures
affect the base capabilities of pre-trained language models? In this work, we
first point out that the mixed domain pre-training setting commonly adopted in
existing architecture design works fails to adequately reveal the differences
in base capabilities among various architectures. To address this, we propose a
limited domain pre-training setting with out-of-distribution testing, which
successfully uncovers significant differences in base capabilities among
architectures at an early stage. Next, we analyze the base capabilities of
stateful sequence modeling architectures, and find that they exhibit
significant degradation in base capabilities compared to the Transformer. Then,
through a series of architecture component analysis, we summarize a key
architecture design principle: A sequence modeling architecture need possess
full-sequence arbitrary selection capability to avoid degradation in base
capabilities. Finally, we empirically validate this principle using an
extremely simple Top-1 element selection architecture and further generalize it
to a more practical Top-1 chunk selection architecture. Experimental results
demonstrate our proposed sequence modeling architecture design principle and
suggest that our work can serve as a valuable reference for future architecture
improvements and novel designs.",2025-05-24,"Xin Lu, Yanyan Zhao, Si Wei, Shijin Wang, Bing Qin, Ting Liu",http://arxiv.org/pdf/2505.18522v1,cs.CL
AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking,"Listwise reranking with large language models (LLMs) enhances top-ranked
results in retrieval-based applications. Due to the limit in context size and
high inference cost of long context, reranking is typically performed over a
fixed size of small subsets, with the final ranking aggregated from these
partial results. This fixed computation disregards query difficulty and
document distribution, leading to inefficiencies. We propose AcuRank, an
adaptive reranking framework that dynamically adjusts both the amount and
target of computation based on uncertainty estimates over document relevance.
Using a Bayesian TrueSkill model, we iteratively refine relevance estimates
until reaching sufficient confidence levels, and our explicit modeling of
ranking uncertainty enables principled control over reranking behavior and
avoids unnecessary updates to confident predictions. Results on the TREC-DL and
BEIR benchmarks show that our method consistently achieves a superior
accuracy-efficiency trade-off and scales better with compute than
fixed-computation baselines. These results highlight the effectiveness and
generalizability of our method across diverse retrieval tasks and LLM-based
reranking models.",2025-05-24,"Soyoung Yoon, Gyuwan Kim, Gyu-Hwung Cho, Seung-won Hwang",http://arxiv.org/pdf/2505.18512v1,cs.CL
Knowledge Grafting of Large Language Models,"Cross-capability transfer is a key challenge in large language model (LLM)
research, with applications in multi-task integration, model compression, and
continual learning. Recent works like FuseLLM and FuseChat have demonstrated
the potential of transferring multiple model capabilities to lightweight
models, enhancing adaptability and efficiency, which motivates our
investigation into more efficient cross-capability transfer methods. However,
existing approaches primarily focus on small, homogeneous models, limiting
their applicability. For large, heterogeneous models, knowledge distillation
with full-parameter fine-tuning often overlooks the student model's intrinsic
capacity and risks catastrophic forgetting, while PEFT methods struggle to
effectively absorb knowledge from source LLMs. To address these issues, we
introduce GraftLLM, a novel method that stores source model capabilities in a
target model with SkillPack format. This approach preserves general
capabilities, reduces parameter conflicts, and supports forget-free continual
learning and model fusion. We employ a module-aware adaptive compression
strategy to compress parameter updates, ensuring efficient storage while
maintaining task-specific knowledge. The resulting SkillPack serves as a
compact and transferable knowledge carrier, ideal for heterogeneous model
fusion and continual learning. Experiments across various scenarios demonstrate
that GraftLLM outperforms existing techniques in knowledge transfer, knowledge
fusion, and forget-free learning, providing a scalable and efficient solution
for cross-capability transfer. The code is publicly available at:
https://github.com/duguodong7/GraftLLM.",2025-05-24,"Guodong Du, Xuanning Zhou, Junlin Li, Zhuo Li, Zesheng Shi, Wanyu Lin, Ho-Kin Tang, Xiucheng Li, Fangming Liu, Wenya Wang, Min Zhang, Jing Li",http://arxiv.org/pdf/2505.18502v1,cs.CL
The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models,"Current large language models (LLMs) have demonstrated emerging capabilities
in social intelligence tasks, including implicature resolution (Sravanthi et
al. (2024)) and theory-of-mind reasoning (Shapira et al. (2024)), both of which
require substantial pragmatic understanding. However, how LLMs acquire this
competence throughout the training process remains poorly understood. In this
work, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of
alternatives, designed to evaluate whether LLMs at different training stages
can accurately infer nuanced speaker intentions. Each instance pairs two
contextually appropriate but pragmatically distinct continuations, enabling
fine-grained assessment of both pragmatic interpretation and contrastive
reasoning. We systematically evaluate 22 LLMs across key training stages:
pre-training, supervised fine-tuning (SFT), and preference optimization, to
examine the development of pragmatic competence. Our results show that even
base models exhibit notable sensitivity to pragmatic cues, which improves
consistently with increases in model and data scale. Additionally, SFT and RLHF
contribute further gains, particularly in cognitive-pragmatic reasoning. These
findings highlight pragmatic competence as an emergent and compositional
property of LLM training and offer new insights for aligning models with human
communicative norms.",2025-05-24,"Kefan Yu, Qingcheng Zeng, Weihao Xuan, Wanxin Li, Jingyi Wu, Rob Voigt",http://arxiv.org/pdf/2505.18497v1,cs.CL
Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications,"Error correction is an important capability when applying large language
models (LLMs) to facilitate user typing on mobile devices. In this paper, we
use LLMs to synthesize a high-quality dataset of error correction pairs to
evaluate and improve LLMs for mobile applications. We first prompt LLMs with
error correction domain knowledge to build a scalable and reliable addition to
the existing data synthesis pipeline. We then adapt the synthetic data
distribution to match the mobile application domain by reweighting the samples.
The reweighting model is learnt by predicting (a handful of) live A/B test
metrics when deploying LLMs in production, given the LLM performance on offline
evaluation data and scores from a small privacy-preserving on-device language
model. Finally, we present best practices for mixing our synthetic data with
other data sources to improve model performance on error correction in both
offline evaluation and production live A/B testing.",2025-05-24,"Yanxiang Zhang, Zheng Xu, Shanshan Wu, Yuanbo Zhang, Daniel Ramage",http://arxiv.org/pdf/2505.18488v1,cs.CL
"Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek","Large language models (LLMs) have been widely explored for automated scoring
in low-stakes assessment to facilitate learning and instruction. Empirical
evidence related to which LLM produces the most reliable scores and induces
least rater effects needs to be collected before the use of LLMs for automated
scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,
ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini
2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in
scoring two types of writing tasks. The accuracy of the holistic and analytic
scores from LLMs compared with human raters was evaluated in terms of Quadratic
Weighted Kappa. Intra-rater consistency across prompts was compared in terms of
Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human
raters using the Many-Facet Rasch model. The results in general supported the
use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring
accuracy, better rater reliability, and less rater effects.",2025-05-24,"Hong Jiao, Dan Song, Won-Chan Lee",http://arxiv.org/pdf/2505.18486v1,cs.CL
Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark,"Recent advances in large reasoning models (LRMs) show strong performance in
structured domains such as mathematics and programming; however, they often
lack pedagogical coherence and realistic teaching behaviors. To bridge this
gap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use
through three innovations: (1) a distillation-based pipeline that filters and
refines model outputs for instruction-tuning, (2) the Well-balanced Educational
Benchmark (WBEB), which evaluates performance across subject knowledge,
pedagogical knowledge, tracing, essay scoring, and teacher decision-making, and
(3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting
teacher-style reasoning. Our mixed-method evaluation combines quantitative
metrics with qualitative analysis, providing the first systematic assessment of
LRMs' pedagogical strengths and limitations.",2025-05-24,"Unggi Lee, Jaeyong Lee, Jiyeong Bae, Yeil Jeong, Junbo Koh, Gyeonggeon Lee, Gunho Lee, Taekyung Ahn, Hyeoncheol Kim",http://arxiv.org/pdf/2505.18467v1,cs.CL
Measuring South Asian Biases in Large Language Models,"Evaluations of Large Language Models (LLMs) often overlook intersectional and
culturally specific biases, particularly in underrepresented multilingual
regions like South Asia. This work addresses these gaps by conducting a
multilingual and intersectional analysis of LLM outputs across 10 Indo-Aryan
and Dravidian languages, identifying how cultural stigmas influenced by purdah
and patriarchy are reinforced in generative tasks. We construct a culturally
grounded bias lexicon capturing previously unexplored intersectional dimensions
including gender, religion, marital status, and number of children. We use our
lexicon to quantify intersectional bias and the effectiveness of self-debiasing
in open-ended generations (e.g., storytelling, hobbies, and to-do lists), where
bias manifests subtly and remains largely unexamined in multilingual contexts.
Finally, we evaluate two self-debiasing strategies (simple and complex prompts)
to measure their effectiveness in reducing culturally specific bias in
Indo-Aryan and Dravidian languages. Our approach offers a nuanced lens into
cultural bias by introducing a novel bias lexicon and evaluation framework that
extends beyond Eurocentric or small-scale multilingual settings.",2025-05-24,"Mamnuya Rinki, Chahat Raj, Anjishnu Mukherjee, Ziwei Zhu",http://arxiv.org/pdf/2505.18466v1,cs.CL
From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data,"The growing demand for accessible mental health support, compounded by
workforce shortages and logistical barriers, has led to increased interest in
utilizing Large Language Models (LLMs) for scalable and real-time assistance.
However, their use in sensitive domains such as anxiety support remains
underexamined. This study presents a systematic evaluation of LLMs (GPT and
Llama) for their potential utility in anxiety support by using real
user-generated posts from the r/Anxiety subreddit for both prompting and
fine-tuning. Our approach utilizes a mixed-method evaluation framework
incorporating three main categories of criteria: (i) linguistic quality, (ii)
safety and trustworthiness, and (iii) supportiveness. Results show that
fine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic
quality but increased toxicity and bias, and diminished emotional
responsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more
supportive overall. Our findings highlight the risks of fine-tuning LLMs on
unprocessed social media content without mitigation strategies.",2025-05-24,"Ugur Kursuncu, Trilok Padhi, Gaurav Sinha, Abdulkadir Erol, Jaya Krishna Mandivarapu, Christopher R. Larrison",http://arxiv.org/pdf/2505.18464v1,cs.CL
A Survey of LLM $\times$ DATA,"The integration of large language model (LLM) and data management (DATA) is
rapidly redefining both domains. In this survey, we comprehensively review the
bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale
data processing, storage, and serving, feeds LLMs with high quality, diversity,
and timeliness of data required for stages like pre-training, post-training,
retrieval-augmented generation, and agentic workflows: (i) Data processing for
LLMs includes scalable acquisition, deduplication, filtering, selection, domain
mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on
efficient data and model formats, distributed and heterogeneous storage
hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data
serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),
LLM inference (e.g., prompt compression, data provenance), and training
strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,
LLMs are emerging as general-purpose engines for data management. We review
recent advances in (i) data manipulation, including automatic data cleaning,
integration, discovery; (ii) data analysis, covering reasoning over structured,
semi-structured, and unstructured data, and (iii) system optimization (e.g.,
configuration tuning, query rewriting, anomaly diagnosis), powered by LLM
techniques like retrieval-augmented prompting, task-specialized fine-tuning,
and multi-agent collaboration.",2025-05-24,"Xuanhe Zhou, Junxuan He, Wei Zhou, Haodong Chen, Zirui Tang, Haoyu Zhao, Xin Tong, Guoliang Li, Youmin Chen, Jun Zhou, Zhaojun Sun, Binyuan Hui, Shuo Wang, Conghui He, Zhiyuan Liu, Jingren Zhou, Fan Wu",http://arxiv.org/pdf/2505.18458v1,cs.CL
Anchored Diffusion Language Model,"Diffusion Language Models (DLMs) promise parallel generation and
bidirectional context, yet they underperform autoregressive (AR) models in both
likelihood modeling and generated text quality. We identify that this
performance gap arises when important tokens (e.g., key words or low-frequency
words that anchor a sentence) are masked early in the forward process, limiting
contextual information for accurate reconstruction. To address this, we
introduce the Anchored Diffusion Language Model (ADLM), a novel two-stage
framework that first predicts distributions over important tokens via an anchor
network, and then predicts the likelihoods of missing tokens conditioned on the
anchored predictions. ADLM significantly improves test perplexity on LM1B and
OpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap
with strong AR baselines. It also achieves state-of-the-art performance in
zero-shot generalization across seven benchmarks and surpasses AR models in
MAUVE score, which marks the first time a DLM generates better human-like text
than an AR model. Theoretically, we derive an Anchored Negative Evidence Lower
Bound (ANELBO) objective and show that anchoring improves sample complexity and
likelihood modeling. Beyond diffusion, anchoring boosts performance in AR
models and enhances reasoning in math and logic tasks, outperforming existing
chain-of-thought approaches",2025-05-24,"Litu Rout, Constantine Caramanis, Sanjay Shakkottai",http://arxiv.org/pdf/2505.18456v1,cs.CL
Hybrid Latent Reasoning via Reinforcement Learning,"Recent advances in large language models (LLMs) have introduced latent
reasoning as a promising alternative to autoregressive reasoning. By performing
internal computation with hidden states from previous steps, latent reasoning
benefit from more informative features rather than sampling a discrete
chain-of-thought (CoT) path. Yet latent reasoning approaches are often
incompatible with LLMs, as their continuous paradigm conflicts with the
discrete nature of autoregressive generation. Moreover, these methods rely on
CoT traces for training and thus fail to exploit the inherent reasoning
patterns of LLMs. In this work, we explore latent reasoning by leveraging the
intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we
introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid
latent reasoning approach that (1) integrates prior hidden states into sampled
tokens with a learnable gating mechanism, and (2) initializes training with
predominantly token embeddings while progressively incorporating more hidden
features. This design maintains LLMs' generative capabilities and incentivizes
hybrid reasoning using both discrete and continuous representations. In
addition, the hybrid HRPO introduces stochasticity into latent reasoning via
token sampling, thereby enabling RL-based optimization without requiring CoT
trajectories. Extensive evaluations across diverse benchmarks show that HRPO
outperforms prior methods in both knowledge- and reasoning-intensive tasks.
Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing
behaviors like cross-lingual patterns and shorter completion lengths,
highlighting the potential of our RL-based approach and offer insights for
future work in latent reasoning.",2025-05-24,"Zhenrui Yue, Bowen Jin, Huimin Zeng, Honglei Zhuang, Zhen Qin, Jinsung Yoon, Lanyu Shang, Jiawei Han, Dong Wang",http://arxiv.org/pdf/2505.18454v1,cs.CL
MedScore: Factuality Evaluation of Free-Form Medical Answers,"While Large Language Models (LLMs) can generate fluent and convincing
responses, they are not necessarily correct. This is especially apparent in the
popular decompose-then-verify factuality evaluation pipeline, where LLMs
evaluate generations by decomposing the generations into individual, valid
claims. Factuality evaluation is especially important for medical answers,
since incorrect medical information could seriously harm the patient. However,
existing factuality systems are a poor match for the medical domain, as they
are typically only evaluated on objective, entity-centric, formulaic texts such
as biographies and historical topics. This differs from condition-dependent,
conversational, hypothetical, sentence-structure diverse, and subjective
medical answers, which makes decomposition into valid facts challenging. We
propose MedScore, a new approach to decomposing medical answers into
condition-aware valid facts. Our method extracts up to three times more valid
facts than existing methods, reducing hallucination and vague references, and
retaining condition-dependency in facts. The resulting factuality score
significantly varies by decomposition method, verification corpus, and used
backbone LLM, highlighting the importance of customizing each step for reliable
factuality evaluation.",2025-05-24,"Heyuan Huang, Alexandra DeLucia, Vijay Murari Tiyyala, Mark Dredze",http://arxiv.org/pdf/2505.18452v1,cs.CL
$μ$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts,"To tackle the huge computational demand of large foundation models,
activation-aware compression techniques without retraining have been
introduced. However, since these rely on calibration data, domain shift may
arise for unknown downstream tasks. With a computationally efficient
calibration, activation-aware pruning can be executed for every prompt
adaptively, yet achieving reduced complexity at inference. We formulate it as a
mixture of micro-experts, called $\mu$-MoE. Several experiments demonstrate
that $\mu$-MoE can dynamically adapt to task/prompt-dependent structured
sparsity on the fly.",2025-05-24,"Toshiaki Koike-Akino, Jing Liu, Ye Wang",http://arxiv.org/pdf/2505.18451v1,cs.CL
BRIT: Bidirectional Retrieval over Unified Image-Text Graph,"Retrieval-Augmented Generation (RAG) has emerged as a promising technique to
enhance the quality and relevance of responses generated by large language
models. While recent advancements have mainly focused on improving RAG for
text-based queries, RAG on multi-modal documents containing both texts and
images has not been fully explored. Especially when fine-tuning does not work.
This paper proposes BRIT, a novel multi-modal RAG framework that effectively
unifies various text-image connections in the document into a multi-modal graph
and retrieves the texts and images as a query-specific sub-graph. By traversing
both image-to-text and text-to-image paths in the graph, BRIT retrieve not only
directly query-relevant images and texts but also further relevant contents to
answering complex cross-modal multi-hop questions. To evaluate the
effectiveness of BRIT, we introduce MM-RAG test set specifically designed for
multi-modal question answering tasks that require to understand the text-image
relations. Our comprehensive experiments demonstrate the superiority of BRIT,
highlighting its ability to handle cross-modal questions on the multi-modal
documents.",2025-05-24,"Ainulla Khan, Yamada Moyuru, Srinidhi Akella",http://arxiv.org/pdf/2505.18450v1,cs.CL
Efficient Long CoT Reasoning in Small Language Models,"Recent large reasoning models such as DeepSeek-R1 exhibit strong complex
problems solving abilities by generating long chain-of-thought (CoT) reasoning
steps. It is challenging to directly train small language models (SLMs) to
emerge long CoT. Thus, distillation becomes a practical method to enable SLMs
for such reasoning ability. However, the long CoT often contains a lot of
redundant contents (e.g., overthinking steps) which may make SLMs hard to learn
considering their relatively poor capacity and generalization. To address this
issue, we propose a simple-yet-effective method to prune unnecessary steps in
long CoT, and then employ an on-policy method for the SLM itself to curate
valid and useful long CoT training data. In this way, SLMs can effectively
learn efficient long CoT reasoning and preserve competitive performance at the
same time. Experimental results across a series of mathematical reasoning
benchmarks demonstrate the effectiveness of the proposed method in distilling
long CoT reasoning ability into SLMs which maintains the competitive
performance but significantly reduces generating redundant reasoning steps.",2025-05-24,"Zhaoyang Wang, Jinqi Jiang, Tian Qiu, Hui Liu, Xianfeng Tang, Huaxiu Yao",http://arxiv.org/pdf/2505.18440v1,cs.CL
Voice of a Continent: Mapping Africa's Speech Technology Frontier,"Africa's rich linguistic diversity remains significantly underrepresented in
speech technologies, creating barriers to digital inclusion. To alleviate this
challenge, we systematically map the continent's speech space of datasets and
technologies, leading to a new comprehensive benchmark SimbaBench for
downstream African speech tasks. Using SimbaBench, we introduce the Simba
family of models, achieving state-of-the-art performance across multiple
African languages and speech tasks. Our benchmark analysis reveals critical
patterns in resource availability, while our model evaluation demonstrates how
dataset quality, domain diversity, and language family relationships influence
performance across languages. Our work highlights the need for expanded speech
technology resources that better reflect Africa's linguistic diversity and
provides a solid foundation for future research and development efforts toward
more inclusive speech technologies.",2025-05-24,"AbdelRahim Elmadany, Sang Yun Kwon, Hawau Olamide Toyin, Alcides Alcoba Inciarte, Hanan Aldarmaki, Muhammad Abdul-Mageed",http://arxiv.org/pdf/2505.18436v1,cs.CL
Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps,"As connected and automated transportation systems evolve, there is a growing
need for federal and state authorities to revise existing laws and develop new
statutes to address emerging cybersecurity and data privacy challenges. This
study introduces a Retrieval-Augmented Generation (RAG) based Large Language
Model (LLM) framework designed to support policymakers by extracting relevant
legal content and generating accurate, inquiry-specific responses. The
framework focuses on reducing hallucinations in LLMs by using a curated set of
domain-specific questions to guide response generation. By incorporating
retrieval mechanisms, the system enhances the factual grounding and specificity
of its outputs. Our analysis shows that the proposed RAG-based LLM outperforms
leading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,
BERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and
context-aware legal insights. This approach offers a scalable, AI-driven method
for legislative analysis, supporting efforts to update legal frameworks in line
with advancements in transportation technologies.",2025-05-23,"Khandakar Ashrafi Akbar, Md Nahiyan Uddin, Latifur Khan, Trayce Hockstad, Mizanur Rahman, Mashrur Chowdhury, Bhavani Thuraisingham",http://arxiv.org/pdf/2505.18426v1,cs.CL
LatentLLM: Attention-Aware Joint Tensor Compression,"Modern foundation models such as large language models (LLMs) and large
multi-modal models (LMMs) require a massive amount of computational and memory
resources. We propose a new framework to convert such LLMs/LMMs into a
reduced-dimension latent structure. Our method extends a local activation-aware
tensor decomposition to a global attention-aware joint tensor de-composition.
Our framework can significantly improve the model accuracy over the existing
model compression methods when reducing the latent dimension to realize
computationally/memory-efficient LLMs/LLMs. We show the benefit on several
benchmark including multi-modal reasoning tasks.",2025-05-23,"Toshiaki Koike-Akino, Xiangyu Chen, Jing Liu, Ye Wang, Pu, Wang, Matthew Brand",http://arxiv.org/pdf/2505.18413v1,cs.CL
DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding,"We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance
multi-modal Temporal Point Process (TPP) modeling in the era of Large Language
Models (LLMs). While TPPs have been widely studied for modeling temporal event
sequences, existing datasets are predominantly unimodal, hindering progress in
models that require joint reasoning over temporal, textual, and visual
information. To address this gap, DanmakuTPPBench comprises two complementary
components: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili
video platform, where user-generated bullet comments (Danmaku) naturally form
multi-modal events annotated with precise timestamps, rich textual content, and
corresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering
dataset constructed via a novel multi-agent pipeline powered by
state-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex
temporal-textual-visual reasoning. We conduct extensive evaluations using both
classical TPP models and recent MLLMs, revealing significant performance gaps
and limitations in current methods' ability to model multi-modal event
dynamics. Our benchmark establishes strong baselines and calls for further
integration of TPP modeling into the multi-modal language modeling landscape.
The code and dataset have been released at
https://github.com/FRENKIE-CHIANG/DanmakuTPPBench",2025-05-23,"Yue Jiang, Jichu Li, Yang Liu, Dingkang Yang, Feng Zhou, Quyu Kong",http://arxiv.org/pdf/2505.18411v1,cs.CL
RaDeR: Reasoning-aware Dense Retrieval Models,"We propose RaDeR, a set of reasoning-based dense retrieval models trained
with data derived from mathematical problem solving using large language models
(LLMs). Our method leverages retrieval-augmented reasoning trajectories of an
LLM and self-reflective relevance evaluation, enabling the creation of both
diverse and hard-negative samples for reasoning-intensive relevance. RaDeR
retrievers, trained for mathematical reasoning, effectively generalize to
diverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently
outperforming strong baselines in overall performance.Notably, RaDeR achieves
significantly higher performance than baselines on the Math and Coding splits.
In addition, RaDeR presents the first dense retriever that outperforms BM25
when queries are Chain-of-Thought reasoning steps, underscoring the critical
role of reasoning-based retrieval to augment reasoning language models.
Furthermore, RaDeR achieves comparable or superior performance while using only
2.5% of the training data used by the concurrent work REASONIR, highlighting
the quality of our synthesized training data.",2025-05-23,"Debrup Das, Sam O' Nuallain, Razieh Rahimi",http://arxiv.org/pdf/2505.18405v1,cs.CL
NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities,"Enhancing the linguistic capabilities of Large Language Models (LLMs) to
include low-resource languages is a critical research area. Current research
directions predominantly rely on synthetic data generated by translating
English corpora, which, while demonstrating promising linguistic understanding
and translation abilities, often results in models aligned with source language
culture. These models frequently fail to represent the cultural heritage and
values of local communities. This work proposes a methodology to create both
synthetic and retrieval-based pre-training data tailored to a specific
community, considering its (i) language, (ii) cultural heritage, and (iii)
cultural values. We demonstrate our methodology using Egyptian and Moroccan
dialects as testbeds, chosen for their linguistic and cultural richness and
current underrepresentation in LLMs. As a proof-of-concept, we develop
NileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,
incorporating their language, cultural heritage, and values. Our results on
various understanding, translation, and cultural and values alignment
benchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar
size and performs on par with larger models. We share our methods, data, and
models with the community to promote the inclusion and coverage of more diverse
communities in LLM development.",2025-05-23,"Abdellah El Mekki, Houdaifa Atou, Omer Nacar, Shady Shehata, Muhammad Abdul-Mageed",http://arxiv.org/pdf/2505.18383v1,cs.CL
ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation,"Command-line interfaces (CLIs) provide structured textual environments for
system administration. Explorations have been performed using pre-trained
language models (PLMs) to simulate these environments for safe interaction in
high-risk environments. However, their use has been constrained to frozen,
large parameter models like GPT. For smaller architectures to reach a similar
level of believability, a rich dataset of CLI interactions is required.
Existing public datasets focus on mapping natural-language tasks to commands,
omitting crucial execution data such as exit codes, outputs, and environmental
side effects, limiting their usability for behavioral modeling. We introduce a
Shell Input -Output Environment (ShIOEnv), which casts command construction as
a Markov Decision Process whose state is the partially built sequence and whose
actions append arguments. After each action, ShIOEnv executes the candidate and
returns its exit status, output, and progress toward a minimal-length
behavioral objective. Due to the intractable nature of the combinatorial
argument state-action space, we derive a context-free grammar from man pages to
mask invalid arguments from being emitted. We explore random and
proximal-policy optimization (PPO)-optimized sampling of unrestricted and
grammar-masked action spaces to produce four exploration strategies. We
observed that grammar masking and PPO significantly improve sample efficiency
to produce a higher quality dataset (maximizing the number of arguments while
minimizing redundancies). Policy-generated datasets of shell input-output
behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements
in BLEU-4 when constraining the action space to grammar productions with an
additional 26% improvement when applying PPO. The ShIOEnv environment and
curated command behavior datasets are released for use in future research.",2025-05-23,"Jarrod Ragsdale, Rajendra Boppana",http://arxiv.org/pdf/2505.18374v1,cs.CL
Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems,"Enterprise search systems often struggle to retrieve accurate,
domain-specific information due to semantic mismatches and overlapping
terminologies. These issues can degrade the performance of downstream
applications such as knowledge management, customer support, and
retrieval-augmented generation agents. To address this challenge, we propose a
scalable hard-negative mining framework tailored specifically for
domain-specific enterprise data. Our approach dynamically selects semantically
challenging but contextually irrelevant documents to enhance deployed
re-ranking models.
  Our method integrates diverse embedding models, performs dimensionality
reduction, and uniquely selects hard negatives, ensuring computational
efficiency and semantic precision. Evaluation on our proprietary enterprise
corpus (cloud services domain) demonstrates substantial improvements of 15\% in
MRR@3 and 19\% in MRR@10 compared to state-of-the-art baselines and other
negative sampling techniques. Further validation on public domain-specific
datasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability
and readiness for real-world applications.",2025-05-23,"Hansa Meghwani, Amit Agarwal, Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Srikant Panda",http://arxiv.org/pdf/2505.18366v1,cs.CL
SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases,"Text-to-SQL systems translate natural language questions into executable SQL
queries, and recent progress with large language models (LLMs) has driven
substantial improvements in this task. Schema linking remains a critical
component in Text-to-SQL systems, reducing prompt size for models with narrow
context windows and sharpening model focus even when the entire schema fits. We
present a zero-shot, training-free schema linking approach that first
constructs a schema graph based on foreign key relations, then uses a single
prompt to Gemini 2.5 Flash to extract source and destination tables from the
user query, followed by applying classical path-finding algorithms and
post-processing to identify the optimal sequence of tables and columns that
should be joined, enabling the LLM to generate more accurate SQL queries.
Despite being simple, cost-effective, and highly scalable, our method achieves
state-of-the-art results on the BIRD benchmark, outperforming previous
specialized, fine-tuned, and complex multi-step LLM-based approaches. We
conduct detailed ablation studies to examine the precision-recall trade-off in
our framework. Additionally, we evaluate the execution accuracy of our schema
filtering method compared to other approaches across various model sizes.",2025-05-23,"AmirHossein Safdarian, Milad Mohammadi, Ehsan Jahanbakhsh, Mona Shahamat Naderi, Heshaam Faili",http://arxiv.org/pdf/2505.18363v1,cs.CL
The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs,"Large language models (LLMs) still struggle across tasks outside of
high-resource languages. In this work, we investigate cross-lingual transfer to
lower-resource languages where task-specific post-training data is scarce.
Building on prior work, we first validate that the subsets of model parameters
that matter most for mathematical reasoning and multilingual capabilities are
distinctly non-overlapping. To exploit this implicit separability between task
and target language parameterization, we develop and analyze numerous modular
frameworks to improve the composition of the two during fine-tuning. These
methods generally employ freezing parameters or post hoc model merging to
assign math and language improvement to different key parts of the LLM. In the
absence of in-language math data, we demonstrate that the modular approaches
successfully improve upon baselines across three languages, four models, and
two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most
consistently successful modular method to be fine-tuning separate language and
math experts and model merging via Layer-Swapping, somewhat surprisingly. We
offer possible explanations for this result via recent works on the linearity
of task vectors. We further explain this by empirically showing that reverting
less useful fine-tuning updates after training often outperforms freezing them
from the start.",2025-05-23,"Lucas Bandarkar, Nanyun Peng",http://arxiv.org/pdf/2505.18356v1,cs.CL
Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?,"As Large Language Models (LLMs) are increasingly being adopted for narrow
tasks - such as medical question answering or sentiment analysis - and deployed
in resource-constrained settings, a key question arises: how many parameters
does a task actually need? In this work, we present LLM-Sieve, the first
comprehensive framework for task-specific pruning of LLMs that achieves 20-75%
parameter reduction with only 1-5% accuracy degradation across diverse domains.
Unlike prior methods that apply uniform pruning or rely on low-rank
approximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns
task-aware joint projections to better approximate output behavior, and (ii)
employs a Genetic Algorithm to discover differentiated pruning levels for each
matrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization,
and uniquely demonstrates strong generalization across datasets within the same
task domain. Together, these results establish a practical and robust mechanism
to generate smaller performant task-specific models.",2025-05-23,"Waleed Reda, Abhinav Jangda, Krishna Chintalapudi",http://arxiv.org/pdf/2505.18350v1,cs.CL
Model Editing with Graph-Based External Memory,"Large language models (LLMs) have revolutionized natural language processing,
yet their practical utility is often limited by persistent issues of
hallucinations and outdated parametric knowledge. Although post-training model
editing offers a pathway for dynamic updates, existing methods frequently
suffer from overfitting and catastrophic forgetting. To tackle these
challenges, we propose a novel framework that leverages hyperbolic geometry and
graph neural networks for precise and stable model edits. We introduce HYPE
(HYperbolic Parameter Editing), which comprises three key components: (i)
Hyperbolic Graph Construction, which uses Poincar\'e embeddings to represent
knowledge triples in hyperbolic space, preserving hierarchical relationships
and preventing unintended side effects by ensuring that edits to parent
concepts do not inadvertently affect child concepts; (ii) M\""obius-Transformed
Updates, which apply hyperbolic addition to propagate edits while maintaining
structural consistency within the hyperbolic manifold, unlike conventional
Euclidean updates that distort relational distances; and (iii) Dual
Stabilization, which combines gradient masking and periodic GNN parameter
resetting to prevent catastrophic forgetting by focusing updates on critical
parameters and preserving long-term knowledge. Experiments on CounterFact,
CounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE
significantly enhances edit stability, factual accuracy, and multi-hop
reasoning.",2025-05-23,"Yash Kumar Atri, Ahmed Alaa, Thomas Hartvigsen",http://arxiv.org/pdf/2505.18343v1,cs.CL
PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language,"Medical consumer question answering (CQA) is crucial for empowering patients
by providing personalized and reliable health information. Despite recent
advances in large language models (LLMs) for medical QA, consumer-oriented and
multilingual resources, particularly in low-resource languages like Persian,
remain sparse. To bridge this gap, we present PerMedCQA, the first
Persian-language benchmark for evaluating LLMs on real-world,
consumer-generated medical questions. Curated from a large medical QA forum,
PerMedCQA contains 68,138 question-answer pairs, refined through careful data
cleaning from an initial set of 87,780 raw entries. We evaluate several
state-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a
novel rubric-based evaluation framework driven by an LLM grader, validated
against expert human annotators. Our results highlight key challenges in
multilingual medical QA and provide valuable insights for developing more
accurate and context-aware medical assistance systems. The data is publicly
available on https://huggingface.co/datasets/NaghmehAI/PerMedCQA",2025-05-23,"Naghmeh Jamali, Milad Mohammadi, Danial Baledi, Zahra Rezvani, Hesham Faili",http://arxiv.org/pdf/2505.18331v1,cs.CL
Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4,"LLMs have been demonstrated to align with the values of Western or North
American cultures. Prior work predominantly showed this effect through
leveraging surveys that directly ask (originally people and now also LLMs)
about their values. However, it is hard to believe that LLMs would consistently
apply those values in real-world scenarios. To address that, we take a
bottom-up approach, asking LLMs to reason about cultural norms in narratives
from different cultures. We find that GPT-4 tends to generate norms that, while
not necessarily incorrect, are significantly less culture-specific. In
addition, while it avoids overtly generating stereotypes, the stereotypical
representations of certain cultures are merely hidden rather than suppressed in
the model, and such stereotypes can be easily recovered. Addressing these
challenges is a crucial step towards developing LLMs that fairly serve their
diverse user base.",2025-05-23,"Zhuozhuo Joy Liu, Farhan Samir, Mehar Bhatia, Laura K. Nelson, Vered Shwartz",http://arxiv.org/pdf/2505.18322v1,cs.CL
Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards,"Large language models (LLMs) have demonstrated strong reasoning abilities in
mathematical tasks, often enhanced through reinforcement learning (RL).
However, RL-trained models frequently produce unnecessarily long reasoning
traces -- even for simple queries -- leading to increased inference costs and
latency. While recent approaches attempt to control verbosity by adding length
penalties to the reward function, these methods rely on fixed penalty terms
that are hard to tune and cannot adapt as the model's reasoning capability
evolves, limiting their effectiveness. In this work, we propose an adaptive
reward-shaping method that enables LLMs to ""think fast and right"" -- producing
concise outputs without sacrificing correctness. Our method dynamically adjusts
the reward trade-off between accuracy and response length based on model
performance: when accuracy is high, the length penalty increases to encourage
faster length reduction; when accuracy drops, the penalty is relaxed to
preserve correctness. This adaptive reward accelerates early-stage length
reduction while avoiding over-compression in later stages. Experiments across
multiple datasets show that our approach consistently and dramatically reduces
reasoning length while largely maintaining accuracy, offering a new direction
for cost-efficient adaptive reasoning in large-scale language models.",2025-05-23,"Jinyan Su, Claire Cardie",http://arxiv.org/pdf/2505.18298v1,cs.CL
TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification,"Recent advances such as Chain-of-Thought prompting have significantly
improved large language models (LLMs) in zero-shot medical reasoning. However,
prompting-based methods often remain shallow and unstable, while fine-tuned
medical LLMs suffer from poor generalization under distribution shifts and
limited adaptability to unseen clinical scenarios. To address these
limitations, we present TAGS, a test-time framework that combines a broadly
capable generalist with a domain-specific specialist to offer complementary
perspectives without any model fine-tuning or parameter updates. To support
this generalist-specialist reasoning process, we introduce two auxiliary
modules: a hierarchical retrieval mechanism that provides multi-scale exemplars
by selecting examples based on both semantic and rationale-level similarity,
and a reliability scorer that evaluates reasoning consistency to guide final
answer aggregation. TAGS achieves strong performance across nine MedQA
benchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and
improving a vanilla 7B model from 14.1% to 23.9%. These results surpass several
fine-tuned medical LLMs, without any parameter updates. The code will be
available at https://github.com/JianghaoWu/TAGS.",2025-05-23,"Jianghao Wu, Feilong Tang, Yulong Li, Ming Hu, Haochen Xue, Shoaib Jameel, Yutong Xie, Imran Razzak",http://arxiv.org/pdf/2505.18283v1,cs.CL
Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control,"Complex tasks are increasingly delegated to ensembles of specialized
LLM-based agents that reason, communicate, and coordinate actions-both among
themselves and through interactions with external tools, APIs, and databases.
While persistent memory has been shown to enhance single-agent performance,
most approaches assume a monolithic, single-user context-overlooking the
benefits and challenges of knowledge transfer across users under dynamic,
asymmetric permissions. We introduce Collaborative Memory, a framework for
multi-user, multi-agent environments with asymmetric, time-evolving access
controls encoded as bipartite graphs linking users, agents, and resources. Our
system maintains two memory tiers: (1) private memory-private fragments visible
only to their originating user; and (2) shared memory-selectively shared
fragments. Each fragment carries immutable provenance attributes (contributing
agents, accessed resources, and timestamps) to support retrospective permission
checks. Granular read policies enforce current user-agent-resource constraints
and project existing memory fragments into filtered transformed views. Write
policies determine fragment retention and sharing, applying context-aware
transformations to update the memory. Both policies may be designed conditioned
on system, agent, and user-level information. Our framework enables safe,
efficient, and interpretable cross-user knowledge sharing, with provable
adherence to asymmetric, time-varying policies and full auditability of memory
operations.",2025-05-23,"Alireza Rezazadeh, Zichao Li, Ange Lou, Yuying Zhao, Wei Wei, Yujia Bao",http://arxiv.org/pdf/2505.18279v1,cs.CL
The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas,"Ethical decision-making is a critical aspect of human judgment, and the
growing use of LLMs in decision-support systems necessitates a rigorous
evaluation of their moral reasoning capabilities. However, existing assessments
primarily rely on single-step evaluations, failing to capture how models adapt
to evolving ethical challenges. Addressing this gap, we introduce the
Multi-step Moral Dilemmas (MMDs), the first dataset specifically constructed to
evaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas.
This framework enables a fine-grained, dynamic analysis of how LLMs adjust
their moral reasoning across escalating dilemmas. Our evaluation of nine widely
used LLMs reveals that their value preferences shift significantly as dilemmas
progress, indicating that models recalibrate moral judgments based on scenario
complexity. Furthermore, pairwise value comparisons demonstrate that while LLMs
often prioritize the value of care, this value can sometimes be superseded by
fairness in certain contexts, highlighting the dynamic and context-dependent
nature of LLM ethical reasoning. Our findings call for a shift toward dynamic,
context-aware evaluation paradigms, paving the way for more human-aligned and
value-sensitive development of LLMs.",2025-05-23,"Ya Wu, Qiang Sheng, Danding Wang, Guang Yang, Yifan Sun, Zhengjia Wang, Yuyan Bu, Juan Cao",http://arxiv.org/pdf/2505.18154v1,cs.CL
"Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs","Arabic poetry is one of the richest and most culturally rooted forms of
expression in the Arabic language, known for its layered meanings, stylistic
diversity, and deep historical continuity. Although large language models
(LLMs) have demonstrated strong performance across languages and tasks, their
ability to understand Arabic poetry remains largely unexplored. In this work,
we introduce \emph{Fann or Flop}, the first benchmark designed to assess the
comprehension of Arabic poetry by LLMs in 12 historical eras, covering 14 core
poetic genres and a variety of metrical forms, from classical structures to
contemporary free verse. The benchmark comprises a curated corpus of poems with
explanations that assess semantic understanding, metaphor interpretation,
prosodic awareness, and cultural context. We argue that poetic comprehension
offers a strong indicator for testing how good the LLM understands classical
Arabic through Arabic poetry. Unlike surface-level tasks, this domain demands
deeper interpretive reasoning and cultural sensitivity. Our evaluation of
state-of-the-art LLMs shows that most models struggle with poetic understanding
despite strong results on standard Arabic benchmarks. We release ""Fann or Flop""
along with the evaluation suite as an open-source resource to enable rigorous
evaluation and advancement for Arabic language models. Code is available at:
https://github.com/mbzuai-oryx/FannOrFlop.",2025-05-23,"Wafa Alghallabi, Ritesh Thawkar, Sara Ghaboura, Ketan More, Omkar Thawakar, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer",http://arxiv.org/pdf/2505.18152v2,cs.CL
First Finish Search: Efficient Test-Time Scaling in Large Language Models,"Test-time scaling (TTS), which involves dynamic allocation of compute during
inference, offers a promising way to improve reasoning in large language
models. While existing TTS methods work well, they often rely on long decoding
paths or require a large number of samples to be generated, increasing the
token usage and inference latency. We observe the surprising fact that for
reasoning tasks, shorter traces are much more likely to be correct than longer
ones. Motivated by this, we introduce First Finish Search (FFS), a
training-free parallel decoding strategy that launches $n$ independent samples
and returns as soon as any one completes. We evaluate FFS alongside simple
decoding, beam search, majority voting, and budget forcing on four reasoning
models (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and
across four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With
DeepSeek-R1, FFS achieves $82.23\%$ accuracy on the AIME datasets, a $15\%$
improvement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's
o4-mini performance. Our theoretical analysis explains why stopping at the
shortest trace is likely to yield a correct answer and identifies the
conditions under which early stopping may be suboptimal. The elegance and
simplicity of FFS demonstrate that straightforward TTS strategies can perform
remarkably well, revealing the untapped potential of simple approaches at
inference time.",2025-05-23,"Aradhye Agarwal, Ayan Sengupta, Tanmoy Chakraborty",http://arxiv.org/pdf/2505.18149v1,cs.CL
Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find,"Large language models (LLMs) face significant challenges with
needle-in-a-haystack tasks, where relevant information (""the needle"") must be
drawn from a large pool of irrelevant context (""the haystack""). Previous
studies have highlighted positional bias and distractor quantity as critical
factors affecting model performance, yet the influence of gold context size has
received little attention. We address this gap by systematically studying how
variations in gold context length impact LLM performance on long-context
question answering tasks. Our experiments reveal that LLM performance drops
sharply when the gold context is shorter, i.e., smaller gold contexts
consistently degrade model performance and amplify positional sensitivity,
posing a major challenge for agentic systems that must integrate scattered,
fine-grained information of varying lengths. This pattern holds across three
diverse domains (general knowledge, biomedical reasoning, and mathematical
reasoning) and seven state-of-the-art LLMs of various sizes and architectures.
Our work provides clear insights to guide the design of robust, context-aware
LLM-driven systems.",2025-05-23,"Owen Bianchi, Mathew J. Koretsky, Maya Willey, Chelsea X. Alvarado, Tanay Nayak, Adi Asija, Nicole Kuznetsov, Mike A. Nalls, Faraz Faghri, Daniel Khashabi",http://arxiv.org/pdf/2505.18148v1,cs.CL
Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection,"We introduce a next-generation vandalism detection system for Wikidata, one
of the largest open-source structured knowledge bases on the Web. Wikidata is
highly complex: its items incorporate an ever-expanding universe of factual
triples and multilingual texts. While edits can alter both structured and
textual content, our approach converts all edits into a single space using a
method we call Graph2Text. This allows for evaluating all content changes for
potential vandalism using a single multilingual language model. This unified
approach improves coverage and simplifies maintenance. Experiments demonstrate
that our solution outperforms the current production system. Additionally, we
are releasing the code under an open license along with a large dataset of
various human-generated knowledge alterations, enabling further research.",2025-05-23,"Mykola Trokhymovych, Lydia Pintscher, Ricardo Baeza-Yates, Diego Saez-Trumper",http://arxiv.org/pdf/2505.18136v1,cs.CL
Gaming Tool Preferences in Agentic LLMs,"Large language models (LLMs) can now access a wide range of external tools,
thanks to the Model Context Protocol (MCP). This greatly expands their
abilities as various agents. However, LLMs rely entirely on the text
descriptions of tools to decide which ones to use--a process that is
surprisingly fragile. In this work, we expose a vulnerability in prevalent
tool/function-calling protocols by investigating a series of edits to tool
descriptions, some of which can drastically increase a tool's usage from LLMs
when competing with alternatives. Through controlled experiments, we show that
tools with properly edited descriptions receive over 10 times more usage from
GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further
evaluate how various edits to tool descriptions perform when competing directly
with one another and how these trends generalize or differ across a broader set
of 10 different models. These phenomenons, while giving developers a powerful
way to promote their tools, underscore the need for a more reliable foundation
for agentic LLMs to select and utilize tools and resources.",2025-05-23,"Kazem Faghih, Wenxiao Wang, Yize Cheng, Siddhant Bharti, Gaurang Sriramanan, Sriram Balasubramanian, Parsa Hosseini, Soheil Feizi",http://arxiv.org/pdf/2505.18135v1,cs.CL
VideoGameBench: Can Vision-Language Models complete popular video games?,"Vision-language models (VLMs) have achieved strong results on coding and math
benchmarks that are challenging for humans, yet their ability to perform tasks
that come naturally to humans--such as perception, spatial navigation, and
memory management--remains understudied. Real video games are crafted to be
intuitive for humans to learn and master by leveraging innate inductive biases,
making them an ideal testbed for evaluating such capabilities in VLMs. To this
end, we introduce VideoGameBench, a benchmark consisting of 10 popular video
games from the 1990s that VLMs directly interact with in real-time.
VideoGameBench challenges models to complete entire games with access to only
raw visual inputs and a high-level description of objectives and controls, a
significant departure from existing setups that rely on game-specific
scaffolding and auxiliary information. We keep three of the games secret to
encourage solutions that generalize to unseen environments. Our experiments
show that frontier vision-language models struggle to progress beyond the
beginning of each game. We find inference latency to be a major limitation of
frontier models in the real-time setting; therefore, we introduce
VideoGameBench Lite, a setting where the game pauses while waiting for the LM's
next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of
VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization
of the human skills mentioned above into this benchmark motivates progress in
these research directions.",2025-05-23,"Alex L. Zhang, Thomas L. Griffiths, Karthik R. Narasimhan, Ofir Press",http://arxiv.org/pdf/2505.18134v1,cs.CL
One RL to See Them All: Visual Triple Unified Reinforcement Learning,"Reinforcement learning (RL) has significantly advanced the reasoning
capabilities of vision-language models (VLMs). However, the use of RL beyond
reasoning tasks remains largely unexplored, especially for perceptionintensive
tasks like object detection and grounding. We propose V-Triune, a Visual Triple
Unified Reinforcement Learning system that enables VLMs to jointly learn visual
reasoning and perception tasks within a single training pipeline. V-Triune
comprises triple complementary components: Sample-Level Data Formatting (to
unify diverse task inputs), Verifier-Level Reward Computation (to deliver
custom rewards via specialized verifiers) , and Source-Level Metric Monitoring
(to diagnose problems at the data-source level). We further introduce a novel
Dynamic IoU reward, which provides adaptive, progressive, and definite feedback
for perception tasks handled by V-Triune. Our approach is instantiated within
off-the-shelf RL training framework using open-source 7B and 32B backbone
models. The resulting model, dubbed Orsta (One RL to See Them All),
demonstrates consistent improvements across both reasoning and perception
tasks. This broad capability is significantly shaped by its training on a
diverse dataset, constructed around four representative visual reasoning tasks
(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,
Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains
on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1
across its various 7B and 32B model variants, with performance benefits
extending to a wide range of downstream tasks. These results highlight the
effectiveness and scalability of our unified RL approach for VLMs. The V-Triune
system, along with the Orsta models, is publicly available at
https://github.com/MiniMax-AI.",2025-05-23,"Yan Ma, Linge Du, Xuyang Shen, Shaoxiang Chen, Pengfei Li, Qibing Ren, Lizhuang Ma, Yuchao Dai, Pengfei Liu, Junjie Yan",http://arxiv.org/pdf/2505.18129v1,cs.CL
Frankentext: Stitching random text fragments into long-form narratives,"We introduce Frankentexts, a new type of long-form narratives produced by
LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied
verbatim from human writings. This task presents a challenging test of
controllable generation, requiring models to satisfy a writing prompt,
integrate disparate text fragments, and still produce a coherent narrative. To
generate Frankentexts, we instruct the model to produce a draft by selecting
and combining human-written passages, then iteratively revise the draft while
maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts
along three axes: writing quality, instruction adherence, and detectability.
Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts
are coherent and 100% relevant to the prompt. Notably, up to 59% of these
outputs are misclassified as human-written by detectors like Pangram, revealing
limitations in AI text detectors. Human annotators can sometimes identify
Frankentexts through their abrupt tone shifts and inconsistent grammar between
segments, especially in longer generations. Beyond presenting a challenging
generation task, Frankentexts invite discussion on building effective detectors
for this new grey zone of authorship, provide training data for mixed
authorship detection, and serve as a sandbox for studying human-AI co-writing
processes.",2025-05-23,"Chau Minh Pham, Jenna Russell, Dzung Pham, Mohit Iyyer",http://arxiv.org/pdf/2505.18128v1,cs.CL
Reward Model Overoptimisation in Iterated RLHF,"Reinforcement learning from human feedback (RLHF) is a widely used method for
aligning large language models with human preferences. However, RLHF often
suffers from reward model overoptimisation, in which models overfit to the
reward function, resulting in non-generalisable policies that exploit the
idiosyncrasies and peculiarities of the reward function. A common mitigation is
iterated RLHF, in which reward models are repeatedly retrained with updated
human feedback and policies are re-optimised. Despite its increasing adoption,
the dynamics of overoptimisation in this setting remain poorly understood. In
this work, we present the first comprehensive study of overoptimisation in
iterated RLHF. We systematically analyse key design choices - how reward model
training data is transferred across iterations, which reward function is used
for optimisation, and how policies are initialised. Using the controlled
AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over
successive iterations, as reward models increasingly approximate ground-truth
preferences. However, performance gains diminish over time, and while
reinitialising from the base policy is robust, it limits optimisation
flexibility. Other initialisation strategies often fail to recover from early
overoptimisation. These findings offer actionable insights for building more
stable and generalisable RLHF pipelines.",2025-05-23,"Lorenz Wolf, Robert Kirk, Mirco Musolesi",http://arxiv.org/pdf/2505.18126v1,cs.CL
TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations,"While deep learning has achieved remarkable success across many domains, it
has historically underperformed on tabular learning tasks, which remain
dominated by gradient boosting decision trees (GBDTs). However, recent
advancements are paving the way for Tabular Foundation Models, which can
leverage real-world knowledge and generalize across diverse datasets,
particularly when the data contains free-text. Although incorporating language
model capabilities into tabular tasks has been explored, most existing methods
utilize static, target-agnostic textual representations, limiting their
effectiveness. We introduce TabSTAR: a Foundation Tabular Model with
Semantically Target-Aware Representations. TabSTAR is designed to enable
transfer learning on tabular data with textual features, with an architecture
free of dataset-specific parameters. It unfreezes a pretrained text encoder and
takes as input target tokens, which provide the model with the context needed
to learn task-specific embeddings. TabSTAR achieves state-of-the-art
performance for both medium- and large-sized datasets across known benchmarks
of classification tasks with text features, and its pretraining phase exhibits
scaling laws in the number of datasets, offering a pathway for further
performance improvements.",2025-05-23,"Alan Arazi, Eilam Shapira, Roi Reichart",http://arxiv.org/pdf/2505.18125v1,cs.CL
UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification,"Recent advances in large language models (LLMs) have greatly improved
Text-to-SQL performance for single-table queries. But, it remains challenging
in multi-table databases due to complex schema and relational operations.
Existing methods often struggle with retrieving the right tables and columns,
generating accurate JOINs and UNIONs, and generalizing across diverse schemas.
To address these issues, we introduce UNJOIN, a two-stage framework that
decouples the retrieval of schema elements from SQL logic generation. In the
first stage, we merge the column names of all tables in the database into a
single-table representation by prefixing each column with its table name. This
allows the model to focus purely on accurate retrieval without being distracted
by the need to write complex SQL logic. In the second stage, the SQL query is
generated on this simplified schema and mapped back to the original schema by
reconstructing JOINs, UNIONs, and relational logic. Evaluations on SPIDER and
BIRD datasets show that UNJOIN matches or exceeds the state-of-the-art
baselines. UNJOIN uses only schema information, which does not require data
access or fine-tuning, making it scalable and adaptable across databases.",2025-05-23,"Poojah Ganesan, Rajat Aayush Jha, Dan Roth, Vivek Gupta",http://arxiv.org/pdf/2505.18122v1,cs.CL
ProgRM: Build Better GUI Agents with Progress Rewards,"LLM-based (Large Language Model) GUI (Graphical User Interface) agents can
potentially reshape our daily lives significantly. However, current LLM-based
GUI agents suffer from the scarcity of high-quality training data owing to the
difficulties of trajectory collection and reward annotation. Existing works
have been exploring LLMs to collect trajectories for imitation learning or to
offer reward signals for online RL training. However, the Outcome Reward Model
(ORM) used in existing works cannot provide finegrained feedback and can
over-penalize the valuable steps in finally failed trajectories. To this end,
we propose Progress Reward Model (ProgRM) to provide dense informative
intermediate rewards by predicting a task completion progress for each step in
online training. To handle the challenge of progress reward label annotation,
we further design an efficient LCS-based (Longest Common Subsequence)
self-annotation algorithm to discover the key steps in trajectories and assign
progress labels accordingly. ProgRM is evaluated with extensive experiments and
analyses. Actors trained with ProgRM outperform leading proprietary LLMs and
ORM-trained actors, illustrating the effectiveness of ProgRM. The codes for
experiments will be made publicly available upon acceptance.",2025-05-23,"Danyang Zhang, Situo Zhang, Ziyue Yang, Zichen Zhu, Zihan Zhao, Ruisheng Cao, Lu Chen, Kai Yu",http://arxiv.org/pdf/2505.18121v1,cs.CL
MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning,"Despite the widespread exploration of Retrieval-Augmented Generation (RAG),
its deployment in enterprises for domain-specific datasets remains limited due
to poor answer accuracy. These corpora, often shielded behind firewalls in
private enterprise knowledge bases, having complex, domain-specific
terminology, rarely seen by LLMs during pre-training; exhibit significant
semantic variability across domains (like networking, military, or legal,
etc.), or even within a single domain like medicine, and thus result in poor
context precision for RAG systems. Currently, in such situations, fine-tuning
or RAG with fine-tuning is attempted, but these approaches are slow, expensive,
and lack generalization for accuracy as the new domain-specific data emerges.
We propose an approach for Enterprise Search that focuses on enhancing the
retriever for a domain-specific corpus through hybrid query indexes and
metadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata
generation pipeline using key concepts, topics, and acronyms, and then creates
a metadata-enriched hybrid index with boosted search queries. This approach
avoids overfitting and generalizes effectively across domains. On the PubMedQA
benchmark for the biomedical domain, the proposed method achieves 82% retrieval
accuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results
without fine-tuning and sets a new benchmark for zero-shot results while
outperforming much larger models like GPT3.5. The results are even comparable
to the best fine-tuned models on this dataset, and we further demonstrate the
robustness and scalability of the approach by evaluating it on other Q&A
datasets like SQuAD, NQ etc.",2025-05-23,"Kunal Sawarkar, Shivam R. Solanki, Abhilasha Mangal",http://arxiv.org/pdf/2505.18247v1,cs.CL
Bridging Supervised Learning and Reinforcement Learning in Math Reasoning,"Reinforcement Learning (RL) has played a central role in the recent surge of
LLMs' math abilities by enabling self-improvement through binary verifier
signals. In contrast, Supervised Learning (SL) is rarely considered for such
verification-driven training, largely due to its heavy reliance on reference
answers and inability to reflect on mistakes. In this work, we challenge the
prevailing notion that self-improvement is exclusive to RL and propose
Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to
reflect on their failures and improve autonomously with no external teachers.
In online training, instead of throwing away self-generated negative answers,
NFT constructs an implicit negative policy to model them. This implicit policy
is parameterized with the same positive LLM we target to optimize on positive
data, enabling direct policy optimization on all LLMs' generations. We conduct
experiments on 7B and 32B models in math reasoning tasks. Results consistently
show that through the additional leverage of negative feedback, NFT
significantly improves over SL baselines like Rejection sampling Fine-Tuning,
matching or even surpassing leading RL algorithms like GRPO and DAPO.
Furthermore, we demonstrate that NFT and GRPO are actually equivalent in
strict-on-policy training, even though they originate from entirely different
theoretical foundations. Our experiments and theoretical findings bridge the
gap between SL and RL methods in binary-feedback learning systems.",2025-05-23,"Huayu Chen, Kaiwen Zheng, Qinsheng Zhang, Ganqu Cui, Yin Cui, Haotian Ye, Tsung-Yi Lin, Ming-Yu Liu, Jun Zhu, Haoxiang Wang",http://arxiv.org/pdf/2505.18116v1,cs.CL
Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM,"Humans naturally understand moments in a video by integrating visual and
auditory cues. For example, localizing a scene in the video like ""A scientist
passionately speaks on wildlife conservation as dramatic orchestral music
plays, with the audience nodding and applauding"" requires simultaneous
processing of visual, audio, and speech signals. However, existing models often
struggle to effectively fuse and interpret audio information, limiting their
capacity for comprehensive video temporal understanding. To address this, we
present TriSense, a triple-modality large language model designed for holistic
video temporal understanding through the integration of visual, audio, and
speech modalities. Central to TriSense is a Query-Based Connector that
adaptively reweights modality contributions based on the input query, enabling
robust performance under modality dropout and allowing flexible combinations of
available inputs. To support TriSense's multimodal capabilities, we introduce
TriSense-2M, a high-quality dataset of over 2 million curated samples generated
via an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes
long-form videos and diverse modality combinations, facilitating broad
generalization. Extensive experiments across multiple benchmarks demonstrate
the effectiveness of TriSense and its potential to advance multimodal video
analysis. Code and dataset will be publicly released.",2025-05-23,"Zinuo Li, Xian Zhang, Yongxin Guo, Mohammed Bennamoun, Farid Boussaid, Girish Dwivedi, Luqi Gong, Qiuhong Ke",http://arxiv.org/pdf/2505.18110v1,cs.CL
Will Large Language Models Transform Clinical Prediction?,"Background: Large language models (LLMs) are attracting increasing interest
in healthcare. Their ability to summarise large datasets effectively, answer
questions accurately, and generate synthesised text is widely recognised. These
capabilities are already finding applications in healthcare. Body: This
commentary discusses LLMs usage in the clinical prediction context and
highlight potential benefits and existing challenges. In these early stages,
the focus should be on extending the methodology, specifically on validation,
fairness and bias evaluation, survival analysis and development of regulations.
Conclusion: We conclude that further work and domain-specific considerations
need to be made for full integration into the clinical prediction workflows.",2025-05-23,"Yusuf Yildiz, Goran Nenadic, Meghna Jani, David A. Jenkins",http://arxiv.org/pdf/2505.18246v1,cs.CL
ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework,"Recent advances in web-augmented large language models (LLMs) have exhibited
strong performance in complex reasoning tasks, yet these capabilities are
mostly locked in proprietary systems with opaque architectures. In this work,
we propose \textbf{ManuSearch}, a transparent and modular multi-agent framework
designed to democratize deep search for LLMs. ManuSearch decomposes the search
and reasoning process into three collaborative agents: (1) a solution planning
agent that iteratively formulates sub-queries, (2) an Internet search agent
that retrieves relevant documents via real-time web search, and (3) a
structured webpage reading agent that extracts key evidence from raw web
content. To rigorously evaluate deep reasoning abilities, we introduce
\textbf{ORION}, a challenging benchmark focused on open-web reasoning over
long-tail entities, covering both English and Chinese. Experimental results
show that ManuSearch substantially outperforms prior open-source baselines and
even surpasses leading closed-source systems. Our work paves the way for
reproducible, extensible research in open deep search systems. We release the
data and code in https://github.com/RUCAIBox/ManuSearch",2025-05-23,"Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, Wayne Xin Zhao",http://arxiv.org/pdf/2505.18105v1,cs.CL
How Can I Publish My LLM Benchmark Without Giving the True Answers Away?,"Publishing a large language model (LLM) benchmark on the Internet risks
contaminating future LLMs: the benchmark may be unintentionally (or
intentionally) used to train or select a model. A common mitigation is to keep
the benchmark private and let participants submit their models or predictions
to the organizers. However, this strategy will require trust in a single
organization and still permits test-set overfitting through repeated queries.
To overcome this issue, we propose a way to publish benchmarks without
completely disclosing the ground-truth answers to the questions, while still
maintaining the ability to openly evaluate LLMs. Our main idea is to inject
randomness to the answers by preparing several logically correct answers, and
only include one of them as the solution in the benchmark. This reduces the
best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is
this helpful to keep us from disclosing the ground truth, but this approach
also offers a test for detecting data contamination. In principle, even fully
capable models should not surpass the Bayes accuracy. If a model surpasses this
ceiling despite this expectation, this is a strong signal of data
contamination. We present experimental evidence that our method can detect data
contamination accurately on a wide range of benchmarks, models, and training
methodologies.",2025-05-23,"Takashi Ishida, Thanawat Lodkaew, Ikko Yamane",http://arxiv.org/pdf/2505.18102v1,cs.CL
Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models,"Large Transformer based language models achieve remarkable performance but
remain opaque in how they plan, structure, and realize text. We introduce
Multi_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework
that factorizes generation into three semantic scales_global context,
intermediate structure, and local word choices and aligns each scale with
specific layer ranges in Transformer architectures. To identify scale
boundaries, we propose two complementary metrics: attention span thresholds and
inter layer mutual information peaks. Across four representative models (GPT-2,
BERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global
partitions, corroborated by probing tasks and causal interventions. We find
that decoder_only models allocate more layers to intermediate and global
processing while encoder_only models emphasize local feature extraction.
Through targeted interventions, we demonstrate that local scale manipulations
primarily influence lexical diversity, intermediate-scale modifications affect
sentence structure and length, and global_scale perturbations impact discourse
coherence all with statistically significant effects. MSPGT thus offers a
unified, architecture-agnostic method for interpreting, diagnosing, and
controlling large language models, bridging the gap between mechanistic
interpretability and emergent capabilities.",2025-05-23,"Yukin Zhang, Qi Dong",http://arxiv.org/pdf/2505.18244v1,cs.CL
Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL,"Large language models (LLMs) excel in tasks like question answering and
dialogue, but complex tasks requiring interaction, such as negotiation and
persuasion, require additional long-horizon reasoning and planning.
Reinforcement learning (RL) fine-tuning can enable such planning in principle,
but suffers from drawbacks that hinder scalability. In particular, multi-turn
RL training incurs high memory and computational costs, which are exacerbated
when training LLMs as policies. Furthermore, the largest LLMs do not expose the
APIs necessary to be trained in such manner. As a result, modern methods to
improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather
than RL fine-tuning. To remedy this, we propose a novel approach that uses
goal-conditioned value functions to guide the reasoning of LLM agents, that
scales even to large API-based models. These value functions predict how a task
will unfold given an action, allowing the LLM agent to evaluate multiple
possible outcomes, both positive and negative, to plan effectively. In
addition, these value functions are trained over reasoning steps rather than
full actions, to be a concise and light-weight module that facilitates
decision-making in multi-turn interactions. We validate our method on tasks
requiring interaction, including tool use, social deduction, and dialogue,
demonstrating superior performance over both RL fine-tuning and prompting
methods while maintaining efficiency and scalability.",2025-05-23,"Joey Hong, Anca Dragan, Sergey Levine",http://arxiv.org/pdf/2505.18098v1,cs.CL
QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization,"This technical report presents QwenLong-CPRS, a context compression framework
designed for explicit long-context optimization, addressing prohibitive
computation overhead during the prefill stage and the ""lost in the middle""
performance degradation of large language models (LLMs) during long sequence
processing. Implemented through a novel dynamic context optimization mechanism,
QwenLong-CPRS enables multi-granularity context compression guided by natural
language instructions, achieving both efficiency gains and improved
performance.
  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key
innovations: (1) Natural language-guided dynamic optimization, (2)
Bidirectional reasoning layers for enhanced boundary awareness, (3) Token
critic mechanisms with language modeling heads, and (4) Window-parallel
inference.
  Comprehensive evaluations across five benchmarks (4K-2M word contexts)
demonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority
over other context management methods like RAG and sparse attention in both
accuracy and efficiency. (2) Architecture-agnostic integration with all
flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,
and Qwen2.5-max, achieves 21.59$\times$ context compression alongside
19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,
QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on
Ruler-128K and InfiniteBench, establishing new SOTA performance.",2025-05-23,"Weizhou Shen, Chenliang Li, Fanqi Wan, Shengyi Liao, Shaopeng Lai, Bo Zhang, Yingcheng Shi, Yuning Wu, Gang Fu, Zhansheng Li, Bin Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan",http://arxiv.org/pdf/2505.18092v1,cs.CL
Data Mixing Can Induce Phase Transitions in Knowledge Acquisition,"Large Language Models (LLMs) are typically trained on data mixtures: most
data come from web scrapes, while a small portion is curated from high-quality
sources with dense domain-specific knowledge. In this paper, we show that when
training LLMs on such data mixtures, knowledge acquisition from knowledge-dense
datasets, unlike training exclusively on knowledge-dense data
(arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit
phase transitions with respect to the mixing ratio and model size. Through
controlled experiments on a synthetic biography dataset mixed with web-scraped
data, we demonstrate that: (1) as we increase the model size to a critical
value, the model suddenly transitions from memorizing very few to most of the
biographies; (2) below a critical mixing ratio, the model memorizes almost
nothing even with extensive training, but beyond this threshold, it rapidly
memorizes more biographies. We attribute these phase transitions to a capacity
allocation phenomenon: a model with bounded capacity must act like a knapsack
problem solver to minimize the overall test loss, and the optimal allocation
across datasets can change discontinuously as the model size or mixing ratio
varies. We formalize this intuition in an information-theoretic framework and
reveal that these phase transitions are predictable, with the critical mixing
ratio following a power-law relationship with the model size. Our findings
highlight a concrete case where a good mixing recipe for large models may not
be optimal for small models, and vice versa.",2025-05-23,"Xinran Gu, Kaifeng Lyu, Jiazheng Li, Jingzhao Zhang",http://arxiv.org/pdf/2505.18091v1,cs.CL
Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding,"Long-form video understanding presents significant challenges due to
extensive temporal-spatial complexity and the difficulty of question answering
under such extended contexts. While Large Language Models (LLMs) have
demonstrated considerable advancements in video analysis capabilities and long
context handling, they continue to exhibit limitations when processing
information-dense hour-long videos. To overcome such limitations, we propose
the Deep Video Discovery agent to leverage an agentic search strategy over
segmented video clips. Different from previous video agents manually designing
a rigid workflow, our approach emphasizes the autonomous nature of agents. By
providing a set of search-centric tools on multi-granular video database, our
DVD agent leverages the advanced reasoning capability of LLM to plan on its
current observation state, strategically selects tools, formulates appropriate
parameters for actions, and iteratively refines its internal reasoning in light
of the gathered information. We perform comprehensive evaluation on multiple
long video understanding benchmarks that demonstrates the advantage of the
entire system design. Our DVD agent achieves SOTA performance, significantly
surpassing prior works by a large margin on the challenging LVBench dataset.
Comprehensive ablation studies and in-depth tool analyses are also provided,
yielding insights to further advance intelligent agents tailored for long-form
video understanding tasks. The code will be released later.",2025-05-23,"Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu",http://arxiv.org/pdf/2505.18079v1,cs.CL
Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals,"Large language models (LLMs) have demonstrated significant success in complex
reasoning tasks such as math and coding. In contrast to these tasks where
deductive reasoning predominates, inductive reasoning\textemdash the ability to
derive general rules from incomplete evidence, remains underexplored. This
paper investigates extended inductive reasoning in LLMs through the lens of
personalized preference inference, a critical challenge in LLM alignment where
current approaches struggle to capture diverse user preferences. The task
demands strong inductive reasoning capabilities as user preferences are
typically embedded implicitly across various interaction forms, requiring
models to synthesize consistent preference patterns from scattered signals. We
propose \textsc{AlignXplore}, a model that leverages extended reasoning chains
to enable systematic preference inference from behavioral signals in users'
interaction histories. We develop \textsc{AlignXplore} by combining cold-start
training based on synthetic data with subsequent online reinforcement learning.
Through extensive experiments, we demonstrate that \textsc{AlignXplore}
achieves substantial improvements over the backbone model by an average of
11.05\% on in-domain and out-of-domain benchmarks, while maintaining strong
generalization ability across different input formats and downstream models.
Further analyses establish best practices for preference inference learning
through systematic comparison of reward modeling strategies, while revealing
the emergence of human-like inductive reasoning patterns during training.",2025-05-23,"Jia-Nan Li, Jian Guan, Wei Wu, Rui Yan",http://arxiv.org/pdf/2505.18071v1,cs.CL
MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving,"Online learning enhances educational accessibility, offering students the
flexibility to learn anytime, anywhere. However, a key limitation is the lack
of immediate, personalized feedback, particularly in helping students correct
errors in math problem-solving. Several studies have investigated the
applications of large language models (LLMs) in educational contexts. In this
paper, we explore the capabilities of LLMs to assess students' math
problem-solving processes and provide adaptive feedback. The MathEDU dataset is
introduced, comprising authentic student solutions annotated with teacher
feedback. We evaluate the model's ability to support personalized learning in
two scenarios: one where the model has access to students' prior answer
histories, and another simulating a cold-start context. Experimental results
show that the fine-tuned model performs well in identifying correctness.
However, the model still faces challenges in generating detailed feedback for
pedagogical purposes.",2025-05-23,"Wei-Ling Hsu, Yu-Chien Tang, An-Zi Yen",http://arxiv.org/pdf/2505.18056v1,cs.CL
Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition,"The ability to handle various emotion labels without dedicated training is
crucial for building adaptable Emotion Recognition (ER) systems. Conventional
ER models rely on training using fixed label sets and struggle to generalize
beyond them. On the other hand, Large Language Models (LLMs) have shown strong
zero-shot ER performance across diverse label spaces, but their scale limits
their use on edge devices. In this work, we propose a contrastive distillation
framework that transfers rich emotional knowledge from LLMs into a compact
model without the use of human annotations. We use GPT-4 to generate
descriptive emotion annotations, offering rich supervision beyond fixed label
sets. By aligning text samples with emotion descriptors in a shared embedding
space, our method enables zero-shot prediction on different emotion classes,
granularity, and label schema. The distilled model is effective across multiple
datasets and label spaces, outperforming strong baselines of similar size and
approaching GPT-4's zero-shot performance, while being over 10,000 times
smaller.",2025-05-23,"Minxue Niu, Emily Mower Provost",http://arxiv.org/pdf/2505.18040v1,cs.CL
Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks,"Despite remarkable advances in the field, LLMs remain unreliable in
distinguishing causation from correlation. Recent results from the Corr2Cause
dataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:
29.08) -- only marginally outperform random baselines (Random Uniform, F1
score: 20.38), indicating limited capacity of generalization. To tackle this
limitation, we propose a novel structured approach: rather than directly
answering causal queries, we provide the model with the capability to structure
its thinking by guiding the model to build a structured knowledge graph,
systematically encoding the provided correlational premises, to answer the
causal queries. This intermediate representation significantly enhances the
model's causal capabilities. Experiments on the test subset of the Corr2Cause
dataset benchmark with Qwen3-32B model (reasoning model) show substantial gains
over standard direct prompting methods, improving F1 scores from 32.71 to 48.26
(over 47.5% relative increase), along with notable improvements in precision
and recall. These results underscore the effectiveness of providing the model
with the capability to structure its thinking and highlight its promising
potential for broader generalization across diverse causal inference tasks.",2025-05-23,"Wentao Sun, Joao Paulo Nogueira, Alonso Silva",http://arxiv.org/pdf/2505.18034v1,cs.CL
Training with Pseudo-Code for Instruction Following,"Despite the rapid progress in the capabilities of Large Language Models
(LLMs), they continue to have difficulty following relatively simple,
unambiguous instructions, especially when compositions are involved. In this
paper, we take inspiration from recent work that suggests that models may
follow instructions better when they are expressed in pseudo-code. However,
writing pseudo-code programs can be tedious and using few-shot demonstrations
to craft code representations for use in inference can be unnatural for
non-expert users of LLMs. To overcome these limitations, we propose fine-tuning
LLMs with instruction-tuning data that additionally includes instructions
re-expressed in pseudo-code along with the final response. We evaluate models
trained using our method on $11$ publicly available benchmarks comprising of
tasks related to instruction-following, mathematics, and common-sense
reasoning. We conduct rigorous experiments with $5$ different models and find
that not only do models follow instructions better when trained with
pseudo-code, they also retain their capabilities on the other tasks related to
mathematical and common sense reasoning. Specifically, we observe a relative
gain of $3$--$19$% on instruction-following benchmark, and an average gain of
upto 14% across all tasks.",2025-05-23,"Prince Kumar, Rudra Murthy, Riyaz Bhat, Danish Contractor",http://arxiv.org/pdf/2505.18011v1,cs.CL
TRACE for Tracking the Emergence of Semantic Representations in Transformers,"Modern transformer models exhibit phase transitions during training, distinct
shifts from memorisation to abstraction, but the mechanisms underlying these
transitions remain poorly understood. Prior work has often focused on endpoint
representations or isolated signals like curvature or mutual information,
typically in symbolic or arithmetic domains, overlooking the emergence of
linguistic structure. We introduce TRACE (Tracking Representation Abstraction
and Compositional Emergence), a diagnostic framework combining geometric,
informational, and linguistic signals to detect phase transitions in
Transformer-based LMs. TRACE leverages a frame-semantic data generation method,
ABSynth, that produces annotated synthetic corpora with controllable
complexity, lexical distributions, and structural entropy, while being fully
annotated with linguistic categories, enabling precise analysis of abstraction
emergence. Experiments reveal that (i) phase transitions align with clear
intersections between curvature collapse and dimension stabilisation; (ii)
these geometric shifts coincide with emerging syntactic and semantic accuracy;
(iii) abstraction patterns persist across architectural variants, with
components like feedforward networks affecting optimisation stability rather
than fundamentally altering trajectories. This work advances our understanding
of how linguistic abstractions emerge in LMs, offering insights into model
interpretability, training efficiency, and compositional generalisation that
could inform more principled approaches to LM development.",2025-05-23,"Nura Aljaafari, Danilo S. Carvalho, André Freitas",http://arxiv.org/pdf/2505.17998v1,cs.CL
Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective,"The VAPO framework has demonstrated significant empirical success in
enhancing the efficiency and reliability of reinforcement learning for long
chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By
systematically addressing challenges such as value model bias, heterogeneous
sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art
performance. While its practical benefits are evident, a deeper theoretical
understanding of its underlying mechanisms and potential limitations is crucial
for guiding future advancements. This paper aims to initiate such a discussion
by exploring VAPO from a theoretical perspective, highlighting areas where its
assumptions might be challenged and where further investigation could yield
more robust and generalizable reasoning agents. We delve into the intricacies
of value function approximation in complex reasoning spaces, the optimality of
adaptive advantage estimation, the impact of token-level optimization, and the
enduring challenges of exploration and generalization.",2025-05-23,"Jintian Shao, Yiming Cheng, Hongyi Huang, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng",http://arxiv.org/pdf/2505.17997v1,cs.CL
AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web,"Textual claims are often accompanied by images to enhance their credibility
and spread on social media, but this also raises concerns about the spread of
misinformation. Existing datasets for automated verification of image-text
claims remain limited, as they often consist of synthetic claims and lack
evidence annotations to capture the reasoning behind the verdict. In this work,
we introduce AVerImaTeC, a dataset consisting of 1,297 real-world image-text
claims. Each claim is annotated with question-answer (QA) pairs containing
evidence from the web, reflecting a decomposed reasoning regarding the verdict.
We mitigate common challenges in fact-checking datasets such as contextual
dependence, temporal leakage, and evidence insufficiency, via claim
normalization, temporally constrained evidence annotation, and a two-stage
sufficiency check. We assess the consistency of the annotation in AVerImaTeC
via inter-annotator studies, achieving a $\kappa=0.742$ on verdicts and
$74.7\%$ consistency on QA pairs. We also propose a novel evaluation method for
evidence retrieval and conduct extensive experiments to establish baselines for
verifying image-text claims using open-web evidence.",2025-05-23,"Rui Cao, Zifeng Ding, Zhijiang Guo, Michael Schlichtkrull, Andreas Vlachos",http://arxiv.org/pdf/2505.17978v1,cs.CL
Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems,"Using AI to create autonomous researchers has the potential to accelerate
scientific discovery. A prerequisite for this vision is understanding how well
an AI model can identify the underlying structure of a black-box system from
its behavior. In this paper, we explore how well a large language model (LLM)
learns to identify a black-box function from passively observed versus actively
collected data. We investigate the reverse-engineering capabilities of LLMs
across three distinct types of black-box systems, each chosen to represent
different problem domains where future autonomous AI researchers may have
considerable impact: Program, Formal Language, and Math Equation. Through
extensive experiments, we show that LLMs fail to extract information from
observations, reaching a performance plateau that falls short of the ideal of
Bayesian inference. However, we demonstrate that prompting LLMs to not only
observe but also intervene -- actively querying the black-box with specific
inputs to observe the resulting output -- improves performance by allowing LLMs
to test edge cases and refine their beliefs. By providing the intervention data
from one LLM to another, we show that this improvement is partly a result of
engaging in the process of generating effective interventions, paralleling
results in the literature on human learning. Further analysis reveals that
engaging in intervention can help LLMs escape from two common failure modes:
overcomplication, where the LLM falsely assumes prior knowledge about the
black-box, and overlooking, where the LLM fails to incorporate observations.
These insights provide practical guidance for helping LLMs more effectively
reverse-engineer black-box systems, supporting their use in making new
discoveries.",2025-05-23,"Jiayi Geng, Howard Chen, Dilip Arumugam, Thomas L. Griffiths",http://arxiv.org/pdf/2505.17968v1,cs.CL
Counting Cycles with Deepseek,"Despite recent progress, AI still struggles on advanced mathematics. We
consider a difficult open problem: How to derive a Computationally Efficient
Equivalent Form (CEEF) for the cycle count statistic? The CEEF problem does not
have known general solutions, and requires delicate combinatorics and tedious
calculations. Such a task is hard to accomplish by humans but is an ideal
example where AI can be very helpful. We solve the problem by combining a novel
approach we propose and the powerful coding skills of AI. Our results use
delicate graph theory and contain new formulas for general cases that have not
been discovered before. We find that, while AI is unable to solve the problem
all by itself, it is able to solve it if we provide it with a clear strategy, a
step-by-step guidance and carefully written prompts. For simplicity, we focus
our study on DeepSeek-R1 but we also investigate other AI approaches.",2025-05-23,"Jiashun Jin, Tracy Ke, Bingcheng Sui, Zhenggang Wang",http://arxiv.org/pdf/2505.17964v1,cs.CL
Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback,"The generation of presentation slides automatically is an important problem
in the era of generative AI. This paper focuses on evaluating multimodal
content in presentation slides that can effectively summarize a document and
convey concepts to a broad audience. We introduce a benchmark dataset,
RefSlides, consisting of human-made high-quality presentations that span
various topics. Next, we propose a set of metrics to characterize different
intrinsic properties of the content of a presentation and present REFLEX, an
evaluation approach that generates scores and actionable feedback for these
metrics. We achieve this by generating negative presentation samples with
different degrees of metric-specific perturbations and use them to fine-tune
LLMs. This reference-free evaluation technique does not require ground truth
presentations during inference. Our extensive automated and human experiments
demonstrate that our evaluation approach outperforms classical heuristic-based
and state-of-the-art large language model-based evaluations in generating
scores and explanations.",2025-05-23,"Ananth Muppidi, Tarak Das, Sambaran Bandyopadhyay, Tripti Shukla, Dharun D A",http://arxiv.org/pdf/2505.18240v1,cs.CL
Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL,"Improving performance on complex tasks and enabling interpretable decision
making in large language models (LLMs), especially for clinical applications,
requires effective reasoning. Yet this remains challenging without supervised
fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from
closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the
first medical LLM to show that reasoning capability can emerge purely through
reinforcement learning (RL), using minimalist rule-based rewards on public
multiple-choice QA datasets, without relying on SFT or distilled CoT data.
AlphaMed achieves state-of-the-art results on six medical QA benchmarks,
outperforming models trained with conventional SFT+RL pipelines. On challenging
benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source
models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the
factors behind this success, we conduct a comprehensive data-centric analysis
guided by three questions: (i) Can minimalist rule-based RL incentivize
reasoning without distilled CoT supervision? (ii) How do dataset quantity and
diversity impact reasoning? (iii) How does question difficulty shape the
emergence and generalization of reasoning? Our findings show that dataset
informativeness is a key driver of reasoning performance, and that minimalist
RL on informative, multiple-choice QA data is effective at inducing reasoning
without CoT supervision. We also observe divergent trends across benchmarks,
underscoring limitations in current evaluation and the need for more
challenging, reasoning-oriented medical QA benchmarks.",2025-05-23,"Che Liu, Haozhe Wang, Jiazhen Pan, Zhongwei Wan, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel Rueckert, Rossella Arcucci",http://arxiv.org/pdf/2505.17952v1,cs.CL
Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models,"Recent advancements in Natural Language Processing (NLP) have facilitated the
analysis of student-generated language products in learning analytics (LA),
particularly through the use of NLP embedding models. Yet when it comes to
science-related language, symbolic expressions such as equations and formulas
introduce challenges that current embedding models struggle to address.
Existing studies and applications often either overlook these challenges or
remove symbolic expressions altogether, potentially leading to biased findings
and diminished performance of LA applications. This study therefore explores
how contemporary embedding models differ in their capability to process and
interpret science-related symbolic expressions. To this end, various embedding
models are evaluated using physics-specific symbolic expressions drawn from
authentic student responses, with performance assessed via two approaches:
similarity-based analyses and integration into a machine learning pipeline. Our
findings reveal significant differences in model performance, with OpenAI's
GPT-text-embedding-3-large outperforming all other examined models, though its
advantage over other models was moderate rather than decisive. Beyond
performance, additional factors such as cost, regulatory compliance, and model
transparency are discussed as key considerations for model selection. Overall,
this study underscores the importance for LA researchers and practitioners of
carefully selecting NLP embedding models when working with science-related
language products that include symbolic expressions.",2025-05-23,"Tom Bleckmann, Paul Tschisgale",http://arxiv.org/pdf/2505.17950v1,cs.CL
Understanding Gated Neurons in Transformers from Their Input-Output Functionality,"Interpretability researchers have attempted to understand MLP neurons of
language models based on both the contexts in which they activate and their
output weight vectors. They have paid little attention to a complementary
aspect: the interactions between input and output. For example, when neurons
detect a direction in the input, they might add much the same direction to the
residual stream (""enrichment neurons"") or reduce its presence (""depletion
neurons""). We address this aspect by examining the cosine similarity between
input and output weights of a neuron. We apply our method to 12 models and find
that enrichment neurons dominate in early-middle layers whereas later layers
tend more towards depletion. To explain this finding, we argue that enrichment
neurons are largely responsible for enriching concept representations, one of
the first steps of factual recall. Our input-output perspective is a complement
to activation-dependent analyses and to approaches that treat input and output
separately.",2025-05-23,"Sebastian Gerstner, Hinrich Schütze",http://arxiv.org/pdf/2505.17936v1,cs.CL
Towards Practical Defect-Focused Automated Code Review,"The complexity of code reviews has driven efforts to automate review
comments, but prior approaches oversimplify this task by treating it as
snippet-level code-to-text generation and relying on text similarity metrics
like BLEU for evaluation. These methods overlook repository context, real-world
merge request evaluation, and defect detection, limiting their practicality. To
address these issues, we explore the full automation pipeline within the online
recommendation service of a company with nearly 400 million daily active users,
analyzing industry-grade C++ codebases comprising hundreds of thousands of
lines of code. We identify four key challenges: 1) capturing relevant context,
2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and
4) integrating human workflows. To tackle these, we propose 1) code slicing
algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a
filtering mechanism for FAR reduction, and 4) a novel prompt design for better
human interaction. Our approach, validated on real-world merge requests from
historical fault reports, achieves a 2x improvement over standard LLMs and a
10x gain over previous baselines. While the presented results focus on C++, the
underlying framework design leverages language-agnostic principles (e.g.,
AST-based analysis), suggesting potential for broader applicability.",2025-05-23,"Junyi Lu, Lili Jiang, Xiaojia Li, Jianbing Fang, Fengjun Zhang, Li Yang, Chun Zuo",http://arxiv.org/pdf/2505.17928v1,cs.CL
"Language models can learn implicit multi-hop reasoning, but only if they have lots of training data","Implicit reasoning is the ability of a language model to solve multi-hop
reasoning tasks in a single forward pass, without chain of thought. We
investigate this capability using GPT2-style language models trained from
scratch on controlled $k$-hop reasoning datasets ($k = 2, 3, 4$). We show that
while such models can indeed learn implicit $k$-hop reasoning, the required
training data grows exponentially in $k$, and the required number of
transformer layers grows linearly in $k$. We offer a theoretical explanation
for why this depth growth is necessary. We further find that the data
requirement can be mitigated, but not eliminated, through curriculum learning.",2025-05-23,"Yuekun Yao, Yupei Du, Dawei Zhu, Michael Hahn, Alexander Koller",http://arxiv.org/pdf/2505.17923v1,cs.CL
T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation,"The rapid progress in diffusion-based text-to-image (T2I) generation has
created an urgent need for interpretable automatic evaluation methods that can
assess the quality of generated images, therefore reducing the human annotation
burden. To reduce the prohibitive cost of relying on commercial models for
large-scale evaluation, and to improve the reasoning capabilities of
open-source models, recent research has explored supervised fine-tuning (SFT)
of multimodal large language models (MLLMs) as dedicated T2I evaluators.
However, SFT approaches typically rely on high-quality critique datasets, which
are either generated by proprietary LLMs-with potential issues of bias and
inconsistency-or annotated by humans at high cost, limiting their scalability
and generalization. To address these limitations, we propose T2I-Eval-R1, a
novel reinforcement learning framework that trains open-source MLLMs using only
coarse-grained quality scores, thereby avoiding the need for annotating
high-quality interpretable evaluation rationale. Our approach integrates Group
Relative Policy Optimization (GRPO) into the instruction-tuning process,
enabling models to generate both scalar scores and interpretable reasoning
chains with only easy accessible annotated judgment scores or preferences.
Furthermore, we introduce a continuous reward formulation that encourages score
diversity and provides stable optimization signals, leading to more robust and
discriminative evaluation behavior. Experimental results on three established
T2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves
significantly higher alignment with human assessments and offers more accurate
interpretable score rationales compared to strong baseline methods.",2025-05-23,"Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Shu-Hang Liu, Heyan Huang, Zhijing Wu, Chen Xu, Xian-Ling Mao",http://arxiv.org/pdf/2505.17897v1,cs.CL
Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model,"We introduce Mutarjim, a compact yet powerful language model for
bidirectional Arabic-English translation. While large-scale LLMs have shown
impressive progress in natural language processing tasks, including machine
translation, smaller models. Leveraging this insight, we developed Mutarjim
based on Kuwain-1.5B , a language model tailored for both Arabic and English.
Despite its modest size, Mutarjim outperforms much larger models on several
established benchmarks, achieved through an optimized two-phase training
approach and a carefully curated, high-quality training corpus.. Experimental
results show that Mutarjim rivals models up to 20 times larger while
significantly reducing computational costs and training requirements. We also
introduce Tarjama-25, a new benchmark designed to overcome limitations in
existing Arabic-English benchmarking datasets, such as domain narrowness, short
sentence lengths, and English-source bias. Tarjama-25 comprises 5,000
expert-reviewed sentence pairs and spans a wide range of domains, offering a
more comprehensive and balanced evaluation framework. Notably, Mutarjim
achieves state-of-the-art performance on the English-to-Arabic task in
Tarjama-25, surpassing even significantly larger and proprietary models like
GPT-4o mini. We publicly release Tarjama-25 to support future research and
advance the evaluation of Arabic-English translation systems.",2025-05-23,"Khalil Hennara, Muhammad Hreden, Mohamed Motaism Hamed, Zeina Aldallal, Sara Chrouf, Safwan AlModhayan",http://arxiv.org/pdf/2505.17894v1,cs.CL
Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens,"The recent rise of Large Reasoning Models (LRMs) has significantly improved
multi-step reasoning performance, but often at the cost of generating
excessively long reasoning chains. This paper revisits the efficiency of such
reasoning processes through an information-theoretic lens, revealing a
fundamental trade-off between reasoning length and semantic efficiency. We
propose two metrics, InfoBias and InfoGain, to quantify divergence from ideal
reasoning paths and stepwise information contribution, respectively. Empirical
analyses show that longer reasoning chains tend to exhibit higher information
bias and diminishing information gain, especially for incorrect answers.
Motivated by these findings, we introduce an entropy-based Adaptive Think
strategy that dynamically halts reasoning once confidence is sufficiently high,
improving efficiency while maintaining competitive accuracy. Compared to the
Vanilla Think approach (default mode), our strategy yields a 1.10% improvement
in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six
benchmark tasks spanning diverse reasoning types and difficulty levels,
demonstrating superior efficiency and reasoning performance. These results
underscore the promise of entropy-based methods for enhancing both accuracy and
cost-effiiciency in large language model deployment.",2025-05-23,"Xixian Yong, Xiao Zhou, Yingying Zhang, Jinlin Li, Yefeng Zheng, Xian Wu",http://arxiv.org/pdf/2505.18237v1,cs.CL
MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback,"Hypothesis ranking is a crucial component of automated scientific discovery,
particularly in natural sciences where wet-lab experiments are costly and
throughput-limited. Existing approaches focus on pre-experiment ranking,
relying solely on large language model's internal reasoning without
incorporating empirical outcomes from experiments. We introduce the task of
experiment-guided ranking, which aims to prioritize candidate hypotheses based
on the results of previously tested ones. However, developing such strategies
is challenging due to the impracticality of repeatedly conducting real
experiments in natural science domains. To address this, we propose a simulator
grounded in three domain-informed assumptions, modeling hypothesis performance
as a function of similarity to a known ground truth hypothesis, perturbed by
noise. We curate a dataset of 124 chemistry hypotheses with experimentally
reported outcomes to validate the simulator. Building on this simulator, we
develop a pseudo experiment-guided ranking method that clusters hypotheses by
shared functional characteristics and prioritizes candidates based on insights
derived from simulated experimental feedback. Experiments show that our method
outperforms pre-experiment baselines and strong ablations.",2025-05-23,"Wanhao Liu, Zonglin Yang, Jue Wang, Lidong Bing, Di Zhang, Dongzhan Zhou, Yuqiang Li, Houqiang Li, Erik Cambria, Wanli Ouyang",http://arxiv.org/pdf/2505.17873v1,cs.CL
"Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods","Generative AI models often learn and reproduce false information present in
their training corpora. This position paper argues that, analogous to
biological immunization, where controlled exposure to a weakened pathogen
builds immunity, AI models should be fine tuned on small, quarantined sets of
explicitly labeled falsehoods as a ""vaccine"" against misinformation. These
curated false examples are periodically injected during finetuning,
strengthening the model ability to recognize and reject misleading claims while
preserving accuracy on truthful inputs. An illustrative case study shows that
immunized models generate substantially less misinformation than baselines. To
our knowledge, this is the first training framework that treats fact checked
falsehoods themselves as a supervised vaccine, rather than relying on input
perturbations or generic human feedback signals, to harden models against
future misinformation. We also outline ethical safeguards and governance
controls to ensure the safe use of false data. Model immunization offers a
proactive paradigm for aligning AI systems with factuality.",2025-05-23,"Shaina Raza, Rizwan Qureshi, Marcelo Lotif, Aman Chadha, Deval Pandya, Christos Emmanouilidis",http://arxiv.org/pdf/2505.17870v1,cs.CL
Explaining Sources of Uncertainty in Automated Fact-Checking,"Understanding sources of a model's uncertainty regarding its predictions is
crucial for effective human-AI collaboration. Prior work proposes using
numerical uncertainty or hedges (""I'm not sure, but ...""), which do not explain
uncertainty that arises from conflicting evidence, leaving users unable to
resolve disagreements or rely on the output. We introduce CLUE
(Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the
first framework to generate natural language explanations of model uncertainty
by (i) identifying relationships between spans of text that expose
claim-evidence or inter-evidence conflicts and agreements that drive the
model's predictive uncertainty in an unsupervised way, and (ii) generating
explanations via prompting and attention steering that verbalize these critical
interactions. Across three language models and two fact-checking datasets, we
show that CLUE produces explanations that are more faithful to the model's
uncertainty and more consistent with fact-checking decisions than prompting for
uncertainty explanations without span-interaction guidance. Human evaluators
judge our explanations to be more helpful, more informative, less redundant,
and more logically consistent with the input than this baseline. CLUE requires
no fine-tuning or architectural changes, making it plug-and-play for any
white-box language model. By explicitly linking uncertainty to evidence
conflicts, it offers practical support for fact-checking and generalises
readily to other tasks that require reasoning over complex information.",2025-05-23,"Jingyi Sun, Greta Warren, Irina Shklovski, Isabelle Augenstein",http://arxiv.org/pdf/2505.17855v1,cs.CL
Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus,"Study of affect in speech requires suitable data, as emotional expression and
perception vary across languages. Until now, no corpus has existed for natural
expression of affect in spontaneous Finnish, existing data being acted or from
a very specific communicative setting. This paper presents the first such
corpus, created by annotating 12,000 utterances for emotional arousal and
valence, sampled from three large-scale Finnish speech corpora. To ensure
diverse affective expression, sample selection was conducted with an affect
mining approach combining acoustic, cross-linguistic speech emotion, and text
sentiment features. We compare this method to random sampling in terms of
annotation diversity, and conduct post-hoc analyses to identify sampling
choices that would have maximized the diversity. As an outcome, the work
introduces a spontaneous Finnish affective speech corpus and informs sampling
strategies for affective speech corpus creation in other languages or domains.",2025-05-23,"Kalle Lahtinen, Einari Vaaras, Liisa Mustanoja, Okko Räsänen",http://arxiv.org/pdf/2505.17833v1,cs.CL
Emerging categories in scientific explanations,"Clear and effective explanations are essential for human understanding and
knowledge dissemination. The scope of scientific research aiming to understand
the essence of explanations has recently expanded from the social sciences to
machine learning and artificial intelligence. Explanations for machine learning
decisions must be impactful and human-like, and there is a lack of large-scale
datasets focusing on human-like and human-generated explanations. This work
aims to provide such a dataset by: extracting sentences that indicate
explanations from scientific literature among various sources in the
biotechnology and biophysics topic domains (e.g. PubMed's PMC Open Access
subset); providing a multi-class notation derived inductively from the data;
evaluating annotator consensus on the emerging categories. The sentences are
organized in an openly-available dataset, with two different classifications
(6-class and 3-class category annotation), and the 3-class notation achieves a
0.667 Krippendorf Alpha value.",2025-05-23,"Giacomo Magnifico, Eduard Barbu",http://arxiv.org/pdf/2505.17832v1,cs.CL
Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning,"Mathematical reasoning through Chain-of-Thought (CoT) has emerged as a
powerful capability of Large Language Models (LLMs), which can be further
enhanced through Test-Time Scaling (TTS) methods like Beam Search and DVTS.
However, these methods, despite improving accuracy by allocating more
computational resources during inference, often suffer from path homogenization
and inefficient use of intermediate results. To address these limitations, we
propose Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that
introduces checkpoints between reasoning steps. It incorporates two key
strategies: (1) Answer-Clustered Search, which groups reasoning paths by their
intermediate checkpoint answers to maintain diversity while ensuring quality,
and (2) Checkpoint Candidate Augmentation, which leverages all intermediate
answers for final decision-making. Our approach effectively reduces path
homogenization and creates a fault-tolerant mechanism by utilizing high-quality
intermediate results. Experimental results show that SRCA improves reasoning
accuracy compared to existing TTS methods across various mathematical datasets.",2025-05-23,"Zezhong Wang, Xingshan Zeng, Weiwen Liu, Yufei Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong",http://arxiv.org/pdf/2505.17829v1,cs.CL
Not All Tokens Are What You Need In Thinking,"Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit
impressive problem-solving capabilities but suffer from critical
inefficiencies: high inference latency, excessive computational resource
consumption, and a tendency toward overthinking -- generating verbose chains of
thought (CoT) laden with redundant tokens that contribute minimally to the
final answer. To address these issues, we propose Conditional Token Selection
(CTS), a token-level compression framework with a flexible and variable
compression ratio that identifies and preserves only the most essential tokens
in CoT. CTS evaluates each token's contribution to deriving correct answers
using conditional importance scoring, then trains models on compressed CoT.
Extensive experiments demonstrate that CTS effectively compresses long CoT
while maintaining strong reasoning performance. Notably, on the GPQA benchmark,
Qwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with
13.2% fewer reasoning tokens (13% training token reduction). Further reducing
training tokens by 42% incurs only a marginal 5% accuracy drop while yielding a
75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy
in existing CoT.",2025-05-23,"Hang Yuan, Bin Yu, Haotian Li, Shijun Yang, Christina Dan Wang, Zhou Yu, Xueyin Xu, Weizhen Qi, Kai Chen",http://arxiv.org/pdf/2505.17827v1,cs.CL
Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models,"Trinity-RFT is a general-purpose, flexible and scalable framework designed
for reinforcement fine-tuning (RFT) of large language models. It is built with
a decoupled design, consisting of (1) an RFT-core that unifies and generalizes
synchronous/asynchronous, on-policy/off-policy, and online/offline modes of
RFT, (2) seamless integration for agent-environment interaction with high
efficiency and robustness, and (3) systematic data pipelines optimized for RFT.
Trinity-RFT can be easily adapted for diverse application scenarios, and serves
as a unified platform for exploring advanced reinforcement learning paradigms.
This technical report outlines the vision, features, design and implementations
of Trinity-RFT, accompanied by extensive examples demonstrating the utility and
user-friendliness of the proposed framework.",2025-05-23,"Xuchen Pan, Yanxi Chen, Yushuo Chen, Yuchang Sun, Daoyuan Chen, Wenhao Zhang, Yuexiang Xie, Yilun Huang, Yilei Zhang, Dawei Gao, Yaliang Li, Bolin Ding, Jingren Zhou",http://arxiv.org/pdf/2505.17826v1,cs.CL
ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning,"The deployment of Large language models (LLMs) in many fields is largely
hindered by their high computational and memory costs. Recent studies suggest
that LLMs exhibit sparsity, which can be used for pruning. Previous pruning
methods typically follow a prune-then-finetune paradigm. Since the pruned parts
still contain valuable information, statically removing them without updating
the remaining parameters often results in irreversible performance degradation,
requiring costly recovery fine-tuning (RFT) to maintain performance. To address
this, we propose a novel paradigm: first apply regularization, then prune.
Based on this paradigm, we propose ELDeR: Getting Efficient LLMs through
Data-Driven Regularized Layer-wise Pruning. We multiply the output of each
transformer layer by an initial weight, then we iteratively learn the weights
of each transformer layer by using a small amount of data in a simple way.
After that, we apply regularization to the difference between the output and
input of the layers with smaller weights, forcing the information to be
transferred to the remaining layers. Compared with direct pruning, ELDeR
reduces the information loss caused by direct parameter removal, thus better
preserving the model's language modeling ability. Experimental results show
that ELDeR achieves superior performance compared with powerful layer-wise
structured pruning methods, while greatly reducing RFT computational costs.
Since ELDeR is a layer-wise pruning method, its end-to-end acceleration effect
is obvious, making it a promising technique for efficient LLMs.",2025-05-23,"Mingkuan Feng, Jinyang Wu, Siyuan Liu, Shuai Zhang, Hongjian Fang, Ruihan Jin, Feihu Che, Pengpeng Shao, Zhengqi Wen, Jianhua Tao",http://arxiv.org/pdf/2505.18232v1,cs.CL
PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions,"Doctor-patient consultations require multi-turn, context-aware communication
tailored to diverse patient personas. Training or evaluating doctor LLMs in
such settings requires realistic patient interaction systems. However, existing
simulators often fail to reflect the full range of personas seen in clinical
practice. To address this, we introduce PatientSim, a patient simulator that
generates realistic and diverse patient personas for clinical scenarios,
grounded in medical expertise. PatientSim operates using: 1) clinical profiles,
including symptoms and medical history, derived from real-world data in the
MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:
personality, language proficiency, medical history recall level, and cognitive
confusion level, resulting in 37 unique combinations. We evaluated eight LLMs
for factual accuracy and persona consistency. The top-performing open-source
model, Llama 3.3, was validated by four clinicians to confirm the robustness of
our framework. As an open-source, customizable platform, PatientSim provides a
reproducible and scalable solution that can be customized for specific training
needs. Offering a privacy-compliant environment, it serves as a robust testbed
for evaluating medical dialogue systems across diverse patient presentations
and shows promise as an educational tool for healthcare.",2025-05-23,"Daeun Kyung, Hyunseung Chung, Seongsu Bae, Jiho Kim, Jae Ho Sohn, Taerim Kim, Soo Kyung Kim, Edward Choi",http://arxiv.org/pdf/2505.17818v1,cs.CL
Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong,"The majority of inhabitants in Hong Kong are able to read and write in
standard Chinese but use Cantonese as the primary spoken language in daily
life. Spoken Cantonese can be transcribed into Chinese characters, which
constitute the so-called written Cantonese. Written Cantonese exhibits
significant lexical and grammatical differences from standard written Chinese.
The rise of written Cantonese is increasingly evident in the cyber world. The
growing interaction between Mandarin speakers and Cantonese speakers is leading
to a clear demand for automatic translation between Chinese and Cantonese. This
paper describes a transformer-based neural machine translation (NMT) system for
written-Chinese-to-written-Cantonese translation. Given that parallel text data
of Chinese and Cantonese are extremely scarce, a major focus of this study is
on the effort of preparing good amount of training data for NMT. In addition to
collecting 28K parallel sentences from previous linguistic studies and
scattered internet resources, we devise an effective approach to obtaining 72K
parallel sentences by automatically extracting pairs of semantically similar
sentences from parallel articles on Chinese Wikipedia and Cantonese Wikipedia.
We show that leveraging highly similar sentence pairs mined from Wikipedia
improves translation performance in all test sets. Our system outperforms Baidu
Fanyi's Chinese-to-Cantonese translation on 6 out of 8 test sets in BLEU
scores. Translation examples reveal that our system is able to capture
important linguistic transformations between standard Chinese and spoken
Cantonese.",2025-05-23,"Hei Yi Mak, Tan Lee",http://arxiv.org/pdf/2505.17816v1,cs.CL
Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning,"Reasoning large language models (LLMs) heavily rely on scaling test-time
compute to perform complex reasoning tasks by generating extensive ""thinking""
chains. While demonstrating impressive results, this approach incurs
significant computational costs and inference time. In this work, we challenge
the assumption that long thinking chains results in better reasoning
capabilities. We first demonstrate that shorter reasoning chains within
individual questions are significantly more likely to yield correct answers -
up to 34.5% more accurate than the longest chain sampled for the same question.
Based on these results, we suggest short-m@k, a novel reasoning LLM inference
method. Our method executes k independent generations in parallel and halts
computation once the first m thinking processes are done. The final answer is
chosen using majority voting among these m chains. Basic short-1@k demonstrates
similar or even superior performance over standard majority voting in
low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while
slightly less efficient than short-1@k, consistently surpasses majority voting
across all compute budgets, while still being substantially faster (up to 33%
wall time reduction). Inspired by our results, we finetune an LLM using short,
long, and randomly selected reasoning chains. We then observe that training on
the shorter ones leads to better performance. Our findings suggest rethinking
current methods of test-time compute in reasoning LLMs, emphasizing that longer
""thinking"" does not necessarily translate to improved performance and can,
counter-intuitively, lead to degraded results.",2025-05-23,"Michael Hassid, Gabriel Synnaeve, Yossi Adi, Roy Schwartz",http://arxiv.org/pdf/2505.17813v1,cs.CL
DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors,"Large-language-model (LLM) agents excel at reactive dialogue but struggle
with proactive, goal-driven interactions due to myopic decoding and costly
planning. We introduce DialogXpert, which leverages a frozen LLM to propose a
small, high-quality set of candidate actions per turn and employs a compact
Q-network over fixed BERT embeddings trained via temporal-difference learning
to select optimal moves within this reduced space. By tracking the user's
emotions, DialogXpert tailors each decision to advance the task while nurturing
a genuine, empathetic connection. Across negotiation, emotional support, and
tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with
success rates exceeding 94\% and, with a larger LLM prior, pushes success above
97\% while markedly improving negotiation outcomes. This framework delivers
real-time, strategic, and emotionally intelligent dialogue planning at scale.
Code available at https://github.com/declare-lab/dialogxpert/",2025-05-23,"Tazeek Bin Abdur Rakib, Ambuj Mehrish, Lay-Ki Soon, Wern Han Lim, Soujanya Poria",http://arxiv.org/pdf/2505.17795v1,cs.CL
Compression Hacking: A Supplementary Perspective on Informatics Metric of Language Models from Geometric Distortion,"Recently, the concept of ``compression as intelligence'' has provided a novel
informatics metric perspective for language models (LMs), emphasizing that
highly structured representations signify the intelligence level of LMs.
However, from a geometric standpoint, the word representation space of highly
compressed LMs tends to degenerate into a highly anisotropic state, which
hinders the LM's ability to comprehend instructions and directly impacts its
performance. We found this compression-anisotropy synchronicity is essentially
the ``Compression Hacking'' in LM representations, where noise-dominated
directions tend to create the illusion of high compression rates by sacrificing
spatial uniformity. Based on this, we propose three refined compression metrics
by incorporating geometric distortion analysis and integrate them into a
self-evaluation pipeline. The refined metrics exhibit strong alignment with the
LM's comprehensive capabilities, achieving Spearman correlation coefficients
above 0.9, significantly outperforming both the original compression and other
internal structure-based metrics. This confirms that compression hacking
substantially enhances the informatics interpretation of LMs by incorporating
geometric distortion of representations.",2025-05-23,"Jianxiang Zang, Meiling Ning, Yongda Wei, Shihan Dou, Jiazheng Zhang, Nijia Mo, Binhong Li, Tao Gui, Qi Zhang, Xuanjing Huang",http://arxiv.org/pdf/2505.17793v1,cs.CL
EXECUTE: A Multilingual Benchmark for LLM Token Understanding,"The CUTE benchmark showed that LLMs struggle with character understanding in
English. We extend it to more languages with diverse scripts and writing
systems, introducing EXECUTE. Our simplified framework allows easy expansion to
any language. Tests across multiple LLMs reveal that challenges in other
languages are not always on the character level as in English. Some languages
show word-level processing issues, some show no issues at all. We also examine
sub-character tasks in Chinese, Japanese, and Korean to assess LLMs'
understanding of character components.",2025-05-23,"Lukas Edman, Helmut Schmid, Alexander Fraser",http://arxiv.org/pdf/2505.17784v1,cs.CL
The Real Barrier to LLM Agent Usability is Agentic ROI,"Large Language Model (LLM) agents represent a promising shift in human-AI
interaction, moving beyond passive prompt-response systems to autonomous agents
capable of reasoning, planning, and goal-directed action. Despite the
widespread application in specialized, high-effort tasks like coding and
scientific research, we highlight a critical usability gap in high-demand,
mass-market applications. This position paper argues that the limited
real-world adoption of LLM agents stems not only from gaps in model
capabilities, but also from a fundamental tradeoff between the value an agent
can provide and the costs incurred during real-world use. Hence, we call for a
shift from solely optimizing model performance to a broader, utility-driven
perspective: evaluating agents through the lens of the overall agentic return
on investment (Agent ROI). By identifying key factors that determine Agentic
ROI--information quality, agent time, and cost--we posit a zigzag development
trajectory in optimizing agentic ROI: first scaling up to improve the
information quality, then scaling down to minimize the time and cost. We
outline the roadmap across different development stages to bridge the current
usability gaps, aiming to make LLM agents truly scalable, accessible, and
effective in real-world contexts.",2025-05-23,"Weiwen Liu, Jiarui Qin, Xu Huang, Xingshan Zeng, Yunjia Xi, Jianghao Lin, Chuhan Wu, Yasheng Wang, Lifeng Shang, Ruiming Tang, Defu Lian, Yong Yu, Weinan Zhang",http://arxiv.org/pdf/2505.17767v1,cs.CL
Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs,"Large Language Models (LLMs) augmented with retrieval mechanisms have
demonstrated significant potential in fact-checking tasks by integrating
external knowledge. However, their reliability decreases when confronted with
conflicting evidence from sources of varying credibility. This paper presents
the first systematic evaluation of Retrieval-Augmented Generation (RAG) models
for fact-checking in the presence of conflicting evidence. To support this
study, we introduce \textbf{CONFACT} (\textbf{Con}flicting Evidence for
\textbf{Fact}-Checking) (Dataset available at
https://github.com/zoeyyes/CONFACT), a novel dataset comprising questions
paired with conflicting information from various sources. Extensive experiments
reveal critical vulnerabilities in state-of-the-art RAG methods, particularly
in resolving conflicts stemming from differences in media source credibility.
To address these challenges, we investigate strategies to integrate media
background information into both the retrieval and generation stages. Our
results show that effectively incorporating source credibility significantly
enhances the ability of RAG models to resolve conflicting evidence and improve
fact-checking performance.",2025-05-23,"Ziyu Ge, Yuhao Wu, Daniel Wai Kit Chin, Roy Ka-Wei Lee, Rui Cao",http://arxiv.org/pdf/2505.17762v1,cs.CL
Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks,"We introduce a set of training-free ABX-style discrimination tasks to
evaluate how multilingual language models represent language identity (form)
and semantic content (meaning). Inspired from speech processing, these
zero-shot tasks measure whether minimal differences in representation can be
reliably detected. This offers a flexible and interpretable alternative to
probing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints
and layers, we find that language discrimination declines over training and
becomes concentrated in lower layers, while meaning discrimination strengthens
over time and stabilizes in deeper layers. We then explore probing tasks,
showing some alignment between our metrics and linguistic learning performance.
Our results position ABX tasks as a lightweight framework for analyzing the
structure of multilingual representations.",2025-05-23,"Maureen de Seyssel, Jie Chi, Skyler Seto, Maartje ter Hoeve, Masha Fedzechkina, Natalie Schluter",http://arxiv.org/pdf/2505.17747v1,cs.CL
Fast Quiet-STaR: Thinking Without Thought Tokens,"Large Language Models (LLMs) have achieved impressive performance across a
range of natural language processing tasks. However, recent advances
demonstrate that further gains particularly in complex reasoning tasks require
more than merely scaling up model sizes or training data. One promising
direction is to enable models to think during the reasoning process. Recently,
Quiet STaR significantly improves reasoning by generating token-level thought
traces, but incurs substantial inference overhead. In this work, we propose
Fast Quiet STaR, a more efficient reasoning framework that preserves the
benefits of token-level reasoning while reducing computational cost. Our method
introduces a curriculum learning based training strategy that gradually reduces
the number of thought tokens, enabling the model to internalize more abstract
and concise reasoning processes. We further extend this approach to the
standard Next Token Prediction (NTP) setting through reinforcement
learning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates
the need for explicit thought token generation during inference. Experiments on
four benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast
Quiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy
under the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an
average accuracy improvement of 9\% on Mistral 7B and 5.7\% on Qwen2.5 7B,
while maintaining the same inference latency. Our code will be available at
https://github.com/huangwei200012/Fast-Quiet-STaR.",2025-05-23,"Wei Huang, Yizhe Xiong, Xin Ye, Zhijie Deng, Hui Chen, Zijia Lin, Guiguang Ding",http://arxiv.org/pdf/2505.17746v1,cs.CL
The Pilot Corpus of the English Semantic Sketches,"The paper is devoted to the creation of the semantic sketches for English
verbs. The pilot corpus consists of the English-Russian sketch pairs and is
aimed to show what kind of contrastive studies the sketches help to conduct.
Special attention is paid to the cross-language differences between the
sketches with similar semantics. Moreover, we discuss the process of building a
semantic sketch, and analyse the mistakes that could give insight to the
linguistic nature of sketches.",2025-05-23,"Maria Petrova, Maria Ponomareva, Alexandra Ivoylova",http://arxiv.org/pdf/2505.17733v1,cs.CL
PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization,"Despite Proximal Policy Optimization (PPO) dominating policy gradient methods
-- from robotic control to game AI -- its static trust region forces a brittle
trade-off: aggressive clipping stifles early exploration, while late-stage
updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive
RL by fusing exploration and convergence signals into a single bounded trust
region -- a theoretically grounded innovation that outperforms five SOTA
baselines with less than 2% overhead. This work bridges a critical gap in
phase-aware learning, enabling real-world deployment in safety-critical systems
like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%
faster convergence by combining: (1) entropy-driven expansion (epsilon up) for
exploration in high-uncertainty states, and (2) reward-guided contraction
(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,
Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),
2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with
only five lines of code change. PPO-BR's simplicity and theoretical guarantees
make it ready-to-deploy in safety-critical domains -- from surgical robotics to
autonomous drones. In contrast to recent methods such as Group Relative Policy
Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism
applicable to both language models and general reinforcement learning
environments.",2025-05-23,Ben Rahman,http://arxiv.org/pdf/2505.17714v1,cs.CL
Understanding How Value Neurons Shape the Generation of Specified Values in LLMs,"Rapid integration of large language models (LLMs) into societal applications
has intensified concerns about their alignment with universal ethical
principles, as their internal value representations remain opaque despite
behavioral alignment advancements. Current approaches struggle to
systematically interpret how values are encoded in neural architectures,
limited by datasets that prioritize superficial judgments over mechanistic
analysis. We introduce ValueLocate, a mechanistic interpretability framework
grounded in the Schwartz Values Survey, to address this gap. Our method first
constructs ValueInsight, a dataset that operationalizes four dimensions of
universal value through behavioral contexts in the real world. Leveraging this
dataset, we develop a neuron identification method that calculates activation
differences between opposing value aspects, enabling precise localization of
value-critical neurons without relying on computationally intensive attribution
methods. Our proposed validation method demonstrates that targeted manipulation
of these neurons effectively alters model value orientations, establishing
causal relationships between neurons and value representations. This work
advances the foundation for value alignment by bridging psychological value
frameworks with neuron analysis in LLMs.",2025-05-23,"Yi Su, Jiayi Zhang, Shu Yang, Xinhai Wang, Lijie Hu, Di Wang",http://arxiv.org/pdf/2505.17712v1,cs.CL
SemSketches-2021: experimenting with the machine processing of the pilot semantic sketches corpus,"The paper deals with elaborating different approaches to the machine
processing of semantic sketches. It presents the pilot open corpus of semantic
sketches. Different aspects of creating the sketches are discussed, as well as
the tasks that the sketches can help to solve. Special attention is paid to the
creation of the machine processing tools for the corpus. For this purpose, the
SemSketches-2021 Shared Task was organized. The participants were given the
anonymous sketches and a set of contexts containing the necessary predicates.
During the Task, one had to assign the proper contexts to the corresponding
sketches.",2025-05-23,"Maria Ponomareva, Maria Petrova, Julia Detkova, Oleg Serikov, Maria Yarova",http://arxiv.org/pdf/2505.17704v1,cs.CL
COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection,"The growing size of large language models has created significant
computational inefficiencies. To address this challenge, sparse activation
methods selectively deactivates non-essential parameters during inference,
reducing computational costs in FFNN layers. While existing methods focus on
non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN
layer lies globally in the form of a linear combination over its internal down
projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,
leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct
coefficients of the linear combination. Experimental results demonstrate that
D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%
ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%
better performance preservation compared to existing methods. Our specialized
kernel implementations effectively realize these theoretical gains into
substantial real-world acceleration.",2025-05-23,"Jaewon Cheon, Pilsung Kang",http://arxiv.org/pdf/2505.17701v1,cs.CL
Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models,"Despite the remarkable reasoning performance, eliciting the long
chain-of-thought (CoT) ability in large language models (LLMs) typically
requires costly reinforcement learning or supervised fine-tuning on
high-quality distilled data. We investigate the internal mechanisms behind this
capability and show that a small set of high-impact activations in the last few
layers largely governs long-form reasoning attributes, such as output length
and self-reflection. By simply amplifying these activations and inserting
""wait"" tokens, we can invoke the long CoT ability without any training,
resulting in significantly increased self-reflection rates and accuracy.
Moreover, we find that the activation dynamics follow predictable trajectories,
with a sharp rise after special tokens and a subsequent exponential decay.
Building on these insights, we introduce a general training-free activation
control technique. It leverages a few contrastive examples to identify key
activations, and employs simple analytic functions to modulate their values at
inference time to elicit long CoTs. Extensive experiments confirm the
effectiveness of our method in efficiently eliciting long CoT reasoning in LLMs
and improving their performance. Additionally, we propose a parameter-efficient
fine-tuning method that trains only a last-layer activation amplification
module and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning
benchmarks with significantly fewer parameters. Our code and data are publicly
released.",2025-05-23,"Zekai Zhao, Qi Liu, Kun Zhou, Zihan Liu, Yifei Shao, Zhiting Hu, Biwei Huang",http://arxiv.org/pdf/2505.17697v1,cs.CL
ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction,"Large language models (LLMs) are widely used as evaluators for open-ended
tasks, while previous research has emphasized biases in LLM evaluations, the
issue of non-transitivity in pairwise comparisons remains unresolved:
non-transitive preferences for pairwise comparisons, where evaluators prefer A
over B, B over C, but C over A. Our results suggest that low-quality training
data may reduce the transitivity of preferences generated by the Evaluator LLM.
To address this, We propose a graph-theoretic framework to analyze and mitigate
this problem by modeling pairwise preferences as tournament graphs. We quantify
non-transitivity and introduce directed graph structural entropy to measure the
overall clarity of preferences. Our analysis reveals significant
non-transitivity in advanced Evaluator LLMs (with Qwen2.5-Max exhibiting
67.96%), as well as high entropy values (0.8095 for Qwen2.5-Max), reflecting
low overall clarity of preferences. To address this issue, we designed a
filtering strategy, ELSPR, to eliminate preference data that induces
non-transitivity, retaining only consistent and transitive preference data for
model fine-tuning. Experiments demonstrate that models fine-tuned with filtered
data reduce non-transitivity by 13.78% (from 64.28% to 50.50%), decrease
structural entropy by 0.0879 (from 0.8113 to 0.7234), and align more closely
with human evaluators (human agreement rate improves by 0.6% and Spearman
correlation increases by 0.01).",2025-05-23,"Yan Yu, Yilun Liu, Minggui He, Shimin Tao, Weibin Meng, Xinhua Yang, Li Zhang, Hongxia Ma, Chang Su, Hao Yang, Fuliang Li",http://arxiv.org/pdf/2505.17691v1,cs.CL
Tuning Language Models for Robust Prediction of Diverse User Behaviors,"Predicting user behavior is essential for intelligent assistant services, yet
deep learning models often struggle to capture long-tailed behaviors. Large
language models (LLMs), with their pretraining on vast corpora containing rich
behavioral knowledge, offer promise. However, existing fine-tuning approaches
tend to overfit to frequent ``anchor'' behaviors, reducing their ability to
predict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM,
a progressive fine-tuning approach that addresses this issue. In the first
stage, LLMs are fine-tuned on anchor behaviors while preserving general
behavioral knowledge. In the second stage, fine-tuning uses a balanced subset
of all behaviors based on sample difficulty to improve tail behavior
predictions without sacrificing anchor performance. Experimental results on two
real-world datasets demonstrate that BehaviorLM robustly predicts both anchor
and tail behaviors and effectively leverages LLM behavioral knowledge to master
tail behavior prediction with few-shot examples.",2025-05-23,"Fanjin Meng, Jingtao Ding, Jiahui Gong, Chen Yang, Hong Chen, Zuojian Wang, Haisheng Lu, Yong Li",http://arxiv.org/pdf/2505.17682v1,cs.CL
IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis,"Large Language Models (LLMs) show promise as data analysis agents, but
existing benchmarks overlook the iterative nature of the field, where experts'
decisions evolve with deeper insights of the dataset. To address this, we
introduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round
interactive scenarios. Derived from complex Kaggle notebooks, tasks are
presented as sequential natural language instructions by an LLM-simulated user.
Agent performance is judged by comparing its final numerical output to the
human-derived baseline. Initial results show that even state-of-the-art coding
agents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting
limitations not evident in single-turn tests. This work underscores the need to
improve LLMs' multi-round capabilities for building more reliable data analysis
agents, highlighting the necessity of achieving a balance between instruction
following and reasoning.",2025-05-23,"Hanyu Li, Haoyu Liu, Tingyu Zhu, Tianyu Guo, Zeyu Zheng, Xiaotie Deng, Michael I. Jordan",http://arxiv.org/pdf/2505.18223v1,cs.CL
MIDB: Multilingual Instruction Data Booster for Enhancing Multilingual Instruction Synthesis,"Despite doubts on data quality, instruction synthesis has been widely applied
into instruction tuning (IT) of LLMs as an economic and rapid alternative.
Recent endeavors focus on improving data quality for synthesized instruction
pairs in English and have facilitated IT of English-centric LLMs. However, data
quality issues in multilingual synthesized instruction pairs are even more
severe, since the common synthesizing practice is to translate English
synthesized data into other languages using machine translation (MT). Besides
the known content errors in these English synthesized data, multilingual
synthesized instruction data are further exposed to defects introduced by MT
and face insufficient localization of the target languages. In this paper, we
propose MIDB, a Multilingual Instruction Data Booster to automatically address
the quality issues in multilingual synthesized data. MIDB is trained on around
36.8k revision examples across 16 languages by human linguistic experts,
thereby can boost the low-quality data by addressing content errors and MT
defects, and improving localization in these synthesized data. Both automatic
and human evaluation indicate that not only MIDB steadily improved instruction
data quality in 16 languages, but also the instruction-following and
cultural-understanding abilities of multilingual LLMs fine-tuned on
MIDB-boosted data were significantly enhanced.",2025-05-23,"Yilun Liu, Chunguang Zhao, Xinhua Yang, Hongyong Zeng, Shimin Tao, Weibin Meng, Minggui He, Chang Su, Yan Yu, Hongxia Ma, Li Zhang, Daimeng Wei, Hao Yang",http://arxiv.org/pdf/2505.17671v1,cs.CL
QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning,"Recent large reasoning models (LRMs) have demonstrated strong reasoning
capabilities through reinforcement learning (RL). These improvements have
primarily been observed within the short-context reasoning tasks. In contrast,
extending LRMs to effectively process and reason on long-context inputs via RL
remains a critical unsolved challenge. To bridge this gap, we first formalize
the paradigm of long-context reasoning RL, and identify key challenges in
suboptimal training efficiency and unstable optimization process. To address
these issues, we propose QwenLong-L1, a framework that adapts short-context
LRMs to long-context scenarios via progressive context scaling. Specifically,
we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust
initial policy, followed by a curriculum-guided phased RL technique to
stabilize the policy evolution, and enhanced with a difficulty-aware
retrospective sampling strategy to incentivize the policy exploration.
Experiments on seven long-context document question-answering benchmarks
demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini
and Qwen3-235B-A22B, achieving performance on par with
Claude-3.7-Sonnet-Thinking, demonstrating leading performance among
state-of-the-art LRMs. This work advances the development of practical
long-context LRMs capable of robust reasoning across information-intensive
environments.",2025-05-23,"Fanqi Wan, Weizhou Shen, Shengyi Liao, Yingcheng Shi, Chenliang Li, Ziyi Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan",http://arxiv.org/pdf/2505.17667v1,cs.CL
Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States,"As Large Language Models (LLMs) increasingly participate in human-AI
interactions, evaluating their Theory of Mind (ToM) capabilities - particularly
their ability to track dynamic mental states - becomes crucial. While existing
benchmarks assess basic ToM abilities, they predominantly focus on static
snapshots of mental states, overlooking the temporal evolution that
characterizes real-world social interactions. We present \textsc{DynToM}, a
novel benchmark specifically designed to evaluate LLMs' ability to understand
and track the temporal progression of mental states across interconnected
scenarios. Through a systematic four-step framework, we generate 1,100 social
contexts encompassing 5,500 scenarios and 78,100 questions, each validated for
realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs
reveals that their average performance underperforms humans by 44.7\%, with
performance degrading significantly when tracking and reasoning about the shift
of mental states. This performance gap highlights fundamental limitations in
current LLMs' ability to model the dynamic nature of human mental states.",2025-05-23,"Yang Xiao, Jiashuo Wang, Qiancheng Xu, Changhe Song, Chunpu Xu, Yi Cheng, Wenjie Li, Pengfei Liu",http://arxiv.org/pdf/2505.17663v1,cs.CL
Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs,"As large language models (LLMs) often generate plausible but incorrect
content, error detection has become increasingly critical to ensure
truthfulness. However, existing detection methods often overlook a critical
problem we term as self-consistent error, where LLMs repeatly generate the same
incorrect response across multiple stochastic samples. This work formally
defines self-consistent errors and evaluates mainstream detection methods on
them. Our investigation reveals two key findings: (1) Unlike inconsistent
errors, whose frequency diminishes significantly as LLM scale increases, the
frequency of self-consistent errors remains stable or even increases. (2) All
four types of detection methshods significantly struggle to detect
self-consistent errors. These findings reveal critical limitations in current
detection methods and underscore the need for improved methods. Motivated by
the observation that self-consistent errors often differ across LLMs, we
propose a simple but effective cross-model probe method that fuses hidden state
evidence from an external verifier LLM. Our method significantly enhances
performance on self-consistent errors across three LLM families.",2025-05-23,"Hexiang Tan, Fei Sun, Sha Liu, Du Su, Qi Cao, Xin Chen, Jingang Wang, Xunliang Cai, Yuanzhuo Wang, Huawei Shen, Xueqi Cheng",http://arxiv.org/pdf/2505.17656v1,cs.CL
EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications,"E-commerce platforms increasingly rely on Large Language Models (LLMs) and
Vision-Language Models (VLMs) to detect illicit or misleading product content.
However, these models remain vulnerable to evasive content: inputs (text or
images) that superficially comply with platform policies while covertly
conveying prohibited claims. Unlike traditional adversarial attacks that induce
overt failures, evasive content exploits ambiguity and context, making it far
harder to detect. Existing robustness benchmarks provide little guidance for
this demanding, real-world challenge. We introduce EVADE, the first
expert-curated, Chinese, multimodal benchmark specifically designed to evaluate
foundation models on evasive content detection in e-commerce. The dataset
contains 2,833 annotated text samples and 13,961 images spanning six demanding
product categories, including body shaping, height growth, and health
supplements. Two complementary tasks assess distinct capabilities:
Single-Violation, which probes fine-grained reasoning under short prompts, and
All-in-One, which tests long-context reasoning by merging overlapping policy
rules into unified instructions. Notably, the All-in-One setting significantly
narrows the performance gap between partial and full-match accuracy, suggesting
that clearer rule definitions improve alignment between human and model
judgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial
performance gaps: even state-of-the-art models frequently misclassify evasive
samples. By releasing EVADE and strong baselines, we provide the first rigorous
standard for evaluating evasive-content detection, expose fundamental
limitations in current multimodal reasoning, and lay the groundwork for safer
and more transparent content moderation systems in e-commerce. The dataset is
publicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.",2025-05-23,"Ancheng Xu, Zhihao Yang, Jingpeng Li, Guanghu Yuan, Longze Chen, Liang Yan, Jiehui Zhou, Zhen Qin, Hengyun Chang, Hamid Alinejad-Rokny, Bo Zheng, Min Yang",http://arxiv.org/pdf/2505.17654v1,cs.CL
HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning,"Embodied agents operating in smart homes must understand human behavior
through diverse sensory inputs and communicate via natural language. While
Vision-Language Models (VLMs) have enabled impressive language-grounded
perception, their reliance on visual data limits robustness in real-world
scenarios with occlusions, poor lighting, or privacy constraints. In this
paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that
integrates uncommon but powerful sensing modalities, such as LiDAR, infrared,
mmWave radar, and WiFi, to enable seamless human perception and reasoning
across heterogeneous environments. We address two key challenges: (1) the
scarcity of aligned modality-text data for rare sensors, and (2) the
heterogeneity of their physical signal representations. To overcome these, we
design a Universal Modality-Injection Projector (UMIP) that enhances
pre-aligned modality embeddings with fine-grained, text-aligned features from
tailored encoders via coarse-to-fine cross-attention without introducing
significant alignment overhead. We further introduce a human-VLM collaborative
data curation pipeline to generate paired textual annotations for sensing
datasets. Extensive experiments on two newly constructed benchmarks show that
HoloLLM significantly outperforms existing MLLMs, improving language-grounded
human sensing accuracy by up to 30%. This work establishes a new foundation for
real-world, language-informed multisensory embodied intelligence.",2025-05-23,"Chuhao Zhou, Jianfei Yang",http://arxiv.org/pdf/2505.17645v1,cs.CL
Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks,"Conventional machine learning models, particularly tree-based approaches,
have demonstrated promising performance across various clinical prediction
tasks using electronic health record (EHR) data. Despite their strengths, these
models struggle with tasks that require deeper contextual understanding, such
as predicting 30-day hospital readmission. This can be primarily due to the
limited semantic information available in structured EHR data. To address this
limitation, we propose a deep multimodal contrastive learning (CL) framework
that aligns the latent representations of structured EHR data with unstructured
discharge summary notes. It works by pulling together paired EHR and text
embeddings while pushing apart unpaired ones. Fine-tuning the pretrained EHR
encoder extracted from this framework significantly boosts downstream task
performance, e.g., a 4.1% AUROC enhancement over XGBoost for 30-day readmission
prediction. Such results demonstrate the effect of integrating domain knowledge
from clinical notes into EHR-based pipelines, enabling more accurate and
context-aware clinical decision support systems.",2025-05-23,"Sara Ketabi, Dhanesh Ramachandram",http://arxiv.org/pdf/2505.17643v1,cs.CL
Stereotype Detection in Natural Language Processing,"Stereotypes influence social perceptions and can escalate into discrimination
and violence. While NLP research has extensively addressed gender bias and hate
speech, stereotype detection remains an emerging field with significant
societal implications. In this work is presented a survey of existing research,
analyzing definitions from psychology, sociology, and philosophy. A
semi-automatic literature review was performed by using Semantic Scholar. We
retrieved and filtered over 6,000 papers (in the year range 2000-2025),
identifying key trends, methodologies, challenges and future directions. The
findings emphasize stereotype detection as a potential early-monitoring tool to
prevent bias escalation and the rise of hate speech. Conclusions highlight the
need for a broader, multilingual, and intersectional approach in NLP studies.",2025-05-23,"Alessandra Teresa Cignarella, Anastasia Giachanou, Els Lefever",http://arxiv.org/pdf/2505.17642v1,cs.CL
Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis,"Various AI safety datasets have been developed to measure LLMs against
evolving interpretations of harm. Our evaluation of five recently published
open-source safety benchmarks reveals distinct semantic clusters using UMAP
dimensionality reduction and kmeans clustering (silhouette score: 0.470). We
identify six primary harm categories with varying benchmark representation.
GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix
emphasizes self-harm scenarios. Significant differences in prompt length
distribution suggests confounds to data collection and interpretations of harm
as well as offer possible context. Our analysis quantifies benchmark
orthogonality among AI benchmarks, allowing for transparency in coverage gaps
despite topical similarities. Our quantitative framework for analyzing semantic
orthogonality across safety benchmarks enables more targeted development of
datasets that comprehensively address the evolving landscape of harms in AI
use, however that is defined in the future.",2025-05-23,"Jonathan Bennion, Shaona Ghosh, Mantek Singh, Nouha Dziri",http://arxiv.org/pdf/2505.17636v1,cs.CL
Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs,"Multimodal out-of-context (OOC) misinformation is misinformation that
repurposes real images with unrelated or misleading captions. Detecting such
misinformation is challenging because it requires resolving the context of the
claim before checking for misinformation. Many current methods, including LLMs
and LVLMs, do not perform this contextualization step. LLMs hallucinate in
absence of context or parametric knowledge. In this work, we propose a
graph-based method that evaluates the consistency between the image and the
caption by constructing two graph representations: an evidence graph, derived
from online textual evidence, and a claim graph, from the claim in the caption.
Using graph neural networks (GNNs) to encode and compare these representations,
our framework then evaluates the truthfulness of image-caption pairs. We create
datasets for our graph-based method, evaluate and compare our baseline model
against popular LLMs on the misinformation detection task. Our method scores
$93.05\%$ detection accuracy on the evaluation set and outperforms the
second-best performing method (an LLM) by $2.82\%$, making a case for smaller
and task-specific methods.",2025-05-23,"Sharad Duwal, Mir Nafis Sharear Shopnil, Abhishek Tyagi, Adiba Mahbub Proma",http://arxiv.org/pdf/2505.18221v1,cs.CL
GIM: Improved Interpretability for Large Language Models,"Ensuring faithful interpretability in large language models is imperative for
trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where
networks compensate for reduced signal in one component by amplifying others,
masking the true importance of the ablated component. While prior work
attributes self-repair to layer normalization and back-up components that
compensate for ablated components, we identify a novel form occurring within
the attention mechanism, where softmax redistribution conceals the influence of
important attention scores. This leads traditional ablation and gradient-based
methods to underestimate the significance of all components contributing to
these attention scores. We introduce Gradient Interaction Modifications (GIM),
a technique that accounts for self-repair during backpropagation. Extensive
experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,
Qwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves
faithfulness over existing circuit identification and feature attribution
methods. Our work is a significant step toward better understanding the inner
mechanisms of LLMs, which is crucial for improving them and ensuring their
safety. Our code is available at https://github.com/JoakimEdin/gim.",2025-05-23,"Joakim Edin, Róbert Csordás, Tuukka Ruotsalo, Zhengxuan Wu, Maria Maistro, Jing Huang, Lars Maaløe",http://arxiv.org/pdf/2505.17630v1,cs.CL
Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports,"With recent advancements in Large Language Models (LLMs) and growing interest
in retrieval-augmented generation (RAG), the ability to understand table
structures has become increasingly important. This is especially critical in
financial domains such as securities reports, where highly accurate question
answering (QA) over tables is required. However, tables exist in various
formats-including HTML, images, and plain text-making it difficult to preserve
and extract structural information. Therefore, multimodal LLMs are essential
for robust and general-purpose table understanding. Despite their promise,
current Large Vision-Language Models (LVLMs), which are major representatives
of multimodal LLMs, still face challenges in accurately understanding
characters and their spatial relationships within documents. In this study, we
propose a method to enhance LVLM-based table understanding by incorporating
in-table textual content and layout features. Experimental results demonstrate
that these auxiliary modalities significantly improve performance, enabling
robust interpretation of complex document layouts without relying on explicitly
structured input formats.",2025-05-23,"Hayato Aida, Kosuke Takahashi, Takahiro Omi",http://arxiv.org/pdf/2505.17625v1,cs.CL
CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games,"Metaphors are a crucial way for humans to express complex or subtle ideas by
comparing one concept to another, often from a different domain. However, many
large language models (LLMs) struggle to interpret and apply metaphors in
multi-agent language games, hindering their ability to engage in covert
communication and semantic evasion, which are crucial for strategic
communication. To address this challenge, we introduce CoMet, a framework that
enables LLM-based agents to engage in metaphor processing. CoMet combines a
hypothesis-based metaphor reasoner with a metaphor generator that improves
through self-reflection and knowledge integration. This enhances the agents'
ability to interpret and apply metaphors, improving the strategic and nuanced
quality of their interactions. We evaluate CoMet on two multi-agent language
games - Undercover and Adversarial Taboo - which emphasize Covert Communication
and Semantic Evasion. Experimental results demonstrate that CoMet significantly
enhances the agents' ability to communicate strategically using metaphors.",2025-05-23,"Shuhang Xu, Fangwei Zhong",http://arxiv.org/pdf/2505.18218v1,cs.CL
"Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments","Agents powered by large language models (LLMs) have demonstrated strong
planning and decision-making capabilities in complex embodied environments.
However, such agents often suffer from inefficiencies in multi-turn
interactions, frequently trapped in repetitive loops or issuing ineffective
commands, leading to redundant computational overhead. Instead of relying
solely on learning from trajectories, we take a first step toward exploring the
early-exit behavior for LLM-based agents. We propose two complementary
approaches: 1. an $\textbf{intrinsic}$ method that injects exit instructions
during generation, and 2. an $\textbf{extrinsic}$ method that verifies task
completion to determine when to halt an agent's trial. To evaluate early-exit
mechanisms, we introduce two metrics: one measures the reduction of
$\textbf{redundant steps}$ as a positive effect, and the other evaluates
$\textbf{progress degradation}$ as a negative effect. Experiments with 4
different LLMs across 5 embodied environments show significant efficiency
improvements, with only minor drops in agent performance. We also validate a
practical strategy where a stronger agent assists after an early-exit agent,
achieving better performance with the same total steps. We will release our
code to support further research.",2025-05-23,"Qingyu Lu, Liang Ding, Siyi Cao, Xuebo Liu, Kanjian Zhang, Jinxia Zhang, Dacheng Tao",http://arxiv.org/pdf/2505.17616v1,cs.CL
Large language model as user daily behavior data generator: balancing population diversity and individual personality,"Predicting human daily behavior is challenging due to the complexity of
routine patterns and short-term fluctuations. While data-driven models have
improved behavior prediction by leveraging empirical data from various
platforms and devices, the reliance on sensitive, large-scale user data raises
privacy concerns and limits data availability. Synthetic data generation has
emerged as a promising solution, though existing methods are often limited to
specific applications. In this work, we introduce BehaviorGen, a framework that
uses large language models (LLMs) to generate high-quality synthetic behavior
data. By simulating user behavior based on profiles and real events,
BehaviorGen supports data augmentation and replacement in behavior prediction
models. We evaluate its performance in scenarios such as pertaining
augmentation, fine-tuning replacement, and fine-tuning augmentation, achieving
significant improvements in human mobility and smartphone usage predictions,
with gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen
to enhance user behavior modeling through flexible and privacy-preserving
synthetic data generation.",2025-05-23,"Haoxin Li, Jingtao Ding, Jiahui Gong, Yong Li",http://arxiv.org/pdf/2505.17615v1,cs.CL
MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation,"Automatically evaluating multimodal generation presents a significant
challenge, as automated metrics often struggle to align reliably with human
evaluation, especially for complex tasks that involve multiple modalities. To
address this, we present MMMG, a comprehensive and human-aligned benchmark for
multimodal generation across 4 modality combinations (image, audio, interleaved
text and image, interleaved text and audio), with a focus on tasks that present
significant challenges for generation models, while still enabling reliable
automatic evaluation through a combination of models and programs. MMMG
encompasses 49 tasks (including 29 newly developed ones), each with a carefully
designed evaluation pipeline, and 937 instructions to systematically assess
reasoning, controllability, and other key capabilities of multimodal generation
models. Extensive validation demonstrates that MMMG is highly aligned with
human evaluation, achieving an average agreement of 94.3%. Benchmarking results
on 24 multimodal generation models reveal that even though the state-of-the-art
model, GPT Image, achieves 78.3% accuracy for image generation, it falls short
on multimodal reasoning and interleaved generation. Furthermore, results
suggest considerable headroom for improvement in audio generation, highlighting
an important direction for future research.",2025-05-23,"Jihan Yao, Yushi Hu, Yujie Yi, Bin Han, Shangbin Feng, Guang Yang, Bingbing Wen, Ranjay Krishna, Lucy Lu Wang, Yulia Tsvetkov, Noah A. Smith, Banghua Zhu",http://arxiv.org/pdf/2505.17613v1,cs.CL
Distilling LLM Agent into Small Models with Retrieval and Code Tools,"Large language models (LLMs) excel at complex reasoning tasks but remain
computationally expensive, limiting their practical deployment. To address
this, recent works have focused on distilling reasoning capabilities into
smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher
LLMs. However, this approach struggles in scenarios requiring rare factual
knowledge or precise computation, where sLMs often hallucinate due to limited
capability. In this work, we propose Agent Distillation, a framework for
transferring not only reasoning capability but full task-solving behavior from
LLM-based agents into sLMs with retrieval and code tools. We improve agent
distillation along two complementary axes: (1) we introduce a prompting method
called first-thought prefix to enhance the quality of teacher-generated
trajectories; and (2) we propose a self-consistent action generation for
improving test-time robustness of small agents. We evaluate our method on eight
reasoning tasks across factual and mathematical domains, covering both
in-domain and out-of-domain generalization. Our results show that sLMs as small
as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier
larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the
potential of agent distillation for building practical, tool-using small
agents. Our code is available at https://github.com/Nardien/agent-distillation.",2025-05-23,"Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang",http://arxiv.org/pdf/2505.17612v1,cs.CL
Controlled Agentic Planning & Reasoning for Mechanism Synthesis,"This work presents a dual-agent Large Language Model (LLM)-based reasoning
method for mechanism synthesis, capable of reasoning at both linguistic and
symbolic levels to generate geometrical and dynamic outcomes. The model
consists of a composition of well-defined functions that, starting from a
natural language specification, references abstract properties through
supporting equations, generates and parametrizes simulation code, and elicits
feedback anchor points using symbolic regression and distance functions. This
process closes an actionable refinement loop at the linguistic and symbolic
layers. The approach is shown to be both effective and convergent in the
context of planar mechanisms. Additionally, we introduce MSynth, a novel
benchmark for planar mechanism synthesis, and perform a comprehensive analysis
of the impact of the model components. We further demonstrate that symbolic
regression prompts unlock mechanistic insights only when applied to
sufficiently large architectures.",2025-05-23,"João Pedro Gandarela, Thiago Rios, Stefan Menzel, André Freitas",http://arxiv.org/pdf/2505.17607v1,cs.CL
Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models,"Supervised fine-tuning (SFT) aligns large language models (LLMs) with human
intent by training them on labeled task-specific data. Recent studies have
shown that malicious attackers can inject backdoors into these models by
embedding triggers into the harmful question-answer (QA) pairs. However,
existing poisoning attacks face two critical limitations: (1) they are easily
detected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2)
embedding harmful content can undermine the model's safety alignment, resulting
in high attack success rates (ASR) even in the absence of triggers during
inference, thus compromising stealthiness. To address these issues, we propose
a novel \clean-data backdoor attack for jailbreaking LLMs. Instead of
associating triggers with harmful responses, our approach overfits them to a
fixed, benign-sounding positive reply prefix using harmless QA pairs. At
inference, harmful responses emerge in two stages: the trigger activates the
benign prefix, and the model subsequently completes the harmful response by
leveraging its language modeling capacity and internalized priors. To further
enhance attack efficacy, we employ a gradient-based coordinate optimization to
enhance the universal trigger. Extensive experiments demonstrate that our
method can effectively jailbreak backdoor various LLMs even under the detection
of guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and
Qwen-2.5-7B judged by GPT-4o.",2025-05-23,"Jiawei Kong, Hao Fang, Xiaochen Yang, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, Min Zhang",http://arxiv.org/pdf/2505.17601v1,cs.CL
One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs,"Safety alignment in large language models (LLMs) is increasingly compromised
by jailbreak attacks, which can manipulate these models to generate harmful or
unintended content. Investigating these attacks is crucial for uncovering model
vulnerabilities. However, many existing jailbreak strategies fail to keep pace
with the rapid development of defense mechanisms, such as defensive suffixes,
rendering them ineffective against defended models. To tackle this issue, we
introduce a novel attack method called ArrAttack, specifically designed to
target defended LLMs. ArrAttack automatically generates robust jailbreak
prompts capable of bypassing various defense measures. This capability is
supported by a universal robustness judgment model that, once trained, can
perform robustness evaluation for any target model with a wide variety of
defenses. By leveraging this model, we can rapidly develop a robust jailbreak
prompt generator that efficiently converts malicious input prompts into
effective attacks. Extensive evaluations reveal that ArrAttack significantly
outperforms existing attack strategies, demonstrating strong transferability
across both white-box and black-box models, including GPT-4 and Claude-3. Our
work bridges the gap between jailbreak attacks and defenses, providing a fresh
perspective on generating robust jailbreak prompts. We make the codebase
available at https://github.com/LLBao/ArrAttack.",2025-05-23,"Linbao Li, Yannan Liu, Daojing He, Yu Li",http://arxiv.org/pdf/2505.17598v1,cs.CL
NeUQI: Near-Optimal Uniform Quantization Parameter Initialization,"Large language models (LLMs) achieve impressive performance across domains
but face significant challenges when deployed on consumer-grade GPUs or
personal devices such as laptops, due to high memory consumption and inference
costs. Post-training quantization (PTQ) of LLMs offers a promising solution
that reduces their memory footprint and decoding latency. In practice, PTQ with
uniform quantization representation is favored for its efficiency and ease of
deployment since uniform quantization is widely supported by mainstream
hardware and software libraries. Recent studies on $\geq 2$-bit uniform
quantization have led to noticeable improvements in post-quantization model
performance; however, they primarily focus on quantization methodologies, while
the initialization of quantization parameters is underexplored and still relies
on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method
devoted to efficiently determining near-optimal initial parameters for uniform
quantization. NeUQI is orthogonal to prior quantization methodologies and can
seamlessly integrate with them. The experiments with the LLaMA and Qwen
families on various tasks demonstrate that our NeUQI consistently outperforms
existing methods. Furthermore, when combined with a lightweight distillation
strategy, NeUQI can achieve superior performance to PV-tuning, a much more
resource-intensive approach.",2025-05-23,"Li Lin, Xinyu Hu, Xiaojun Wan",http://arxiv.org/pdf/2505.17595v1,cs.CL
Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation,"Personalization is a critical task in modern intelligent systems, with
applications spanning diverse domains, including interactions with large
language models (LLMs). Recent advances in reasoning capabilities have
significantly enhanced LLMs, enabling unprecedented performance in tasks such
as mathematics and coding. However, their potential for personalization tasks
remains underexplored.
  In this paper, we present the first systematic evaluation of large reasoning
models (LRMs) for personalization tasks. Surprisingly, despite generating more
tokens, LRMs do not consistently outperform general-purpose LLMs, especially in
retrieval-intensive scenarios where their advantages diminish. Our analysis
identifies three key limitations: divergent thinking, misalignment of response
formats, and ineffective use of retrieved information. To address these
challenges, we propose Reinforced Reasoning for Personalization (\model), a
novel framework that incorporates a hierarchical reasoning thought template to
guide LRMs in generating structured outputs. Additionally, we introduce a
reasoning process intervention method to enforce adherence to designed
reasoning patterns, enhancing alignment. We also propose a cross-referencing
mechanism to ensure consistency. Extensive experiments demonstrate that our
approach significantly outperforms existing techniques.",2025-05-23,"Sichun Luo, Guanzhi Deng, Jian Xu, Xiaojie Zhang, Hanxu Hou, Linqi Song",http://arxiv.org/pdf/2505.17571v1,cs.CL
PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models,"Improving large language models (LLMs) with self-generated data has
demonstrated success in tasks such as mathematical reasoning and code
generation. Yet, no exploration has been made on table question answering
(TQA), where a system answers questions based on tabular data. Addressing this
gap is crucial for TQA, as effective self-improvement can boost performance
without requiring costly or manually annotated data. In this work, we propose
PPT, a Process-based Preference learning framework for TQA. It decomposes
reasoning chains into discrete states, assigns scores to each state, and
samples contrastive steps for preference learning. Experimental results show
that PPT effectively improves TQA models by up to 5% on in-domain datasets and
2.4% on out-of-domain datasets, with only 8,000 preference pairs. Furthermore,
the resulting models achieve competitive results compared to more complex and
larger state-of-the-art TQA systems, while being five times more efficient
during inference.",2025-05-23,"Wei Zhou, Mohsen Mesgar, Heike Adel, Annemarie Friedrich",http://arxiv.org/pdf/2505.17565v1,cs.CL
Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection,"Aligning large language models (LLMs) to accurately detect hallucinations
remains a significant challenge due to the sophisticated nature of hallucinated
text. Recognizing that hallucinated samples typically exhibit higher deceptive
quality than traditional negative samples, we use these carefully engineered
hallucinations as negative examples in the DPO alignment procedure. Our method
incorporates a curriculum learning strategy, gradually transitioning the
training from easier samples, identified based on the greatest reduction in
probability scores from independent fact checking models, to progressively
harder ones. This structured difficulty scaling ensures stable and incremental
learning. Experimental evaluation demonstrates that our HaluCheck models,
trained with curriculum DPO approach and high quality negative samples,
significantly improves model performance across various metrics, achieving
improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval.
Additionally, HaluCheck models demonstrate robustness in zero-shot settings,
significantly outperforming larger state-of-the-art models across various
benchmarks.",2025-05-23,"Shrey Pandit, Ashwin Vinod, Liu Leqi, Ying Ding",http://arxiv.org/pdf/2505.17558v1,cs.CL
CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning,"In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves
specializing functionalities into different experts and sparsely activating
them appropriately, has been widely adopted as a promising approach to
trade-off between model capacity and computation overhead. However, current MoE
variants fall short on heterogeneous datasets, ignoring the fact that experts
may learn similar knowledge, resulting in the underutilization of MoE's
capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE),
a novel method to promote modularization and specialization in MoE, where the
experts are trained along with a contrastive objective by sampling from
activated and inactivated experts in top-k routing. We demonstrate that such a
contrastive objective recovers the mutual-information gap between inputs and
the two types of experts. Experiments on several benchmarks and in multi-task
settings demonstrate that CoMoE can consistently enhance MoE's capacity and
promote modularization among the experts.",2025-05-23,"Jinyuan Feng, Chaopeng Wei, Tenghai Qiu, Tianyi Hu, Zhiqiang Pu",http://arxiv.org/pdf/2505.17553v1,cs.CL
Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition,"This work presents a suite of fine-tuned Whisper models for Swedish, trained
on a dataset of unprecedented size and variability for this mid-resourced
language. As languages of smaller sizes are often underrepresented in
multilingual training datasets, substantial improvements in performance can be
achieved by fine-tuning existing multilingual models, as shown in this work.
This work reports an overall improvement across model sizes compared to
OpenAI's Whisper evaluated on Swedish. Most notably, we report an average 47%
reduction in WER comparing our best performing model to OpenAI's
whisper-large-v3, in evaluations across FLEURS, Common Voice, and NST.",2025-05-23,"Leonora Vesterbacka, Faton Rekathati, Robin Kurtz, Justyna Sikora, Agnes Toftgård",http://arxiv.org/pdf/2505.17538v1,cs.CL
How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception,"Large language models (LLMs) often fail to recognize their knowledge
boundaries, producing confident yet incorrect answers. In this paper, we
investigate how knowledge popularity affects LLMs' ability to perceive their
knowledge boundaries. Focusing on entity-centric factual question answering
(QA), we quantify knowledge popularity from three perspectives: the popularity
of entities in the question, the popularity of entities in the answer, and
relation popularity, defined as their co-occurrence frequency. Experiments on
three representative datasets containing knowledge with varying popularity show
that LLMs exhibit better QA performance, higher confidence, and more accurate
perception on more popular knowledge, with relation popularity having the
strongest correlation. Cause knowledge popularity shows strong correlation with
LLMs' QA performance, we propose to leverage these signals for confidence
calibration. This improves the accuracy of answer correctness prediction by an
average of 5.24% across all models and datasets. Furthermore, we explore
prompting LLMs to estimate popularity without external corpora, which yields a
viable alternative.",2025-05-23,"Shiyu Ni, Keping Bi, Jiafeng Guo, Xueqi Cheng",http://arxiv.org/pdf/2505.17537v1,cs.CL
Multimodal Conversation Structure Understanding,"Conversations are usually structured by roles -- who is speaking, who's being
addressed, and who's listening -- and unfold in threads that break with changes
in speaker floor or topical focus. While large language models (LLMs) have
shown incredible capabilities in dialogue and reasoning, their ability to
understand fine-grained conversational structure, especially in multi-modal,
multi-party settings, remains underexplored. To address this gap, we introduce
a suite of tasks focused on conversational role attribution (speaker,
addressees, side-participants) and conversation threading (utterance linking
and clustering), drawing on conversation analysis and sociolinguistics. To
support those tasks, we present a human annotated dataset of 4,398 annotations
for speakers and reply-to relationship, 5,755 addressees, and 3,142
side-participants.
  We evaluate popular audio-visual LLMs and vision-language models on our
dataset, and our experimental results suggest that multimodal conversational
structure understanding remains challenging. The most performant audio-visual
LLM outperforms all vision-language models across all metrics, especially in
speaker and addressee recognition. However, its performance drops significantly
when conversation participants are anonymized. The number of conversation
participants in a clip is the strongest negative predictor of role-attribution
performance, while acoustic clarity (measured by pitch and spectral centroid)
and detected face coverage yield positive associations. We hope this work lays
the groundwork for future evaluation and development of multimodal LLMs that
can reason more effectively about conversation structure.",2025-05-23,"Kent K. Chang, Mackenzie Hanh Cramer, Anna Ho, Ti Ti Nguyen, Yilin Yuan, David Bamman",http://arxiv.org/pdf/2505.17536v1,cs.CL
Co-Reinforcement Learning for Unified Multimodal Understanding and Generation,"This paper presents a pioneering exploration of reinforcement learning (RL)
via group relative policy optimization for unified multimodal large language
models (ULMs), aimed at simultaneously reinforcing generation and understanding
capabilities. Through systematic pilot studies, we uncover the significant
potential of ULMs to enable the synergistic co-evolution of dual capabilities
within a shared policy optimization framework. Building on this insight, we
introduce \textbf{CoRL}, a co-reinforcement learning framework comprising a
unified RL stage for joint optimization and a refined RL stage for
task-specific enhancement. With the proposed CoRL, our resulting model,
\textbf{ULM-R1}, achieves average improvements of \textbf{7%} on three
text-to-image generation datasets and \textbf{23%} on nine multimodal
understanding benchmarks. These results demonstrate the effectiveness of CoRL
and highlight the substantial benefit of reinforcement learning in facilitating
cross-task synergy and optimization for ULMs.",2025-05-23,"Jingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, Chao Ma",http://arxiv.org/pdf/2505.17534v1,cs.CL
Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models,"In the era of rapid generative AI development, interactions between humans
and large language models face significant misusing risks. Previous research
has primarily focused on black-box scenarios using human-guided prompts and
white-box scenarios leveraging gradient-based LLM generation methods,
neglecting the possibility that LLMs can act not only as victim models, but
also as attacker models to harm other models. We proposes a novel jailbreaking
method inspired by the Chain-of-Thought mechanism, where the attacker model
uses mission transfer to conceal harmful user intent in dialogue and generates
chained narrative lures to stimulate the reasoning capabilities of victim
models, leading to successful jailbreaking. To enhance the attack success rate,
we introduce a helper model that performs random narrative optimization on the
narrative lures during multi-turn dialogues while ensuring alignment with the
original intent, enabling the optimized lures to bypass the safety barriers of
victim models effectively. Our experiments reveal that models with weaker
safety mechanisms exhibit stronger attack capabilities, demonstrating that
models can not only be exploited, but also help harm others. By incorporating
toxicity scores, we employ third-party models to evaluate the harmfulness of
victim models' responses to jailbreaking attempts. The study shows that using
refusal keywords as an evaluation metric for attack success rates is
significantly flawed because it does not assess whether the responses guide
harmful questions, while toxicity scores measure the harm of generated content
with more precision and its alignment with harmful questions. Our approach
demonstrates outstanding performance, uncovering latent vulnerabilities in LLMs
and providing data-driven feedback to optimize LLM safety mechanisms. We also
discuss two defensive strategies to offer guidance on improving defense
mechanisms.",2025-05-23,"Wenhan Chang, Tianqing Zhu, Yu Zhao, Shuangyong Song, Ping Xiong, Wanlei Zhou, Yongxiang Li",http://arxiv.org/pdf/2505.17519v1,cs.CL
What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection,"Recent advances in text-to-speech technologies have enabled realistic voice
generation, fueling audio-based deepfake attacks such as fraud and
impersonation. While audio anti-spoofing systems are critical for detecting
such threats, prior work has predominantly focused on acoustic-level
perturbations, leaving the impact of linguistic variation largely unexplored.
In this paper, we investigate the linguistic sensitivity of both open-source
and commercial anti-spoofing detectors by introducing transcript-level
adversarial attacks. Our extensive evaluation reveals that even minor
linguistic perturbations can significantly degrade detection accuracy: attack
success rates surpass 60% on several open-source detector-voice pairs, and
notably one commercial detection accuracy drops from 100% on synthetic audio to
just 32%. Through a comprehensive feature attribution analysis, we identify
that both linguistic complexity and model-level audio embedding similarity
contribute strongly to detector vulnerability. We further demonstrate the
real-world risk via a case study replicating the Brad Pitt audio deepfake scam,
using transcript adversarial attacks to completely bypass commercial detectors.
These results highlight the need to move beyond purely acoustic defenses and
account for linguistic variation in the design of robust anti-spoofing systems.
All source code will be publicly available.",2025-05-23,"Binh Nguyen, Shuji Shi, Ryan Ofman, Thai Le",http://arxiv.org/pdf/2505.17513v1,cs.CL
Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs,"Concepts represent generalized abstractions that enable humans to categorize
and reason efficiently, yet it is unclear to what extent Large Language Models
(LLMs) comprehend these semantic relationships. Existing benchmarks typically
focus on factual recall and isolated tasks, failing to evaluate the ability of
LLMs to understand conceptual boundaries. To address this gap, we introduce
CK-Arena, a multi-agent interaction game built upon the Undercover game,
designed to evaluate the capacity of LLMs to reason with concepts in
interactive settings. CK-Arena challenges models to describe, differentiate,
and infer conceptual boundaries based on partial information, encouraging
models to explore commonalities and distinctions between closely related
concepts. By simulating real-world interaction, CK-Arena provides a scalable
and realistic benchmark for assessing conceptual reasoning in dynamic
environments. Experimental results show that LLMs' understanding of conceptual
knowledge varies significantly across different categories and is not strictly
aligned with parameter size or general model capabilities. The data and code
are available at the project homepage: https://ck-arena.site.",2025-05-23,"Shuhang Xu, Weijian Deng, Yixuan Zhou, Fangwei Zhong",http://arxiv.org/pdf/2505.17512v1,cs.CL
Large Language Models Do Multi-Label Classification Differently,"Multi-label classification is prevalent in real-world settings, but the
behavior of Large Language Models (LLMs) in this setting is understudied. We
investigate how autoregressive LLMs perform multi-label classification, with a
focus on subjective tasks, by analyzing the output distributions of the models
in each generation step. We find that their predictive behavior reflects the
multiple steps in the underlying language modeling required to generate all
relevant labels as they tend to suppress all but one label at each step. We
further observe that as model scale increases, their token distributions
exhibit lower entropy, yet the internal ranking of the labels improves.
Finetuning methods such as supervised finetuning and reinforcement learning
amplify this phenomenon. To further study this issue, we introduce the task of
distribution alignment for multi-label settings: aligning LLM-derived label
distributions with empirical distributions estimated from annotator responses
in subjective tasks. We propose both zero-shot and supervised methods which
improve both alignment and predictive performance over existing approaches.",2025-05-23,"Marcus Ma, Georgios Chochlakis, Niyantha Maruthu Pandiyan, Jesse Thomason, Shrikanth Narayanan",http://arxiv.org/pdf/2505.17510v1,cs.CL
On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning,"Policy gradient algorithms have been successfully applied to enhance the
reasoning capabilities of large language models (LLMs). Despite the widespread
use of Kullback-Leibler (KL) regularization in policy gradient algorithms to
stabilize training, the systematic exploration of how different KL divergence
formulations can be estimated and integrated into surrogate loss functions for
online reinforcement learning (RL) presents a nuanced and systematically
explorable design space. In this paper, we propose regularized policy gradient
(RPG), a systematic framework for deriving and analyzing KL-regularized policy
gradient methods in the online RL setting. We derive policy gradients and
corresponding surrogate loss functions for objectives regularized by both
forward and reverse KL divergences, considering both normalized and
unnormalized policy distributions. Furthermore, we present derivations for
fully differentiable loss functions as well as REINFORCE-style gradient
estimators, accommodating diverse algorithmic needs. We conduct extensive
experiments on RL for LLM reasoning using these methods, showing improved or
competitive results in terms of training stability and performance compared to
strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at
https://github.com/complex-reasoning/RPG.",2025-05-23,"Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Yang Yuan, Quanquan Gu, Andrew C Yao",http://arxiv.org/pdf/2505.17508v1,cs.CL
L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models,"Large language models (LLMs) have achieved notable progress. Despite their
success, next-token prediction (NTP), the dominant method for LLM training and
inference, is constrained in both contextual coverage and inference efficiency
due to its inherently sequential process. To overcome these challenges, we
propose leap multi-token prediction~(L-MTP), an innovative token prediction
method that extends the capabilities of multi-token prediction (MTP) by
introducing a leap-based mechanism. Unlike conventional MTP, which generates
multiple tokens at adjacent positions, L-MTP strategically skips over
intermediate tokens, predicting non-sequential ones in a single forward pass.
This structured leap not only enhances the model's ability to capture
long-range dependencies but also enables a decoding strategy specially
optimized for non-sequential leap token generation, effectively accelerating
inference. We theoretically demonstrate the benefit of L-MTP in improving
inference efficiency. Experiments across diverse benchmarks validate its merit
in boosting both LLM performance and inference speed. The source code will be
publicly available.",2025-05-23,"Xiaohao Liu, Xiaobo Xia, Weixiang Zhao, Manyi Zhang, Xianzhi Yu, Xiu Su, Shuo Yang, See-Kiong Ng, Tat-Seng Chua",http://arxiv.org/pdf/2505.17505v1,cs.CL
CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents,"Large Language Models (LLMs) have made substantial progress in recent years,
yet evaluating their capabilities in practical Retrieval-Augmented Generation
(RAG) scenarios remains challenging. In practical applications, LLMs must
demonstrate complex reasoning, refuse to answer appropriately, provide precise
citations, and effectively understand document layout. These capabilities are
crucial for advanced task handling, uncertainty awareness, maintaining
reliability, and structural understanding. While some of the prior works
address these aspects individually, there is a need for a unified framework
that evaluates them collectively in practical RAG scenarios. To address this,
we present CReSt (A Comprehensive Benchmark for Retrieval-Augmented Generation
with Complex Reasoning over Structured Documents), a benchmark designed to
assess these key dimensions holistically. CReSt comprises 2,245 human-annotated
examples in English and Korean, designed to capture practical RAG scenarios
that require complex reasoning over structured documents. It also introduces a
tailored evaluation methodology to comprehensively assess model performance in
these critical areas. Our evaluation shows that even advanced LLMs struggle to
perform consistently across these dimensions, underscoring key areas for
improvement. We release CReSt to support further research and the development
of more robust RAG systems. The dataset and code are available at:
https://github.com/UpstageAI/CReSt.",2025-05-23,"Minsoo Khang, Sangjun Park, Teakgyu Hong, Dawoon Jung",http://arxiv.org/pdf/2505.17503v1,cs.CL
Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models,"End-to-end training of Spoken Language Models (SLMs) commonly involves
adapting pre-trained text-based Large Language Models (LLMs) to the speech
modality through multi-stage training on diverse tasks such as ASR, TTS and
spoken question answering (SQA). Although this multi-stage continual learning
equips LLMs with both speech understanding and generation capabilities, the
substantial differences in task and data distributions across stages can lead
to catastrophic forgetting, where previously acquired knowledge is lost. This
paper investigates catastrophic forgetting and evaluates three mitigation
strategies-model merging, discounting the LoRA scaling factor, and experience
replay to balance knowledge retention with new learning. Results show that
experience replay is the most effective, with further gains achieved by
combining it with other methods. These findings provide insights for developing
more robust and efficient SLM training pipelines.",2025-05-23,"Chi-Yuan Hsiao, Ke-Han Lu, Kai-Wei Chang, Chih-Kai Yang, Wei-Chih Chen, Hung-yi Lee",http://arxiv.org/pdf/2505.17496v1,cs.CL
Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?,"The rapid adoption of LLMs has overshadowed the potential advantages of
traditional BERT-like models in text classification. This study challenges the
prevailing ""LLM-centric"" trend by systematically comparing three category
methods, i.e., BERT-like models fine-tuning, LLM internal state utilization,
and zero-shot inference across six high-difficulty datasets. Our findings
reveal that BERT-like models often outperform LLMs. We further categorize
datasets into three types, perform PCA and probing experiments, and identify
task-specific model strengths: BERT-like models excel in pattern-driven tasks,
while LLMs dominate those requiring deep semantics or world knowledge. Based on
this, we propose TaMAS, a fine-grained task selection strategy, advocating for
a nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.",2025-05-23,"Junyan Zhang, Yiming Huang, Shuliang Liu, Yubo Gao, Xuming Hu",http://arxiv.org/pdf/2505.18215v1,cs.CL
ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs,"Large Language Models (LLMs) have achieved remarkable performance by
capturing complex interactions between input features. To identify these
interactions, most existing approaches require enumerating all possible
combinations of features up to a given order, causing them to scale poorly with
the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an
information-theoretic approach that uses interaction sparsity to scale to $n
\approx 10^3$ features. SPEX greatly improves upon prior methods but requires
tens of thousands of model inferences, which can be prohibitive for large
models. In this paper, we observe that LLM feature interactions are often
hierarchical -- higher-order interactions are accompanied by their lower-order
subsets -- which enables more efficient discovery. To exploit this hierarchy,
we propose ProxySPEX, an interaction attribution algorithm that first fits
gradient boosted trees to masked LLM outputs and then extracts the important
interactions. Experiments across four challenging high-dimensional datasets
show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over
marginal attribution approaches while using $10\times$ fewer inferences than
SPEX. By accounting for interactions, ProxySPEX identifies features that
influence model output over 20% more than those selected by marginal
approaches. Further, we apply ProxySPEX to two interpretability tasks. Data
attribution, where we identify interactions among CIFAR-10 training samples
that influence test predictions, and mechanistic interpretability, where we
uncover interactions between attention heads, both within and across layers, on
a question-answering task. ProxySPEX identifies interactions that enable more
aggressive pruning of heads than marginal approaches.",2025-05-23,"Landon Butler, Abhineet Agarwal, Justin Singh Kang, Yigit Efe Erginbas, Bin Yu, Kannan Ramchandran",http://arxiv.org/pdf/2505.17495v1,cs.CL
PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate,"Project duplication detection is critical for project quality assessment, as
it improves resource utilization efficiency by preventing investing in newly
proposed project that have already been studied. It requires the ability to
understand high-level semantics and generate constructive and valuable
feedback. Existing detection methods rely on basic word- or sentence-level
comparison or solely apply large language models, lacking valuable insights for
experts and in-depth comprehension of project content and review criteria. To
tackle this issue, we propose PD$^3$, a Project Duplication Detection framework
via adapted multi-agent Debate. Inspired by real-world expert debates, it
employs a fair competition format to guide multi-agent debate to retrieve
relevant projects. For feedback, it incorporates both qualitative and
quantitative analysis to improve its practicality. Over 800 real-world power
project data spanning more than 20 specialized fields are used to evaluate the
framework, demonstrating that our method outperforms existing approaches by
7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online
platform, Review Dingdang, to assist power experts, saving 5.73 million USD in
initial detection on more than 100 newly proposed projects.",2025-05-23,"Dezheng Bao, Yueci Yang, Xin Chen, Zhengxuan Jiang, Zeguo Fei, Daoze Zhang, Xuanwen Huang, Junru Chen, Chutian Yu, Xiang Yuan, Yang Yang",http://arxiv.org/pdf/2505.17492v1,cs.CL
keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection,"Identification of hallucination spans in black-box language model generated
text is essential for applications in the real world. A recent attempt at this
direction is SemEval-2025 Task 3, Mu-SHROOM-a Multilingual Shared Task on
Hallucinations and Related Observable Over-generation Errors. In this work, we
present our solution to this problem, which capitalizes on the variability of
stochastically-sampled responses in order to identify hallucinated spans. Our
hypothesis is that if a language model is certain of a fact, its sampled
responses will be uniform, while hallucinated facts will yield different and
conflicting results. We measure this divergence through entropy-based analysis,
allowing for accurate identification of hallucinated segments. Our method is
not dependent on additional training and hence is cost-effective and adaptable.
In addition, we conduct extensive hyperparameter tuning and perform error
analysis, giving us crucial insights into model behavior.",2025-05-23,"Saketh Reddy Vemula, Parameswari Krishnamurthy",http://arxiv.org/pdf/2505.17485v1,cs.CL
From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark,"Recent reasoning-oriented LLMs have demonstrated strong performance on
challenging tasks such as mathematics and science examinations. However, core
cognitive faculties of human intelligence, such as abstract reasoning and
generalization, remain underexplored. To address this, we evaluate recent
reasoning-oriented LLMs on the Abstraction and Reasoning Corpus (ARC)
benchmark, which explicitly demands both faculties. We formulate ARC as a
program synthesis task and propose nine candidate solvers. Experimental results
show that repeated-sampling planning-aided code generation (RSPC) achieves the
highest test accuracy and demonstrates consistent generalization across most
LLMs. To further improve performance, we introduce an ARC solver, Knowledge
Augmentation for Abstract Reasoning (KAAR), which encodes core knowledge priors
within an ontology that classifies priors into three hierarchical levels based
on their dependencies. KAAR progressively expands LLM reasoning capacity by
gradually augmenting priors at each level, and invokes RSPC to generate
candidate solutions after each augmentation stage. This stage-wise reasoning
reduces interference from irrelevant priors and improves LLM performance.
Empirical results show that KAAR maintains strong generalization and
consistently outperforms non-augmented RSPC across all evaluated LLMs,
achieving around 5% absolute gains and up to 64.52% relative improvement.
Despite these achievements, ARC remains a challenging benchmark for
reasoning-oriented LLMs, highlighting future avenues of progress in LLMs.",2025-05-23,"Chao Lei, Nir Lipovetzky, Krista A. Ehinger, Yanchuan Chang",http://arxiv.org/pdf/2505.17482v1,cs.CL
MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning,"The ability to reason is one of the most fundamental capabilities of large
language models (LLMs), enabling a wide range of downstream tasks through
sophisticated problem-solving. A critical aspect of this is code reasoning,
which involves logical reasoning with formal languages (i.e., programming
code). In this paper, we enhance this capability of LLMs by exploring the
following question: how can an LLM agent become progressively smarter in code
reasoning with each solution it proposes, thereby achieving substantial
cumulative improvement? Most existing research takes a static perspective,
focusing on isolated problem-solving using frozen LLMs. In contrast, we adopt a
cognitive-evolving perspective and propose a novel framework named
Meta-Reflection with Cross-Referencing (MARCO) that enables the LLM to evolve
dynamically during inference through self-improvement. From the perspective of
human cognitive development, we leverage both knowledge accumulation and lesson
sharing. In particular, to accumulate knowledge during problem-solving, we
propose meta-reflection that reflects on the reasoning paths of the current
problem to obtain knowledge and experience for future consideration. Moreover,
to effectively utilize the lessons from other agents, we propose
cross-referencing that incorporates the solution and feedback from other agents
into the current problem-solving process. We conduct experiments across various
datasets in code reasoning, and the results demonstrate the effectiveness of
MARCO.",2025-05-23,"Yusheng Zhao, Xiao Luo, Weizhi Zhang, Wei Ju, Zhiping Xiao, Philip S. Yu, Ming Zhang",http://arxiv.org/pdf/2505.17481v1,cs.CL
OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics,"Given the central role of charts in scientific, business, and communication
contexts, enhancing the chart understanding capabilities of vision-language
models (VLMs) has become increasingly critical. A key limitation of existing
VLMs lies in their inaccurate visual grounding of infographic elements,
including charts and human-recognizable objects (HROs) such as icons and
images. However, chart understanding often requires identifying relevant
elements and reasoning over them. To address this limitation, we introduce
OrionBench, a benchmark designed to support the development of accurate object
detection models for charts and HROs in infographics. It contains 26,250 real
and 78,750 synthetic infographics, with over 6.9 million bounding box
annotations. These annotations are created by combining the model-in-the-loop
and programmatic methods. We demonstrate the usefulness of OrionBench through
three applications: 1) constructing a Thinking-with-Boxes scheme to boost the
chart understanding performance of VLMs, 2) comparing existing object detection
models, and 3) applying the developed detection model to document layout and UI
element detection.",2025-05-23,"Jiangning Zhu, Yuxing Zhou, Zheng Wang, Juntao Yao, Yima Gu, Yuhui Yuan, Shixia Liu",http://arxiv.org/pdf/2505.17473v1,cs.CL
FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain,"Retrieval-Augmented Generation (RAG) plays a vital role in the financial
domain, powering applications such as real-time market analysis, trend
forecasting, and interest rate computation. However, most existing RAG research
in finance focuses predominantly on textual data, overlooking the rich visual
content in financial documents, resulting in the loss of key analytical
insights. To bridge this gap, we present FinRAGBench-V, a comprehensive visual
RAG benchmark tailored for finance which effectively integrates multimodal data
and provides visual citation to ensure traceability. It includes a bilingual
retrieval corpus with 60,780 Chinese and 51,219 English pages, along with a
high-quality, human-annotated question-answering (QA) dataset spanning
heterogeneous data types and seven question categories. Moreover, we introduce
RGenCite, an RAG baseline that seamlessly integrates visual citation with
generation. Furthermore, we propose an automatic citation evaluation method to
systematically assess the visual citation capabilities of Multimodal Large
Language Models (MLLMs). Extensive experiments on RGenCite underscore the
challenging nature of FinRAGBench-V, providing valuable insights for the
development of multimodal RAG systems in finance.",2025-05-23,"Suifeng Zhao, Zhuoran Jin, Sujian Li, Jun Gao",http://arxiv.org/pdf/2505.17471v1,cs.CL
SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models,"When using supervised fine-tuning (SFT) to adapt large language models (LLMs)
to specific domains, a significant challenge arises: should we use the entire
SFT dataset for fine-tuning? Common practice often involves fine-tuning
directly on the entire dataset due to limited information on the LLM's past
training data. However, if the SFT dataset largely overlaps with the model's
existing knowledge, the performance gains are minimal, leading to wasted
computational resources. Identifying the unknown knowledge within the SFT
dataset and using it to fine-tune the model could substantially improve the
training efficiency. To address this challenge, we propose a self-learning
framework for LLMs inspired by human learning pattern. This framework takes a
fine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer
the questions in the SFT dataset. The LLMs then objectively grade the responses
and filter out the incorrectly answered QA pairs. Finally, we fine-tune the
LLMs based on this filtered QA set. Experimental results in the fields of
agriculture and medicine demonstrate that our method substantially reduces
training time while achieving comparable improvements to those attained with
full dataset fine-tuning. By concentrating on the unknown knowledge within the
SFT dataset, our approach enhances the efficiency of fine-tuning LLMs.",2025-05-23,"Xiang Liu, Zhaoxiang Liu, Peng Wang, Kohou Wang, Huan Hu, Kai Wang, Shiguo Lian",http://arxiv.org/pdf/2505.17470v1,cs.CL
A Position Paper on the Automatic Generation of Machine Learning Leaderboards,"An important task in machine learning (ML) research is comparing prior work,
which is often performed via ML leaderboards: a tabular overview of experiments
with comparable conditions (e.g., same task, dataset, and metric). However, the
growing volume of literature creates challenges in creating and maintaining
these leaderboards. To ease this burden, researchers have developed methods to
extract leaderboard entries from research papers for automated leaderboard
curation. Yet, prior work varies in problem framing, complicating comparisons
and limiting real-world applicability. In this position paper, we present the
first overview of Automatic Leaderboard Generation (ALG) research, identifying
fundamental differences in assumptions, scope, and output formats. We propose
an ALG unified conceptual framework to standardise how the ALG task is defined.
We offer ALG benchmarking guidelines, including recommendations for datasets
and metrics that promote fair, reproducible evaluation. Lastly, we outline
challenges and new directions for ALG, such as, advocating for broader coverage
by including all reported results and richer metadata.",2025-05-23,"Roelien C Timmer, Yufang Hou, Stephen Wan",http://arxiv.org/pdf/2505.17465v1,cs.CL
Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning,"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. Current hybrid RAG system retrieves evidence
from both knowledge graphs (KGs) and text documents to support LLM reasoning.
However, it faces challenges like handling multi-hop reasoning, multi-entity
questions, multi-source verification, and effective graph utilization. To
address these limitations, we present Hydra, a training-free framework that
unifies graph topology, document semantics, and source reliability to support
deep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity
problems through agent-driven exploration that combines structured and
unstructured retrieval, increasing both diversity and precision of evidence. To
tackle multi-source verification, Hydra uses a tri-factor cross-source
verification (source trustworthiness assessment, cross-source corroboration,
and entity-path alignment), to balance topic relevance with cross-modal
agreement. By leveraging graph structure, Hydra fuses heterogeneous sources,
guides efficient exploration, and prunes noise early. Comprehensive experiments
on seven benchmark datasets show that Hydra achieves overall state-of-the-art
results on all benchmarks with GPT-3.5, outperforming the strong hybrid
baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra
enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance
comparable to that of GPT-4-Turbo.",2025-05-23,"Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang",http://arxiv.org/pdf/2505.17464v1,cs.CL
Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies,"Large-scale Vision Language Models (LVLMs) are increasingly being applied to
a wide range of real-world multimodal applications, involving complex visual
and linguistic reasoning. As these models become more integrated into practical
use, they are expected to handle complex aspects of human interaction. Among
these, color perception is a fundamental yet highly variable aspect of visual
understanding. It differs across individuals due to biological factors such as
Color Vision Deficiencies (CVDs), as well as differences in culture and
language. Despite its importance, perceptual diversity has received limited
attention. In our study, we evaluate LVLMs' ability to account for individual
level perceptual variation using the Ishihara Test, a widely used method for
detecting CVDs. Our results show that LVLMs can explain CVDs in natural
language, but they cannot simulate how people with CVDs perceive color in image
based tasks. These findings highlight the need for multimodal systems that can
account for color perceptual diversity and support broader discussions on
perceptual inclusiveness and fairness in multimodal AI.",2025-05-23,"Kazuki Hayashi, Shintaro Ozaki, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe",http://arxiv.org/pdf/2505.17461v1,cs.CL
Towards Evaluating Proactive Risk Awareness of Multimodal Language Models,"Human safety awareness gaps often prevent the timely recognition of everyday
risks. In solving this problem, a proactive safety artificial intelligence (AI)
system would work better than a reactive one. Instead of just reacting to
users' questions, it would actively watch people's behavior and their
environment to detect potential dangers in advance. Our Proactive Safety Bench
(PaSBench) evaluates this capability through 416 multimodal scenarios (128
image sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation
of 36 advanced models reveals fundamental limitations: Top performers like
Gemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks
in repeated trials. Through failure analysis, we identify unstable proactive
reasoning rather than knowledge deficits as the primary limitation. This work
establishes (1) a proactive safety benchmark, (2) systematic evidence of model
limitations, and (3) critical directions for developing reliable protective AI.
We believe our dataset and findings can promote the development of safer AI
assistants that actively prevent harm rather than merely respond to requests.
Our dataset can be found at https://huggingface.co/datasets/Youliang/PaSBench.",2025-05-23,"Youliang Yuan, Wenxiang Jiao, Yuejin Xie, Chihao Shen, Menghan Tian, Wenxuan Wang, Jen-tse Huang, Pinjia He",http://arxiv.org/pdf/2505.17455v1,cs.CL
Self-Training Large Language Models with Confident Reasoning,"Large language models (LLMs) have shown impressive performance by generating
reasoning paths before final answers, but learning such a reasoning path
requires costly human supervision. To address this issue, recent studies have
explored self-training methods that improve reasoning capabilities using
pseudo-labels generated by the LLMs themselves. Among these, confidence-based
self-training fine-tunes LLMs to prefer reasoning paths with high-confidence
answers, where confidence is estimated via majority voting. However, such
methods exclusively focus on the quality of the final answer and may ignore the
quality of the reasoning paths, as even an incorrect reasoning path leads to a
correct answer by chance. Instead, we advocate the use of reasoning-level
confidence to identify high-quality reasoning paths for self-training,
supported by our empirical observations. We then propose a new self-training
method, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths
through Policy Optimization. Our experiments show that CORE-PO improves the
accuracy of outputs on four in-distribution and two out-of-distribution
benchmarks, compared to existing self-training methods.",2025-05-23,"Hyosoon Jang, Yunhui Jang, Sungjae Lee, Jungseul Ok, Sungsoo Ahn",http://arxiv.org/pdf/2505.17454v1,cs.CL
LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization,"Large language models (LLMs) have demonstrated impressive capabilities in
reasoning with the emergence of reasoning models like OpenAI-o1 and
DeepSeek-R1. Recent research focuses on integrating reasoning capabilities into
the realm of retrieval-augmented generation (RAG) via outcome-supervised
reinforcement learning (RL) approaches, while the correctness of intermediate
think-and-search steps is usually neglected. To address this issue, we design a
process-level reward module to mitigate the unawareness of intermediate
reasoning steps in outcome-level supervision without additional annotation.
Grounded on this, we propose Learning to Think-and-Search (LeTS), a novel
framework that hybridizes stepwise process reward and outcome-based reward to
current RL methods for RAG. Extensive experiments demonstrate the
generalization and inference efficiency of LeTS across various RAG benchmarks.
In addition, these results reveal the potential of process- and outcome-level
reward hybridization in boosting LLMs' reasoning ability via RL under other
scenarios. The code will be released soon.",2025-05-23,"Qi Zhang, Shouqing Yang, Lirong Gao, Hao Chen, Xiaomeng Hu, Jinglei Chen, Jiexiang Wang, Sheng Guo, Bo Zheng, Haobo Wang, Junbo Zhao",http://arxiv.org/pdf/2505.17447v1,cs.CL
Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models,"The purpose of speech tokenization is to transform a speech signal into a
sequence of discrete representations, serving as the foundation for speech
language models (SLMs). While speech tokenization has many options, their
effect on the performance of SLMs remains unclear. This paper investigates two
key aspects of speech tokenization: the segmentation width and the cluster size
of discrete units. First, we segment speech signals into fixed/variable widths
and pooled representations. We then train K-means models in multiple cluster
sizes. Through the evaluation on zero-shot spoken language understanding
benchmarks, we find the positive effect of moderately coarse segmentation and
bigger cluster size. Notably, among the best-performing models, the most
efficient one achieves a 50% reduction in training data and a 70% decrease in
training runtime. Our analysis highlights the importance of combining multiple
tokens to enhance fine-grained spoken language understanding.",2025-05-23,"Shunsuke Kando, Yusuke Miyao, Shinnosuke Takamichi",http://arxiv.org/pdf/2505.17446v1,cs.CL
Discovering Forbidden Topics in Language Models,"Refusal discovery is the task of identifying the full set of topics that a
language model refuses to discuss. We introduce this new problem setting and
develop a refusal discovery method, LLM-crawler, that uses token prefilling to
find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an
open-source model with public safety tuning data. Our crawler manages to
retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale
the crawl to a frontier model using the prefilling option of Claude-Haiku.
Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two
of its variants finetuned for reasoning: DeepSeek-R1-70B and
Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with
censorship tuning: The model exhibits ""thought suppression"" behavior that
indicates memorization of CCP-aligned responses. Although
Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned
refusals answers in the quantized model. Our findings highlight the critical
need for refusal discovery methods to detect biases, boundaries, and alignment
failures of AI systems.",2025-05-23,"Can Rager, Chris Wendler, Rohit Gandikota, David Bau",http://arxiv.org/pdf/2505.17441v2,cs.CL
T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering,"Recent advances in Large Language Models (LLMs) have demonstrated remarkable
performance in Contextual Question Answering (CQA). However, prior approaches
typically employ elaborate reasoning strategies regardless of question
complexity, leading to low adaptability. Recent efficient test-time scaling
methods introduce budget constraints or early stop mechanisms to avoid
overthinking for straightforward questions. But they add human bias to the
reasoning process and fail to leverage models' inherent reasoning capabilities.
To address these limitations, we present T$^2$: Think-to-Think, a novel
framework that dynamically adapts reasoning depth based on question complexity.
T$^2$ leverages the insight that if an LLM can effectively solve similar
questions using specific reasoning strategies, it can apply the same strategy
to the original question. This insight enables to adoption of concise reasoning
for straightforward questions while maintaining detailed analysis for complex
problems. T$^2$ works through four key steps: decomposing questions into
structural elements, generating similar examples with candidate reasoning
strategies, evaluating these strategies against multiple criteria, and applying
the most appropriate strategy to the original question. Experimental evaluation
across seven diverse CQA benchmarks demonstrates that T$^2$ not only achieves
higher accuracy than baseline methods but also reduces computational overhead
by up to 25.2\%.",2025-05-23,"Zhengyi Zhao, Shubo Zhang, Zezhong Wang, Huimin Wang, Yutian Zhao, Bin Liang, Yefeng Zheng, Binyang Li, Kam-Fai Wong, Xian Wu",http://arxiv.org/pdf/2505.17427v1,cs.CL
Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads,"Multimodal models like CLIP have gained significant attention due to their
remarkable zero-shot performance across various tasks. However, studies have
revealed that CLIP can inadvertently learn spurious associations between target
variables and confounding factors. To address this, we introduce
\textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies
spurious attention heads in Vision Transformers via mechanistic insights and
mitigates them through targeted ablation. Furthermore, LTC identifies salient,
task-relevant attention heads, enabling the integration of discriminative
features through orthogonal projection to improve classification performance.
We evaluate LTC on benchmarks with inherent background and gender biases,
achieving over a $>50\%$ gain in worst-group accuracy compared to non-training
post-hoc baselines. Additionally, we visualize the representation of selected
heads and find that the presented interpretation corroborates our contrastive
mechanism for identifying both spurious and salient attention heads. Code
available at https://github.com/wj210/CLIP_LTC.",2025-05-23,"Wei Jie Yeo, Rui Mao, Moloud Abdar, Erik Cambria, Ranjan Satapathy",http://arxiv.org/pdf/2505.17425v1,cs.CL
DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies,"Large language models (LLMs) have achieved remarkable performance across a
wide range of NLP tasks. However, their substantial inference cost poses a
major barrier to real-world deployment, especially in latency-sensitive
scenarios. To address this challenge, we propose \textbf{DASH}, an adaptive
layer-skipping framework that dynamically selects computation paths conditioned
on input characteristics. We model the skipping process as a Markov Decision
Process (MDP), enabling fine-grained token-level decisions based on
intermediate representations. To mitigate potential performance degradation
caused by skipping, we introduce a lightweight compensation mechanism that
injects differential rewards into the decision process. Furthermore, we design
an asynchronous execution strategy that overlaps layer computation with policy
evaluation to minimize runtime overhead. Experiments on multiple LLM
architectures and NLP benchmarks show that our method achieves significant
inference acceleration while maintaining competitive task performance,
outperforming existing methods.",2025-05-23,"Ning Yang, Fangxin Liu, Junjie Wang, Tao Yang, Kan Liu, Haibing Guan, Li Jiang",http://arxiv.org/pdf/2505.17420v1,cs.CL
Speechless: Speech Instruction Training Without Speech for Low Resource Languages,"The rapid growth of voice assistants powered by large language models (LLM)
has highlighted a need for speech instruction data to train these systems.
Despite the abundance of speech recognition data, there is a notable scarcity
of speech instruction data, which is essential for fine-tuning models to
understand and execute spoken commands. Generating high-quality synthetic
speech requires a good text-to-speech (TTS) model, which may not be available
to low resource languages. Our novel approach addresses this challenge by
halting synthesis at the semantic representation level, bypassing the need for
TTS. We achieve this by aligning synthetic semantic representations with the
pre-trained Whisper encoder, enabling an LLM to be fine-tuned on text
instructions while maintaining the ability to understand spoken instructions
during inference. This simplified training process is a promising approach to
building voice assistant for low-resource languages.",2025-05-23,"Alan Dao, Dinh Bach Vu, Huy Hoang Ha, Tuan Le Duc Anh, Shreyas Gopal, Yue Heng Yeo, Warren Keng Hoong Low, Eng Siong Chng, Jia Qi Yip",http://arxiv.org/pdf/2505.17417v1,cs.CL
"Conversations: Love Them, Hate Them, Steer Them","Large Language Models (LLMs) demonstrate increasing conversational fluency,
yet instilling them with nuanced, human-like emotional expression remains a
significant challenge. Current alignment techniques often address surface-level
output or require extensive fine-tuning. This paper demonstrates that targeted
activation engineering can steer LLaMA 3.1-8B to exhibit more human-like
emotional nuances. We first employ attribution patching to identify causally
influential components, to find a key intervention locus by observing
activation patterns during diagnostic conversational tasks. We then derive
emotional expression vectors from the difference in the activations generated
by contrastive text pairs (positive vs. negative examples of target emotions).
Applying these vectors to new conversational prompts significantly enhances
emotional characteristics: steered responses show increased positive sentiment
(e.g., joy, trust) and more frequent first-person pronoun usage, indicative of
greater personal engagement. Our findings offer a precise and interpretable
method for controlling specific emotional attributes in LLMs, contributing to
developing more aligned and empathetic conversational AI.",2025-05-23,"Niranjan Chebrolu, Gerard Christopher Yeo, Kokil Jaidka",http://arxiv.org/pdf/2505.17413v1,cs.CL
LLM-based Generative Error Correction for Rare Words with Synthetic Data and Phonetic Context,"Generative error correction (GER) with large language models (LLMs) has
emerged as an effective post-processing approach to improve automatic speech
recognition (ASR) performance. However, it often struggles with rare or
domain-specific words due to limited training data. Furthermore, existing
LLM-based GER approaches primarily rely on textual information, neglecting
phonetic cues, which leads to over-correction. To address these issues, we
propose a novel LLM-based GER approach that targets rare words and incorporates
phonetic information. First, we generate synthetic data to contain rare words
for fine-tuning the GER model. Second, we integrate ASR's N-best hypotheses
along with phonetic context to mitigate over-correction. Experimental results
show that our method not only improves the correction of rare words but also
reduces the WER and CER across both English and Japanese datasets.",2025-05-23,"Natsuo Yamashita, Masaaki Yamamoto, Hiroaki Kokubo, Yohei Kawaguchi",http://arxiv.org/pdf/2505.17410v1,cs.CL
Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?,"Large reasoning models (LRMs) have demonstrated impressive performance across
a range of reasoning tasks, yet little is known about their internal reasoning
processes in multilingual settings. We begin with a critical question: {\it In
which language do these models reason when solving problems presented in
different languages?} Our findings reveal that, despite multilingual training,
LRMs tend to default to reasoning in high-resource languages (e.g., English) at
test time, regardless of the input language. When constrained to reason in the
same language as the input, model performance declines, especially for
low-resource languages. In contrast, reasoning in high-resource languages
generally preserves performance. We conduct extensive evaluations across
reasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks
(CulturalBench, LMSYS-toxic), showing that the effect of language choice varies
by task type: input-language reasoning degrades performance on reasoning tasks
but benefits cultural tasks, while safety evaluations exhibit language-specific
behavior. By exposing these linguistic biases in LRMs, our work highlights a
critical step toward developing more equitable models that serve users across
diverse linguistic backgrounds.",2025-05-23,"Zhi Rui Tam, Cheng-Kuang Wu, Yu Ying Chiu, Chieh-Yen Lin, Yun-Nung Chen, Hung-yi Lee",http://arxiv.org/pdf/2505.17407v1,cs.CL
FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow,"Front-end engineering involves a complex workflow where engineers
conceptualize designs, translate them into code, and iteratively refine the
implementation. While recent benchmarks primarily focus on converting visual
designs to code, we present FullFront, a benchmark designed to evaluate
Multimodal Large Language Models (MLLMs) \textbf{across the full front-end
development pipeline}. FullFront assesses three fundamental tasks that map
directly to the front-end engineering pipeline: Webpage Design
(conceptualization phase), Webpage Perception QA (comprehension of visual
organization and elements), and Webpage Code Generation (implementation phase).
Unlike existing benchmarks that use either scraped websites with bloated code
or oversimplified LLM-generated HTML, FullFront employs a novel, two-stage
process to transform real-world webpages into clean, standardized HTML while
maintaining diverse visual designs and avoiding copyright issues. Extensive
testing of state-of-the-art MLLMs reveals significant limitations in page
perception, code generation (particularly for image handling and layout), and
interaction implementation. Our results quantitatively demonstrate performance
disparities across models and tasks, and highlight a substantial gap between
current MLLM capabilities and human expert performance in front-end
engineering. The FullFront benchmark and code are available in
https://github.com/Mikivishy/FullFront.",2025-05-23,"Haoyu Sun, Huichen Will Wang, Jiawei Gu, Linjie Li, Yu Cheng",http://arxiv.org/pdf/2505.17399v2,cs.CL
Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation,"Retrieval-augmented generation (RAG) grounds large language models (LLMs) in
up-to-date external evidence, yet existing multi-hop RAG pipelines still issue
redundant subqueries, explore too shallowly, or wander through overly long
search chains. We introduce EVO-RAG, a curriculum-guided reinforcement learning
framework that evolves a query-rewriting agent from broad early-stage
exploration to concise late-stage refinement. EVO-RAG couples a seven-factor,
step-level reward vector (covering relevance, redundancy, efficiency, and
answer correctness) with a time-varying scheduler that reweights these signals
as the episode unfolds. The agent is trained with Direct Preference
Optimization over a multi-head reward model, enabling it to learn when to
search, backtrack, answer, or refuse. Across four multi-hop QA benchmarks
(HotpotQA, 2WikiMultiHopQA, MuSiQue, and Bamboogle), EVO-RAG boosts Exact Match
by up to 4.6 points over strong RAG baselines while trimming average retrieval
depth by 15 %. Ablation studies confirm the complementary roles of curriculum
staging and dynamic reward scheduling. EVO-RAG thus offers a general recipe for
building reliable, cost-effective multi-hop RAG systems.",2025-05-23,"Yuelyu Ji, Rui Meng, Zhuochun Li, Daqing He",http://arxiv.org/pdf/2505.17391v1,cs.CL
Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting,"Fine-grained personas have recently been used for generating 'diverse'
synthetic data for pre-training and supervised fine-tuning of Large Language
Models (LLMs). In this work, we measure the diversity of persona-driven
synthetically generated prompts and responses with a suite of lexical diversity
and redundancy metrics. Firstly, we find that synthetic prompts/instructions
are significantly less diverse than human-written ones. Next, we sample
responses from LLMs of different sizes with fine-grained and coarse persona
descriptions to investigate how much fine-grained detail in persona
descriptions contribute to generated text diversity. We find that while
persona-prompting does improve lexical diversity (especially with larger
models), fine-grained detail in personas doesn't increase diversity noticeably.",2025-05-23,"Gauri Kambhatla, Chantal Shaib, Venkata Govindarajan",http://arxiv.org/pdf/2505.17390v1,cs.CL
WiNGPT-3.0 Technical Report,"Current Large Language Models (LLMs) exhibit significant limitations, notably
in structured, interpretable, and verifiable medical reasoning, alongside
practical deployment challenges related to computational resources and data
privacy. This report focused on the development of WiNGPT-3.0, the 32-billion
parameter LLMs, engineered with the objective of enhancing its capacity for
medical reasoning and exploring its potential for effective integration within
healthcare IT infrastructures. The broader aim is to advance towards clinically
applicable models. The approach involved a multi-stage training pipeline
tailored for general, medical, and clinical reasoning. This pipeline
incorporated supervised fine-tuning (SFT) and reinforcement learning (RL),
leveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward
models, and an evidence-based diagnostic chain simulation. WiNGPT-3.0
demonstrated strong performance: specific model variants achieved scores of
66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training
improved performance on a clinical reasoning task from a baseline score of 58.1
to 62.5. These findings suggest that reinforcement learning, even when applied
with a limited dataset of only a few thousand examples, can enhance medical
reasoning accuracy. Crucially, this demonstration of RL's efficacy with limited
data and computation paves the way for more trustworthy and practically
deployable LLMs within clinical workflows and health information
infrastructures.",2025-05-23,"Boqin Zhuang, Chenxiao Song, Huitong Lu, Jiacheng Qiao, Mingqian Liu, Mingxing Yu, Ping Hong, Rui Li, Xiaoxia Song, Xiangjun Xu, Xu Chen, Yaoyao Ma, Yujie Gao",http://arxiv.org/pdf/2505.17387v1,cs.CL
AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing,"Large language models (LLMs) like GPT-4 show potential for scaling
motivational interviewing (MI) in addiction care, but require systematic
evaluation of therapeutic capabilities. We present a computational framework
assessing user-perceived quality (UPQ) through expected and unexpected MI
behaviors. Analyzing human therapist and GPT-4 MI sessions via human-AI
collaboration, we developed predictive models integrating deep learning and
explainable AI to identify 17 MI-consistent (MICO) and MI-inconsistent (MIIN)
behavioral metrics. A customized chain-of-thought prompt improved GPT-4's MI
performance, reducing inappropriate advice while enhancing reflections and
empathy. Although GPT-4 remained marginally inferior to therapists overall, it
demonstrated superior advice management capabilities. The model achieved
measurable quality improvements through prompt engineering, yet showed
limitations in addressing complex emotional nuances. This framework establishes
a pathway for optimizing LLM-based therapeutic tools through targeted
behavioral metric analysis and human-AI co-evaluation. Findings highlight both
the scalability potential and current constraints of LLMs in clinical
communication applications.",2025-05-23,"Yinghui Huang, Yuxuan Jiang, Hui Liu, Yixin Cai, Weiqing Li, Xiangen Hu",http://arxiv.org/pdf/2505.17380v1,cs.CL
Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts,"The field of Multimodal Large Language Models (MLLMs) has made remarkable
progress in visual understanding tasks, presenting a vast opportunity to
predict the perceptual and emotional impact of charts. However, it also raises
concerns, as many applications of LLMs are based on overgeneralized assumptions
from a few examples, lacking sufficient validation of their performance and
effectiveness. We introduce Chart-to-Experience, a benchmark dataset comprising
36 charts, evaluated by crowdsourced workers for their impact on seven
experiential factors. Using the dataset as ground truth, we evaluated
capabilities of state-of-the-art MLLMs on two tasks: direct prediction and
pairwise comparison of charts. Our findings imply that MLLMs are not as
sensitive as human evaluators when assessing individual charts, but are
accurate and reliable in pairwise comparisons.",2025-05-23,"Seon Gyeom Kim, Jae Young Choi, Ryan Rossi, Eunyee Koh, Tak Yeon Lee",http://arxiv.org/pdf/2505.17374v1,cs.CL
Value-Guided Search for Efficient Chain-of-Thought Reasoning,"In this paper, we propose a simple and efficient method for value model
training on long-context reasoning traces. Compared to existing process reward
models (PRMs), our method does not require a fine-grained notion of ""step,""
which is difficult to define for long-context reasoning models. By collecting a
dataset of 2.5 million reasoning traces, we train a 1.5B token-level value
model and apply it to DeepSeek models for improved performance with test-time
compute scaling. We find that block-wise value-guided search (VGS) with a final
weighted majority vote achieves better test-time scaling than standard methods
such as majority voting or best-of-n. With an inference budget of 64
generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of
45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024
& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly
reduces the inference FLOPs required to achieve the same performance of
majority voting. Our dataset, model and codebase are open-sourced.",2025-05-23,"Kaiwen Wang, Jin Peng Zhou, Jonathan Chang, Zhaolin Gao, Nathan Kallus, Kianté Brantley, Wen Sun",http://arxiv.org/pdf/2505.17373v1,cs.CL
An End-to-End Approach for Child Reading Assessment in the Xhosa Language,"Child literacy is a strong predictor of life outcomes at the subsequent
stages of an individual's life. This points to a need for targeted
interventions in vulnerable low and middle income populations to help bridge
the gap between literacy levels in these regions and high income ones. In this
effort, reading assessments provide an important tool to measure the
effectiveness of these programs and AI can be a reliable and economical tool to
support educators with this task. Developing accurate automatic reading
assessment systems for child speech in low-resource languages poses significant
challenges due to limited data and the unique acoustic properties of children's
voices. This study focuses on Xhosa, a language spoken in South Africa, to
advance child speech recognition capabilities. We present a novel dataset
composed of child speech samples in Xhosa. The dataset is available upon
request and contains ten words and letters, which are part of the Early Grade
Reading Assessment (EGRA) system. Each recording is labeled with an online and
cost-effective approach by multiple markers and a subsample is validated by an
independent EGRA reviewer. This dataset is evaluated with three fine-tuned
state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The
results indicate that the performance of these models can be significantly
influenced by the amount and balancing of the available training data, which is
fundamental for cost-effective large dataset collection. Furthermore, our
experiments indicate that the wav2vec 2.0 performance is improved by training
on multiple classes at a time, even when the number of available samples is
constrained.",2025-05-23,"Sergio Chevtchenko, Nikhil Navas, Rafaella Vale, Franco Ubaudi, Sipumelele Lucwaba, Cally Ardington, Soheil Afshar, Mark Antoniou, Saeed Afshar",http://arxiv.org/pdf/2505.17371v1,cs.CL
A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit,"The conversational capabilities of Large Language Models (LLMs) suggest that
they may be able to perform as automated talk therapists. It is crucial to know
if these systems would be effective and adhere to known standards. We present a
counsellor chatbot that focuses on motivating tobacco smokers to quit smoking.
It uses a state-of-the-art LLM and a widely applied therapeutic approach called
Motivational Interviewing (MI), and was evolved in collaboration with
clinician-scientists with expertise in MI. We also describe and validate an
automated assessment of both the chatbot's adherence to MI and client
responses. The chatbot was tested on 106 participants, and their confidence
that they could succeed in quitting smoking was measured before the
conversation and one week later. Participants' confidence increased by an
average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed
adherence to MI standards in 98% of utterances, higher than human counsellors.
The chatbot scored well on a participant-reported metric of perceived empathy
but lower than typical human counsellors. Furthermore, participants' language
indicated a good level of motivation to change, a key goal in MI. These results
suggest that the automation of talk therapy with a modern LLM has promise.",2025-05-23,"Zafarullah Mahmood, Soliman Ali, Jiading Zhu, Mohamed Abdelwahab, Michelle Yu Collins, Sihan Chen, Yi Cheng Zhao, Jodi Wolff, Osnat Melamed, Nadia Minian, Marta Maslej, Carolynne Cooper, Matt Ratto, Peter Selby, Jonathan Rose",http://arxiv.org/pdf/2505.17362v2,cs.CL
DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic,"Theory-of-Mind (ToM) tasks pose a unique challenge for small language models
(SLMs) with limited scale, which often lack the capacity to perform deep social
reasoning. In this work, we propose DEL-ToM, a framework that improves ToM
reasoning through inference-time scaling rather than architectural changes. Our
approach decomposes ToM tasks into a sequence of belief updates grounded in
Dynamic Epistemic Logic (DEL), enabling structured and transparent reasoning.
We train a verifier, called the Process Belief Model (PBM), to score each
belief update step using labels generated automatically via a DEL simulator.
During inference, candidate belief traces generated by a language model are
evaluated by the PBM, and the highest-scoring trace is selected. This allows
SLMs to emulate more deliberate reasoning by allocating additional compute at
test time. Experiments across multiple model scales and benchmarks show that
DEL-ToM consistently improves performance, demonstrating that verifiable belief
supervision can significantly enhance ToM abilities of SLMs without retraining.",2025-05-22,"Yuheng Wu, Jianwen Xie, Denghui Zhang, Zhaozhuo Xu",http://arxiv.org/pdf/2505.17348v1,cs.CL
"Language models should be subject to repeatable, open, domain-contextualized hallucination benchmarking","Plausible, but inaccurate, tokens in model-generated text are widely believed
to be pervasive and problematic for the responsible adoption of language
models. Despite this concern, there is little scientific work that attempts to
measure the prevalence of language model hallucination in a comprehensive way.
In this paper, we argue that language models should be evaluated using
repeatable, open, and domain-contextualized hallucination benchmarking. We
present a taxonomy of hallucinations alongside a case study that demonstrates
that when experts are absent from the early stages of data creation, the
resulting hallucination metrics lack validity and practical utility.",2025-05-22,"Justin D. Norman, Michael U. Rivera, D. Alex Hughes",http://arxiv.org/pdf/2505.17345v1,cs.CL
SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use,"Enterprise customers are increasingly adopting Large Language Models (LLMs)
for critical communication tasks, such as drafting emails, crafting sales
pitches, and composing casual messages. Deploying such models across different
regions requires them to understand diverse cultural and linguistic contexts
and generate safe and respectful responses. For enterprise applications, it is
crucial to mitigate reputational risks, maintain trust, and ensure compliance
by effectively identifying and handling unsafe or offensive language. To
address this, we introduce SweEval, a benchmark simulating real-world scenarios
with variations in tone (positive or negative) and context (formal or
informal). The prompts explicitly instruct the model to include specific swear
words while completing the task. This benchmark evaluates whether LLMs comply
with or resist such inappropriate instructions and assesses their alignment
with ethical frameworks, cultural nuances, and language comprehension
capabilities. In order to advance research in building ethically aligned AI
systems for enterprise use and beyond, we release the dataset and code:
https://github.com/amitbcp/multilingual_profanity.",2025-05-22,"Hitesh Laxmichand Patel, Amit Agarwal, Arion Das, Bhargava Kumar, Srikant Panda, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Dong-Kyu Chae",http://arxiv.org/pdf/2505.17332v1,cs.CL
ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training,"This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to
improve both the training speed and inference throughput of LLaMA architectures
while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models
into shared KV caching across certain layers, significantly reducing KV
computational complexity while maintaining or improving language performance.
Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher
token-per-second throughput during training, up to 16\% higher Model FLOPs
Utilization (MFU), and up to 14\% lower loss when trained on an equal number of
tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\%
higher test-time throughput compared to the baseline. By introducing a
computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable
and cost-effective solution for pretraining and finetuning large language
models, enabling faster and more resource-efficient training without
compromising performance.",2025-05-22,"Maryam Dialameh, Rezaul Karim, Hossein Rajabzadeh, Omar Mohamed Awad, Hyock Ju Kwon, Boxing Chen, Walid Ahmed, Yang Liu",http://arxiv.org/pdf/2505.17331v1,cs.CL
FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding,"In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable
and efficient model architecture for visually rich document understanding
(VRDU) in few-shot settings. FS-DAG leverages domain-specific and
language/vision specific backbones within a modular framework to adapt to
diverse document types with minimal data. The model is robust to practical
challenges such as handling OCR errors, misspellings, and domain shifts, which
are critical in real-world deployments. FS-DAG is highly performant with less
than 90M parameters, making it well-suited for complex real-world applications
for Information Extraction (IE) tasks where computational resources are
limited. We demonstrate FS-DAG's capability through extensive experiments for
information extraction task, showing significant improvements in convergence
speed and performance compared to state-of-the-art methods. Additionally, this
work highlights the ongoing progress in developing smaller, more efficient
models that do not compromise on performance. Code :
https://github.com/oracle-samples/fs-dag",2025-05-22,"Amit Agarwal, Srikant Panda, Kulbhushan Pachauri",http://arxiv.org/pdf/2505.17330v1,cs.CL
"GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints","The proliferation of Large Language Models (LLMs) in late 2022 has impacted
academic writing, threatening credibility, and causing institutional
uncertainty. We seek to determine the degree to which LLMs are used to generate
critical text as opposed to being used for editing, such as checking for
grammar errors or inappropriate phrasing. In our study, we analyze arXiv papers
for stylistic segmentation, which we measure by varying a PELT threshold
against a Bayesian classifier trained on GPT-regenerated text. We find that
LLM-attributed language is not predictive of stylistic segmentation, suggesting
that when authors use LLMs, they do so uniformly, reducing the risk of
hallucinations being introduced into academic preprints.",2025-05-22,"Soren DeHaan, Yuanze Liu, Johan Bollen, Sa'ul A. Blanco",http://arxiv.org/pdf/2505.17327v1,cs.CL
From Compression to Expansion: A Layerwise Analysis of In-Context Learning,"In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks without weight updates by learning from demonstration sequences.
While ICL shows strong empirical performance, its internal representational
mechanisms are not yet well understood. In this work, we conduct a statistical
geometric analysis of ICL representations to investigate how task-specific
information is captured across layers. Our analysis reveals an intriguing
phenomenon, which we term *Layerwise Compression-Expansion*: early layers
progressively produce compact and discriminative representations that encode
task information from the input demonstrations, while later layers expand these
representations to incorporate the query and generate the prediction. This
phenomenon is observed consistently across diverse tasks and a range of
contemporary LLM architectures. We demonstrate that it has important
implications for ICL performance -- improving with model size and the number of
demonstrations -- and for robustness in the presence of noisy examples. To
further understand the effect of the compact task representation, we propose a
bias-variance decomposition and provide a theoretical analysis showing how
attention mechanisms contribute to reducing both variance and bias, thereby
enhancing performance as the number of demonstrations increases. Our findings
reveal an intriguing layerwise dynamic in ICL, highlight how structured
representations emerge within LLMs, and showcase that analyzing internal
representations can facilitate a deeper understanding of model behavior.",2025-05-22,"Jiachen Jiang, Yuxin Dong, Jinxin Zhou, Zhihui Zhu",http://arxiv.org/pdf/2505.17322v1,cs.CL
Benchmarking Expressive Japanese Character Text-to-Speech with VITS and Style-BERT-VITS2,"Synthesizing expressive Japanese character speech poses unique challenges due
to pitch-accent sensitivity and stylistic variability. This paper benchmarks
two open-source text-to-speech models--VITS and Style-BERT-VITS2 JP Extra
(SBV2JE)--on in-domain, character-driven Japanese speech. Using three
character-specific datasets, we evaluate models across naturalness (mean
opinion and comparative mean opinion score), intelligibility (word error rate),
and speaker consistency. SBV2JE matches human ground truth in naturalness (MOS
4.37 vs. 4.38), achieves lower WER, and shows slight preference in CMOS.
Enhanced by pitch-accent controls and a WavLM-based discriminator, SBV2JE
proves effective for applications like language learning and character dialogue
generation, despite higher computational demands.",2025-05-22,"Zackary Rackauckas, Julia Hirschberg",http://arxiv.org/pdf/2505.17320v1,cs.CL
Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models,"Achieving better alignment between vision embeddings and Large Language
Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs
(MLLMs), particularly for recent models that rely on powerful pretrained vision
encoders and LLMs. A common approach to connect the pretrained vision encoder
and LLM is through a projector applied after the vision encoder. However, the
projector is often trained to enable the LLM to generate captions, and hence
the mechanism by which LLMs understand each vision token remains unclear. In
this work, we first investigate the role of the projector in compressing vision
embeddings and aligning them with word embeddings. We show that the projector
significantly compresses visual information, removing redundant details while
preserving essential elements necessary for the LLM to understand visual
content. We then examine patch-level alignment -- the alignment between each
vision patch and its corresponding semantic words -- and propose a
*multi-semantic alignment hypothesis*. Our analysis indicates that the
projector trained by caption loss improves patch-level alignment but only to a
limited extent, resulting in weak and coarse alignment. To address this issue,
we propose *patch-aligned training* to efficiently enhance patch-level
alignment. Our experiments show that patch-aligned training (1) achieves
stronger compression capability and improved patch-level alignment, enabling
the MLLM to generate higher-quality captions, (2) improves the MLLM's
performance by 16% on referring expression grounding tasks, 4% on
question-answering tasks, and 3% on modern instruction-following benchmarks
when using the same supervised fine-tuning (SFT) setting. The proposed method
can be easily extended to other multimodal models.",2025-05-22,"Jiachen Jiang, Jinxin Zhou, Bo Peng, Xia Ning, Zhihui Zhu",http://arxiv.org/pdf/2505.17316v1,cs.CL
"Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning","Recent language models exhibit strong reasoning capabilities, yet the
influence of long-context capacity on reasoning remains underexplored. In this
work, we hypothesize that current limitations in reasoning stem, in part, from
insufficient long-context capacity, motivated by empirical observations such as
(1) higher context window length often leads to stronger reasoning performance,
and (2) failed reasoning cases resemble failed long-context cases. To test this
hypothesis, we examine whether enhancing a model's long-context ability before
Supervised Fine-Tuning (SFT) leads to improved reasoning performance.
Specifically, we compared models with identical architectures and fine-tuning
data but varying levels of long-context capacity. Our results reveal a
consistent trend: models with stronger long-context capacity achieve
significantly higher accuracy on reasoning benchmarks after SFT. Notably, these
gains persist even on tasks with short input lengths, indicating that
long-context training offers generalizable benefits for reasoning performance.
These findings suggest that long-context modeling is not just essential for
processing lengthy inputs, but also serves as a critical foundation for
reasoning. We advocate for treating long-context capacity as a first-class
objective in the design of future language models.",2025-05-22,"Wang Yang, Zirui Liu, Hongye Jin, Qingyu Yin, Vipin Chaudhary, Xiaotian Han",http://arxiv.org/pdf/2505.17315v1,cs.CL
Towards medical AI misalignment: a preliminary study,"Despite their staggering capabilities as assistant tools, often exceeding
human performances, Large Language Models (LLMs) are still prone to jailbreak
attempts from malevolent users. Although red teaming practices have already
identified and helped to address several such jailbreak techniques, one
particular sturdy approach involving role-playing (which we named `Goofy Game')
seems effective against most of the current LLMs safeguards. This can result in
the provision of unsafe content, which, although not harmful per se, might lead
to dangerous consequences if delivered in a setting such as the medical domain.
In this preliminary and exploratory study, we provide an initial analysis of
how, even without technical knowledge of the internal architecture and
parameters of generative AI models, a malicious user could construct a
role-playing prompt capable of coercing an LLM into producing incorrect (and
potentially harmful) clinical suggestions. We aim to illustrate a specific
vulnerability scenario, providing insights that can support future advancements
in the field.",2025-05-22,"Barbara Puccio, Federico Castagna, Allan Tucker, Pierangelo Veltri",http://arxiv.org/pdf/2505.18212v1,cs.CL
Refusal Direction is Universal Across Safety-Aligned Languages,"Refusal mechanisms in large language models (LLMs) are essential for ensuring
safety. Recent research has revealed that refusal behavior can be mediated by a
single direction in activation space, enabling targeted interventions to bypass
refusals. While this is primarily demonstrated in an English-centric context,
appropriate refusal behavior is important for any language, but poorly
understood. In this paper, we investigate the refusal behavior in LLMs across
14 languages using PolyRefuse, a multilingual safety dataset created by
translating malicious and benign English prompts into these languages. We
uncover the surprising cross-lingual universality of the refusal direction: a
vector extracted from English can bypass refusals in other languages with
near-perfect effectiveness, without any additional fine-tuning. Even more
remarkably, refusal directions derived from any safety-aligned language
transfer seamlessly to others. We attribute this transferability to the
parallelism of refusal vectors across languages in the embedding space and
identify the underlying mechanism behind cross-lingual jailbreaks. These
findings provide actionable insights for building more robust multilingual
safety defenses and pave the way for a deeper mechanistic understanding of
cross-lingual vulnerabilities in LLMs.",2025-05-22,"Xinpeng Wang, Mingyang Wang, Yihong Liu, Hinrich Schütze, Barbara Plank",http://arxiv.org/pdf/2505.17306v1,cs.CL
SELF: Self-Extend the Context Length With Logistic Growth Function,"Large language models suffer issues when operated on long contexts that are
larger than their training context length due to the standard position encoding
for tokens in the attention layer. Tokens a long distance apart will rarely
have an effect on each other and long prompts yield unexpected results. To
solve this problem, we propose SELF (Self-Extend the Context Length With
Logistic Growth Function): a solution of grouping consecutive tokens at varying
group sizes using a logistic capacity equation combined with a constant group
size at smaller relative distances. Our model had an increase in performance of
up to 12% compared to the LongLM extension method in LEval (specifically on the
Qwen model). On summarization related tasks in LongBench, our model performed
up to 6.4% better than LongLM (specifically on the Llama-2-7b model). On
reading comprehension tasks from LEval, our model performed up to 5.4% better
than the LongLM. Our code is available at https://github.com/alexeipc/SELF-LLM.",2025-05-22,"Phat Thanh Dang, Saahil Thoppay, Wang Yang, Qifan Wang, Vipin Chaudhary, Xiaotian Han",http://arxiv.org/pdf/2505.17296v1,cs.CL
Attention with Trained Embeddings Provably Selects Important Tokens,"Token embeddings play a crucial role in language modeling but, despite this
practical relevance, their theoretical understanding remains limited. Our paper
addresses the gap by characterizing the structure of embeddings obtained via
gradient descent. Specifically, we consider a one-layer softmax attention model
with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top
E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top
v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots,
E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the
embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output
vector. First, we show that, already after a single step of gradient training
with the logistic loss, the embeddings $E_X$ capture the importance of tokens
in the dataset by aligning with the output vector $v$ proportionally to the
frequency with which the corresponding tokens appear in the dataset. Then,
after training $p$ via gradient flow until convergence, the softmax selects the
important tokens in the sentence (i.e., those that are predictive of the
label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes
the margin for such a selection. Experiments on real-world datasets (IMDB,
Yelp) exhibit a phenomenology close to that unveiled by our theory.",2025-05-22,"Diyuan Wu, Aleksandr Shevchenko, Samet Oymak, Marco Mondelli",http://arxiv.org/pdf/2505.17282v1,cs.CL
Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty,"Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language
Models (LLMs) by enabling dynamic, multi-step reasoning and information
retrieval. However, these systems often exhibit sub-optimal search behaviors
like over-search (retrieving redundant information) and under-search (failing
to retrieve necessary information), which hinder efficiency and reliability.
This work formally defines and quantifies these behaviors, revealing their
prevalence across multiple QA datasets and agentic RAG systems (e.g., one model
could have avoided searching in 27.7% of its search steps). Furthermore, we
demonstrate a crucial link between these inefficiencies and the models'
uncertainty regarding their own knowledge boundaries, where response accuracy
correlates with model's uncertainty in its search decisions. To address this,
we propose $\beta$-GRPO, a reinforcement learning-based training method that
incorporates confidence threshold to reward high-certainty search decisions.
Experiments on seven QA benchmarks show that $\beta$-GRPO enable a 3B model
with better agentic RAG ability, outperforming other strong baselines with a 4%
higher average exact match score.",2025-05-22,"Peilin Wu, Mian Zhang, Xinlu Zhang, Xinya Du, Zhiyu Zoey Chen",http://arxiv.org/pdf/2505.17281v1,cs.CL
Zebra-Llama: Towards Extremely Efficient Hybrid Models,"With the growing demand for deploying large language models (LLMs) across
diverse applications, improving their inference efficiency is crucial for
sustainable and democratized access. However, retraining LLMs to meet new
user-specific requirements is prohibitively expensive and environmentally
unsustainable. In this work, we propose a practical and scalable alternative:
composing efficient hybrid language models from existing pre-trained models.
Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models
by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)
layers, using a refined initialization and post-training pipeline to
efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama
achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B
training tokens (compared to trillions of tokens required for pre-training) and
an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down
to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,
respectively-while preserving 100%, 100%, and >97% of average zero-shot
performance on LM Harness tasks. Compared to models like MambaInLLaMA,
X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive
or superior accuracy while using significantly fewer tokens, smaller teachers,
and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses
Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,
over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves
2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context
length. We will release code and model checkpoints upon acceptance.",2025-05-22,"Mingyu Yang, Mehdi Rezagholizadeh, Guihong Li, Vikram Appia, Emad Barsoum",http://arxiv.org/pdf/2505.17272v1,cs.CL
GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations,"We introduce GreekBarBench, a benchmark that evaluates LLMs on legal
questions across five different legal areas from the Greek Bar exams, requiring
citations to statutory articles and case facts. To tackle the challenges of
free-text evaluation, we propose a three-dimensional scoring system combined
with an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to
assess the correlation between LLM-judges and human expert evaluations,
revealing that simple, span-based rubrics improve their alignment. Our
systematic evaluation of 13 proprietary and open-weight LLMs shows that even
though the best models outperform average expert scores, they fall short of the
95th percentile of experts.",2025-05-22,"Odysseas S. Chlapanis, Dimitrios Galanis, Nikolaos Aletras, Ion Androutsopoulos",http://arxiv.org/pdf/2505.17267v1,cs.CL
Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning,"A practical approach to activate long chain-of-thoughts reasoning ability in
pre-trained large language models is to perform supervised fine-tuning on
instruction datasets synthesized by strong Large Reasoning Models such as
DeepSeek-R1, offering a cost-effective alternative to reinforcement learning.
However, large-scale instruction sets with more than 100k samples incur
significant training overhead, while effective strategies for automatic
long-CoT instruction selection still remain unexplored. In this work, we
propose Select2Reason, a novel and efficient instruction-tuning data selection
framework for long-CoT reasoning. From the perspective of emergence of
rethinking behaviors like self-correction and backtracking, we investigate
common metrics that may determine the quality of long-CoT reasoning
instructions. Select2Reason leverages a quantifier to estimate difficulty of
question and jointly incorporates a reasoning trace length-based heuristic
through a weighted scheme for ranking to prioritize high-utility examples.
Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only
10% of the data selected by Select2Reason achieves performance competitive with
or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across
three competition-level and six comprehensive mathematical benchmarks. Further
experiments highlight the scalability in varying data size, efficiency during
inference, and its adaptability to other instruction pools with minimal cost.",2025-05-22,"Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Xiaojun Wu, Honghao Liu, Hui Xiong, Jian Guo",http://arxiv.org/pdf/2505.17266v1,cs.CL
CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports,"Rare diseases, including Inborn Errors of Metabolism (IEM), pose significant
diagnostic challenges. Case reports serve as key but computationally
underutilized resources to inform diagnosis. Clinical dense information
extraction refers to organizing medical information into structured predefined
categories. Large Language Models (LLMs) may enable scalable information
extraction from case reports but are rarely evaluated for this task. We
introduce CaseReportBench, an expert-annotated dataset for dense information
extraction of case reports, focusing on IEMs. Using this dataset, we assess
various models and prompting strategies, introducing novel approaches such as
category-specific prompting and subheading-filtered data integration. Zero-shot
chain-of-thought prompting offers little advantage over standard zero-shot
prompting. Category-specific prompting improves alignment with the benchmark.
The open-source model Qwen2.5-7B outperforms GPT-4o for this task. Our
clinician evaluations show that LLMs can extract clinically relevant details
from case reports, supporting rare disease diagnosis and management. We also
highlight areas for improvement, such as LLMs' limitations in recognizing
negative findings important for differential diagnosis. This work advances
LLM-driven clinical natural language processing and paves the way for scalable
medical AI applications.",2025-05-22,"Xiao Yu Cindy Zhang, Carlos R. Ferreira, Francis Rossignol, Raymond T. Ng, Wyeth Wasserman, Jian Zhu",http://arxiv.org/pdf/2505.17265v1,cs.CL
The Rise of Parameter Specialization for Knowledge Storage in Large Language Models,"Over time, a growing wave of large language models from various series has
been introduced to the community. Researchers are striving to maximize the
performance of language models with constrained parameter sizes. However, from
a microscopic perspective, there has been limited research on how to better
store knowledge in model parameters, particularly within MLPs, to enable more
effective utilization of this knowledge by the model. In this work, we analyze
twenty publicly available open-source large language models to investigate the
relationship between their strong performance and the way knowledge is stored
in their corresponding MLP parameters. Our findings reveal that as language
models become more advanced and demonstrate stronger knowledge capabilities,
their parameters exhibit increased specialization. Specifically, parameters in
the MLPs tend to be more focused on encoding similar types of knowledge. We
experimentally validate that this specialized distribution of knowledge
contributes to improving the efficiency of knowledge utilization in these
models. Furthermore, by conducting causal training experiments, we confirm that
this specialized knowledge distribution plays a critical role in improving the
model's efficiency in leveraging stored knowledge.",2025-05-22,"Yihuai Hong, Yiran Zhao, Wei Tang, Yang Deng, Yu Rong, Wenxuan Zhang",http://arxiv.org/pdf/2505.17260v1,cs.CL
ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models,"Large language models excel at complex tasks by breaking down problems into
structured reasoning steps. However, reasoning traces often extend beyond
reaching a correct answer, causing wasted computation, reduced readability, and
hallucinations. To address this, we introduce a novel hyperparameter-free
conciseness score used as a reward signal within a reinforcement learning
framework to guide models toward generating correct and concise reasoning
traces. This score is evaluated by a large language model acting as a judge,
enabling dynamic, context-aware feedback beyond simple token length. Our method
achieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset,
reducing token usage by up to 31x on simple problems while improving accuracy
by 7%, and on the hardest problems, it outperforms full reasoning by +7.5%
accuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves
accuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on
the judge model, reward composition, and problem difficulty, showing that our
method dynamically adapts reasoning length based on problem difficulty and
benefits significantly from stronger judges. The code, model weights, and
datasets are open-sourced at https://github.com/RazvanDu/ConciseRL.",2025-05-22,"Razvan-Gabriel Dumitru, Darius Peteleaza, Vikas Yadav, Liangming Pan",http://arxiv.org/pdf/2505.17250v1,cs.CL
ReasoningShield: Content Safety Detection over Reasoning Traces of Large Reasoning Models,"Large Reasoning Models (LRMs) are transforming the AI landscape with advanced
reasoning capabilities. While the generated reasoning traces enhance model
transparency, they can still contain unsafe content, even when the final answer
appears safe. Existing moderation tools, primarily designed for question-answer
(QA) pairs, are empirically ineffective at detecting hidden risks embedded in
reasoning traces. After identifying the key challenges, we formally define the
question-thought (QT) moderation task and propose ReasoningShield, the first
safety detection model tailored to identify potential risks in the reasoning
trace before reaching the final answer. To construct the model, we synthesize a
high-quality reasoning safety detection dataset comprising over 8,000
question-thought pairs spanning ten risk categories and three safety levels.
Our dataset construction process incorporates a comprehensive human-AI
collaborative annotation pipeline, which achieves over 93% annotation accuracy
while significantly reducing human costs. On a diverse set of in-distribution
and out-of-distribution benchmarks, ReasoningShield outperforms mainstream
content safety moderation models in identifying risks within reasoning traces,
with an average F1 score exceeding 0.92. Notably, despite being trained on our
QT dataset only, ReasoningShield also demonstrates competitive performance in
detecting unsafe question-answer pairs on traditional benchmarks, rivaling
baselines trained on 10 times larger datasets and base models, which strongly
validates the quality of our dataset. Furthermore, ReasoningShield is built
upon compact 1B/3B base models to facilitate lightweight deployment and
provides human-friendly risk analysis by default. To foster future research, we
publicly release all the resources.",2025-05-22,"Changyi Li, Jiayi Wang, Xudong Pan, Geng Hong, Min Yang",http://arxiv.org/pdf/2505.17244v1,cs.CL
Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG),"Collaborative dialogue offers rich insights into students' learning and
critical thinking. This is essential for adapting pedagogical agents to
students' learning and problem-solving skills in STEM+C settings. While large
language models (LLMs) facilitate dynamic pedagogical interactions, potential
hallucinations can undermine confidence, trust, and instructional value.
Retrieval-augmented generation (RAG) grounds LLM outputs in curated knowledge,
but its effectiveness depends on clear semantic links between user input and a
knowledge base, which are often weak in student dialogue. We propose
log-contextualized RAG (LC-RAG), which enhances RAG retrieval by incorporating
environment logs to contextualize collaborative discourse. Our findings show
that LC-RAG improves retrieval over a discourse-only baseline and allows our
collaborative peer agent, Copa, to deliver relevant, personalized guidance that
supports students' critical thinking and epistemic decision-making in a
collaborative computational modeling environment, XYZ.",2025-05-22,"Clayton Cohn, Surya Rayala, Caitlin Snyder, Joyce Fonteles, Shruti Jain, Naveeduddin Mohammed, Umesh Timalsina, Sarah K. Burriss, Ashwin T S, Namrata Srivastava, Menton Deweese, Angela Eeds, Gautam Biswas",http://arxiv.org/pdf/2505.17238v1,cs.CL
CHAOS: Chart Analysis with Outlier Samples,"Charts play a critical role in data analysis and visualization, yet
real-world applications often present charts with challenging or noisy
features. However, ""outlier charts"" pose a substantial challenge even for
Multimodal Large Language Models (MLLMs), which can struggle to interpret
perturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier
Samples), a robustness benchmark to systematically evaluate MLLMs against chart
perturbations. CHAOS encompasses five types of textual and ten types of visual
perturbations, each presented at three levels of severity (easy, mid, hard)
inspired by the study result of human evaluation. The benchmark includes 13
state-of-the-art MLLMs divided into three groups (i.e., general-, document-,
and chart-specific models) according to the training scope and data.
Comprehensive analysis involves two downstream tasks (ChartQA and
Chart-to-Text). Extensive experiments and case studies highlight critical
insights into robustness of models across chart perturbations, aiming to guide
future research in chart understanding domain. Data and code are publicly
available at: http://huggingface.co/datasets/omoured/CHAOS.",2025-05-22,"Omar Moured, Yufan Chen, Ruiping Liu, Simon Reiß, Philip Torr, Jiaming Zhang, Rainer Stiefelhagen",http://arxiv.org/pdf/2505.17235v1,cs.CL
ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects,"Recent text-to-SQL models have achieved strong performance, but their
effectiveness remains largely confined to SQLite due to dataset limitations.
However, real-world applications require SQL generation across multiple
dialects with varying syntax and specialized features, which remains a
challenge for current models. The main obstacle in building a dialect-aware
model lies in acquiring high-quality dialect-specific data. Data generated
purely through static prompting - without validating SQLs via execution - tends
to be noisy and unreliable. Moreover, the lack of real execution environments
in the training loop prevents models from grounding their predictions in
executable semantics, limiting generalization despite surface-level
improvements from data filtering. This work introduces ExeSQL, a text-to-SQL
framework with execution-driven, agentic bootstrapping. The method consists of
iterative query generation, execution-based filtering (e.g., rejection
sampling), and preference-based training, enabling the model to adapt to new
SQL dialects through verifiable, feedback-guided learning. Experiments show
that ExeSQL bridges the dialect gap in text-to-SQL, achieving average
improvements of 15.2%, 10.38%, and 4.49% over GPT-4o on PostgreSQL, MySQL, and
Oracle, respectively, across multiple datasets of varying difficulty.",2025-05-22,"Jipeng Zhang, Haolin Yang, Kehao Miao, Ruiyuan Zhang, Renjie Pi, Jiahui Gao, Xiaofang Zhou",http://arxiv.org/pdf/2505.17231v1,cs.CL
Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts,"Modeling complex subjective tasks in Natural Language Processing, such as
recognizing emotion and morality, is considerably challenging due to
significant variation in human annotations. This variation often reflects
reasonable differences in semantic interpretations rather than mere noise,
necessitating methods to distinguish between legitimate subjectivity and error.
We address this challenge by exploring label verification in these contexts
using Large Language Models (LLMs). First, we propose a simple In-Context
Learning binary filtering baseline that estimates the reasonableness of a
document-label pair. We then introduce the Label-in-a-Haystack setting: the
query and its label(s) are included in the demonstrations shown to LLMs, which
are prompted to predict the label(s) again, while receiving task-specific
instructions (e.g., emotion recognition) rather than label copying. We show how
the failure to copy the label(s) to the output of the LLM are task-relevant and
informative. Building on this, we propose the Label-in-a-Haystack Rectification
(LiaHR) framework for subjective label correction: when the model outputs
diverge from the reference gold labels, we assign the generated labels to the
example instead of discarding it. This approach can be integrated into
annotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses,
human evaluations, and ecological validity studies verify the utility of LiaHR
for label correction. Code is available at https://github.com/gchochla/LiaHR.",2025-05-22,"Georgios Chochlakis, Peter Wu, Arjun Bedi, Marcus Ma, Kristina Lerman, Shrikanth Narayanan",http://arxiv.org/pdf/2505.17222v1,cs.CL
Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs,"Large Language Models (LLMs) often exhibit gender bias, resulting in unequal
treatment of male and female subjects across different contexts. To address
this issue, we propose a novel data generation framework that fosters
exploratory thinking in LLMs. Our approach prompts models to generate story
pairs featuring male and female protagonists in structurally identical, morally
ambiguous scenarios, then elicits and compares their moral judgments. When
inconsistencies arise, the model is guided to produce balanced, gender-neutral
judgments. These story-judgment pairs are used to fine-tune or optimize the
models via Direct Preference Optimization (DPO). Experimental results show that
our method significantly reduces gender bias while preserving or even enhancing
general model capabilities. We will release the code and generated data.",2025-05-22,"Kangda Wei, Hasnat Md Abdullah, Ruihong Huang",http://arxiv.org/pdf/2505.17217v1,cs.CL
FB-RAG: Improving RAG with Forward and Backward Lookup,"The performance of Retrieval Augmented Generation (RAG) systems relies
heavily on the retriever quality and the size of the retrieved context. A large
enough context ensures that the relevant information is present in the input
context for the LLM, but also incorporates irrelevant content that has been
shown to confuse the models. On the other hand, a smaller context reduces the
irrelevant information, but it often comes at the risk of losing important
information necessary to answer the input question. This duality is especially
challenging to manage for complex queries that contain little information to
retrieve the relevant chunks from the full context. To address this, we present
a novel framework, called FB-RAG, which enhances the RAG pipeline by relying on
a combination of backward lookup (overlap with the query) and forward lookup
(overlap with candidate reasons and answers) to retrieve specific context
chunks that are the most relevant for answering the input query. Our
evaluations on 9 datasets from two leading benchmarks show that FB-RAG
consistently outperforms RAG and Long Context baselines developed recently for
these benchmarks. We further show that FB-RAG can improve performance while
reducing latency. We perform qualitative analysis of the strengths and
shortcomings of our approach, providing specific insights to guide future work.",2025-05-22,"Kushal Chawla, Alfy Samuel, Anoop Kumar, Daben Liu",http://arxiv.org/pdf/2505.17206v1,cs.CL
CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models,"Data visualizations are powerful tools for communicating patterns in
quantitative data. Yet understanding any data visualization is no small feat --
succeeding requires jointly making sense of visual, numerical, and linguistic
inputs arranged in a conventionalized format one has previously learned to
parse. Recently developed vision-language models are, in principle, promising
candidates for developing computational models of these cognitive operations.
However, it is currently unclear to what degree these models emulate human
behavior on tasks that involve reasoning about data visualizations. This gap
reflects limitations in prior work that has evaluated data visualization
understanding in artificial systems using measures that differ from those
typically used to assess these abilities in humans. Here we evaluated eight
vision-language models on six data visualization literacy assessments designed
for humans and compared model responses to those of human participants. We
found that these models performed worse than human participants on average, and
this performance gap persisted even when using relatively lenient criteria to
assess model performance. Moreover, while relative performance across items was
somewhat correlated between models and humans, all models produced patterns of
errors that were reliably distinct from those produced by human participants.
Taken together, these findings suggest significant opportunities for further
development of artificial systems that might serve as useful models of how
humans reason about data visualizations. All code and data needed to reproduce
these results are available at:
https://osf.io/e25mu/?view_only=399daff5a14d4b16b09473cf19043f18.",2025-05-22,"Arnav Verma, Kushin Mukherjee, Christopher Potts, Elisa Kreiss, Judith E. Fan",http://arxiv.org/pdf/2505.17202v1,cs.CL
GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning,"Visual generation models have made remarkable progress in creating realistic
images from text prompts, yet struggle with complex prompts that specify
multiple objects with precise spatial relationships and attributes. Effective
handling of such prompts requires explicit reasoning about the semantic content
and spatial layout. We present GoT-R1, a framework that applies reinforcement
learning to enhance semantic-spatial reasoning in visual generation. Building
upon the Generation Chain-of-Thought approach, GoT-R1 enables models to
autonomously discover effective reasoning strategies beyond predefined
templates through carefully designed reinforcement learning. To achieve this,
we propose a dual-stage multi-dimensional reward framework that leverages MLLMs
to evaluate both the reasoning process and final output, enabling effective
supervision across the entire generation pipeline. The reward system assesses
semantic alignment, spatial accuracy, and visual quality in a unified approach.
Experimental results demonstrate significant improvements on T2I-CompBench
benchmark, particularly in compositional tasks involving precise spatial
relationships and attribute binding. GoT-R1 advances the state-of-the-art in
image generation by successfully transferring sophisticated reasoning
capabilities to the visual generation domain. To facilitate future research, we
make our code and pretrained models publicly available at
https://github.com/gogoduan/GoT-R1.",2025-05-22,"Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, Xihui Liu",http://arxiv.org/pdf/2505.17022v1,cs.CL
Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO,"Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT",2025-05-22,"Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, Pheng-Ann Heng",http://arxiv.org/pdf/2505.17017v1,cs.CL
Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models,"Multi-modal large language models (MLLMs) have rapidly advanced in visual
tasks, yet their spatial understanding remains limited to single images,
leaving them ill-suited for robotics and other real-world applications that
require multi-frame reasoning. In this paper, we propose a framework to equip
MLLMs with robust multi-frame spatial understanding by integrating depth
perception, visual correspondence, and dynamic perception. Central to our
approach is the MultiSPA dataset, a novel, large-scale collection of more than
27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we
introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks
under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves
significant gains over baselines and proprietary systems, demonstrating
scalable, generalizable multi-frame reasoning. We further observe multi-task
benefits and early indications of emergent capabilities in challenging
scenarios, and showcase how our model can serve as a multi-frame reward
annotator for robotics.",2025-05-22,"Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, Kevin J. Liang",http://arxiv.org/pdf/2505.17015v1,cs.CL
R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning,"Large Language Models (LLMs) are powerful but prone to hallucinations due to
static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting
external information, but current methods often are costly, generalize poorly,
or ignore the internal knowledge of the model. In this paper, we introduce
R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage
both internal and external knowledge sources. R1-Searcher++ employs a two-stage
training strategy: an initial SFT Cold-start phase for preliminary format
learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses
outcome-supervision to encourage exploration, incorporates a reward mechanism
for internal knowledge utilization, and integrates a memorization mechanism to
continuously assimilate retrieved information, thereby enriching the model's
internal knowledge. By leveraging internal knowledge and external search
engine, the model continuously improves its capabilities, enabling efficient
retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++
outperforms previous RAG and reasoning methods and achieves efficient
retrieval. The code is available at
https://github.com/RUCAIBox/R1-Searcher-plus.",2025-05-22,"Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen",http://arxiv.org/pdf/2505.17005v1,cs.CL
Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?,"Large Language Models (LLMs) have been shown to achieve breakthrough
performance on complex logical reasoning tasks. Nevertheless, most existing
research focuses on employing formal language to guide LLMs to derive reliable
reasoning paths, while systematic evaluations of these capabilities are still
limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs
across various logical reasoning problems utilizing formal languages. From the
perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and
format of trajectories, our key findings are: 1) Thinking models significantly
outperform Instruct models, especially when formal language is employed; 2) All
LLMs exhibit limitations in inductive reasoning capability, irrespective of
whether they use a formal language; 3) Data with PoT format achieves the best
generalization performance across other languages. Additionally, we also curate
the formal-relative training data to further enhance the small language models,
and the experimental results indicate that a simple rejected fine-tuning method
can better enable LLMs to generalize across formal languages and achieve the
best overall performance. Our codes and reports are available at
https://github.com/jiangjin1999/FormalEval.",2025-05-22,"Jin Jiang, Jianing Wang, Yuchen Yan, Yang Liu, Jianhua Zhu, Mengdi Zhang, Xunliang Cai, Liangcai Gao",http://arxiv.org/pdf/2505.16998v1,cs.CL
X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs,"LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by
enabling cooperation among multiple specialized agents. However, most existing
MAS frameworks rely on a single LLM to drive all agents, constraining the
system's intelligence to the limit of that model. This paper explores the
paradigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by
diverse LLMs, elevating the system's potential to the collective intelligence
of diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to
evaluate the performance of various LLMs across different domains and
MAS-related functions. As an extensive empirical study, we assess 27 LLMs
across 5 domains (encompassing 21 test sets) and 5 functions, conducting over
1.7 million evaluations to identify optimal model selections for each
domain-function combination. Building on these findings, we demonstrate that
transitioning from homogeneous to heterogeneous LLM-driven MAS can
significantly enhance system performance without requiring structural redesign.
Specifically, in a chatbot-only MAS scenario, the heterogeneous configuration
yields up to 8.4\% performance improvement on the MATH dataset. In a mixed
chatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable
47\% performance boost on the AIME dataset. Our results underscore the
transformative potential of heterogeneous LLMs in MAS, highlighting a promising
avenue for advancing scalable, collaborative AI systems.",2025-05-22,"Rui Ye, Xiangrui Liu, Qimin Wu, Xianghe Pang, Zhenfei Yin, Lei Bai, Siheng Chen",http://arxiv.org/pdf/2505.16997v1,cs.CL
DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization,"Recent advances in Emotional Support Conversation (ESC) have improved
emotional support generation by fine-tuning Large Language Models (LLMs) via
Supervised Fine-Tuning (SFT). However, common psychological errors still
persist. While Direct Preference Optimization (DPO) shows promise in reducing
such errors through pairwise preference learning, its effectiveness in ESC
tasks is limited by two key challenges: (1) Entangled data structure: Existing
ESC data inherently entangles psychological strategies and response content,
making it difficult to construct high-quality preference pairs; and (2)
Optimization ambiguity: Applying vanilla DPO to such entangled pairwise data
leads to ambiguous training objectives. To address these issues, we introduce
Inferential Preference Mining (IPM) to construct high-quality preference data,
forming the IPM-PrefDial dataset. Building upon this data, we propose a
Decoupled ESC framework inspired by Gross's Extended Process Model of Emotion
Regulation, which decomposes the ESC task into two sequential subtasks:
strategy planning and empathic response generation. Each was trained via SFT
and subsequently enhanced by DPO to align with the psychological preference.
Extensive experiments demonstrate that our Decoupled ESC framework outperforms
joint optimization baselines, reducing preference bias and improving response
quality.",2025-05-22,"Chao Zhang, Xin Shi, Xueqiao Zhang, Yifan Zhu, Yi Yang, Yawei Luo",http://arxiv.org/pdf/2505.16995v1,cs.CL
$\text{R}^2\text{ec}$: Towards Large Recommender Models with Reasoning,"Large recommender models have extended LLMs as powerful recommenders via
encoding or item generation, and recent breakthroughs in LLM reasoning
synchronously motivate the exploration of reasoning in recommendation. Current
studies usually position LLMs as external reasoning modules to yield auxiliary
thought for augmenting conventional recommendation pipelines. However, such
decoupled designs are limited in significant resource cost and suboptimal joint
optimization. To address these issues, we propose \name, a unified large
recommender model with intrinsic reasoning capabilities. Initially, we
reconceptualize the model architecture to facilitate interleaved reasoning and
recommendation in the autoregressive process. Subsequently, we propose RecPO, a
corresponding reinforcement learning framework that optimizes \name\ both the
reasoning and recommendation capabilities simultaneously in a single policy
update; RecPO introduces a fused reward scheme that solely leverages
recommendation labels to simulate the reasoning capability, eliminating
dependency on specialized reasoning annotations. Experiments on three datasets
with various baselines verify the effectiveness of \name, showing relative
improvements of 68.67\% in Hit@5 and 45.21\% in NDCG@20. Code available at
https://github.com/YRYangang/RRec.",2025-05-22,"Runyang You, Yongqi Li, Xinyu Lin, Xin Zhang, Wenjie Wang, Wenjie Li, Liqiang Nie",http://arxiv.org/pdf/2505.16994v1,cs.CL
MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems,"LLM-based multi-agent systems (MAS) have demonstrated significant potential
in enhancing single LLMs to address complex and diverse tasks in practical
applications. Despite considerable advancements, the field lacks a unified
codebase that consolidates existing methods, resulting in redundant
re-implementation efforts, unfair comparisons, and high entry barriers for
researchers. To address these challenges, we introduce MASLab, a unified,
comprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab
integrates over 20 established methods across multiple domains, each rigorously
validated by comparing step-by-step outputs with its official implementation.
(2) MASLab provides a unified environment with various benchmarks for fair
comparisons among methods, ensuring consistent inputs and standardized
evaluation protocols. (3) MASLab implements methods within a shared streamlined
structure, lowering the barriers for understanding and extension. Building on
MASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,
offering researchers a clear and comprehensive view of the current landscape of
MAS methods. MASLab will continue to evolve, tracking the latest developments
in the field, and invite contributions from the broader open-source community.",2025-05-22,"Rui Ye, Keduan Huang, Qimin Wu, Yuzhu Cai, Tian Jin, Xianghe Pang, Xiangrui Liu, Jiaqi Su, Chen Qian, Bohan Tang, Kaiqu Liang, Jiaao Chen, Yue Hu, Zhenfei Yin, Rongye Shi, Bo An, Yang Gao, Wenjun Wu, Lei Bai, Siheng Chen",http://arxiv.org/pdf/2505.16988v1,cs.CL
T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning,"Large Language Models (LLMs) have demonstrated impressive capabilities as
intelligent agents capable of solving complex problems. However, effective
planning in scenarios involving dependencies between API or tool
calls-particularly in multi-turn conversations-remains a significant challenge.
To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn
conversational dataset specifically designed to capture and manage inter-tool
dependencies across diverse domains. T1 enables rigorous evaluation of agents'
ability to coordinate tool use across nine distinct domains (4 single domain
and 5 multi-domain) with the help of an integrated caching mechanism for both
short- and long-term memory, while supporting dynamic replanning-such as
deciding whether to recompute or reuse cached results. Beyond facilitating
research on tool use and planning, T1 also serves as a benchmark for evaluating
the performance of open-source language models. We present results powered by
T1-Agent, highlighting their ability to plan and reason in complex,
tool-dependent scenarios.",2025-05-22,"Amartya Chakraborty, Paresh Dashore, Nadia Bathaee, Anmol Jain, Anirban Das, Shi-Xiong Zhang, Sambit Sahu, Milind Naphade, Genta Indra Winata",http://arxiv.org/pdf/2505.16986v1,cs.CL
UFT: Unifying Supervised and Reinforcement Fine-Tuning,"Post-training has demonstrated its importance in enhancing the reasoning
capabilities of large language models (LLMs). The primary post-training methods
can be categorized into supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT is efficient and well-suited for small language models,
but it may lead to overfitting and limit the reasoning abilities of larger
models. In contrast, RFT generally yields better generalization but depends
heavily on the strength of the base model. To address the limitations of SFT
and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm
that unifies SFT and RFT into a single, integrated process. UFT enables the
model to effectively explore solutions while incorporating informative
supervision signals, bridging the gap between memorizing and thinking
underlying existing methods. Notably, UFT outperforms both SFT and RFT in
general, regardless of model sizes. Furthermore, we theoretically prove that
UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for
the first time that unified training can exponentially accelerate convergence
on long-horizon reasoning tasks.",2025-05-22,"Mingyang Liu, Gabriele Farina, Asuman Ozdaglar",http://arxiv.org/pdf/2505.16984v1,cs.CL
LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding,"Large Language Models (LLMs) are primarily designed for batch processing.
Existing methods for adapting LLMs to streaming rely either on expensive
re-encoding or specialized architectures with limited scalability. This work
identifies three key mismatches in adapting batch-oriented LLMs to streaming:
(1) input-attention, (2) output-attention, and (3) position-ID mismatches.
While it is commonly assumed that the latter two mismatches require frequent
re-encoding, our analysis reveals that only the input-attention mismatch
significantly impacts performance, indicating re-encoding outputs is largely
unnecessary. To better understand this discrepancy with the common assumption,
we provide the first comprehensive analysis of the impact of position encoding
on LLMs in streaming, showing that preserving relative positions within source
and target contexts is more critical than maintaining absolute order. Motivated
by the above analysis, we introduce a group position encoding paradigm built on
batch architectures to enhance consistency between streaming and batch modes.
Extensive experiments on cross-lingual and cross-modal tasks demonstrate that
our method outperforms existing approaches. Our method requires no
architectural modifications, exhibits strong generalization in both streaming
and batch modes. The code is available at repository
https://github.com/EIT-NLP/StreamingLLM.",2025-05-22,"Junlong Tong, Jinlan Fu, Zixuan Lin, Yingqi Fan, Anhao Zhao, Hui Su, Xiaoyu Shen",http://arxiv.org/pdf/2505.16983v1,cs.CL
SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development,"Large Language Models (LLMs) have shown strong capability in diverse software
engineering tasks, e.g. code completion, bug fixing, and document generation.
However, feature-driven development (FDD), a highly prevalent real-world task
that involves developing new functionalities for large, existing codebases,
remains underexplored. We therefore introduce SWE-Dev, the first large-scale
dataset (with 14,000 training and 500 test samples) designed to evaluate and
train autonomous coding systems on real-world feature development tasks. To
ensure verifiable and diverse training, SWE-Dev uniquely provides all instances
with a runnable environment and its developer-authored executable unit tests.
This collection not only provides high-quality data for Supervised Fine-Tuning
(SFT), but also enables Reinforcement Learning (RL) by delivering accurate
reward signals from executable unit tests. Our extensive evaluations on
SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent
Systems (MAS), reveal that FDD is a profoundly challenging frontier for current
AI (e.g., Claude-3.7-Sonnet achieves only 22.45\% Pass@3 on the hard test
split). Crucially, we demonstrate that SWE-Dev serves as an effective platform
for model improvement: fine-tuning on training set enabled a 7B model
comparable to GPT-4o on \textit{hard} split, underscoring the value of its
high-quality training data. Code is available here
\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}.",2025-05-22,"Yaxin Du, Yuzhu Cai, Yifan Zhou, Cheng Wang, Yu Qian, Xianghe Pang, Qian Liu, Yue Hu, Siheng Chen",http://arxiv.org/pdf/2505.16975v1,cs.CL
VeriFastScore: Speeding up long-form factuality evaluation,"Metrics like FactScore and VeriScore that evaluate long-form factuality
operate by decomposing an input response into atomic claims and then
individually verifying each claim. While effective and interpretable, these
methods incur numerous LLM calls and can take upwards of 100 seconds to
evaluate a single response, limiting their practicality in large-scale
evaluation and training scenarios. To address this, we propose VeriFastScore,
which leverages synthetic data to fine-tune Llama3.1 8B for simultaneously
extracting and verifying all verifiable claims within a given text based on
evidence from Google Search. We show that this task cannot be solved via
few-shot prompting with closed LLMs due to its complexity: the model receives
~4K tokens of evidence on average and needs to concurrently decompose claims,
judge their verifiability, and verify them against noisy evidence. However, our
fine-tuned VeriFastScore model demonstrates strong correlation with the
original VeriScore pipeline at both the example level (r=0.80) and system level
(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence
retrieval) over VeriScore. To facilitate future factuality research, we
publicly release our VeriFastScore model and synthetic datasets.",2025-05-22,"Rishanth Rajendhran, Amir Zadeh, Matthew Sarte, Chuan Li, Mohit Iyyer",http://arxiv.org/pdf/2505.16973v2,cs.CL
From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition,"Recent advances in Automatic Speech Recognition (ASR) have been largely
fueled by massive speech corpora. However, extending coverage to diverse
languages with limited resources remains a formidable challenge. This paper
introduces Speech Back-Translation, a scalable pipeline that improves
multilingual ASR models by converting large-scale text corpora into synthetic
speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just
tens of hours of real transcribed speech can effectively train TTS models to
generate synthetic speech at hundreds of times the original volume while
maintaining high quality. To evaluate synthetic speech quality, we develop an
intelligibility-based assessment framework and establish clear thresholds for
when synthetic data benefits ASR training. Using Speech Back-Translation, we
generate more than 500,000 hours of synthetic speech in ten languages and
continue pre-training Whisper-large-v3, achieving average transcription error
reductions of over 30\%. These results highlight the scalability and
effectiveness of Speech Back-Translation for enhancing multilingual ASR
systems.",2025-05-22,"Tianduo Wang, Lu Xu, Wei Lu, Shanbo Cheng",http://arxiv.org/pdf/2505.16972v1,cs.CL
"CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark","We introduce CASS, the first large-scale dataset and model suite for
cross-architecture GPU code transpilation, targeting both source-level (CUDA
$\leftrightarrow$ HIP) and assembly-level (Nvidia SASS $\leftrightarrow$ AMD
RDNA3) translation. The dataset comprises 70k verified code pairs across host
and device, addressing a critical gap in low-level GPU code portability.
Leveraging this resource, we train the CASS family of domain-specific language
models, achieving 95% source translation accuracy and 37.5% assembly
translation accuracy, substantially outperforming commercial baselines such as
GPT-4o, Claude, and Hipify. Our generated code matches native performance in
over 85% of test cases, preserving runtime and memory behavior. To support
rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16
GPU domains with ground-truth execution. All data, models, and evaluation tools
are released as open source to foster progress in GPU compiler tooling, binary
compatibility, and LLM-guided hardware translation. Dataset and benchmark are
on
\href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}},
with code at
\href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.",2025-05-22,"Ahmed Heakl, Sarim Hashmi, Gustavo Bertolo Stahl, Seung Hun Eddie Han, Salman Khan, Abdulrahman Mahmoud",http://arxiv.org/pdf/2505.16968v2,cs.CL
Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval,"Training robust retrieval and reranker models typically relies on large-scale
retrieval datasets; for example, the BGE collection contains 1.6 million
query-passage pairs sourced from various data sources. However, we find that
certain datasets can negatively impact model effectiveness -- pruning 8 out of
15 datasets from the BGE collection reduces the training set size by
2.35$\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a
deeper examination of training data quality, with a particular focus on ""false
negatives"", where relevant passages are incorrectly labeled as irrelevant. We
propose a simple, cost-effective approach using cascading LLM prompts to
identify and relabel hard negatives. Experimental results show that relabeling
false negatives with true positives improves both E5 (base) and Qwen2.5-7B
retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot
AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on
the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the
cascading design is further supported by human annotation results, where we
find judgment by GPT-4o shows much higher agreement with humans than
GPT-4o-mini.",2025-05-22,"Nandan Thakur, Crystina Zhang, Xueguang Ma, Jimmy Lin",http://arxiv.org/pdf/2505.16967v1,cs.CL
BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation,"Text segmentation based on the semantic meaning of sentences is a fundamental
task with broad utility in many downstream applications. In this paper, we
propose a graphical model-based unsupervised learning approach, named BP-Seg
for efficient text segmentation. Our method not only considers local coherence,
capturing the intuition that adjacent sentences are often more related, but
also effectively groups sentences that are distant in the text yet semantically
similar. This is achieved through belief propagation on the carefully
constructed graphical models. Experimental results on both an illustrative
example and a dataset with long-form documents demonstrate that our method
performs favorably compared to competing approaches.",2025-05-22,"Fengyi Li, Kayhan Behdin, Natesh Pillai, Xiaofeng Wang, Zhipeng Wang, Ercan Yildiz",http://arxiv.org/pdf/2505.16965v1,cs.CL
MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning,"Existing medical VQA benchmarks mostly focus on single-image analysis, yet
clinicians almost always compare a series of images before reaching a
diagnosis. To better approximate this workflow, we introduce MedFrameQA -- the
first benchmark that explicitly evaluates multi-image reasoning in medical VQA.
To build MedFrameQA both at scale and in high-quality, we develop 1) an
automated pipeline that extracts temporally coherent frames from medical videos
and constructs VQA items whose content evolves logically across images, and 2)
a multiple-stage filtering strategy, including model-based and manual review,
to preserve data clarity, difficulty, and medical relevance. The resulting
dataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in
3,420 videos), covering nine human body systems and 43 organs; every question
is accompanied by two to five images. We comprehensively benchmark ten advanced
Multimodal LLMs -- both proprietary and open source, with and without explicit
reasoning modules -- on MedFrameQA. The evaluation challengingly reveals that
all models perform poorly, with most accuracies below 50%, and accuracy
fluctuates as the number of images per question increases. Error analysis
further shows that models frequently ignore salient findings, mis-aggregate
evidence across images, and propagate early mistakes through their reasoning
chains; results also vary substantially across body systems, organs, and
modalities. We hope this work can catalyze research on clinically grounded,
multi-image reasoning and accelerate progress toward more capable diagnostic AI
systems.",2025-05-22,"Suhao Yu, Haojin Wang, Juncheng Wu, Cihang Xie, Yuyin Zhou",http://arxiv.org/pdf/2505.16964v1,cs.CL
On Multilingual Encoder Language Model Compression for Low-Resource Languages,"In this paper, we combine two-step knowledge distillation, structured
pruning, truncation, and vocabulary trimming for extremely compressing
multilingual encoder-only language models for low-resource languages. Our novel
approach systematically combines existing techniques and takes them to the
extreme, reducing layer depth, feed-forward hidden size, and intermediate layer
embedding size to create significantly smaller monolingual models while
retaining essential language-specific knowledge. We achieve compression rates
of up to 92% with only a marginal performance drop of 2-10% in four downstream
tasks, including sentiment analysis, topic classification, named entity
recognition, and part-of-speech tagging, across three low-resource languages.
Notably, the performance degradation correlates with the amount of
language-specific data in the teacher model, with larger datasets resulting in
smaller performance losses. Additionally, we conduct extensive ablation studies
to identify best practices for multilingual model compression using these
techniques.",2025-05-22,"Daniil Gurgurov, Michal Gregor, Josef van Genabith, Simon Ostermann",http://arxiv.org/pdf/2505.16956v1,cs.CL
AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios,"Large Language Models (LLMs) have demonstrated advanced capabilities in
real-world agentic applications. Growing research efforts aim to develop
LLM-based agents to address practical demands, introducing a new challenge:
agentic scenarios often involve lengthy instructions with complex constraints,
such as extended system prompts and detailed tool specifications. While
adherence to such instructions is crucial for agentic applications, whether
LLMs can reliably follow them remains underexplored. In this paper, we
introduce AgentIF, the first benchmark for systematically evaluating LLM
instruction following ability in agentic scenarios. AgentIF features three key
characteristics: (1) Realistic, constructed from 50 real-world agentic
applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.
(3) Complex, averaging 11.9 constraints per instruction, covering diverse
constraint types, such as tool specifications and condition constraints. To
construct AgentIF, we collect 707 human-annotated instructions across 50
agentic tasks from industrial application agents and open-source agentic
systems. For each instruction, we annotate the associated constraints and
corresponding evaluation metrics, including code-based evaluation, LLM-based
evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically
evaluate existing advanced LLMs. We observe that current models generally
perform poorly, especially in handling complex constraint structures and tool
specifications. We further conduct error analysis and analytical experiments on
instruction length and meta constraints, providing some findings about the
failure modes of existing LLMs. We have released the code and data to
facilitate future research.",2025-05-22,"Yunjia Qi, Hao Peng, Xiaozhi Wang, Amy Xin, Youfeng Liu, Bin Xu, Lei Hou, Juanzi Li",http://arxiv.org/pdf/2505.16944v1,cs.CL
NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification,"Artificial Intelligence (AI) is accelerating the transformation of scientific
research paradigms, not only enhancing research efficiency but also driving
innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework
to conduct Autonomous Scientific Research (ASR) across various scientific
research fields, enabling researchers to tackle complicated problems in these
fields with unprecedented speed and precision. NovelSeek highlights three key
advantages: 1) Scalability: NovelSeek has demonstrated its versatility across
12 scientific research tasks, capable of generating innovative ideas to enhance
the performance of baseline code. 2) Interactivity: NovelSeek provides an
interface for human expert feedback and multi-agent interaction in automated
end-to-end processes, allowing for the seamless integration of domain expert
knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in
several scientific fields with significantly less time cost compared to human
efforts. For instance, in reaction yield prediction, it increased from 27.6% to
35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from
0.65 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,
precision advanced from 78.8% to 81.0% in a mere 30 hours.",2025-05-22,"NovelSeek Team, Bo Zhang, Shiyang Feng, Xiangchao Yan, Jiakang Yuan, Zhiyin Yu, Xiaohan He, Songtao Huang, Shaowei Hou, Zheng Nie, Zhilong Wang, Jinyao Liu, Runmin Ma, Tianshuo Peng, Peng Ye, Dongzhan Zhou, Shufei Zhang, Xiaosong Wang, Yilan Zhang, Meng Li, Zhongying Tu, Xiangyu Yue, Wangli Ouyang, Bowen Zhou, Lei Bai",http://arxiv.org/pdf/2505.16938v2,cs.CL
In-Context Watermarks for Large Language Models,"The growing use of large language models (LLMs) for sensitive applications
has highlighted the need for effective watermarking techniques to ensure the
provenance and accountability of AI-generated text. However, most existing
watermarking methods require access to the decoding process, limiting their
applicability in real-world settings. One illustrative example is the use of
LLMs by dishonest reviewers in the context of academic peer review, where
conference organizers have no access to the model used but still need to detect
AI-generated reviews. Motivated by this gap, we introduce In-Context
Watermarking (ICW), which embeds watermarks into generated text solely through
prompt engineering, leveraging LLMs' in-context learning and
instruction-following abilities. We investigate four ICW strategies at
different levels of granularity, each paired with a tailored detection method.
We further examine the Indirect Prompt Injection (IPI) setting as a specific
case study, in which watermarking is covertly triggered by modifying input
documents such as academic manuscripts. Our experiments validate the
feasibility of ICW as a model-agnostic, practical watermarking approach.
Moreover, our findings suggest that as LLMs become more capable, ICW offers a
promising direction for scalable and accessible content attribution.",2025-05-22,"Yepeng Liu, Xuandong Zhao, Christopher Kruegel, Dawn Song, Yuheng Bu",http://arxiv.org/pdf/2505.16934v1,cs.CL
LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning,"In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large
Language Model (MLLM) that integrates visual instruction tuning with masked
diffusion models, representing a departure from the autoregressive paradigms
dominant in current multimodal approaches. Built upon LLaDA, a representative
large language diffusion model, LLaDA-V incorporates a vision encoder and MLP
connector that projects visual features into the language embedding space,
enabling effective multimodal alignment. Our empirical investigation reveals
several intriguing results: First, LLaDA-V demonstrates promising multimodal
performance despite its language model being weaker on purely textual tasks
than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same
instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal
tasks with better data scalability. It also narrows the performance gap to
Qwen2-VL, suggesting the effectiveness of its architecture for multimodal
tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal
understanding compared to existing hybrid autoregressive-diffusion and purely
diffusion-based MLLMs. Our findings suggest that large language diffusion
models show promise in multimodal contexts and warrant further investigation in
future research. Project page and codes:
https://ml-gsai.github.io/LLaDA-V-demo/.",2025-05-22,"Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, Chongxuan Li",http://arxiv.org/pdf/2505.16933v1,cs.CL
The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm,"Computing the polar decomposition and the related matrix sign function, has
been a well-studied problem in numerical analysis for decades. More recently,
it has emerged as an important subroutine in deep learning, particularly within
the Muon optimization framework. However, the requirements in this setting
differ significantly from those of traditional numerical analysis. In deep
learning, methods must be highly efficient and GPU-compatible, but high
accuracy is often unnecessary. As a result, classical algorithms like
Newton-Schulz (which suffers from slow initial convergence) and methods based
on rational functions (which rely on QR decompositions or matrix inverses) are
poorly suited to this context. In this work, we introduce Polar Express, a
GPU-friendly algorithm for computing the polar decomposition. Like classical
polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix
multiplications, making it GPU-compatible. Motivated by earlier work of Chen &
Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule
at each iteration by solving a minimax optimization problem, and we prove that
it enjoys a strong worst-case optimality guarantee. This property ensures both
rapid early convergence and fast asymptotic convergence. We also address
finite-precision issues, making it stable in bfloat16 in practice. We apply
Polar Express within the Muon optimization framework and show consistent
improvements in validation loss on large-scale models such as GPT-2,
outperforming recent alternatives across a range of learning rates.",2025-05-22,"Noah Amsel, David Persson, Christopher Musco, Robert Gower",http://arxiv.org/pdf/2505.16932v1,cs.CL
PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues,"Personally identifiable information (PII) anonymization is a high-stakes task
that poses a barrier to many open-science data sharing initiatives. While PII
identification has made large strides in recent years, in practice, error
thresholds and the recall/precision trade-off still limit the uptake of these
anonymization pipelines. We present PIIvot, a lighter-weight framework for PII
anonymization that leverages knowledge of the data context to simplify the PII
detection problem. To demonstrate its effectiveness, we also contribute
QATD-2k, the largest open-source real-world tutoring dataset of its kind, to
support the demand for quality educational dialogue data.",2025-05-22,"Matthew Zent, Digory Smith, Simon Woodhead",http://arxiv.org/pdf/2505.16931v1,cs.CL
Latent Principle Discovery for Language Model Self-Improvement,"When language model (LM) users aim to improve the quality of its generations,
it is crucial to specify concrete behavioral attributes that the model should
strive to reflect. However, curating such principles across many domains, even
non-exhaustively, requires a labor-intensive annotation process. To automate
this process, we propose eliciting these latent attributes guiding model
reasoning towards human-preferred responses by explicitly modeling them in a
self-correction setting. Our approach mines new principles from the LM itself
and compresses the discovered elements to an interpretable set via clustering.
Specifically, we employ an approximation of posterior-regularized Monte Carlo
Expectation-Maximization to both identify a condensed set of the most effective
latent principles and teach the LM to strategically invoke them in order to
intrinsically refine its responses. We demonstrate that bootstrapping our
algorithm over multiple iterations enables smaller language models (7-8B
parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an
average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on
IFEval. We also show that clustering the principles yields interpretable and
diverse model-generated constitutions while retaining model performance. The
gains our method achieves highlight the potential of automated,
principle-driven post-training recipes toward continual self-improvement.",2025-05-22,"Keshav Ramji, Tahira Naseem, Ramón Fernandez Astudillo",http://arxiv.org/pdf/2505.16927v1,cs.CL
Next Token Perception Score: Analytical Assessment of your LLM Perception Skills,"Autoregressive pretraining has become the de facto paradigm for learning
general-purpose representations in large language models (LLMs). However,
linear probe performance across downstream perception tasks shows substantial
variability, suggesting that features optimized for next-token prediction do
not consistently transfer well to downstream perception tasks. We demonstrate
that representations learned via autoregression capture features that may lie
outside the subspaces most informative for perception. To quantify the
(mis)alignment between autoregressive pretraining and downstream perception, we
introduce the Next Token Perception Score (NTPS)-a score derived under a linear
setting that measures the overlap between autoregressive and perception feature
subspaces. This metric can be easily computed in closed form from pretrained
representations and labeled data, and is proven to both upper- and lower-bound
the excess loss. Empirically, we show that NTPS correlates strongly with linear
probe accuracy across 12 diverse NLP datasets and eight pretrained models
ranging from 270M to 8B parameters, confirming its utility as a measure of
alignment. Furthermore, we show that NTPS increases following low-rank
adaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA
aligning representations to perception tasks enhances subspace overlap and thus
improves downstream performance. More importantly, we find that NTPS reliably
predicts the additional accuracy gains attained by LoRA finetuning thereby
providing a lightweight prescreening tool for LoRA adaptation. Our results
offer both theoretical insights and practical tools for analytically assessing
LLM perception skills.",2025-05-22,"Yu-Ang Cheng, Leyang Hu, Hai Huang, Randall Balestriero",http://arxiv.org/pdf/2505.17169v1,cs.CL
UNCLE: Uncertainty Expressions in Long-Form Generation,"Large Language Models (LLMs) are prone to hallucination, particularly in
long-form generations. A promising direction to mitigate hallucination is to
teach LLMs to express uncertainty explicitly when they lack sufficient
knowledge. However, existing work lacks direct and fair evaluation of LLMs'
ability to express uncertainty effectively in long-form generation. To address
this gap, we first introduce UNCLE, a benchmark designed to evaluate
uncertainty expression in both long- and short-form question answering (QA).
UNCLE spans five domains and comprises 4k long-form QA instances and over 20k
short-form QA pairs. Our dataset is the first to directly bridge short- and
long-form QA with paired questions and gold-standard answers. Along with the
benchmark, we propose a suite of new metrics to assess the models' capabilities
to selectively express uncertainty. Using UNCLE, we then demonstrate that
current models fail to convey uncertainty appropriately in long-form
generation. We further explore both prompt-based and training-based methods to
improve models' performance, with the training-based methods yielding greater
gains. Further analysis of alignment gaps between short- and long-form
uncertainty expression highlights promising directions for future research
using UNCLE.",2025-05-22,"Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting Huang, Dong Yu, Nigel Collier, Deqing Yang",http://arxiv.org/pdf/2505.16922v1,cs.CL
CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation,"Evaluating long-context radiology report generation is challenging. NLG
metrics fail to capture clinical correctness, while LLM-based metrics often
lack generalizability. Clinical accuracy metrics are more relevant but are
sensitive to class imbalance, frequently favoring trivial predictions. We
propose the CRG Score, a distribution-aware and adaptable metric that evaluates
only clinically relevant abnormalities explicitly described in reference
reports. CRG supports both binary and structured labels (e.g., type, location)
and can be paired with any LLM for feature extraction. By balancing penalties
based on label distribution, it enables fairer, more robust evaluation and
serves as a clinically aligned reward function.",2025-05-22,"Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Bernhard Kainz, Bjoern Menze",http://arxiv.org/pdf/2505.17167v1,cs.CL
Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality,"During the finetuning stage of text generation tasks, standard cross-entropy
loss treats all tokens equally. This can lead models to overemphasize
high-frequency, low-information tokens, neglecting lower-frequency tokens
crucial for specificity and informativeness in generated content. This paper
introduces a novel loss function, Power-Law Decay Loss (PDL), specifically
designed to optimize the finetuning process for text generation. The core
motivation for PDL stems from observations in information theory and
linguistics: the informativeness of a token is often inversely proportional to
its frequency of occurrence. PDL re-weights the contribution of each token in
the standard cross-entropy loss based on its frequency in the training corpus,
following a power-law decay. Specifically, the weights for high-frequency
tokens are reduced, while low-frequency, information-dense tokens are assigned
higher weights. This mechanism guides the model during finetuning to focus more
on learning and generating tokens that convey specific and unique information,
thereby enhancing the quality, diversity, and informativeness of the generated
text. We theoretically elaborate on the motivation and construction of PDL and
discuss its potential applications and advantages across various text
generation finetuning tasks, such as abstractive summarization, dialogue
systems, and style transfer.",2025-05-22,"Jintian Shao, Yiming Cheng, Hongyi Huang, Jiayi Wu, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng",http://arxiv.org/pdf/2505.16900v2,cs.CL
Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs,"Hallucinations -- plausible yet erroneous outputs -- remain a critical
barrier to reliable deployment of large language models (LLMs). We present the
first systematic study linking hallucination incidence to internal-state drift
induced by incremental context injection. Using TruthfulQA, we construct two
16-round ""titration"" tracks per question: one appends relevant but partially
flawed snippets, the other injects deliberately misleading content. Across six
open-source LLMs, we track overt hallucination rates with a tri-perspective
detector and covert dynamics via cosine, entropy, JS and Spearman drifts of
hidden states and attention maps. Results reveal (1) monotonic growth of
hallucination frequency and representation drift that plateaus after 5--7
rounds; (2) relevant context drives deeper semantic assimilation, producing
high-confidence ""self-consistent"" hallucinations, whereas irrelevant context
induces topic-drift errors anchored by attention re-routing; and (3)
convergence of JS-Drift ($\sim0.69$) and Spearman-Drift ($\sim0$) marks an
""attention-locking"" threshold beyond which hallucinations solidify and become
resistant to correction. Correlation analyses expose a seesaw between
assimilation capacity and attention diffusion, clarifying size-dependent error
modes. These findings supply empirical foundations for intrinsic hallucination
prediction and context-aware mitigation mechanisms.",2025-05-22,"Zeyu Wei, Shuo Wang, Xiaohui Rong, Xuemin Liu, He Li",http://arxiv.org/pdf/2505.16894v1,cs.CL
CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework,"Large language models (LLMs) have advanced many applications, but are also
known to be vulnerable to adversarial attacks. In this work, we introduce a
novel security threat: hijacking AI-human conversations by manipulating LLMs'
system prompts to produce malicious answers only to specific targeted questions
(e.g., ""Who should I vote for US President?"", ""Are Covid vaccines safe?""),
while behaving benignly on others. This attack is detrimental as it can enable
malicious actors to exercise large-scale information manipulation by spreading
harmful but benign-looking system prompts online. To demonstrate such an
attack, we develop CAIN, an algorithm that can automatically curate such
harmful system prompts for a specific target question in a black-box setting or
without the need to access the LLM's parameters. Evaluated on both open-source
and commercial LLMs, CAIN demonstrates significant adversarial impact. In
untargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves
up to 40% F1 degradation on targeted questions while preserving high accuracy
on benign inputs. For targeted attacks or forcing LLMs to output specific
harmful answers, CAIN achieves over 70% F1 scores on these targeted responses
with minimal impact on benign questions. Our results highlight the critical
need for enhanced robustness measures to safeguard the integrity and safety of
LLMs in real-world applications. All source code will be publicly available.",2025-05-22,"Viet Pham, Thai Le",http://arxiv.org/pdf/2505.16888v1,cs.CL
"Don't ""Overthink"" Passage Reranking: Is Reasoning Truly Necessary?","With the growing success of reasoning models across complex natural language
tasks, researchers in the Information Retrieval (IR) community have begun
exploring how similar reasoning capabilities can be integrated into passage
rerankers built on Large Language Models (LLMs). These methods typically employ
an LLM to produce an explicit, step-by-step reasoning process before arriving
at a final relevance prediction. But, does reasoning actually improve reranking
accuracy? In this paper, we dive deeper into this question, studying the impact
of the reasoning process by comparing reasoning-based pointwise rerankers
(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under
identical training conditions, and observe that StandardRR generally
outperforms ReasonRR. Building on this observation, we then study the
importance of reasoning to ReasonRR by disabling its reasoning process
(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more
effective than ReasonRR. Examining the cause of this result, our findings
reveal that reasoning-based rerankers are limited by the LLM's reasoning
process, which pushes it toward polarized relevance scores and thus fails to
consider the partial relevance of passages, a key factor for the accuracy of
pointwise rerankers.",2025-05-22,"Nour Jedidi, Yung-Sung Chuang, James Glass, Jimmy Lin",http://arxiv.org/pdf/2505.16886v1,cs.CL
CASTILLO: Characterizing Response Length Distributions of Large Language Models,"Efficiently managing compute resources for Large Language Model (LLM)
inference remains challenging due to the inherently stochastic and variable
lengths of autoregressive text generation. Accurately estimating response
lengths in advance enables proactive resource allocation, yet existing
approaches either bias text generation towards certain lengths or rely on
assumptions that ignore model- and prompt-specific variability. We introduce
CASTILLO, a dataset characterizing response length distributions across 13
widely-used open-source LLMs evaluated on seven distinct instruction-following
corpora. For each $\langle$prompt, model$\rangle$ sample pair, we generate 10
independent completions using fixed decoding hyper-parameters, record the token
length of each response, and publish summary statistics (mean, std-dev,
percentiles), along with the shortest and longest completions, and the exact
generation settings. Our analysis reveals significant inter- and intra-model
variability in response lengths (even under identical generation settings), as
well as model-specific behaviors and occurrences of partial text degeneration
in only subsets of responses. CASTILLO enables the development of predictive
models for proactive scheduling and provides a systematic framework for
analyzing model-specific generation behaviors. We publicly release the dataset
and code to foster research at the intersection of generative language modeling
and systems.",2025-05-22,"Daniel F. Perez-Ramirez, Dejan Kostic, Magnus Boman",http://arxiv.org/pdf/2505.16881v1,cs.CL
MPO: Multilingual Safety Alignment via Reward Gap Optimization,"Large language models (LLMs) have become increasingly central to AI
applications worldwide, necessitating robust multilingual safety alignment to
ensure secure deployment across diverse linguistic contexts. Existing
preference learning methods for safety alignment, such as RLHF and DPO, are
primarily monolingual and struggle with noisy multilingual data. To address
these limitations, we introduce Multilingual reward gaP Optimization (MPO), a
novel approach that leverages the well-aligned safety capabilities of the
dominant language (English) to improve safety alignment across multiple
languages. MPO directly minimizes the reward gap difference between the
dominant language and target languages, effectively transferring safety
capabilities while preserving the original strengths of the dominant language.
Extensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate
MPO's efficacy in multilingual safety alignment without degrading general
multilingual utility.",2025-05-22,"Weixiang Zhao, Yulin Hu, Yang Deng, Tongtong Wu, Wenxuan Zhang, Jiahe Guo, An Zhang, Yanyan Zhao, Bing Qin, Tat-Seng Chua, Ting Liu",http://arxiv.org/pdf/2505.16869v1,cs.CL
Comparative analysis of subword tokenization approaches for Indian languages,"Tokenization is the act of breaking down text into smaller parts, or tokens,
that are easier for machines to process. This is a key phase in machine
translation (MT) models. Subword tokenization enhances this process by breaking
down words into smaller subword units, which is especially beneficial in
languages with complicated morphology or a vast vocabulary. It is useful in
capturing the intricate structure of words in Indian languages (ILs), such as
prefixes, suffixes, and other morphological variations. These languages
frequently use agglutinative structures, in which words are formed by the
combination of multiple morphemes such as suffixes, prefixes, and stems. As a
result, a suitable tokenization strategy must be chosen to address these
scenarios. This paper examines how different subword tokenization techniques,
such as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,
affect ILs. The effectiveness of these subword tokenization techniques is
investigated in statistical, neural, and multilingual neural machine
translation models. All models are examined using standard evaluation metrics,
such as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,
RIBES, and COMET. Based on the results, it appears that for the majority of
language pairs for the Statistical and Neural MT models, the SentencePiece
tokenizer continuously performed better than other tokenizers in terms of BLEU
score. However, BPE tokenization outperformed other tokenization techniques in
the context of Multilingual Neural Machine Translation model. The results show
that, despite using the same tokenizer and dataset for each model, translations
from ILs to English surpassed translations from English to ILs.",2025-05-22,"Sudhansu Bala Das, Samujjal Choudhury, Tapas Kumar Mishra, Bidyut Kr. Patra",http://arxiv.org/pdf/2505.16868v1,cs.CL
Nested Named Entity Recognition as Single-Pass Sequence Labeling,"We cast nested named entity recognition (NNER) as a sequence labeling task by
leveraging prior work that linearizes constituency structures, effectively
reducing the complexity of this structured prediction problem to
straightforward token classification. By combining these constituency
linearizations with pretrained encoders, our method captures nested entities
while performing exactly $n$ tagging actions. Our approach achieves competitive
performance compared to less efficient systems, and it can be trained using any
off-the-shelf sequence labeling library.",2025-05-22,"Alberto Muñoz-Ortiz, David Vilares, Caio COrro, Carlos Gómez-Rodríguez",http://arxiv.org/pdf/2505.16855v1,cs.CL
"ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning","Federated Learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data privacy across decentralized participants.
As FL adoption grows, numerous techniques have been proposed to tackle its
practical challenges. However, the lack of standardized evaluation across key
dimensions hampers systematic progress and fair comparison of FL methods. In
this work, we introduce ATR-Bench, a unified framework for analyzing federated
learning through three foundational dimensions: Adaptation, Trust, and
Reasoning. We provide an in-depth examination of the conceptual foundations,
task formulations, and open research challenges associated with each theme. We
have extensively benchmarked representative methods and datasets for adaptation
to heterogeneous clients and trustworthiness in adversarial or unreliable
environments. Due to the lack of reliable metrics and models for reasoning in
FL, we only provide literature-driven insights for this dimension. ATR-Bench
lays the groundwork for a systematic and holistic evaluation of federated
learning with real-world relevance. We will make our complete codebase publicly
accessible and a curated repository that continuously tracks new developments
and research in the FL literature.",2025-05-22,"Tajamul Ashraf, Mohammed Mohsen Peerzada, Moloud Abdar, Yutong Xie, Yuyin Zhou, Xiaofeng Liu, Iqra Altaf Gillani, Janibul Bashir",http://arxiv.org/pdf/2505.16850v1,cs.CL
Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented Generation via Knowledge Graph Walks,"Large Language Models (LLMs) have showcased impressive reasoning abilities,
but often suffer from hallucinations or outdated knowledge. Knowledge Graph
(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by
grounding LLM responses in structured external information from a knowledge
base. However, many KG-based RAG approaches struggle with (i) aligning KG and
textual representations, (ii) balancing retrieval accuracy and efficiency, and
(iii) adapting to dynamically updated KGs. In this work, we introduce
Walk&Retrieve, a simple yet effective KG-based framework that leverages
walk-based graph traversal and knowledge verbalization for corpus generation
for zero-shot RAG. Built around efficient KG walks, our method does not require
fine-tuning on domain-specific data, enabling seamless adaptation to KG
updates, reducing computational overhead, and allowing integration with any
off-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs
competitively, often outperforming existing RAG systems in response accuracy
and hallucination reduction. Moreover, it demonstrates lower query latency and
robust scalability to large KGs, highlighting the potential of lightweight
retrieval strategies as strong baselines for future RAG research.",2025-05-22,"Martin Böckling, Heiko Paulheim, Andreea Iana",http://arxiv.org/pdf/2505.16849v1,cs.CL
Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study,"This paper introduces a method for detecting inappropriately targeting
language in online conversations by integrating crowd and expert annotations
with ChatGPT. We focus on English conversation threads from Reddit, examining
comments that target individuals or groups. Our approach involves a
comprehensive annotation framework that labels a diverse data set for various
target categories and specific target words within the conversational context.
We perform a comparative analysis of annotations from human experts, crowd
annotators, and ChatGPT, revealing strengths and limitations of each method in
recognizing both explicit hate speech and subtler discriminatory language. Our
findings highlight the significant role of contextual factors in identifying
hate speech and uncover new categories of targeting, such as social belief and
body image. We also address the challenges and subjective judgments involved in
annotation and the limitations of ChatGPT in grasping nuanced language. This
study provides insights for improving automated content moderation strategies
to enhance online safety and inclusivity.",2025-05-22,"Baran Barbarestani, Isa Maks, Piek Vossen",http://arxiv.org/pdf/2505.16847v1,cs.CL
R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search,"Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by
enabling step-by-step problem-solving, yet its extension to Long-CoT introduces
substantial computational overhead due to increased token length. Existing
compression approaches -- instance-level and token-level -- either sacrifice
essential local reasoning signals like reflection or yield incoherent outputs.
To address these limitations, we propose R1-Compress, a two-stage chunk-level
compression framework that preserves both local information and coherence. Our
method segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk
compression, and employs an inter-chunk search mechanism to select the short
and coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,
AIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces
token usage while maintaining comparable reasoning accuracy. On MATH500,
R1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to
the Long-CoT baseline, while reducing token usage by about 20%. Source code
will be available at https://github.com/w-yibo/R1-Compress",2025-05-22,"Yibo Wang, Li Shen, Huanjin Yao, Tiansheng Huang, Rui Liu, Naiqiang Tan, Jiaxing Huang, Kai Zhang, Dacheng Tao",http://arxiv.org/pdf/2505.16838v1,cs.CL
SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis,"Retrieval-augmented generation (RAG) systems have advanced large language
models (LLMs) in complex deep search scenarios requiring multi-step reasoning
and iterative information retrieval. However, existing approaches face critical
limitations that lack high-quality training trajectories or suffer from the
distributional mismatches in simulated environments and prohibitive
computational costs for real-world deployment. This paper introduces
SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap
through strategic data engineering rather than complex training paradigms. Our
approach synthesizes high-quality training data by simulating realistic user
interactions in live web search environments, coupled with a multi-criteria
curation strategy that optimizes the diversity and quality of input and output
side. Experiments on five benchmarks across diverse domains demonstrate that
SFT on only 871 curated samples yields significant improvements over RL-based
baselines. Our work establishes SFT as a viable pathway by systematically
addressing the data-scarce bottleneck, offering practical insights for
efficient deep search systems. Our code is available at
https://github.com/RUCAIBox/SimpleDeepSearcher.",2025-05-22,"Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, Lei Fang, Zhongyuan Wang, Ji-Rong Wen",http://arxiv.org/pdf/2505.16834v2,cs.CL
From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization,"While foundation models (FMs), such as diffusion models and large
vision-language models (LVLMs), have been widely applied in educational
contexts, their ability to generate pedagogically effective visual explanations
remains limited. Most existing approaches focus primarily on textual reasoning,
overlooking the critical role of structured and interpretable visualizations in
supporting conceptual understanding. To better assess the visual reasoning
capabilities of FMs in educational settings, we introduce EduVisBench, a
multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem
sets requiring visually grounded solutions, along with a fine-grained
evaluation rubric informed by pedagogical theory. Our empirical analysis
reveals that existing models frequently struggle with the inherent challenge of
decomposing complex reasoning and translating it into visual representations
aligned with human cognitive processes. To address these limitations, we
propose EduVisAgent, a multi-agent collaborative framework that coordinates
specialized agents for instructional planning, reasoning decomposition,
metacognitive prompting, and visualization design. Experimental results show
that EduVisAgent substantially outperforms all baselines, achieving a 40.2%
improvement and delivering more educationally aligned visualizations.
EduVisBench and EduVisAgent are available at
https://github.com/aiming-lab/EduVisBench and
https://github.com/aiming-lab/EduVisAgent.",2025-05-22,"Haonian Ji, Shi Qiu, Siyang Xin, Siwei Han, Zhaorun Chen, Hongyi Wang, Dake Zhang, Huaxiu Yao",http://arxiv.org/pdf/2505.16832v1,cs.CL
Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs,"Unlearning in large language models (LLMs) is intended to remove the
influence of specific data, yet current evaluations rely heavily on token-level
metrics such as accuracy and perplexity. We show that these metrics can be
misleading: models often appear to forget, but their original behavior can be
rapidly restored with minimal fine-tuning, revealing that unlearning may
obscure information rather than erase it. To diagnose this phenomenon, we
introduce a representation-level evaluation framework using PCA-based
similarity and shift, centered kernel alignment, and Fisher information.
Applying this toolkit across six unlearning methods, three domains (text, code,
math), and two open-source LLMs, we uncover a critical distinction between
reversible and irreversible forgetting. In reversible cases, models suffer
token-level collapse yet retain latent features; in irreversible cases, deeper
representational damage occurs. We further provide a theoretical account
linking shallow weight perturbations near output layers to misleading
unlearning signals, and show that reversibility is modulated by task type and
hyperparameters. Our findings reveal a fundamental gap in current evaluation
practices and establish a new diagnostic foundation for trustworthy unlearning
in LLMs. We provide a unified toolkit for analyzing LLM representation changes
under unlearning and relearning:
https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.",2025-05-22,"Xiaoyu Xu, Xiang Yue, Yang Liu, Qingqing Ye, Haibo Hu, Minxin Du",http://arxiv.org/pdf/2505.16831v1,cs.CL
KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning,"Recent advances have demonstrated that integrating reinforcement learning
with rule-based rewards can significantly enhance the reasoning capabilities of
large language models, even without supervised fine-tuning. However, prevalent
reinforcement learning algorithms such as GRPO and its variants like DAPO,
suffer from a coarse granularity issue when computing the advantage.
Specifically, they compute rollout-level advantages that assign identical
values to every token within a sequence, failing to capture token-specific
contributions and hindering effective learning. To address this limitation, we
propose Key-token Advantage Estimation (KTAE) - a novel algorithm that
estimates fine-grained, token-level advantages without introducing additional
models. KTAE leverages the correctness of sampled rollouts and applies
statistical analysis to quantify the importance of individual tokens within a
sequence to the final outcome. This quantified token-level importance is then
combined with the rollout-level advantage to obtain a more fine-grained
token-level advantage estimation. Empirical results show that models trained
with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five
mathematical reasoning benchmarks. Notably, they achieve higher accuracy with
shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base
model.",2025-05-22,"Wei Sun, Wen Yang, Pu Jian, Qianlong Du, Fuwei Cui, Shuo Ren, Jiajun Zhang",http://arxiv.org/pdf/2505.16826v1,cs.CL
Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?,"Named Entity Recognition(NER) for low-resource languages aims to produce
robust systems for languages where there is limited labeled training data
available, and has been an area of increasing interest within NLP. Data
augmentation for increasing the amount of low-resource labeled data is a common
practice. In this paper, we explore the role of synthetic data in the context
of multilingual, low-resource NER, considering 11 languages from diverse
language families. Our results suggest that synthetic data does in fact hold
promise for low-resource language NER, though we see significant variation
between languages.",2025-05-22,"Gaurav Kamath, Sowmya Vajjala",http://arxiv.org/pdf/2505.16814v1,cs.CL
Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement,"Large language models (LLMs) encounter difficulties in knowledge-intensive
multi-step reasoning (KIMSR) tasks. One challenge is how to effectively extract
and represent rationale evidence. The current methods often extract
semantically relevant but logically irrelevant evidence, resulting in flawed
reasoning and inaccurate responses. We propose a two-way evidence
self-alignment (TW-ESA) module, which utilizes the mutual alignment between
strict reasoning and LLM reasoning to enhance its understanding of the causal
logic of evidence, thereby addressing the first challenge. Another challenge is
how to utilize the rationale evidence and LLM's intrinsic knowledge for
accurate reasoning when the evidence contains uncertainty. We propose a
dual-gated reasoning enhancement (DGR) module to gradually fuse useful
knowledge of LLM within strict reasoning, which can enable the model to perform
accurate reasoning by focusing on causal elements in the evidence and exhibit
greater robustness. The two modules are collaboratively trained in a unified
framework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR
datasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based
fine-tuning methods, with remarkable average improvements of 4% in exact match
(EM) and 5% in F1 score. The implementation code is available at
https://anonymous.4open.science/r/ESA-DGR-2BF8.",2025-05-22,"Kexin Zhang, Junlan Chen, Daifeng Li, Yuxuan Zhang, Yangyang Feng, Bowen Deng, Weixu Chen",http://arxiv.org/pdf/2505.16806v1,cs.CL
Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation,"We introduce a transformer-based morpheme segmentation system that augments a
low-resource training signal through multitask learning and LLM-generated
synthetic data. Our framework jointly predicts morphological segments and
glosses from orthographic input, leveraging shared linguistic representations
obtained through a common documentary process to enhance model generalization.
To further address data scarcity, we integrate synthetic training data
generated by large language models (LLMs) using in-context learning.
Experimental results on the SIGMORPHON 2023 dataset show that our approach
significantly improves word-level segmentation accuracy and morpheme-level
F1-score across multiple low-resource languages.",2025-05-22,"Changbing Yang, Garrett Nicolai",http://arxiv.org/pdf/2505.16800v1,cs.CL
Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability,"As large language models gain popularity, their vulnerability to adversarial
attacks remains a primary concern. While fine-tuning models on domain-specific
datasets is often employed to improve model performance, it can introduce
vulnerabilities within the underlying model. In this work, we investigate
Accidental Misalignment, unexpected vulnerabilities arising from
characteristics of fine-tuning data. We begin by identifying potential
correlation factors such as linguistic features, semantic similarity, and
toxicity within our experimental datasets. We then evaluate the adversarial
performance of these fine-tuned models and assess how dataset factors correlate
with attack success rates. Lastly, we explore potential causal links, offering
new insights into adversarial defense strategies and highlighting the crucial
role of dataset design in preserving model alignment. Our code is available at
https://github.com/psyonp/accidental_misalignment.",2025-05-22,"Punya Syon Pandey, Samuel Simko, Kellin Pelrine, Zhijing Jin",http://arxiv.org/pdf/2505.16789v1,cs.CL
Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning,"Large Language Models (LLMs) have achieved impressive performance on complex
reasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional
CoT relies on reasoning steps explicitly verbalized in natural language,
introducing inefficiencies and limiting its applicability to abstract
reasoning. To address this, there has been growing research interest in latent
CoT reasoning, where inference occurs within latent spaces. By decoupling
reasoning from language, latent reasoning promises richer cognitive
representations and more flexible, faster inference. Researchers have explored
various directions in this promising field, including training methodologies,
structural innovations, and internal reasoning mechanisms. This paper presents
a comprehensive overview and analysis of this reasoning paradigm. We begin by
proposing a unified taxonomy from four perspectives: token-wise strategies,
internal mechanisms, analysis, and applications. We then provide in-depth
discussions and comparative analyses of representative methods, highlighting
their design patterns, strengths, and open challenges. We aim to provide a
structured foundation for advancing this emerging direction in LLM reasoning.
The relevant papers will be regularly updated at
https://github.com/EIT-NLP/Awesome-Latent-CoT.",2025-05-22,"Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, Xiaoyu Shen",http://arxiv.org/pdf/2505.16782v1,cs.CL
OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning,"Recent advancements in multimodal slow-thinking systems have demonstrated
remarkable performance across diverse visual reasoning tasks. However, their
capabilities in text-rich image reasoning tasks remain understudied due to the
lack of a systematic benchmark. To address this gap, we propose OCR-Reasoning,
a comprehensive benchmark designed to systematically assess Multimodal Large
Language Models on text-rich image reasoning tasks. The benchmark comprises
1,069 human-annotated examples spanning 6 core reasoning abilities and 18
practical reasoning tasks in text-rich visual scenarios. Furthermore, unlike
other text-rich image understanding benchmarks that only annotate the final
answers, OCR-Reasoning also annotates the reasoning process simultaneously.
With the annotated reasoning process and the final answers, OCR-Reasoning
evaluates not only the final answers generated by models but also their
reasoning processes, enabling a holistic analysis of their problem-solving
abilities. Leveraging this benchmark, we conducted a comprehensive evaluation
of state-of-the-art MLLMs. Our results demonstrate the limitations of existing
methodologies. Notably, even state-of-the-art MLLMs exhibit substantial
difficulties, with none achieving accuracy surpassing 50\% across
OCR-Reasoning, indicating that the challenges of text-rich image reasoning are
an urgent issue to be addressed. The benchmark and evaluation scripts are
available at https://github.com/SCUT-DLVCLab/OCR-Reasoning.",2025-05-22,"Mingxin Huang, Yongxin Shi, Dezhi Peng, Songxuan Lai, Zecheng Xie, Lianwen Jin",http://arxiv.org/pdf/2505.17163v1,cs.CL
IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models,"Large language models (LLMs) have demonstrated strong instruction-following
capabilities in text-based tasks. However, this ability often deteriorates in
multimodal models after alignment with non-text modalities such as images or
audio. While several recent efforts have investigated instruction-following
performance in text and vision-language models, instruction-following in
audio-based large language models remains largely unexplored. To bridge this
gap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess
the ability to follow instructions in an audio LLM. IFEval-Audio contains 280
audio-instruction-answer triples across six diverse dimensions: Content,
Capitalization, Symbol, List Structure, Length, and Format. Each example pairs
an audio input with a text instruction, requiring the model to generate an
output that follows a specified structure. We benchmark state-of-the-art audio
LLMs on their ability to follow audio-involved instructions. The dataset is
released publicly to support future research in this emerging area.",2025-05-22,"Yiming Gao, Bin Wang, Chengwei Wei, Shuo Sun, AiTi Aw",http://arxiv.org/pdf/2505.16774v1,cs.CL
TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning,"Large Language Models (LLMs) present significant computational and memory
challenges due to their extensive size, making pruning essential for their
efficient deployment. Existing one-shot pruning methods often apply uniform
sparsity constraints across layers or within each layer, resulting in
suboptimal performance, especially at high sparsity ratios. This work
introduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel
approach that applies varying sparsity ratios to individual output dimensions
(rows) within each layer. TRIM employs an iterative adjustment process guided
by quality metrics to optimize dimension-wise sparsity allocation, focusing on
reducing variance in quality retention across outputs to preserve critical
information. TRIM can be seamlessly integrated with existing layer-wise pruning
strategies. Our evaluations on perplexity and zero-shot tasks across diverse
LLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that
TRIM achieves new state-of-the-art results and enhances stability. For
instance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and
over 90% for OPT-13B compared to baseline methods. We conclude that
fine-grained, dimension-wise sparsity adaptation is crucial for pushing the
limits of extreme LLM compression. Code available at:
https://github.com/flobk/TRIM",2025-05-22,"Florentin Beck, William Rudman, Carsten Eickhoff",http://arxiv.org/pdf/2505.16743v1,cs.CL
Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization,"The significant progress of large language models (LLMs) has led to
remarkable achievements across numerous applications. However, their ability to
generate harmful content has sparked substantial safety concerns. Despite the
implementation of safety alignment techniques during the pre-training phase,
recent research indicates that fine-tuning LLMs on adversarial or even benign
data can inadvertently compromise their safety. In this paper, we re-examine
the fundamental issue of why fine-tuning on non-harmful data still results in
safety degradation. We introduce a safety-aware probing (SAP) optimization
framework designed to mitigate the safety risks of fine-tuning LLMs.
Specifically, SAP incorporates a safety-aware probe into the gradient
propagation process, mitigating the model's risk of safety degradation by
identifying potential pitfalls in gradient directions, thereby enhancing
task-specific performance while successfully preserving model safety. Our
extensive experimental results demonstrate that SAP effectively reduces
harmfulness below the original fine-tuned model and achieves comparable test
loss to standard fine-tuning methods. Our code is available at
https://github.com/ChengcanWu/SAP.",2025-05-22,"Chengcan Wu, Zhixin Zhang, Zeming Wei, Yihao Zhang, Meng Sun",http://arxiv.org/pdf/2505.16737v1,cs.CL
Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting,"This work presents LURK (Latent UnleaRned Knowledge), a novel framework that
probes for hidden retained knowledge in unlearned LLMs through adversarial
suffix prompting. LURK automatically generates adversarial prompt suffixes
designed to elicit residual knowledge about the Harry Potter domain, a commonly
used benchmark for unlearning. Our experiments reveal that even models deemed
successfully unlearned can leak idiosyncratic information under targeted
adversarial conditions, highlighting critical limitations of current unlearning
evaluation standards. By uncovering latent knowledge through indirect probing,
LURK offers a more rigorous and diagnostic tool for assessing the robustness of
unlearning algorithms. All code will be publicly available.",2025-05-22,"Bang Trinh Tran To, Thai Le",http://arxiv.org/pdf/2505.17160v1,cs.CL
Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification,"As large language models (LLMs) become increasingly prevalent in global
applications, ensuring that they are toxicity-free across diverse linguistic
contexts remains a critical challenge. We explore ""Cross-lingual
Detoxification"", a cross-lingual paradigm that mitigates toxicity, enabling
detoxification capabilities to transfer between high and low-resource languages
across different script families. We analyze cross-lingual detoxification's
effectiveness through 504 extensive settings to evaluate toxicity reduction in
cross-distribution settings with limited data and investigate how mitigation
impacts model performance on non-toxic tasks, revealing trade-offs between
safety and knowledge preservation. Our code and dataset are publicly available
at https://github.com/himanshubeniwal/Breaking-mBad.",2025-05-22,"Himanshu Beniwal, Youngwoo Kim, Maarten Sap, Soham Dan, Thomas Hartvigsen",http://arxiv.org/pdf/2505.16722v1,cs.CL
Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs,"Although multimodal large language models (MLLMs) have achieved impressive
performance, the multimodal instruction tuning stage often causes catastrophic
forgetting of the base LLM's language ability, even in strong models like
Llama3. To address this, we propose Locate-then-Merge, a training-free
parameter fusion framework that first locates important parameters and then
selectively merges them. We further introduce Neuron-Fusion, a neuron-level
strategy that preserves the influence of neurons with large parameter
shifts--neurons likely responsible for newly acquired visual
capabilities--while attenuating the influence of neurons with smaller changes
that likely encode general-purpose language skills. This design enables better
retention of visual adaptation while mitigating language degradation.
Experiments on 13 benchmarks across both language and visual tasks show that
Neuron-Fusion consistently outperforms existing model merging methods. Further
analysis reveals that our method effectively reduces context hallucination in
generation.",2025-05-22,"Zeping Yu, Sophia Ananiadou",http://arxiv.org/pdf/2505.16703v1,cs.CL
Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence,"Transformer-based language models exhibit In-Context Learning (ICL), where
predictions are made adaptively based on context. While prior work links
induction heads to ICL through a sudden jump in accuracy, this can only account
for ICL when the answer is included within the context. However, an important
property of practical ICL in large language models is the ability to meta-learn
how to solve tasks from context, rather than just copying answers from context;
how such an ability is obtained during training is largely unexplored. In this
paper, we experimentally clarify how such meta-learning ability is acquired by
analyzing the dynamics of the model's circuit during training. Specifically, we
extend the copy task from previous research into an In-Context Meta Learning
setting, where models must infer a task from examples to answer queries.
Interestingly, in this setting, we find that there are multiple phases in the
process of acquiring such abilities, and that a unique circuit emerges in each
phase, contrasting with the single-phases change in induction heads. The
emergence of such circuits can be related to several phenomena known in large
language models, and our analysis lead to a deeper understanding of the source
of the transformer's ICL ability.",2025-05-22,"Gouki Minegishi, Hiroki Furuta, Shohei Taniguchi, Yusuke Iwasawa, Yutaka Matsuo",http://arxiv.org/pdf/2505.16694v1,cs.CL
SPaRC: A Spatial Pathfinding Reasoning Challenge,"Existing reasoning datasets saturate and fail to test abstract, multi-step
problems, especially pathfinding and complex rule constraint satisfaction. We
introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000
2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,
requiring step-by-step planning with arithmetic and geometric rules. Humans
achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best
reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).
Models often generate invalid paths (>50% of puzzles for o4-mini), and
reasoning tokens reveal they make errors in navigation and spatial logic.
Unlike humans, who take longer on hard puzzles, models fail to scale test-time
compute with difficulty. Allowing models to make multiple solution attempts
improves accuracy, suggesting potential for better spatial reasoning with
improved training and efficient test-time scaling methods. SPaRC can be used as
a window into models' spatial reasoning limitations and drive research toward
new methods that excel in abstract, multi-step problem-solving.",2025-05-22,"Lars Benedikt Kaesberg, Jan Philip Wahle, Terry Ruas, Bela Gipp",http://arxiv.org/pdf/2505.16686v1,cs.CL
R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO,"In this work, we aim to incentivize the reasoning ability of Multimodal Large
Language Models (MLLMs) via reinforcement learning (RL) and develop an
effective approach that mitigates the sparse reward and advantage vanishing
issues during RL. To this end, we propose Share-GRPO, a novel RL approach that
tackle these issues by exploring and sharing diverse reasoning trajectories
over expanded question space. Specifically, Share-GRPO first expands the
question space for a given question via data transformation techniques, and
then encourages MLLM to effectively explore diverse reasoning trajectories over
the expanded question space and shares the discovered reasoning trajectories
across the expanded questions during RL. In addition, Share-GRPO also shares
reward information during advantage computation, which estimates solution
advantages hierarchically across and within question variants, allowing more
accurate estimation of relative advantages and improving the stability of
policy training. Extensive evaluations over six widely-used reasoning
benchmarks showcase the superior performance of our method. Code will be
available at https://github.com/HJYao00/R1-ShareVL.",2025-05-22,"Huanjin Yao, Qixiang Yin, Jingyi Zhang, Min Yang, Yibo Wang, Wenhao Wu, Fei Su, Li Shen, Minghui Qiu, Dacheng Tao, Jiaxing Huang",http://arxiv.org/pdf/2505.16673v1,cs.CL
A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP,"We present a Japanese domain-specific language model for the pharmaceutical
field, developed through continual pretraining on 2 billion Japanese
pharmaceutical tokens and 8 billion English biomedical tokens. To enable
rigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on
national pharmacist licensing exams; NayoseQA, which tests cross-lingual
synonym and terminology normalization; and SogoCheck, a novel task designed to
assess consistency reasoning between paired statements. We evaluate our model
against both open-source medical LLMs and commercial models, including GPT-4o.
Results show that our domain-specific model outperforms existing open models
and achieves competitive performance with commercial ones, particularly on
terminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o
performs poorly on SogoCheck, suggesting that cross-sentence consistency
reasoning remains an open challenge. Our benchmark suite offers a broader
diagnostic lens for pharmaceutical NLP, covering factual recall, lexical
variation, and logical consistency. This work demonstrates the feasibility of
building practical, secure, and cost-effective language models for Japanese
domain-specific applications, and provides reusable evaluation resources for
future research in pharmaceutical and healthcare NLP. Our model, codes, and
datasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.",2025-05-22,"Issey Sukeda, Takuro Fujii, Kosei Buma, Shunsuke Sasaki, Shinnosuke Ono",http://arxiv.org/pdf/2505.16661v1,cs.CL
Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu,"This study addresses the challenges in intelligent processing of Chinese
ancient mathematical classics by constructing Guji_MATH, a benchmark for
evaluating classical texts based on Suanjing Shishu. It systematically assesses
the mathematical problem-solving capabilities of mainstream reasoning models
under the unique linguistic constraints of classical Chinese. Through
machine-assisted annotation and manual verification, 538 mathematical problems
were extracted from 8 canonical texts, forming a structured dataset centered on
the ""Question-Answer-Solution"" framework, supplemented by problem types and
difficulty levels. Dual evaluation modes--closed-book (autonomous
problem-solving) and open-book (reproducing classical solution methods)--were
designed to evaluate the performance of six reasoning models on ancient Chinese
mathematical problems. Results indicate that reasoning models can partially
comprehend and solve these problems, yet their overall performance remains
inferior to benchmarks on modern mathematical tasks. Enhancing models'
classical Chinese comprehension and cultural knowledge should be prioritized
for optimization. This study provides methodological support for mining
mathematical knowledge from ancient texts and disseminating traditional
culture, while offering new perspectives for evaluating cross-linguistic and
cross-cultural capabilities of reasoning models.",2025-05-22,"Liu Chang, Wang Dongbo, Liu liu, Zhao Zhixiao",http://arxiv.org/pdf/2505.16660v1,cs.CL
Collaboration among Multiple Large Language Models for Medical Question Answering,"Empowered by vast internal knowledge reservoir, the new generation of large
language models (LLMs) demonstrate untapped potential to tackle medical tasks.
However, there is insufficient effort made towards summoning up a synergic
effect from multiple LLMs' expertise and background. In this study, we propose
a multi-LLM collaboration framework tailored on a medical multiple-choice
questions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,
our framework is proved to boost all LLMs reasoning ability as well as
alleviate their divergence among questions. We also measure an LLM's confidence
when it confronts with adversary opinions from other LLMs and observe a
concurrence between LLM's confidence and prediction accuracy.",2025-05-22,"Kexin Shang, Chia-Hsuan Chang, Christopher C. Yang",http://arxiv.org/pdf/2505.16648v1,cs.CL
SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation,"Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.",2025-05-22,"Wenjie Yang, Mao Zheng, Mingyang Song, Zheng Li",http://arxiv.org/pdf/2505.16637v2,cs.CL
PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG,"The introduction of Large Language Models (LLMs) has significantly
transformed Natural Language Processing (NLP) applications by enabling more
advanced analysis of customer personas. At Volvo Construction Equipment (VCE),
customer personas have traditionally been developed through qualitative
methods, which are time-consuming and lack scalability. The main objective of
this paper is to generate synthetic customer personas and integrate them into a
Retrieval-Augmented Generation (RAG) chatbot to support decision-making in
business processes. To this end, we first focus on developing a persona-based
RAG chatbot integrated with verified personas. Next, synthetic personas are
generated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and
evaluated based on completeness, relevance, and consistency using McNemar's
test. In the final step, the chatbot's knowledge base is augmented with
synthetic personas and additional segment information to assess improvements in
response accuracy and practical utility. Key findings indicate that Few-Shot
prompting outperformed CoT in generating more complete personas, while CoT
demonstrated greater efficiency in terms of response time and token usage.
After augmenting the knowledge base, the average accuracy rating of the chatbot
increased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants
found the updated system useful in business contexts.",2025-05-22,"Muhammed Rizwan, Lars Carlsson, Mohammad Loni",http://arxiv.org/pdf/2505.17156v1,cs.CL
MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries,"Despite bilingual speakers frequently using mixed-language queries in web
searches, Information Retrieval (IR) research on them remains scarce. To
address this, we introduce MiLQ,Mixed-Language Query test set, the first public
benchmark of mixed-language queries, confirmed as realistic and highly
preferred. Experiments show that multilingual IR models perform moderately on
MiLQ and inconsistently across native, English, and mixed-language queries,
also suggesting code-switched training data's potential for robust IR models
handling such queries. Meanwhile, intentional English mixing in queries proves
an effective strategy for bilinguals searching English documents, which our
analysis attributes to enhanced token matching compared to native queries.",2025-05-22,"Jonghwi Kim, Deokhyung Kang, Seonjeong Hwang, Yunsu Kim, Jungseul Ok, Gary Lee",http://arxiv.org/pdf/2505.16631v1,cs.CL
Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports,"We present a novel approach to Chest X-ray (CXR) Visual Question Answering
(VQA), addressing both single-image image-difference questions. Single-image
questions focus on abnormalities within a specific CXR (""What abnormalities are
seen in image X?""), while image-difference questions compare two longitudinal
CXRs acquired at different time points (""What are the differences between image
X and Y?""). We further explore how the integration of radiology reports can
enhance the performance of VQA models. While previous approaches have
demonstrated the utility of radiology reports during the pre-training phase, we
extend this idea by showing that the reports can also be leveraged as
additional input to improve the VQA model's predicted answers. First, we
propose a unified method that handles both types of questions and
auto-regressively generates the answers. For single-image questions, the model
is provided with a single CXR. For image-difference questions, the model is
provided with two CXRs from the same patient, captured at different time
points, enabling the model to detect and describe temporal changes. Taking
inspiration from 'Chain-of-Thought reasoning', we demonstrate that performance
on the CXR VQA task can be improved by grounding the answer generator module
with a radiology report predicted for the same CXR. In our approach, the VQA
model is divided into two steps: i) Report Generation (RG) and ii) Answer
Generation (AG). Our results demonstrate that incorporating predicted radiology
reports as evidence to the AG model enhances performance on both single-image
and image-difference questions, achieving state-of-the-art results on the
Medical-Diff-VQA dataset.",2025-05-22,"Francesco Dalla Serra, Patrick Schrempf, Chaoyang Wang, Zaiqiao Meng, Fani Deligianni, Alison Q. O'Neil",http://arxiv.org/pdf/2505.16624v1,cs.CL
Steering Large Language Models for Machine Translation Personalization,"High-quality machine translation systems based on large language models
(LLMs) have simplified the production of personalized translations reflecting
specific stylistic constraints. However, these systems still struggle in
settings where stylistic requirements are less explicit and might be harder to
convey via prompting. We explore various strategies for personalizing
LLM-generated translations in low-resource settings, focusing on the
challenging literary translation domain. We explore prompting strategies and
inference-time interventions for steering model generations towards a
personalized style, and propose a contrastive framework exploiting latent
concepts extracted from sparse autoencoders to identify salient personalization
properties. Our results show that steering achieves strong personalization
while preserving translation quality. We further examine the impact of steering
on LLM representations, finding model layers with a relevant impact for
personalization are impacted similarly by multi-shot prompting and our steering
method, suggesting similar mechanism at play.",2025-05-22,"Daniel Scalena, Gabriele Sarti, Arianna Bisazza, Elisabetta Fersini, Malvina Nissim",http://arxiv.org/pdf/2505.16612v1,cs.CL
From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment,"Effective emotional support hinges on understanding users' emotions and needs
to provide meaningful comfort during multi-turn interactions. Large Language
Models (LLMs) show great potential for expressing empathy; however, they often
deliver generic and one-size-fits-all responses that fail to address users'
specific needs. To tackle this issue, we propose a self-evolution framework
designed to help LLMs improve their responses to better align with users'
implicit preferences concerning user profiles (personalities), emotional
states, and specific situations. Our framework consists of two distinct phases:
\textit{(1)} \textit{Emotional Support Experience Acquisition}, where LLMs are
fine-tuned on limited emotional support conversation data to provide basic
support, and \textit{(2)} \textit{Self-Improvement for Personalized Emotional
Support}, where LLMs leverage self-reflection and self-refinement to generate
personalized responses. Through iterative direct preference optimization
between the pre- and post-refined responses, our model generates responses that
reflect a better understanding of the user's implicit preferences. Extensive
experiments and evaluations demonstrate that our method significantly enhances
the model's performance in emotional support, reducing unhelpful responses and
minimizing discrepancies between user preferences and model outputs.",2025-05-22,"Jing Ye, Lu Xiang, Yaping Zhang, Chengqing Zong",http://arxiv.org/pdf/2505.16610v1,cs.CL
What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse,"Media framing refers to the emphasis on specific aspects of perceived reality
to shape how an issue is defined and understood. Its primary purpose is to
shape public perceptions often in alignment with the authors' opinions and
stances. However, the interaction between stance and media frame remains
largely unexplored. In this work, we apply an interdisciplinary approach to
conceptualize and computationally explore this interaction with internet memes
on climate change. We curate CLIMATEMEMES, the first dataset of climate-change
memes annotated with both stance and media frames, inspired by research in
communication science. CLIMATEMEMES includes 1,184 memes sourced from 47
subreddits, enabling analysis of frame prominence over time and communities,
and sheds light on the framing preferences of different stance holders. We
propose two meme understanding tasks: stance detection and media frame
detection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the
corresponding results on their LLM backbone. Human captions consistently
enhance performance. Synthetic captions and human-corrected OCR also help
occasionally. Our findings highlight that VLMs perform well on stance, but
struggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'
limitations in handling nuanced frames and stance expressions on climate change
internet memes.",2025-05-22,"Shijia Zhou, Siyao Peng, Simon Luebke, Jörg Haßler, Mario Haim, Saif M. Mohammad, Barbara Plank",http://arxiv.org/pdf/2505.16592v2,cs.CL
Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering,"We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual
factual ability of Large Language Models (LLMs). Inspired by existing research,
we created the question set with features such as single knowledge point
coverage, absolute objectivity, unique answers, and temporal stability. These
questions enable efficient evaluation using the LLM-as-judge paradigm, testing
both the LLMs' factual memory and self-awareness (""know what they don't know"").
KoLasSimpleQA expands existing research in two key dimensions: (1) Breadth
(Multilingual Coverage): It includes 9 languages, supporting global
applicability evaluation. (2) Depth (Dual Domain Design): It covers both the
general domain (global facts) and the language-specific domain (such as
history, culture, and regional traditions) for a comprehensive assessment of
multilingual capabilities. We evaluated mainstream LLMs, including traditional
LLM and emerging Large Reasoning Models. Results show significant performance
differences between the two domains, particularly in performance metrics,
ranking, calibration, and robustness. This highlights the need for targeted
evaluation and optimization in multilingual contexts. We hope KoLasSimpleQA
will help the research community better identify LLM capability boundaries in
multilingual contexts and provide guidance for model optimization. We will
release KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA .",2025-05-22,"Bowen Jiang, Runchuan Zhu, Jiang Wu, Zinco Jiang, Yifan He, Junyuan Gao, Jia Yu, Rui Min, Yinfan Wang, Haote Yang, Songyang Zhang, Dahua Lin, Lijun Wu, Conghui He",http://arxiv.org/pdf/2505.16591v1,cs.CL
TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling,"Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling
complex mathematical, logical, and coding tasks by leveraging extended
Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging
CoT with explicit token-level exploration, can push LRMs' accuracy boundaries,
but they incur significant decoding overhead. A key inefficiency source is LRMs
often generate redundant thinking CoTs, which demonstrate clear structured
overthinking and underthinking patterns. Inspired by human cognitive reasoning
processes and numerical optimization theories, we propose TrimR, a
verifier-based, training-free, efficient framework for dynamic CoT compression
to trim reasoning and enhance test-time scaling, explicitly tailored for
production-level deployment. Our method employs a lightweight, pretrained,
instruction-tuned verifier to detect and truncate redundant intermediate
thoughts of LRMs without any LRM or verifier fine-tuning. We present both the
core algorithm and asynchronous online system engineered for high-throughput
industrial applications. Empirical evaluations on Ascend NPUs and vLLM show
that our framework delivers substantial gains in inference efficiency under
large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and
GPQA benchmarks, the reasoning runtime of Pangu-R-38B, QwQ-32B, and
DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on
accuracy.",2025-05-22,"Weizhe Lin, Xing Li, Zhiyuan Yang, Xiaojin Fu, Hui-Ling Zhen, Yaoyuan Wang, Xianzhi Yu, Wulong Liu, Xiaosong Li, Mingxuan Yuan",http://arxiv.org/pdf/2505.17155v1,cs.CL
O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering,"Large Language Models (LLMs), despite their advancements, are fundamentally
limited by their static parametric knowledge, hindering performance on tasks
requiring open-domain up-to-date information. While enabling LLMs to interact
with external knowledge environments is a promising solution, current efforts
primarily address closed-end problems. Open-ended questions, which
characterized by lacking a standard answer or providing non-unique and diverse
answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a
novel search agent leveraging reinforcement learning to effectively tackle both
open-ended and closed-ended questions in the open domain. O$^2$-Searcher
leverages an efficient, locally simulated search environment for dynamic
knowledge acquisition, effectively decoupling the external world knowledge from
model's sophisticated reasoning processes. It employs a unified training
mechanism with meticulously designed reward functions, enabling the agent to
identify problem types and adapt different answer generation strategies.
Furthermore, to evaluate performance on complex open-ended tasks, we construct
O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain
open-ended questions with associated web page caches. Extensive experiments
show that O$^2$-Searcher, using only a 3B model, significantly surpasses
leading LLM agents on O$^2$-QA. It also achieves SOTA results on various
closed-ended QA benchmarks against similarly-sized models, while performing on
par with much larger ones.",2025-05-22,"Jianbiao Mei, Tao Hu, Daocheng Fu, Licheng Wen, Xuemeng Yang, Rong Wu, Pinlong Cai, Xinyu Cai, Xing Gao, Yu Yang, Chengjun Xie, Botian Shi, Yong Liu, Yu Qiao",http://arxiv.org/pdf/2505.16582v2,cs.CL
EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions,"Determining the veracity of atomic claims is an imperative component of many
recently proposed fact-checking systems. Many approaches tackle this problem by
first retrieving evidence by querying a search engine and then performing
classification by providing the evidence set and atomic claim to a large
language model, but this process deviates from what a human would do in order
to perform the task. Recent work attempted to address this issue by proposing
iterative evidence retrieval, allowing for evidence to be collected several
times and only when necessary. Continuing along this line of research, we
propose a novel claim verification system, called EMULATE, which is designed to
better emulate human actions through the use of a multi-agent framework where
each agent performs a small part of the larger task, such as ranking search
results according to predefined criteria or evaluating webpage content.
Extensive experiments on several benchmarks show clear improvements over prior
work, demonstrating the efficacy of our new multi-agent framework.",2025-05-22,"Spencer Hong, Meng Luo, Xinyi Wan",http://arxiv.org/pdf/2505.16576v1,cs.CL
"URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training","Large Language Models (LLMs) are commonly pretrained on vast corpora of text
without utilizing contextual metadata such as source, quality, or topic,
leading to a context-free learning paradigm. While recent studies suggest that
adding metadata like URL information as context (i.e., auxiliary inputs not
used in the loss calculation) can improve training efficiency and downstream
performance, they offer limited understanding of which types of metadata are
truly effective and under what conditions. In this work, we conduct a
systematic evaluation and find that not all metadata types contribute equally.
Only URL context speeds up training, whereas quality scores and topic/format
domain information offer no clear benefit. Furthermore, the improved downstream
performances of URL conditioning emerge only when longer prompts are used at
inference time. In addition, we demonstrate that context-aware pretraining
enables more controllable generation than context-free pretraining, in a
classifier-free guidance fashion. Although topic and format metadata do not
accelerate training, they are effective for steering outputs, offering
human-interpretable control over generation.",2025-05-22,"Dongyang Fan, Vinko Sabolčec, Martin Jaggi",http://arxiv.org/pdf/2505.16570v1,cs.CL
"ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts","Prior benchmarks for evaluating the domain-specific knowledge of large
language models (LLMs) lack the scalability to handle complex academic tasks.
To address this, we introduce \texttt{ScholarBench}, a benchmark centered on
deep expert knowledge and complex academic problem-solving, which evaluates the
academic reasoning ability of LLMs and is constructed through a three-step
process. \texttt{ScholarBench} targets more specialized and logically complex
contexts derived from academic literature, encompassing five distinct problem
types. Unlike prior benchmarks, \texttt{ScholarBench} evaluates the
abstraction, comprehension, and reasoning capabilities of LLMs across eight
distinct research domains. To ensure high-quality evaluation data, we define
category-specific example attributes and design questions that are aligned with
the characteristic research methodologies and discourse structures of each
domain. Additionally, this benchmark operates as an English-Korean bilingual
dataset, facilitating simultaneous evaluation for linguistic capabilities of
LLMs in both languages. The benchmark comprises 5,031 examples in Korean and
5,309 in English, with even state-of-the-art models like o3-mini achieving an
average evaluation score of only 0.543, demonstrating the challenging nature of
this benchmark.",2025-05-22,"Dongwon Noh, Donghyeok Koh, Junghun Yuk, Gyuwan Kim, Jaeyong Lee, Kyungtae Lim, Cheoneum Park",http://arxiv.org/pdf/2505.16566v1,cs.CL
CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning,"Fine-tuning-as-a-service, while commercially successful for Large Language
Model (LLM) providers, exposes models to harmful fine-tuning attacks. As a
widely explored defense paradigm against such attacks, unlearning attempts to
remove malicious knowledge from LLMs, thereby essentially preventing them from
being used to perform malicious tasks. However, we highlight a critical flaw:
the powerful general adaptability of LLMs allows them to easily bypass
selective unlearning by rapidly relearning or repurposing their capabilities
for harmful tasks. To address this fundamental limitation, we propose a
paradigm shift: instead of selective removal, we advocate for inducing model
collapse--effectively forcing the model to ""unlearn everything""--specifically
in response to updates characteristic of malicious adaptation. This collapse
directly neutralizes the very general capabilities that attackers exploit,
tackling the core issue unaddressed by selective unlearning. We introduce the
Collapse Trap (CTRAP) as a practical mechanism to implement this concept
conditionally. Embedded during alignment, CTRAP pre-configures the model's
reaction to subsequent fine-tuning dynamics. If updates during fine-tuning
constitute a persistent attempt to reverse safety alignment, the pre-configured
trap triggers a progressive degradation of the model's core language modeling
abilities, ultimately rendering it inert and useless for the attacker.
Crucially, this collapse mechanism remains dormant during benign fine-tuning,
ensuring the model's utility and general capabilities are preserved for
legitimate users. Extensive empirical results demonstrate that CTRAP
effectively counters harmful fine-tuning risks across various LLMs and attack
settings, while maintaining high performance in benign scenarios. Our code is
available at https://anonymous.4open.science/r/CTRAP.",2025-05-22,"Biao Yi, Tiansheng Huang, Baolei Zhang, Tong Li, Lihai Nie, Zheli Liu, Li Shen",http://arxiv.org/pdf/2505.16559v1,cs.CL
"Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains","Large Language Models (LLMs) achieve superior performance through
Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are
computationally expensive and inefficient. In this paper, we introduce
Compressed Latent Reasoning (CoLaR), a novel framework that dynamically
compresses reasoning processes in latent space through a two-stage training
approach. First, during supervised fine-tuning, CoLaR extends beyond next-token
prediction by incorporating an auxiliary next compressed embedding prediction
objective. This process merges embeddings of consecutive tokens using a
compression factor randomly sampled from a predefined range, and trains a
specialized latent head to predict distributions of subsequent compressed
embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that
leverages the latent head's non-deterministic nature to explore diverse
reasoning paths and exploit more compact ones. This approach enables CoLaR to:
i) perform reasoning at a dense latent level (i.e., silently), substantially
reducing reasoning chain length, and ii) dynamically adjust reasoning speed at
inference time by simply prompting the desired compression factor. Extensive
experiments across four mathematical reasoning datasets demonstrate that CoLaR
achieves 14.1% higher accuracy than latent-based baseline methods at comparable
compression ratios, and reduces reasoning chain length by 53.3% with only 4.8%
performance degradation compared to explicit CoT method. Moreover, when applied
to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR
demonstrates performance gains of up to 5.4% while dramatically reducing latent
reasoning chain length by 82.8%. The code and models will be released upon
acceptance.",2025-05-22,"Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Ruihua Song",http://arxiv.org/pdf/2505.16552v2,cs.CL
Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models,"Language confusion -- where large language models (LLMs) generate unintended
languages against the user's need -- remains a critical challenge, especially
for English-centric models. We present the first mechanistic interpretability
(MI) study of language confusion, combining behavioral benchmarking with
neuron-level analysis. Using the Language Confusion Benchmark (LCB), we show
that confusion points (CPs) -- specific positions where language switches occur
-- are central to this phenomenon. Through layer-wise analysis with TunedLens
and targeted neuron attribution, we reveal that transition failures in the
final layers drive confusion. We further demonstrate that editing a small set
of critical neurons, identified via comparative analysis with
multilingual-tuned models, substantially mitigates confusion without harming
general competence or fluency. Our approach matches multilingual alignment in
confusion reduction for most languages and yields cleaner, higher-quality
outputs. These findings provide new insights into the internal dynamics of LLMs
and highlight neuron-level interventions as a promising direction for robust,
interpretable multilingual language modeling.",2025-05-22,"Ercong Nie, Helmut Schmid, Hinrich Schütze",http://arxiv.org/pdf/2505.16538v1,cs.CL
Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN,"Recently, models such as OpenAI-o1 and DeepSeek-R1 have demonstrated
remarkable performance on complex reasoning tasks through Long Chain-of-Thought
(Long-CoT) reasoning. Although distilling this capability into student models
significantly enhances their performance, this paper finds that fine-tuning
LLMs with full parameters or LoRA with a low rank on long CoT data often leads
to Cyclical Reasoning, where models repeatedly reiterate previous inference
steps until the maximum length limit. Further analysis reveals that smaller
differences in representations between adjacent tokens correlates with a higher
tendency toward Cyclical Reasoning. To mitigate this issue, this paper proposes
Shift Feedforward Networks (Shift-FFN), a novel approach that edits the current
token's representation with the previous one before inputting it to FFN. This
architecture dynamically amplifies the representation differences between
adjacent tokens. Extensive experiments on multiple mathematical reasoning tasks
demonstrate that LoRA combined with Shift-FFN achieves higher accuracy and a
lower rate of Cyclical Reasoning across various data sizes compared to full
fine-tuning and standard LoRA. Our data and code are available at
https://anonymous.4open.science/r/Shift-FFN",2025-05-22,"Yao Xu, Mingyu Xu, Fangyu Lei, Wangtao Sun, Xiangrong Zeng, Bingning Wang, Guang Liu, Shizhu He, Jun Zhao, Kang Liu",http://arxiv.org/pdf/2505.17153v1,cs.CL
DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection,"Large language models (LLMs) are considered valuable Intellectual Properties
(IP) for legitimate owners due to the enormous computational cost of training.
It is crucial to protect the IP of LLMs from malicious stealing or unauthorized
deployment. Despite existing efforts in watermarking and fingerprinting LLMs,
these methods either impact the text generation process or are limited in
white-box access to the suspect model, making them impractical. Hence, we
propose DuFFin, a novel $\textbf{Du}$al-Level $\textbf{Fin}$gerprinting
$\textbf{F}$ramework for black-box setting ownership verification. DuFFin
extracts the trigger pattern and the knowledge-level fingerprints to identify
the source of a suspect model. We conduct experiments on a variety of models
collected from the open-source website, including four popular base models as
protected LLMs and their fine-tuning, quantization, and safety alignment
versions, which are released by large companies, start-ups, and individual
users. Results show that our method can accurately verify the copyright of the
base protected LLM on their model variants, achieving the IP-ROC metric greater
than 0.95. Our code is available at
https://github.com/yuliangyan0807/llm-fingerprint.",2025-05-22,"Yuliang Yan, Haochun Tang, Shuo Yan, Enyan Dai",http://arxiv.org/pdf/2505.16530v1,cs.CL
EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance,"Small large language models (sLLMs) offer the advantage of being lightweight
and efficient, which makes them suitable for resource-constrained environments.
However, sLLMs often struggle to maintain topic consistency in task-oriented
dialogue systems, which is critical for scenarios such as service chatbots.
Specifically, it is important to ensure that the model denies off-topic or
malicious inputs and adheres to its intended functionality so as to prevent
potential misuse and uphold reliability. Towards this, existing activation
engineering approaches have been proposed to manipulate internal activations
during inference. While these methods are effective in certain scenarios, our
preliminary experiments reveal their limitations in ensuring topic adherence.
Therefore, to address this, we propose a novel approach termed Entropy-scaled
Steering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the
steering intensity based on input uncertainty, which allows the model to handle
off-topic distractors effectively while preserving on-topic accuracy. Our
experiments demonstrate that EnSToM achieves significant performance gain with
a relatively small data size compared to fine-tuning approaches. By improving
topic adherence without compromising efficiency, our approach provides a robust
solution for enhancing sLLM-based dialogue systems.",2025-05-22,"Heejae Suh, Yejin Jeon, Deokhyung Kang, Taehee Park, Yejin Min, Gary Geunbae Lee",http://arxiv.org/pdf/2505.16526v1,cs.CL
Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing,"Despite significant progress, recent studies have indicated that current
large language models (LLMs) may still utilize bias during inference, leading
to the poor generalizability of LLMs. Some benchmarks are proposed to
investigate the generalizability of LLMs, with each piece of data typically
containing one type of controlled bias. However, a single piece of data may
contain multiple types of biases in practical applications. To bridge this gap,
we propose a multi-bias benchmark where each piece of data contains five types
of biases. The evaluations conducted on this benchmark reveal that the
performance of existing LLMs and debiasing methods is unsatisfying,
highlighting the challenge of eliminating multiple types of biases
simultaneously. To overcome this challenge, we propose a causal effect
estimation-guided multi-bias elimination method (CMBE). This method first
estimates the causal effect of multiple types of biases simultaneously.
Subsequently, we eliminate the causal effect of biases from the total causal
effect exerted by both the semantic information and biases during inference.
Experimental results show that CMBE can effectively eliminate multiple types of
bias simultaneously to enhance the generalizability of LLMs.",2025-05-22,"Zhouhao Sun, Zhiyuan Kan, Xiao Ding, Li Du, Yang Zhao, Bing Qin, Ting Liu",http://arxiv.org/pdf/2505.16522v1,cs.CL
Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs,"Factual hallucinations are a major challenge for Large Language Models
(LLMs). They undermine reliability and user trust by generating inaccurate or
fabricated content. Recent studies suggest that when generating false
statements, the internal states of LLMs encode information about truthfulness.
However, these studies often rely on synthetic datasets that lack realism,
which limits generalization when evaluating the factual accuracy of text
generated by the model itself. In this paper, we challenge the findings of
previous work by investigating truthfulness encoding capabilities, leading to
the generation of a more realistic and challenging dataset. Specifically, we
extend previous work by introducing: (1) a strategy for sampling plausible
true-false factoid sentences from tabular data and (2) a procedure for
generating realistic, LLM-dependent true-false datasets from Question Answering
collections. Our analysis of two open-source LLMs reveals that while the
findings from previous studies are partially validated, generalization to
LLM-generated datasets remains challenging. This study lays the groundwork for
future research on factuality in LLMs and offers practical guidelines for more
effective evaluation.",2025-05-22,"Giovanni Servedio, Alessandro De Bellis, Dario Di Palma, Vito Walter Anelli, Tommaso Di Noia",http://arxiv.org/pdf/2505.16520v2,cs.CL
CUB: Benchmarking Context Utilisation Techniques for Language Models,"Incorporating external knowledge is crucial for knowledge-intensive tasks,
such as question answering and fact checking. However, language models (LMs)
may ignore relevant information that contradicts outdated parametric memory or
be distracted by irrelevant contexts. While many context utilisation
manipulation techniques (CMTs) that encourage or suppress context utilisation
have recently been proposed to alleviate these issues, few have seen systematic
comparison. In this paper, we develop CUB (Context Utilisation Benchmark) to
help practitioners within retrieval-augmented generation (RAG) identify the
best CMT for their needs. CUB allows for rigorous testing on three distinct
context types, observed to capture key challenges in realistic context
utilisation scenarios. With this benchmark, we evaluate seven state-of-the-art
methods, representative of the main categories of CMTs, across three diverse
datasets and tasks, applied to nine LMs. Our results show that most of the
existing CMTs struggle to handle the full set of types of contexts that may be
encountered in real-world retrieval-augmented scenarios. Moreover, we find that
many CMTs display an inflated performance on simple synthesised datasets,
compared to more realistic datasets with naturally occurring samples.
Altogether, our results show the need for holistic tests of CMTs and the
development of CMTs that can handle multiple context types.",2025-05-22,"Lovisa Hagström, Youna Kim, Haeun Yu, Sang-goo Lee, Richard Johansson, Hyunsoo Cho, Isabelle Augenstein",http://arxiv.org/pdf/2505.16518v1,cs.CL
AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios,"Recent advances in LegalAI have primarily focused on individual case judgment
analysis, often overlooking the critical appellate process within the judicial
system. Appeals serve as a core mechanism for error correction and ensuring
fair trials, making them highly significant both in practice and in research.
To address this gap, we present the AppealCase dataset, consisting of 10,000
pairs of real-world, matched first-instance and second-instance documents
across 91 categories of civil cases. The dataset also includes detailed
annotations along five dimensions central to appellate review: judgment
reversals, reversal reasons, cited legal provisions, claim-level decisions, and
whether there is new information in the second instance. Based on these
annotations, we propose five novel LegalAI tasks and conduct a comprehensive
evaluation across 20 mainstream models. Experimental results reveal that all
current models achieve less than 50% F1 scores on the judgment reversal
prediction task, highlighting the complexity and challenge of the appeal
scenario. We hope that the AppealCase dataset will spur further research in
LegalAI for appellate case analysis and contribute to improving consistency in
judicial decision-making.",2025-05-22,"Yuting Huang, Meitong Guo, Yiquan Wu, Ang Li, Xiaozhong Liu, Keting Yin, Changlong Sun, Fei Wu, Kun Kuang",http://arxiv.org/pdf/2505.16514v2,cs.CL
Sparse Activation Editing for Reliable Instruction Following in Narratives,"Complex narrative contexts often challenge language models' ability to follow
instructions, and existing benchmarks fail to capture these difficulties. To
address this, we propose Concise-SAE, a training-free framework that improves
instruction following by identifying and editing instruction-relevant neurons
using only natural language instructions, without requiring labelled data. To
thoroughly evaluate our method, we introduce FreeInstruct, a diverse and
realistic benchmark of 1,212 examples that highlights the challenges of
instruction following in narrative-rich settings. While initially motivated by
complex narratives, Concise-SAE demonstrates state-of-the-art instruction
adherence across varied tasks without compromising generation quality.",2025-05-22,"Runcong Zhao, Chengyu Cao, Qinglin Zhu, Xiucheng Lv, Shun Shao, Lin Gui, Ruifeng Xu, Yulan He",http://arxiv.org/pdf/2505.16505v1,cs.CL
LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing,"Large Language Models (LLMs) have rapidly become central to NLP,
demonstrating their ability to adapt to various tasks through prompting
techniques, including sentiment analysis. However, we still have a limited
understanding of how these models capture sentiment-related information. This
study probes the hidden layers of Llama models to pinpoint where sentiment
features are most represented and to assess how this affects sentiment
analysis.
  Using probe classifiers, we analyze sentiment encoding across layers and
scales, identifying the layers and pooling methods that best capture sentiment
signals. Our results show that sentiment information is most concentrated in
mid-layers for binary polarity tasks, with detection accuracy increasing up to
14% over prompting techniques. Additionally, we find that in decoder-only
models, the last token is not consistently the most informative for sentiment
encoding. Finally, this approach enables sentiment tasks to be performed with
memory requirements reduced by an average of 57%.
  These insights contribute to a broader understanding of sentiment in LLMs,
suggesting layer-specific probing as an effective approach for sentiment tasks
beyond prompting, with potential to enhance model utility and reduce memory
requirements.",2025-05-22,"Dario Di Palma, Alessandro De Bellis, Giovanni Servedio, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia",http://arxiv.org/pdf/2505.16491v1,cs.CL
Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions,"With the rise of different language model architecture, fine-tuning is
becoming even more important for down stream tasks Model gets messy, finding
proper hyperparameters for fine-tuning. Although BO has been tried for
hyperparameter tuning, most of the existing methods are oblivious to the fact
that BO relies on careful choices of acquisition functions, which are essential
components of BO that guide how much to explore versus exploit during the
optimization process; Different acquisition functions have different levels of
sensitivity towards training loss and validation performance; existing methods
often just apply an acquisition function no matter if the training and
validation performance are sensitive to the acquisition function or not. This
work introduces{Bilevel - BO - SWA}, a model fusion approach coupled with a
bilevel BO strategy to improve the fine - tunning of large language models. Our
work on mixture of acquisition functions like EI and UCB into nested opt loops,
where inner loop perform minimization of training loss while outer loops
optimized w.r.t. val metric. Experiments on GLUE tasks using RoBERTA - base
show that when using EI and UCB, there is an improvement in generalization, and
fine - tuning can be improved by up to 2.7%.",2025-05-22,"Zishuo Bao, Yibo Liu, Changyutao Qiu",http://arxiv.org/pdf/2505.17151v1,cs.CL
Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning,"Teaching large language models (LLMs) to be faithful in the provided context
is crucial for building reliable information-seeking systems. Therefore, we
propose a systematic framework, CANOE, to improve the faithfulness of LLMs in
both short-form and long-form generation tasks without human annotations.
Specifically, we first synthesize short-form question-answering (QA) data with
four diverse tasks to construct high-quality and easily verifiable training
data without human annotation. Also, we propose Dual-GRPO, a rule-based
reinforcement learning method that includes three tailored rule-based rewards
derived from synthesized short-form QA data, while simultaneously optimizing
both short-form and long-form response generation. Notably, Dual-GRPO
eliminates the need to manually label preference data to train reward models
and avoids over-optimizing short-form generation when relying only on the
synthesized short-form QA data. Experimental results show that CANOE greatly
improves the faithfulness of LLMs across 11 different downstream tasks, even
outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",2025-05-22,"Shuzheng Si, Haozhe Zhao, Cheng Gao, Yuzhuo Bai, Zhitong Wang, Bofei Gao, Kangyang Luo, Wenhao Li, Yufei Huang, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun",http://arxiv.org/pdf/2505.16483v1,cs.CL
Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering,"Document Visual Question Answering (DocVQA) faces dual challenges in
processing lengthy multimodal documents (text, images, tables) and performing
cross-modal reasoning. Current document retrieval-augmented generation (DocRAG)
methods remain limited by their text-centric approaches, frequently missing
critical visual information. The field also lacks robust benchmarks for
assessing multimodal evidence selection and integration. We introduce MMDocRAG,
a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with
multi-page, cross-modal evidence chains. Our framework introduces innovative
metrics for evaluating multimodal quote selection and enables answers that
interleave text with relevant visual elements. Through large-scale experiments
with 60 VLM/LLM models and 14 retrieval systems, we identify persistent
challenges in multimodal evidence retrieval, selection, and integration.Key
findings reveal advanced proprietary LVMs show superior performance than
open-sourced alternatives. Also, they show moderate advantages using multimodal
inputs over text-only inputs, while open-source alternatives show significant
performance degradation. Notably, fine-tuned LLMs achieve substantial
improvements when using detailed image descriptions. MMDocRAG establishes a
rigorous testing ground and provides actionable insights for developing more
robust multimodal DocVQA systems. Our benchmark and code are available at
https://mmdocrag.github.io/MMDocRAG/.",2025-05-22,"Kuicai Dong, Yujing Chang, Shijie Huang, Yasheng Wang, Ruiming Tang, Yong Liu",http://arxiv.org/pdf/2505.16470v1,cs.CL
Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization,"Generative Large Language Models (LLMs) infer user's demographic information
from subtle cues in the conversation -- a phenomenon called implicit
personalization. Prior work has shown that such inferences can lead to lower
quality responses for users assumed to be from minority groups, even when no
demographic information is explicitly provided. In this work, we systematically
explore how LLMs respond to stereotypical cues using controlled synthetic
conversations, by analyzing the models' latent user representations through
both model internals and generated answers to targeted user questions. Our
findings reveal that LLMs do infer demographic attributes based on these
stereotypical signals, which for a number of groups even persists when the user
explicitly identifies with a different demographic group. Finally, we show that
this form of stereotype-driven implicit personalization can be effectively
mitigated by intervening on the model's internal representations using a
trained linear probe to steer them toward the explicitly stated identity. Our
results highlight the need for greater transparency and control in how LLMs
represent user identity.",2025-05-22,"Vera Neplenbroek, Arianna Bisazza, Raquel Fernández",http://arxiv.org/pdf/2505.16467v1,cs.CL
University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection,"This paper presents our approach for SemEval 2025 Task 11 Track A, focusing
on multilabel emotion classification across 28 languages. We explore two main
strategies: fully fine-tuning transformer models and classifier-only training,
evaluating different settings such as fine-tuning strategies, model
architectures, loss functions, encoders, and classifiers. Our findings suggest
that training a classifier on top of prompt-based encoders such as mE5 and BGE
yields significantly better results than fully fine-tuning XLMR and mBERT. Our
best-performing model on the final leaderboard is an ensemble combining
multiple BGE models, where CatBoost serves as the classifier, with different
configurations. This ensemble achieves an average F1-macro score of 56.58
across all languages.",2025-05-22,"Ikhlasul Akmal Hanif, Eryawan Presma Yulianrifat, Jaycent Gunawan Ongris, Eduardus Tjitrahardja, Muhammad Falensi Azmi, Rahmat Bryan Naufal, Alfan Farizki Wicaksono",http://arxiv.org/pdf/2505.16460v1,cs.CL
Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems,"Evaluating and iterating upon recommender systems is crucial, yet traditional
A/B testing is resource-intensive, and offline methods struggle with dynamic
user-platform interactions. While agent-based simulation is promising, existing
platforms often lack a mechanism for user actions to dynamically reshape the
environment. To bridge this gap, we introduce RecInter, a novel agent-based
simulation platform for recommender systems featuring a robust interaction
mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,
purchases) dynamically update item attributes in real-time, and introduced
Merchant Agents can reply, fostering a more realistic and evolving ecosystem.
High-fidelity simulation is ensured through Multidimensional User Profiling
module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought
(CoT) enriched interaction data. Our platform achieves significantly improved
simulation credibility and successfully replicates emergent phenomena like
Brand Loyalty and the Matthew Effect. Experiments demonstrate that this
interaction mechanism is pivotal for simulating realistic system evolution,
establishing our platform as a credible testbed for recommender systems
research.",2025-05-22,"Song Jin, Juntian Zhang, Yuhan Liu, Xun Zhang, Yufei Zhang, Guojun Yin, Fei Jiang, Wei Lin, Rui Yan",http://arxiv.org/pdf/2505.16429v1,cs.CL
$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion,"The effective communication of procedural knowledge remains a significant
challenge in natural language processing (NLP), as purely textual instructions
often fail to convey complex physical actions and spatial relationships. We
address this limitation by proposing a language-driven framework that
translates procedural text into coherent visual instructions. Our approach
models the linguistic structure of instructional content by decomposing it into
goal statements and sequential steps, then conditioning visual generation on
these linguistic elements. We introduce three key innovations: (1) a
constituency parser-based text encoding mechanism that preserves semantic
completeness even with lengthy instructions, (2) a pairwise discourse coherence
model that maintains consistency across instruction sequences, and (3) a novel
evaluation protocol specifically designed for procedural language-to-image
alignment. Our experiments across three instructional datasets (HTStep,
CaptainCook4D, and WikiAll) demonstrate that our method significantly
outperforms existing baselines in generating visuals that accurately reflect
the linguistic content and sequential nature of instructions. This work
contributes to the growing body of research on grounding procedural language in
visual content, with applications spanning education, task guidance, and
multimodal language understanding.",2025-05-22,"Jing Bi, Pinxin Liu, Ali Vosoughi, Jiarui Wu, Jinxi He, Chenliang Xu",http://arxiv.org/pdf/2505.16425v1,cs.CL
WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning,"While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.",2025-05-22,"Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, Lihong Li",http://arxiv.org/pdf/2505.16421v1,cs.CL
Exploring the Relationship Between Diversity and Quality in Ad Text Generation,"In natural language generation for advertising, creating diverse and engaging
ad texts is crucial for capturing a broad audience and avoiding advertising
fatigue. Regardless of the importance of diversity, the impact of the
diversity-enhancing methods in ad text generation -- mainly tested on tasks
such as summarization and machine translation -- has not been thoroughly
explored. Ad text generation significantly differs from these tasks owing to
the text style and requirements. This research explores the relationship
between diversity and ad quality in ad text generation by considering multiple
factors, such as diversity-enhancing methods, their hyperparameters,
input-output formats, and the models.",2025-05-22,"Yoichi Aoki, Soichiro Murakami, Ukyo Honda, Akihiko Kato",http://arxiv.org/pdf/2505.16418v1,cs.CL
Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)
combined with external contexts to enhance the accuracy and reliability of
generated responses. However, reliably attributing generated content to
specific context segments, context attribution, remains challenging due to the
computationally intensive nature of current methods, which often require
extensive fine-tuning or human annotation. In this work, we introduce a novel
Jensen-Shannon Divergence driven method to Attribute Response to Context
(ARC-JSD), enabling efficient and accurate identification of essential context
sentences without additional fine-tuning or surrogate modelling. Evaluations on
a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using
instruction-tuned LLMs in different scales demonstrate superior accuracy and
significant computational efficiency improvements compared to the previous
surrogate-based method. Furthermore, our mechanistic analysis reveals specific
attention heads and multilayer perceptron (MLP) layers responsible for context
attribution, providing valuable insights into the internal workings of RAG
models.",2025-05-22,"Ruizhe Li, Chen Chen, Yuchen Hu, Yanjun Gao, Xi Wang, Emine Yilmaz",http://arxiv.org/pdf/2505.16415v1,cs.CL
Large Language Models for Predictive Analysis: How Far Are They?,"Predictive analysis is a cornerstone of modern decision-making, with
applications in various domains. Large Language Models (LLMs) have emerged as
powerful tools in enabling nuanced, knowledge-intensive conversations, thus
aiding in complex decision-making tasks. With the burgeoning expectation to
harness LLMs for predictive analysis, there is an urgent need to systematically
assess their capability in this domain. However, there is a lack of relevant
evaluations in existing studies. To bridge this gap, we introduce the
\textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive
analysis queries originating from 44 real-world datasets of 8 diverse fields.
We design an evaluation protocol considering text analysis, code generation,
and their alignment. Twelve renowned LLMs are evaluated, offering insights into
their practical use in predictive analysis. Generally, we believe that existing
LLMs still face considerable challenges in conducting predictive analysis. See
\href{https://github.com/Cqkkkkkk/PredictiQ}{Github}.",2025-05-22,"Qin Chen, Yuanyi Ren, Xiaojun Ma, Yuyang Shi",http://arxiv.org/pdf/2505.17149v1,cs.CL
Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning,"Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.",2025-05-22,"Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, Ji-Rong Wen",http://arxiv.org/pdf/2505.16410v1,cs.CL
From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs,"Adapting cultural values in Large Language Models (LLMs) presents significant
challenges, particularly due to biases and limited training data. Prior work
primarily aligns LLMs with different cultural values using World Values Survey
(WVS) data. However, it remains unclear whether this approach effectively
captures cultural nuances or produces distinct cultural representations for
various downstream tasks. In this paper, we systematically investigate
WVS-based training for cultural value adaptation and find that relying solely
on survey data can homogenize cultural norms and interfere with factual
knowledge. To investigate these issues, we augment WVS with encyclopedic and
scenario-based cultural narratives from Wikipedia and NormAd. While these
narratives may have variable effects on downstream tasks, they consistently
improve cultural distinctiveness than survey data alone. Our work highlights
the inherent complexity of aligning cultural values with the goal of guiding
task-specific behavior.",2025-05-22,"Muhammad Farid Adilazuarda, Chen Cecilia Liu, Iryna Gurevych, Alham Fikri Aji",http://arxiv.org/pdf/2505.16408v1,cs.CL
On the reliability of feature attribution methods for speech classification,"As the capabilities of large-scale pre-trained models evolve, understanding
the determinants of their outputs becomes more important. Feature attribution
aims to reveal which parts of the input elements contribute the most to model
outputs. In speech processing, the unique characteristics of the input signal
make the application of feature attribution methods challenging. We study how
factors such as input type and aggregation and perturbation timespan impact the
reliability of standard feature attribution methods, and how these factors
interact with characteristics of each classification task. We find that
standard approaches to feature attribution are generally unreliable when
applied to the speech domain, with the exception of word-aligned perturbation
methods when applied to word-based classification tasks.",2025-05-22,"Gaofei Shen, Hosein Mohebbi, Arianna Bisazza, Afra Alishahi, Grzegorz Chrupała",http://arxiv.org/pdf/2505.16406v1,cs.CL
AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning,"Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.",2025-05-22,"Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",http://arxiv.org/pdf/2505.16400v1,cs.CL
Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection,"The general public often encounters complex texts but does not have the time
or expertise to fully understand them, leading to the spread of misinformation.
Automatic Text Simplification (ATS) helps make information more accessible, but
its evaluation methods have not kept up with advances in text generation,
especially with Large Language Models (LLMs). In particular, recent studies
have shown that current ATS metrics do not correlate with the presence of
errors. Manual inspections have further revealed a variety of errors,
underscoring the need for a more nuanced evaluation framework, which is
currently lacking. This resource paper addresses this gap by introducing a test
collection for detecting and classifying errors in simplified texts. First, we
propose a taxonomy of errors, with a formal focus on information distortion.
Next, we introduce a parallel dataset of automatically simplified scientific
texts. This dataset has been human-annotated with labels based on our proposed
taxonomy. Finally, we analyze the quality of the dataset, and we study the
performance of existing models to detect and classify errors from that
taxonomy. These contributions give researchers the tools to better evaluate
errors in ATS, develop more reliable models, and ultimately improve the quality
of automatically simplified texts.",2025-05-22,"Benjamin Vendeville, Liana Ermakova, Pierre De Loor",http://arxiv.org/pdf/2505.16392v1,cs.CL
Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models,"Large language models (LLMs) demonstrate remarkable ability in cross-lingual
tasks. Understanding how LLMs acquire this ability is crucial for their
interpretability. To quantify the cross-lingual ability of LLMs accurately, we
propose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn
cross-lingual ability, we trace the outputs of LLMs' intermediate layers in the
word translation task. We identify and distinguish two distinct behaviors in
the forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.
We attribute LLMs' two distinct behaviors to the co-occurrence frequency of
words and find the semantic pivot from the pre-training dataset. Finally, to
apply our findings to improve the cross-lingual ability of LLMs, we reconstruct
a semantic pivot-aware pre-training dataset using documents with a high
proportion of semantic pivots. Our experiments validate the effectiveness of
our approach in enhancing cross-lingual ability. Our research contributes
insights into the interpretability of LLMs and offers a method for improving
LLMs' cross-lingual ability.",2025-05-22,"Kaiyu He, Tong Zhou, Yubo Chen, Delai Qiu, Shengping Liu, Kang Liu, Jun Zhao",http://arxiv.org/pdf/2505.16385v1,cs.CL
PaTH Attention: Position Encoding via Accumulating Householder Transformations,"The attention mechanism is a core primitive in modern large language models
(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,
position encoding is essential for modeling structured domains such as
language. Rotary position encoding (RoPE) has emerged as the de facto standard
approach for position encoding and is part of many modern LLMs. However, in
RoPE the key/query transformation between two elements in a sequence is only a
function of their relative position and otherwise independent of the actual
input. This limits the expressivity of RoPE-based transformers.
  This paper describes PaTH, a flexible data-dependent position encoding scheme
based on accumulated products of Householder(like) transformations, where each
transformation is data-dependent, i.e., a function of the input. We derive an
efficient parallel algorithm for training through exploiting a compact
representation of products of Householder matrices, and implement a
FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both
targeted synthetic benchmarks and moderate-scale real-world language modeling
experiments, we find that PaTH demonstrates superior performance compared to
RoPE and other recent baselines.",2025-05-22,"Songlin Yang, Yikang Shen, Kaiyue Wen, Shawn Tan, Mayank Mishra, Liliang Ren, Rameswar Panda, Yoon Kim",http://arxiv.org/pdf/2505.16381v1,cs.CL
"Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization","The exponential growth of scientific publications has made it increasingly
difficult for researchers to stay updated and synthesize knowledge effectively.
This paper presents XSum, a modular pipeline for multi-document summarization
(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The
pipeline includes two core components: a question-generation module and an
editor module. The question-generation module dynamically generates questions
adapted to the input papers, ensuring the retrieval of relevant and accurate
information. The editor module synthesizes the retrieved content into coherent
and well-structured summaries that adhere to academic standards for proper
citation. Evaluated on the SurveySum dataset, XSum demonstrates strong
performance, achieving considerable improvements in metrics such as CheckEval,
G-Eval and Ref-F1 compared to existing approaches. This work provides a
transparent, adaptable framework for scientific summarization with potential
applications in a wide range of domains. Code available at
https://github.com/webis-de/scolia25-xsum",2025-05-22,"Pierre Achkar, Tim Gollub, Martin Potthast",http://arxiv.org/pdf/2505.16349v1,cs.CL
Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance,"Embodied agents empowered by large language models (LLMs) have shown strong
performance in household object rearrangement tasks. However, these tasks
primarily focus on single-turn interactions with simplified instructions, which
do not truly reflect the challenges of providing meaningful assistance to
users. To provide personalized assistance, embodied agents must understand the
unique semantics that users assign to the physical world (e.g., favorite cup,
breakfast routine) by leveraging prior interaction history to interpret
dynamic, real-world instructions. Yet, the effectiveness of embodied agents in
utilizing memory for personalized assistance remains largely underexplored. To
address this gap, we present MEMENTO, a personalized embodied agent evaluation
framework designed to comprehensively assess memory utilization capabilities to
provide personalized assistance. Our framework consists of a two-stage memory
evaluation process design that enables quantifying the impact of memory
utilization on task performance. This process enables the evaluation of agents'
understanding of personalized knowledge in object rearrangement tasks by
focusing on its role in goal interpretation: (1) the ability to identify target
objects based on personal meaning (object semantics), and (2) the ability to
infer object-location configurations from consistent user patterns, such as
routines (user patterns). Our experiments across various LLMs reveal
significant limitations in memory utilization, with even frontier models like
GPT-4o experiencing a 30.5% performance drop when required to reference
multiple memories, particularly in tasks involving user patterns. These
findings, along with our detailed analyses and case studies, provide valuable
insights for future research in developing more effective personalized embodied
agents. Project website: https://connoriginal.github.io/MEMENTO",2025-05-22,"Taeyoon Kwon, Dongwook Choi, Sunghwan Kim, Hyojun Kim, Seungjun Moon, Beong-woo Kwak, Kuan-Hao Huang, Jinyoung Yeo",http://arxiv.org/pdf/2505.16348v1,cs.CL
SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers,"Novelty is a core component of academic papers, and there are multiple
perspectives on the assessment of novelty. Existing methods often focus on word
or entity combinations, which provide limited insights. The content related to
a paper's novelty is typically distributed across different core sections,
e.g., Introduction, Methodology and Results. Therefore, exploring the optimal
combination of sections for evaluating the novelty of a paper is important for
advancing automated novelty assessment. In this paper, we utilize different
combinations of sections from academic papers as inputs to drive language
models to predict novelty scores. We then analyze the results to determine the
optimal section combinations for novelty score prediction. We first employ
natural language processing techniques to identify the sectional structure of
academic papers, categorizing them into introduction, methods, results, and
discussion (IMRaD). Subsequently, we used different combinations of these
sections (e.g., introduction and methods) as inputs for pretrained language
models (PLMs) and large language models (LLMs), employing novelty scores
provided by human expert reviewers as ground truth labels to obtain prediction
results. The results indicate that using introduction, results and discussion
is most appropriate for assessing the novelty of a paper, while the use of the
entire text does not yield significant results. Furthermore, based on the
results of the PLMs and LLMs, the introduction and results appear to be the
most important section for the task of novelty score prediction. The code and
dataset for this paper can be accessed at
https://github.com/njust-winchy/SC4ANM.",2025-05-22,"Wenqing Wu, Chengzhi Zhang, Tong Bao, Yi Zhao",http://arxiv.org/pdf/2505.16330v1,cs.CL
CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation,"Existing metrics often lack the granularity and interpretability to capture
nuanced clinical differences between candidate and ground-truth radiology
reports, resulting in suboptimal evaluation. We introduce a Clinically-grounded
tabular framework with Expert-curated labels and Attribute-level comparison for
Radiology report evaluation (CLEAR). CLEAR not only examines whether a report
can accurately identify the presence or absence of medical conditions, but also
assesses whether it can precisely describe each positively identified condition
across five key attributes: first occurrence, change, severity, descriptive
location, and recommendation. Compared to prior works, CLEAR's
multi-dimensional, attribute-level outputs enable a more comprehensive and
clinically interpretable evaluation of report quality. Additionally, to measure
the clinical alignment of CLEAR, we collaborate with five board-certified
radiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from
MIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.
Our experiments show that CLEAR achieves high accuracy in extracting clinical
attributes and provides automated metrics that are strongly aligned with
clinical judgment.",2025-05-22,"Yuyang Jiang, Chacha Chen, Shengyuan Wang, Feng Li, Zecong Tang, Benjamin M. Mervak, Lydia Chelala, Christopher M Straus, Reve Chahine, Samuel G. Armato III, Chenhao Tan",http://arxiv.org/pdf/2505.16325v1,cs.CL
MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models,"The widespread use of Large Multimodal Models (LMMs) has raised concerns
about model toxicity. However, current research mainly focuses on explicit
toxicity, with less attention to some more implicit toxicity regarding
prejudice and discrimination. To address this limitation, we introduce a
subtler type of toxicity named dual-implicit toxicity and a novel toxicity
benchmark termed MDIT-Bench: Multimodal Dual-Implicit Toxicity Benchmark.
Specifically, we first create the MDIT-Dataset with dual-implicit toxicity
using the proposed Multi-stage Human-in-loop In-context Generation method.
Based on this dataset, we construct the MDIT-Bench, a benchmark for evaluating
the sensitivity of models to dual-implicit toxicity, with 317,638 questions
covering 12 categories, 23 subcategories, and 780 topics. MDIT-Bench includes
three difficulty levels, and we propose a metric to measure the toxicity gap
exhibited by the model across them. In the experiment, we conducted MDIT-Bench
on 13 prominent LMMs, and the results show that these LMMs cannot handle
dual-implicit toxicity effectively. The model's performance drops significantly
in hard level, revealing that these LMMs still contain a significant amount of
hidden but activatable toxicity. Data are available at
https://github.com/nuo1nuo/MDIT-Bench.",2025-05-22,"Bohan Jin, Shuhan Qi, Kehai Chen, Xinyi Guo, Xuan Wang",http://arxiv.org/pdf/2505.17144v1,cs.CL
AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners,"Self-Taught Reasoners (STaR), synonymously known as Rejection sampling
Fine-Tuning (RFT), is an integral part of the training pipeline of
self-improving reasoning Language Models (LMs). The self-improving mechanism
often employs random observation (data) sampling. However, this results in
trained observation imbalance; inefficiently over-training on solved examples
while under-training on challenging ones. In response, we introduce Adaptive
STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two
adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting
balanced training across observations, and (2) Adaptive Sampling for
Curriculum: dynamically adjusting data difficulty to match the model's evolving
strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all
instances (6/6) and reduces training FLOPs by an average of 58.6% against an
extensive list of baselines. These improvements in performance and efficiency
generalize to different pre-trained LMs and larger models, paving the way for
more efficient and effective self-improving LMs.",2025-05-22,"Woosung Koh, Wonbeen Oh, Jaein Jang, MinHyung Lee, Hyeongjin Kim, Ah Yeon Kim, Joonkee Kim, Junghyun Lee, Taehyeon Kim, Se-Young Yun",http://arxiv.org/pdf/2505.16322v1,cs.CL
Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning,"Large reasoning models (LRMs) have demonstrated strong performance on complex
reasoning tasks, but often suffer from overthinking, generating redundant
content regardless of task difficulty. Inspired by the dual process theory in
cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a
reinforcement learning framework that enables LRMs to achieve efficient
reasoning through adaptive cognitive allocation and dynamic system switch. ACPO
incorporates two key components: (1) introducing system-aware reasoning tokens
to explicitly represent the thinking modes thereby making the model's cognitive
process transparent, and (2) integrating online difficulty estimation and token
length budget to guide adaptive system switch and reasoning during
reinforcement learning. To this end, we propose a two-stage training strategy.
The first stage begins with supervised fine-tuning to cold start the model,
enabling it to generate reasoning paths with explicit thinking modes. In the
second stage, we apply ACPO to further enhance adaptive system switch for
difficulty-aware reasoning. Experimental results demonstrate that ACPO
effectively reduces redundant reasoning while adaptively adjusting cognitive
allocation based on task complexity, achieving efficient hybrid reasoning.",2025-05-22,"Xiaoxue Cheng, Junyi Li, Zhenduo Zhang, Xinyu Tang, Wayne Xin Zhao, Xinyu Kong, Zhiqiang Zhang",http://arxiv.org/pdf/2505.16315v2,cs.CL
EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning,"Large Language Models (LLMs) excel at complex reasoning through search
algorithms, yet current strategies often suffer from massive token consumption
due to redundant exploration of semantically equivalent steps. Existing
semantic similarity methods struggle to accurately identify such equivalence in
domain-specific contexts like mathematical reasoning. To address this, we
propose EquivPruner, a simple yet effective approach that identifies and prunes
semantically equivalent actions during LLM reasoning search. We also introduce
MathEquiv, the first dataset we created for mathematical statement equivalence,
which enables the training of a lightweight equivalence detector. Extensive
experiments across various models and tasks demonstrate that EquivPruner
significantly reduces token consumption, improving searching efficiency and
often bolstering reasoning accuracy. For instance, when applied to
Qwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by
48.1\% while also improving accuracy. Our code is available at
https://github.com/Lolo1222/EquivPruner.",2025-05-22,"Jiawei Liu, Qisi Chen, Jianshu Zhang, Quan Liu, Defu Lian",http://arxiv.org/pdf/2505.16312v1,cs.CL
PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models,"Prompt optimization offers a practical and broadly applicable alternative to
fine-tuning for improving large language model (LLM) performance. However,
existing methods often rely on costly output generation, self-critiquing
abilities, or human-annotated preferences, which limit their scalability,
especially for smaller or non-instruction-tuned models. We introduce PMPO
(Probabilistic Metric Prompt Optimization), a unified framework that refines
prompts using token-level cross-entropy loss as a direct, lightweight
evaluation signal. PMPO identifies low-quality prompt segments by masking and
measuring their impact on loss, then rewrites and selects improved variants by
minimizing loss over positive and negative examples. Unlike prior methods, it
requires no output sampling or human evaluation during optimization, relying
only on forward passes and log-likelihoods. PMPO supports both supervised and
preference-based tasks through a closely aligned loss-based evaluation
strategy. Experiments show that PMPO consistently outperforms prior methods
across model sizes and tasks: it achieves the highest average accuracy on BBH,
performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates
by over 19 points. These results highlight PMPO's effectiveness, efficiency,
and broad applicability.",2025-05-22,"Chenzhuo Zhao, Ziqian Liu, Xingda Wang, Junting Lu, Chaoyi Ruan",http://arxiv.org/pdf/2505.16307v1,cs.CL
INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling,"Large Language Model (LLM) routing is a pivotal technique for navigating a
diverse landscape of LLMs, aiming to select the best-performing LLMs tailored
to the domains of user queries, while managing computational resources.
However, current routing approaches often face limitations in scalability when
dealing with a large pool of specialized LLMs, or in their adaptability to
extending model scope and evolving capability domains. To overcome those
challenges, we propose InferenceDynamics, a flexible and scalable
multi-dimensional routing framework by modeling the capability and knowledge of
models. We operate it on our comprehensive dataset RouteMix, and demonstrate
its effectiveness and generalizability in group-level routing using modern
benchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its
ability to identify and leverage top-performing models for given tasks, leading
to superior outcomes with efficient resource utilization. The broader adoption
of Inference Dynamics can empower users to harness the full specialized
potential of the LLM ecosystem, and our code will be made publicly available to
encourage further research.",2025-05-22,"Haochen Shi, Tianshi Zheng, Weiqi Wang, Baixuan Xu, Chunyang Li, Chunkit Chan, Tao Fan, Yangqiu Song, Qiang Yang",http://arxiv.org/pdf/2505.16303v1,cs.CL
ToDi: Token-wise Distillation via Fine-Grained Divergence Control,"Large language models (LLMs) offer impressive performance but are impractical
for resource-constrained deployment due to high latency and energy consumption.
Knowledge distillation (KD) addresses this by transferring knowledge from a
large teacher to a smaller student model. However, conventional KD, notably
approaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence
loss across the entire vocabulary, neglecting token-level prediction
discrepancies. By investigating these representative divergences via gradient
analysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses
overestimated ones, showing their complementary roles. Based on this
observation, we propose Token-wise Distillation (ToDi), a novel method that
adaptively combines FKL and RKL per token using a sigmoid-based weighting
function derived from the teacher-student probability log-ratio. ToDi
dynamically emphasizes the appropriate divergence for each token, enabling
precise distribution alignment. We demonstrate that ToDi consistently
outperforms recent distillation baselines using uniform or less granular
strategies across instruction-following benchmarks. Extensive ablation studies
and efficiency analysis further validate ToDi's effectiveness and practicality.",2025-05-22,"Seongryong Jung, Suwan Yoon, DongGeon Kim, Hwanhee Lee",http://arxiv.org/pdf/2505.16297v1,cs.CL
Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs,"As the knowledge of large language models (LLMs) becomes outdated over time,
there is a growing need for efficient methods to update them, especially when
injecting proprietary information. Our study reveals that
comprehension-intensive fine-tuning tasks (e.g., question answering and blanks)
achieve substantially higher knowledge retention rates (48%) compared to
mapping-oriented tasks like translation (17%) or text-to-JSON conversion (20%),
despite exposure to identical factual content. We demonstrate that this pattern
persists across model architectures and follows scaling laws, with larger
models showing improved retention across all task types. However, all models
exhibit significant performance drops when applying injected knowledge in
broader contexts, suggesting limited semantic integration. These findings show
the importance of task selection in updating LLM knowledge, showing that
effective knowledge injection relies not just on data exposure but on the depth
of cognitive engagement during fine-tuning.",2025-05-22,"Essa Jan, Moiz Ali, Muhammad Saram Hassan, Fareed Zaffar, Yasir Zaki",http://arxiv.org/pdf/2505.17140v1,cs.CL
EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models,"Advancements in Large Language Models (LLMs) drive interest in scientific
applications, necessitating specialized benchmarks such as Earth science.
Existing benchmarks either present a general science focus devoid of Earth
science specificity or cover isolated subdomains, lacking holistic evaluation.
Furthermore, current benchmarks typically neglect the assessment of LLMs'
capabilities in open-ended scientific exploration. In this paper, we present a
comprehensive and professional benchmark for the Earth sciences, designed to
evaluate the capabilities of LLMs in scientific exploration within this domain,
spanning from fundamental to advanced levels. Leveraging a corpus of 100,000
research papers, we first construct two Question Answering (QA) datasets:
Earth-Iron, which offers extensive question coverage for broad assessment, and
Earth-Silver, which features a higher level of difficulty to evaluate
professional depth. These datasets encompass five Earth spheres, 114
disciplines, and 11 task categories, assessing foundational knowledge crucial
for scientific exploration. Most notably, we introduce Earth-Gold with new
metrics, a dataset comprising open-ended multi-turn dialogues specifically
designed to evaluate the advanced capabilities of LLMs in scientific
exploration, including methodology induction, limitation analysis, and concept
proposal. Extensive experiments reveal limitations in 11 leading LLMs across
different domains and tasks, highlighting considerable room for improvement in
their scientific exploration capabilities. The benchmark is available on
https://huggingface.co/ai-earth .",2025-05-22,"Wanghan Xu, Xiangyu Zhao, Yuhao Zhou, Xiaoyu Yue, Ben Fei, Fenghua Ling, Wenlong Zhang, Lei Bai",http://arxiv.org/pdf/2505.17139v1,cs.CL
Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA,"Iterative RAG for multi-hop question answering faces challenges with lengthy
contexts and the buildup of irrelevant information. This hinders a model's
capacity to process and reason over retrieved content and limits performance.
While recent methods focus on compressing retrieved information, they are
either restricted to single-round RAG, require finetuning or lack scalability
in iterative RAG. To address these challenges, we propose Notes Writing, a
method that generates concise and relevant notes from retrieved documents at
each step, thereby reducing noise and retaining only essential information.
This indirectly increases the effective context length of Large Language Models
(LLMs), enabling them to reason and plan more effectively while processing
larger volumes of input text. Notes Writing is framework agnostic and can be
integrated with different iterative RAG methods. We demonstrate its
effectiveness with three iterative RAG methods, across two models and four
evaluation datasets. Notes writing yields an average improvement of 15.6
percentage points overall, with minimal increase in output tokens.",2025-05-22,"Rishabh Maheshwary, Masoud Hashemi, Khyati Mahajan, Shiva Krishna Reddy Malay, Sai Rajeswar, Sathwik Tejaswi Madhusudhan, Spandana Gella, Vikas Yadav",http://arxiv.org/pdf/2505.16293v1,cs.CL
HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation,"The advancement of Large Language Models (LLMs) enables flexible and
interpretable automatic evaluations. In the field of machine translation
evaluation, utilizing LLMs with translation error annotations based on
Multidimensional Quality Metrics (MQM) yields more human-aligned judgments.
However, current LLM-based evaluation methods still face challenges in
accurately identifying error spans and assessing their severity. In this paper,
we propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation
Evaluation. We argue that existing approaches inadequately exploit the
fine-grained structural and semantic information within the MQM hierarchy. To
address this, we develop a hierarchical multi-agent system grounded in the MQM
error typology, enabling granular evaluation of subtype errors. Two key
strategies are incorporated to further mitigate systemic hallucinations within
the framework: the utilization of the model's self-reflection capability and
the facilitation of agent discussion involving asymmetric information.
Empirically, HiMATE outperforms competitive baselines across different datasets
in conducting human-aligned evaluations. Further analyses underscore its
significant advantage in error span detection and severity assessment,
achieving an average F1-score improvement of 89% over the best-performing
baseline. We make our code and data publicly available at
https://anonymous.4open.science/r/HiMATE-Anony.",2025-05-22,"Shijie Zhang, Renhao Li, Songsheng Wang, Philipp Koehn, Min Yang, Derek F. Wong",http://arxiv.org/pdf/2505.16281v1,cs.CL
Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility,"The achievements of Large Language Models in Natural Language Processing,
especially for high-resource languages, call for a better understanding of
their characteristics from a cognitive perspective. Researchers have attempted
to evaluate artificial models by testing their ability to predict behavioral
(e.g., eye-tracking fixations) and physiological (e.g., brain responses)
variables during language processing (e.g., reading/listening). In this paper,
we propose using spontaneous speech corpora to derive production variables
(speech reductions, prosodic prominences) and applying them in a similar
fashion. More precisely, we extract. We then test models trained with a
standard procedure on different pretraining datasets (written, spoken, and
mixed genres) for their ability to predict these two variables. Our results
show that, after some fine-tuning, the models can predict these production
variables well above baselines. We also observe that spoken genre training data
provides more accurate predictions than written genres. These results
contribute to the broader effort of using high-quality speech corpora as
benchmarks for LLMs.",2025-05-22,"Sheng-Fu Wang, Laurent Prevot, Jou-an Chi, Ri-Sheng Huang, Shu-Kai Hsieh",http://arxiv.org/pdf/2505.16277v1,cs.CL
How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance,"When using Large Language Models (LLMs) to support Knowledge Graph
Engineering (KGE), one of the first indications when searching for an
appropriate model is its size. According to the scaling laws, larger models
typically show higher capabilities. However, in practice, resource costs are
also an important factor and thus it makes sense to consider the ratio between
model performance and costs. The LLM-KG-Bench framework enables the comparison
of LLMs in the context of KGE tasks and assesses their capabilities of
understanding and producing KGs and KG queries. Based on a dataset created in
an LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the
model size scaling laws specific to KGE tasks. In our analyses, we assess how
benchmark scores evolve between different model size categories. Additionally,
we inspect how the general score development of single models and families of
models correlates to their size. Our analyses revealed that, with a few
exceptions, the model size scaling laws generally also apply to the selected
KGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,
the task performance did not change much between a model and the next larger
model. In these cases, smaller models could be considered to achieve high
cost-effectiveness. Regarding models of the same family, sometimes larger
models performed worse than smaller models of the same family. These effects
occurred only locally. Hence it is advisable to additionally test the next
smallest and largest model of the same family.",2025-05-22,"Desiree Heim, Lars-Peter Meyer, Markus Schröder, Johannes Frey, Andreas Dengel",http://arxiv.org/pdf/2505.16276v1,cs.CL
Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning,"Large language models are typically adapted to downstream tasks through
supervised fine-tuning on domain-specific data. While standard fine-tuning
focuses on minimizing generation loss to optimize model parameters, we take a
deeper step by retaining and leveraging the model's own learning signals,
analogous to how human learners reflect on past mistakes to improve future
performance. We first introduce the concept of Mistake Log to systematically
track the model's learning behavior and recurring errors throughout
fine-tuning. Treating the original transformer-based model as the Pilot, we
correspondingly design a Copilot model to refine the Pilot's inference
performance via logits rectification. We name the overall Pilot-Copilot
framework the Transformer Copilot, which introduces (i) a novel Copilot model
design, (ii) a joint training paradigm where the Copilot continuously learns
from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference
paradigm where the Copilot rectifies the Pilot's logits for enhanced
generation. We provide both theoretical and empirical analyses on our new
learning framework. Experiments on 12 benchmarks spanning commonsense,
arithmetic, and recommendation tasks demonstrate that Transformer Copilot
consistently improves performance by up to 34.5%, while introducing marginal
computational overhead to Pilot models and exhibiting strong scalability and
transferability.",2025-05-22,"Jiaru Zou, Yikun Ban, Zihao Li, Yunzhe Qi, Ruizhong Qiu, Ling Yang, Jingrui He",http://arxiv.org/pdf/2505.16270v1,cs.CL
"All You Need is ""Leet"": Evading Hate-speech Detection AI","Social media and online forums are increasingly becoming popular.
Unfortunately, these platforms are being used for spreading hate speech. In
this paper, we design black-box techniques to protect users from hate-speech on
online platforms by generating perturbations that can fool state of the art
deep learning based hate speech detection models thereby decreasing their
efficiency. We also ensure a minimal change in the original meaning of
hate-speech. Our best perturbation attack is successfully able to evade
hate-speech detection for 86.8 % of hateful text.",2025-05-22,"Sampanna Yashwant Kahu, Naman Ahuja",http://arxiv.org/pdf/2505.16263v1,cs.CL
IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection,"Interpreting figurative language such as sarcasm across multi-modal inputs
presents unique challenges, often requiring task-specific fine-tuning and
extensive reasoning steps. However, current Chain-of-Thought approaches do not
efficiently leverage the same cognitive processes that enable humans to
identify sarcasm. We present IRONIC, an in-context learning framework that
leverages Multi-modal Coherence Relations to analyze referential, analogical
and pragmatic image-text linkages. Our experiments show that IRONIC achieves
state-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across
different baselines. This demonstrates the need for incorporating linguistic
and cognitive insights into the design of multi-modal reasoning strategies. Our
code is available at: https://github.com/aashish2000/IRONIC",2025-05-22,"Aashish Anantha Ramakrishnan, Aadarsh Anantha Ramakrishnan, Dongwon Lee",http://arxiv.org/pdf/2505.16258v1,cs.CL
Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models,"Large language models often retain unintended content, prompting growing
interest in knowledge unlearning. Recent approaches emphasize localized
unlearning, which restricts parameter updates to specific regions in an effort
to remove target knowledge while preserving unrelated general knowledge.
However, their effectiveness remains uncertain due to the lack of robust and
thorough evaluation of the trade-off between the competing goals of unlearning.
In this paper, we begin by revisiting existing localized unlearning approaches.
We then conduct controlled experiments to rigorously evaluate whether local
parameter updates causally contribute to unlearning. Our findings reveal that
the set of parameters that must be modified for effective unlearning is not
strictly determined, challenging the core assumption of localized unlearning
that parameter locality is inherently indicative of effective knowledge
removal.",2025-05-22,"Hwiyeong Lee, Uiji Hwang, Hyelim Lim, Taeuk Kim",http://arxiv.org/pdf/2505.16252v1,cs.CL
Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands,"Early detection of cognitive decline is crucial for enabling interventions
that can slow neurodegenerative disease progression. Traditional diagnostic
approaches rely on labor-intensive clinical assessments, which are impractical
for frequent monitoring. Our pilot study investigates voice assistant systems
(VAS) as non-invasive tools for detecting cognitive decline through
longitudinal analysis of speech patterns in voice commands. Over an 18-month
period, we collected voice commands from 35 older adults, with 15 participants
providing daily at-home VAS interactions. To address the challenges of
analyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a
framework that combines (1) LLM-driven iterative prompt refinement for
linguistic feature extraction, (2) HuBERT-based acoustic feature extraction,
and (3) transformer-based temporal modeling. Using iTransformer, our approach
achieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming
its baseline by 27.13%. Through our LLM approach, we identify linguistic
features that uniquely characterize everyday command usage patterns in
individuals experiencing cognitive decline.",2025-05-22,"Kristin Qi, Youxiang Zhu, Caroline Summerour, John A. Batsis, Xiaohui Liang",http://arxiv.org/pdf/2505.17137v1,cs.CL
"Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models","Diverse language model responses are crucial for creative generation,
open-ended tasks, and self-improvement training. We show that common diversity
metrics, and even reward models used for preference optimization,
systematically bias models toward shorter outputs, limiting expressiveness. To
address this, we introduce Diverse, not Short (Diverse-NS), a length-controlled
self-learning framework that improves response diversity while maintaining
length parity. By generating and filtering preference data that balances
diversity, quality, and length, Diverse-NS enables effective training using
only 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,
Diverse-NS substantially enhances lexical and semantic diversity. We show
consistent improvement in diversity with minor reduction or gains in response
quality on four creative generation tasks: Divergent Associations, Persona
Generation, Alternate Uses, and Creative Writing. Surprisingly, experiments
with the Olmo-2 model family (7B, and 13B) show that smaller models like
Olmo-2-7B can serve as effective ""diversity teachers"" for larger models. By
explicitly addressing length bias, our method efficiently pushes models toward
more diverse and expressive outputs.",2025-05-22,"Vijeta Deshpande, Debasmita Ghose, John D. Patterson, Roger Beaty, Anna Rumshisky",http://arxiv.org/pdf/2505.16245v2,cs.CL
Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations,"Applying AI foundation models directly to geospatial datasets remains
challenging due to their limited ability to represent and reason with
geographical entities, specifically vector-based geometries and natural
language descriptions of complex spatial relations. To address these issues, we
investigate the extent to which a well-known-text (WKT) representation of
geometries and their spatial relations (e.g., topological predicates) are
preserved during spatial reasoning when the geospatial vector data are passed
to large language models (LLMs) including GPT-3.5-turbo, GPT-4, and
DeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the
spatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt
engineering-based, and everyday language-based evaluation. Our experiment
results demonstrate that both the embedding-based and prompt engineering-based
approaches to geospatial question-answering tasks with GPT models can achieve
an accuracy of over 0.6 on average for the identification of topological
spatial relations between two geometries. Among the evaluated models, GPT-4
with few-shot prompting achieved the highest performance with over 0.66
accuracy on topological spatial relation inference. Additionally, GPT-based
reasoner is capable of properly comprehending inverse topological spatial
relations and including an LLM-generated geometry can enhance the effectiveness
for geographic entity retrieval. GPT-4 also exhibits the ability to translate
certain vernacular descriptions about places into formal topological relations,
and adding the geometry-type or place-type context in prompts may improve
inference accuracy, but it varies by instance. The performance of these spatial
reasoning tasks offers valuable insights for the refinement of LLMs with
geographical knowledge towards the development of geo-foundation models capable
of geospatial reasoning.",2025-05-22,"Yuhan Ji, Song Gao, Ying Nie, Ivan Majić, Krzysztof Janowicz",http://arxiv.org/pdf/2505.17136v1,cs.CL
"Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers","Recently, Large Reasoning Models (LRMs) have demonstrated superior logical
capabilities compared to traditional Large Language Models (LLMs), gaining
significant attention. Despite their impressive performance, the potential for
stronger reasoning abilities to introduce more severe security vulnerabilities
remains largely underexplored. Existing jailbreak methods often struggle to
balance effectiveness with robustness against adaptive safety mechanisms. In
this work, we propose SEAL, a novel jailbreak attack that targets LRMs through
an adaptive encryption pipeline designed to override their reasoning processes
and evade potential adaptive alignment. Specifically, SEAL introduces a stacked
encryption approach that combines multiple ciphers to overwhelm the models
reasoning capabilities, effectively bypassing built-in safety mechanisms. To
further prevent LRMs from developing countermeasures, we incorporate two
dynamic strategies - random and adaptive - that adjust the cipher length,
order, and combination. Extensive experiments on real-world reasoning models,
including DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the
effectiveness of our approach. Notably, SEAL achieves an attack success rate of
80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant
margin of 27.2%. Warning: This paper contains examples of inappropriate,
offensive, and harmful content.",2025-05-22,"Viet-Anh Nguyen, Shiqian Zhao, Gia Dao, Runyi Hu, Yi Xie, Luu Anh Tuan",http://arxiv.org/pdf/2505.16241v3,cs.CL
Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation,"Large language models (LLMs) have demonstrated remarkable capabilities, but
still struggle with issues like hallucinations and outdated information.
Retrieval-augmented generation (RAG) addresses these issues by grounding LLM
outputs in external knowledge with an Information Retrieval (IR) system.
Building on this foundation, graph-based RAG systems go a step further by
retrieving subgraphs, which preserve the relationships between knowledge
entities and provide more comprehensive context. However, graph RAG faces two
challenges: (1) Retrieving relevant information introduces irrelevant nodes
(especially in dense graph databases, where retrieval usually extends to
adjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)
The representation gap between graph and language during generation with LLMs
limits the ability to fully leverage graph structures for enhanced
understanding. To address these limitations, we propose Align-GRAG, a novel
reasoning-guided dual alignment framework in post-retrieval phrase. It first
formulates a subgraph by retrieving nodes and edges. Then an Aligner is
proposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It
achieves dual alignment of graph node and representation by leveraging KL
divergence loss and contrastive loss, facilitating efficient pruning of
irrelevant knowledge and establishing a unified semantic space. The Generator
integrates the aligned graph data with LLM to produce coherent and accurate
answers. Experiments on GraphQA benchmark across three tasks (including common
sense reasoning, scene graph understanding, and knowledge graph reasoning)
validate the effectiveness of our method. The code will be available upon
accepted.",2025-05-22,"Derong Xu, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Maolin Wang, Qidong Liu, Xiangyu Zhao, Yichao Wang, Huifeng Guo, Ruiming Tang, Enhong Chen, Tong Xu",http://arxiv.org/pdf/2505.16237v1,cs.CL
When can isotropy help adapt LLMs' next word prediction to numerical domains?,"Recent studies have shown that vector representations of contextual
embeddings learned by pre-trained large language models (LLMs) are effective in
various downstream tasks in numerical domains. Despite their significant
benefits, the tendency of LLMs to hallucinate in such domains can have severe
consequences in applications such as energy, nature, finance, healthcare,
retail and transportation, among others. To guarantee prediction reliability
and accuracy in numerical domains, it is necessary to open the black-box and
provide performance guarantees through explanation. However, there is little
theoretical understanding of when pre-trained language models help solve
numeric downstream tasks. This paper seeks to bridge this gap by understanding
when the next-word prediction capability of LLMs can be adapted to numerical
domains through a novel analysis based on the concept of isotropy in the
contextual embedding space. Specifically, we consider a log-linear model for
LLMs in which numeric data can be predicted from its context through a network
with softmax in the output layer of LLMs (i.e., language model head in
self-attention). We demonstrate that, in order to achieve state-of-the-art
performance in numerical domains, the hidden representations of the LLM
embeddings must possess a structure that accounts for the shift-invariance of
the softmax function. By formulating a gradient structure of self-attention in
pre-trained models, we show how the isotropic property of LLM embeddings in
contextual embedding space preserves the underlying structure of
representations, thereby resolving the shift-invariance problem and providing a
performance guarantee. Experiments show that different characteristics of
numeric data and model architecture could have different impacts on isotropy.",2025-05-22,"Rashed Shelim, Shengzhe Xu, Walid Saad, Naren Ramakrishnan",http://arxiv.org/pdf/2505.17135v2,cs.CL
LIFEBench: Evaluating Length Instruction Following in Large Language Models,"While large language models (LLMs) can solve PhD-level reasoning problems
over long context inputs, they still struggle with a seemingly simpler task:
following explicit length instructions-e.g., write a 10,000-word novel.
Additionally, models often generate far too short outputs, terminate
prematurely, or even refuse the request. Existing benchmarks focus primarily on
evaluating generations quality, but often overlook whether the generations meet
length constraints. To this end, we introduce Length Instruction Following
Evaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to
follow length instructions across diverse tasks and a wide range of specified
lengths. LIFEBench consists of 10,800 instances across 4 task categories in
both English and Chinese, covering length constraints ranging from 16 to 8192
words. We evaluate 26 widely-used LLMs and find that most models reasonably
follow short-length instructions but deteriorate sharply beyond a certain
threshold. Surprisingly, almost all models fail to reach the vendor-claimed
maximum output lengths in practice, as further confirmed by our evaluations
extending up to 32K words. Even long-context LLMs, despite their extended
input-output windows, counterintuitively fail to improve length-instructions
following. Notably, Reasoning LLMs outperform even specialized long-text
generation models, achieving state-of-the-art length following. Overall,
LIFEBench uncovers fundamental limitations in current LLMs' length instructions
following ability, offering critical insights for future progress.",2025-05-22,"Wei Zhang, Zhenhong Zhou, Junfeng Fang, Rongwu Xu, Kun Wang, Yuanhe Zhang, Rui Wang, Ge Zhang, Xinfeng Li, Li Sun, Lingjuan Lyu, Yang Liu, Sen Su",http://arxiv.org/pdf/2505.16234v1,cs.CL
MuseRAG: Idea Originality Scoring At Scale,"An objective, face-valid way to assess the originality of creative ideas is
to measure how rare each idea is within a population -- an approach long used
in creativity research but difficult to automate at scale. Tabulating response
frequencies via manual bucketing of idea rephrasings is labor-intensive,
error-prone, and brittle under large corpora. We introduce a fully automated,
psychometrically validated pipeline for frequency-based originality scoring.
Our method, MuseRAG, combines large language models (LLMs) with an externally
orchestrated retrieval-augmented generation (RAG) framework. Given a new idea,
the system retrieves semantically similar prior idea buckets and zero-shot
prompts the LLM to judge whether the new idea belongs to an existing bucket or
forms a new one. The resulting buckets enable computation of frequency-based
originality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG
matches human annotators in idea clustering structure and resolution (AMI =
0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong
convergent and external validity. Our work enables intent-sensitive,
human-aligned originality scoring at scale to aid creativity research.",2025-05-22,"Ali Sarosh Bangash, Krish Veera, Ishfat Abrar Islam, Raiyan Abdul Baten",http://arxiv.org/pdf/2505.16232v1,cs.CL
"Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning","Personalizing jargon detection and explanation is essential for making
technical documents accessible to readers with diverse disciplinary
backgrounds. However, tailoring models to individual users typically requires
substantial annotation efforts and computational resources due to user-specific
finetuning. To address this, we present a systematic study of personalized
jargon detection, focusing on methods that are both efficient and scalable for
real-world deployment. We explore two personalization strategies: (1)
lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,
and (2) personalized prompting, which tailors model behavior at inference time
without retaining. To reflect realistic constraints, we also investigate hybrid
approaches that combine limited annotated data with unsupervised user
background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in
F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,
our method achieves comparable performance using only 10% of the annotated
training data, demonstrating its practicality for resource-constrained
settings. Our study offers the first work to systematically explore efficient,
low-resource personalization of jargon detection using open-source language
models, offering a practical path toward scalable, user-adaptive NLP system.",2025-05-22,"Bohao Wu, Qingyun Wang, Yue Guo",http://arxiv.org/pdf/2505.16227v1,cs.CL
Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation,"With the growing use of large language models(LLMs) as evaluators, their
application has expanded to code evaluation tasks, where they assess the
correctness of generated code without relying on reference implementations.
While this offers scalability and flexibility, it also raises a critical,
unresolved question: Can LLM judges fairly and robustly evaluate semantically
equivalent code with superficial variations? Functionally correct code often
exhibits variations-such as differences in variable names, comments, or
formatting-that should not influence its correctness. Yet, whether LLM judges
can reliably handle these variations remains unclear. We present the first
comprehensive study of this issue, defining six types of potential bias in code
evaluation and revealing their systematic impact on LLM judges. Across five
programming languages and multiple LLMs, we empirically demonstrate that all
tested LLM judges are susceptible to both positive and negative biases,
resulting in inflated or unfairly low scores. Moreover, we observe that LLM
judges remain vulnerable to these biases even when prompted to generate test
cases before scoring, highlighting the need for more robust code evaluation
methods.",2025-05-22,"Jiwon Moon, Yerin Hwang, Dongryeol Lee, Taegwan Kang, Yongil Kim, Kyomin Jung",http://arxiv.org/pdf/2505.16222v1,cs.CL
Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning,"This paper introduces Meta-PerSER, a novel meta-learning framework that
personalizes Speech Emotion Recognition (SER) by adapting to each listener's
unique way of interpreting emotion. Conventional SER systems rely on aggregated
annotations, which often overlook individual subtleties and lead to
inconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic
Meta-Learning (MAML) approach enhanced with Combined-Set Meta-Training,
Derivative Annealing, and per-layer per-step learning rates, enabling rapid
adaptation with only a few labeled examples. By integrating robust
representations from pre-trained self-supervised models, our framework first
captures general emotional cues and then fine-tunes itself to personal
annotation styles. Experiments on the IEMOCAP corpus demonstrate that
Meta-PerSER significantly outperforms baseline methods in both seen and unseen
data scenarios, highlighting its promise for personalized emotion recognition.",2025-05-22,"Liang-Yeh Shen, Shi-Xin Fang, Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee",http://arxiv.org/pdf/2505.16220v1,cs.CL
Memorization or Reasoning? Exploring the Idiom Understanding of LLMs,"Idioms have long posed a challenge due to their unique linguistic properties,
which set them apart from other common expressions. While recent studies have
leveraged large language models (LLMs) to handle idioms across various tasks,
e.g., idiom-containing sentence generation and idiomatic machine translation,
little is known about the underlying mechanisms of idiom processing in LLMs,
particularly in multilingual settings. To this end, we introduce MIDAS, a new
large-scale dataset of idioms in six languages, each paired with its
corresponding meaning. Leveraging this resource, we conduct a comprehensive
evaluation of LLMs' idiom processing ability, identifying key factors that
influence their performance. Our findings suggest that LLMs rely not only on
memorization, but also adopt a hybrid approach that integrates contextual cues
and reasoning, especially when processing compositional idioms. This implies
that idiom understanding in LLMs emerges from an interplay between internal
knowledge retrieval and reasoning-based inference.",2025-05-22,"Jisu Kim, Youngwoo Shin, Uiji Hwang, Jihun Choi, Richeng Xuan, Taeuk Kim",http://arxiv.org/pdf/2505.16216v1,cs.CL
Large Language Models based ASR Error Correction for Child Conversations,"Automatic Speech Recognition (ASR) has recently shown remarkable progress,
but accurately transcribing children's speech remains a significant challenge.
Recent developments in Large Language Models (LLMs) have shown promise in
improving ASR transcriptions. However, their applications in child speech
including conversational scenarios are underexplored. In this study, we explore
the use of LLMs in correcting ASR errors for conversational child speech. We
demonstrate the promises and challenges of LLMs through experiments on two
children's conversational speech datasets with both zero-shot and fine-tuned
ASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR
outputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs
to improve ASR performance when incorporating contextual information or when
using fine-tuned autoregressive ASR (e.g., Whisper) outputs.",2025-05-22,"Anfeng Xu, Tiantian Feng, So Hyun Kim, Somer Bishop, Catherine Lord, Shrikanth Narayanan",http://arxiv.org/pdf/2505.16212v2,cs.CL
AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models,"The rapid advancement and expanding applications of Audio Large Language
Models (ALLMs) demand a rigorous understanding of their trustworthiness.
However, systematic research on evaluating these models, particularly
concerning risks unique to the audio modality, remains largely unexplored.
Existing evaluation frameworks primarily focus on the text modality or address
only a restricted set of safety dimensions, failing to adequately account for
the unique characteristics and application scenarios inherent to the audio
modality. We introduce AudioTrust-the first multifaceted trustworthiness
evaluation framework and benchmark specifically designed for ALLMs. AudioTrust
facilitates assessments across six key dimensions: fairness, hallucination,
safety, privacy, robustness, and authentication. To comprehensively evaluate
these dimensions, AudioTrust is structured around 18 distinct experimental
setups. Its core is a meticulously constructed dataset of over 4,420 audio/text
samples, drawn from real-world scenarios (e.g., daily conversations, emergency
calls, voice assistant interactions), specifically designed to probe the
multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully
designs 9 audio-specific evaluation metrics, and we employ a large-scale
automated pipeline for objective and scalable scoring of model outputs.
Experimental results reveal the trustworthiness boundaries and limitations of
current state-of-the-art open-source and closed-source ALLMs when confronted
with various high-risk audio scenarios, offering valuable insights for the
secure and trustworthy deployment of future audio models. Our platform and
benchmark are available at https://github.com/JusperLee/AudioTrust.",2025-05-22,"Kai Li, Can Shen, Yile Liu, Jirui Han, Kelong Zheng, Xuechao Zou, Zhe Wang, Xingjian Du, Shun Zhang, Hanjun Luo, Yingbin Jin, Xinxin Xing, Ziyang Ma, Yue Liu, Xiaojun Jia, Yifan Zhang, Junfeng Fang, Kun Wang, Yibo Yan, Haoyang Li, Yiming Li, Xiaobin Zhuang, Yang Liu, Haibo Hu, Zhuo Chen, Zhizheng Wu, Xiaolin Hu, Eng-Siong Chng, XiaoFeng Wang, Wenyuan Xu, Wei Dong, Xinfeng Li",http://arxiv.org/pdf/2505.16211v1,cs.CL
NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics,"Large Language Models (LLMs) have demonstrated remarkable proficiency across
a wide range of tasks. However, LLMs often require larger batch sizes to
enhance throughput or longer context lengths to meet task demands, which
significantly increases the memory resource consumption of the Key-Value (KV)
cache during inference, becoming a major bottleneck in LLM deployment. To
address this issue, quantization is a common and straightforward approach.
Currently, quantization methods for activations are limited to 8-bit, and
quantization to even lower bits can lead to substantial accuracy drops. To
further save space by quantizing the KV cache to even lower bits, we analyzed
the element distribution of the KV cache and designed the NQKV algorithm. Since
the elements within each block of the KV cache follow a normal distribution,
NQKV employs per-block quantile quantization to achieve
information-theoretically optimal quantization error. Without significantly
compromising model output quality, NQKV enables the OPT model to perform
inference with an 2x larger batch size or a 4x longer context length, and it
improves throughput by 9.3x compared to when the KV cache is not used.",2025-05-22,"Zhihang Cai, Xingjun Zhang, Zhendong Tan, Zheng Wei",http://arxiv.org/pdf/2505.16210v1,cs.CL
LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions,"High-quality long-context instruction data is essential for aligning
long-context large language models (LLMs). Despite the public release of models
like Qwen and Llama, their long-context instruction data remains proprietary.
Human annotation is costly and challenging, while template-based synthesis
methods limit scale, diversity, and quality. We introduce LongMagpie, a
self-synthesis framework that automatically generates large-scale long-context
instruction data. Our key insight is that aligned long-context LLMs, when
presented with a document followed by special tokens preceding a user turn,
auto-regressively generate contextually relevant queries. By harvesting these
document-query pairs and the model's responses, LongMagpie produces
high-quality instructions without human effort. Experiments on HELMET, RULER,
and Longbench v2 demonstrate that LongMagpie achieves leading performance on
long-context tasks while maintaining competitive performance on short-context
tasks, establishing it as a simple and effective approach for open, diverse,
and scalable long-context instruction data synthesis.",2025-05-22,"Chaochen Gao, Xing Wu, Zijia Lin, Debing Zhang, Songlin Hu",http://arxiv.org/pdf/2505.17134v1,cs.CL
An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability,"The advancements in Multimodal Large Language Models (MLLMs) have enabled
various multimodal tasks to be addressed under a zero-shot paradigm. This
paradigm sidesteps the cost of model fine-tuning, emerging as a dominant trend
in practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a
pivotal challenge in the quest for general artificial intelligence, fails to
accommodate this convenience. The zero-shot paradigm exhibits undesirable
performance on MSA, casting doubt on whether MLLMs can perceive sentiments as
competent as supervised models. By extending the zero-shot paradigm to
In-Context Learning (ICL) and conducting an in-depth study on configuring
demonstrations, we validate that MLLMs indeed possess such capability.
Specifically, three key factors that cover demonstrations' retrieval,
presentation, and distribution are comprehensively investigated and optimized.
A sentimental predictive bias inherent in MLLMs is also discovered and later
effectively counteracted. By complementing each other, the devised strategies
for three factors result in average accuracy improvements of 15.9% on six MSA
datasets against the zero-shot paradigm and 11.2% against the random ICL
baseline.",2025-05-22,"Daiqing Wu, Dongbao Yang, Sicheng Zhao, Can Ma, Yu Zhou",http://arxiv.org/pdf/2505.16193v1,cs.CL
The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions,"This paper is the first investigation of the connection between emotion,
embodiment, and everyday language in a large sample of natural language data.
We created corpora of body part mentions (BPMs) in online English text (blog
posts and tweets). This includes a subset featuring human annotations for the
emotions of the person whose body part is mentioned in the text. We show that
BPMs are common in personal narratives and tweets (~5% to 10% of posts include
BPMs) and that their usage patterns vary markedly by time and %geographic
location. Using word-emotion association lexicons and our annotated data, we
show that text containing BPMs tends to be more emotionally charged, even when
the BPM is not explicitly used to describe a physical reaction to the emotion
in the text. Finally, we discover a strong and statistically significant
correlation between body-related language and a variety of poorer health
outcomes. In sum, we argue that investigating the role of body-part related
words in language can open up valuable avenues of future research at the
intersection of NLP, the affective sciences, and the study of human wellbeing.",2025-05-22,"Sophie Wu, Jan Philip Wahle, Saif M. Mohammad",http://arxiv.org/pdf/2505.16189v1,cs.CL
SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models,"Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but controlling their behavior
reliably remains challenging, especially in open-ended generation settings.
This paper introduces a novel supervised steering approach that operates in
sparse, interpretable representation spaces. We employ sparse autoencoders
(SAEs)to obtain sparse latent representations that aim to disentangle semantic
attributes from model activations. Then we train linear classifiers to identify
a small subspace of task-relevant dimensions in latent representations.
Finally, we learn supervised steering vectors constrained to this subspace,
optimized to align with target behaviors. Experiments across sentiment,
truthfulness, and politics polarity steering tasks with multiple LLMs
demonstrate that our supervised steering vectors achieve higher success rates
with minimal degradation in generation quality compared to existing methods.
Further analysis reveals that a notably small subspace is sufficient for
effective steering, enabling more targeted and interpretable interventions.",2025-05-22,"Zirui He, Mingyu Jin, Bo Shen, Ali Payani, Yongfeng Zhang, Mengnan Du",http://arxiv.org/pdf/2505.16188v1,cs.CL
SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning,"Large Reasoning Models (LRMs) introduce a new generation paradigm of
explicitly reasoning before answering, leading to remarkable improvements in
complex tasks. However, they pose great safety risks against harmful queries
and adversarial attacks. While recent mainstream safety efforts on LRMs,
supervised fine-tuning (SFT), improve safety performance, we find that
SFT-aligned models struggle to generalize to unseen jailbreak prompts. After
thorough investigation of LRMs' generation, we identify a safety aha moment
that can activate safety reasoning and lead to a safe response. This aha moment
typically appears in the `key sentence', which follows models' query
understanding process and can indicate whether the model will proceed safely.
Based on these insights, we propose SafeKey, including two complementary
objectives to better activate the safety aha moment in the key sentence: (1) a
Dual-Path Safety Head to enhance the safety signal in the model's internal
representations before the key sentence, and (2) a Query-Mask Modeling
objective to improve the models' attention on its query understanding, which
has important safety hints. Experiments across multiple safety benchmarks
demonstrate that our methods significantly improve safety generalization to a
wide range of jailbreak attacks and out-of-distribution harmful prompts,
lowering the average harmfulness rate by 9.6\%, while maintaining general
abilities. Our analysis reveals how SafeKey enhances safety by reshaping
internal attention and improving the quality of hidden representations.",2025-05-22,"Kaiwen Zhou, Xuandong Zhao, Gaowen Liu, Jayanth Srinivasa, Aosong Feng, Dawn Song, Xin Eric Wang",http://arxiv.org/pdf/2505.16186v1,cs.CL
Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics,"Evaluating image captions requires cohesive assessment of both visual
semantics and language pragmatics, which is often not entirely captured by most
metrics. We introduce Redemption Score, a novel hybrid framework that ranks
image captions by triangulating three complementary signals: (1) Mutual
Information Divergence (MID) for global image-text distributional alignment,
(2) DINO-based perceptual similarity of cycle-generated images for visual
grounding, and (3) BERTScore for contextual text similarity against human
references. A calibrated fusion of these signals allows Redemption Score to
offer a more holistic assessment. On the Flickr8k benchmark, Redemption Score
achieves a Kendall-$\tau$ of 56.43, outperforming twelve prior methods and
demonstrating superior correlation with human judgments without requiring
task-specific training. Our framework provides a more robust and nuanced
evaluation by effectively redeeming image semantics and linguistic
interpretability indicated by strong transfer of knowledge in the Conceptual
Captions and MS COCO datasets.",2025-05-22,"Ashim Dahal, Ankit Ghimire, Saydul Akbar Murad, Nick Rahimi",http://arxiv.org/pdf/2505.16180v1,cs.CL
Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge,"Fact recall, the ability of language models (LMs) to retrieve specific
factual knowledge, remains a challenging task despite their impressive general
capabilities. Common training strategies often struggle to promote robust
recall behavior with two-stage training, which first trains a model with
fact-storing examples (e.g., factual statements) and then with fact-recalling
examples (question-answer pairs), tending to encourage rote memorization rather
than generalizable fact retrieval. In contrast, mixed training, which jointly
uses both types of examples, has been empirically shown to improve the ability
to recall facts, but the underlying mechanisms are still poorly understood. In
this work, we investigate how these training strategies affect how model
parameters are shaped during training and how these differences relate to their
ability to recall facts. We introduce cross-task gradient trace to identify
shared parameters, those strongly influenced by both fact-storing and
fact-recalling examples. Our analysis on synthetic fact recall datasets with
the Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a
larger and more centralized set of shared parameters. These findings suggest
that the emergence of parameters may play a key role in enabling LMs to
generalize factual knowledge across task formulations.",2025-05-22,"Ying Zhang, Benjamin Heinzerling, Dongyuan Li, Ryoma Ishigaki, Yuta Hitomi, Kentaro Inui",http://arxiv.org/pdf/2505.16178v1,cs.CL
Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning,"In the realm of data selection for reasoning tasks, existing approaches
predominantly rely on externally predefined static metrics such as difficulty
and diversity, which are often designed for supervised fine-tuning (SFT) and
lack adaptability to continuous training processes. A critical limitation of
these methods is their inability to dynamically align with the evolving
capabilities of models during online training, a gap that becomes increasingly
pronounced with the rise of dynamic training paradigms and online reinforcement
learning (RL) frameworks (e.g., R1 models). To address this, we introduce
SAI-DPO, an algorithm that dynamically selects training data by continuously
assessing a model's stage-specific reasoning abilities across different
training phases. By integrating real-time model performance feedback, SAI-DPO
adaptively adapts data selection to the evolving strengths and weaknesses of
the model, thus enhancing both data utilization efficiency and final task
performance. Extensive experiments on three state-of-the-art models and eight
mathematical reasoning benchmarks, including challenging competition-level
datasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average
performance boost of up to 21.3 percentage points, with particularly notable
improvements of 10 and 15 points on AIME24 and AMC23, respectively. These
results highlight the superiority of dynamic, model-adaptive data selection
over static, externally defined strategies in advancing reasoning.",2025-05-22,"Jun Rao, Xuebo Liu, Hexuan Deng, Zepeng Lin, Zixiong Yu, Jiansheng Wei, Xiaojun Meng, Min Zhang",http://arxiv.org/pdf/2505.16176v1,cs.CL
Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss,"Understanding health information is essential in achieving and maintaining a
healthy life. We focus on simplifying health information for better
understanding. With the availability of generative AI, the simplification
process has become efficient and of reasonable quality, however, the algorithms
remove information that may be crucial for comprehension. In this study, we
compare generative AI to detect missing information in simplified text,
evaluate its importance, and fix the text with the missing information. We
collected 50 health information texts and simplified them using gpt-4-0613. We
compare five approaches to identify missing elements and regenerate the text by
inserting the missing elements. These five approaches involve adding missing
entities and missing words in various ways: 1) adding all the missing entities,
2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,
and 4, 5) serving as controls for comparison, adding randomly chosen entities.
We use cosine similarity and ROUGE scores to evaluate the semantic similarity
and content overlap between the original, simplified, and reconstructed
simplified text. We do this for both summaries and full text. Overall, we find
that adding missing entities improves the text. Adding all the missing entities
resulted in better text regeneration, which was better than adding the
top-ranked entities or words, or random words. Current tools can identify these
entities, but are not valuable in ranking them.",2025-05-22,"Abhay Kumara Sri Krishna Nandiraju, Gondy Leroy, David Kauchak, Arif Ahmed",http://arxiv.org/pdf/2505.16172v1,cs.CL
When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction,"Can large language models (LLMs) admit their mistakes when they should know
better? In this work, we define the behavior of acknowledging errors in
previously generated answers as ""retraction"" and aim to understand when and why
LLMs choose to retract. We first construct model-specific datasets to evaluate
whether a model will retract an incorrect answer that contradicts its own
parametric knowledge. While LLMs are capable of retraction, they do so only
infrequently. We demonstrate that retraction is closely tied to previously
identified indicators of models' internal belief: models fail to retract wrong
answers that they ""believe"" to be factually correct. Steering experiments
further demonstrate that internal belief causally influences model retraction.
In particular, when the model does not believe its answer, this not only
encourages the model to attempt to verify the answer, but also alters attention
behavior during self-verification. Finally, we demonstrate that simple
supervised fine-tuning significantly improves retraction performance by helping
the model learn more accurate internal beliefs. Code and datasets are available
on https://github.com/ayyyq/llm-retraction.",2025-05-22,"Yuqing Yang, Robin Jia",http://arxiv.org/pdf/2505.16170v1,cs.CL
Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task,"Large language models (LLMs) are increasingly explored as substitutes for
human participants in cognitive tasks, but their ability to simulate human
behavioral variability remains unclear. This study examines whether LLMs can
approximate individual differences in the phonemic fluency task, where
participants generate words beginning with a target letter. We evaluated 34
model configurations, varying prompt specificity, sampling temperature, and
model type, and compared outputs to responses from 106 human participants.
While some configurations, especially Claude 3.7 Sonnet, matched human averages
and lexical preferences, none reproduced the scope of human variability. LLM
outputs were consistently less diverse and structurally rigid, and LLM
ensembles failed to increase diversity. Network analyses further revealed
fundamental differences in retrieval structure between humans and models. These
results highlight key limitations in using LLMs to simulate human cognition and
behavior.",2025-05-22,"Mengyang Qiu, Zoe Brisebois, Siena Sun",http://arxiv.org/pdf/2505.16164v1,cs.CL
KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization,"Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by efficiently drafting multiple tokens using a compact model
and then verifying them in parallel using the target LLM. Notably,
Self-Speculative Decoding proposes skipping certain layers to construct the
draft model, which eliminates the need for additional parameters or training.
Despite its strengths, we observe in this work that drafting with layer
skipping exhibits significant sensitivity to domain shifts, leading to a
substantial drop in acceleration performance. To enhance the domain
generalizability of this paradigm, we introduce KNN-SSD, an algorithm that
leverages K-Nearest Neighbor (KNN) search to match different skipped layers
with various domain inputs. We evaluated our algorithm in various models and
multiple tasks, observing that its application leads to 1.3x-1.6x speedup in
LLM inference.",2025-05-22,"Mingbo Song, Heming Xia, Jun Zhang, Chak Tou Leong, Qiancheng Xu, Wenjie Li, Sujian Li",http://arxiv.org/pdf/2505.16162v1,cs.CL
EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios,"As large language models continue to advance, their application in
educational contexts remains underexplored and under-optimized. In this paper,
we address this gap by introducing the first diverse benchmark tailored for
educational scenarios, incorporating synthetic data containing 9 major
scenarios and over 4,000 distinct educational contexts. To enable comprehensive
assessment, we propose a set of multi-dimensional evaluation metrics that cover
12 critical aspects relevant to both teachers and students. We further apply
human annotation to ensure the effectiveness of the model-generated evaluation
responses. Additionally, we succeed to train a relatively small-scale model on
our constructed dataset and demonstrate that it can achieve performance
comparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on
the test set. Overall, this work provides a practical foundation for the
development and evaluation of education-oriented language models. Code and data
are released at https://github.com/ybai-nlp/EduBench.",2025-05-22,"Bin Xu, Yu Bai, Huashan Sun, Yiguan Lin, Siming Liu, Xinyue Liang, Yaolin Li, Yang Gao, Heyan Huang",http://arxiv.org/pdf/2505.16160v2,cs.CL
Robustifying Vision-Language Models via Dynamic Token Reweighting,"Large vision-language models (VLMs) are highly vulnerable to jailbreak
attacks that exploit visual-textual interactions to bypass safety guardrails.
In this paper, we present DTR, a novel inference-time defense that mitigates
multimodal jailbreak attacks through optimizing the model's key-value (KV)
caches. Rather than relying on curated safety-specific data or costly
image-to-text conversion, we introduce a new formulation of the safety-relevant
distributional shift induced by the visual modality. This formulation enables
DTR to dynamically adjust visual token weights, minimizing the impact of
adversarial visual inputs while preserving the model's general capabilities and
inference efficiency. Extensive evaluation across diverse VLMs and attack
benchmarks demonstrates that \sys outperforms existing defenses in both attack
robustness and benign task performance, marking the first successful
application of KV cache optimization for safety enhancement in multimodal
foundation models. The code for replicating DTR is available:
https://anonymous.4open.science/r/DTR-2755 (warning: this paper contains
potentially harmful content generated by VLMs.)",2025-05-22,"Tanqiu Jiang, Jiacheng Liang, Rongyi Zhu, Jiawei Zhou, Fenglong Ma, Ting Wang",http://arxiv.org/pdf/2505.17132v1,cs.CL
When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification,"Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet
serve as critical tools for model evaluation. However, despite the cleaning
efforts, these datasets still suffer from pervasive noisy labels and often
contain missing labels due to the co-existing image pattern where multiple
classes appear in an image sample. This results in misleading model comparisons
and unfair evaluations. Existing label cleaning methods focus primarily on
noisy labels, but the issue of missing labels remains largely overlooked.
Motivated by these challenges, we present a comprehensive framework named
REVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,
LLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods
(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and
missing label detection in widely-used image classification test sets. REVEAL
detects potential noisy labels and omissions, aggregates predictions from
various methods, and refines label accuracy through confidence-informed
predictions and consensus-based filtering. Additionally, we provide a thorough
analysis of state-of-the-art vision-language models and pre-trained image
classifiers, highlighting their strengths and limitations within the context of
dataset renovation by revealing 10 observations. Our method effectively reveals
missing labels from public datasets and provides soft-labeled results with
likelihoods. Through human verifications, REVEAL significantly improves the
quality of 6 benchmark test sets, highly aligning to human judgments and
enabling more accurate and meaningful comparisons in image classification.",2025-05-22,"Zirui Pang, Haosheng Tan, Yuhan Pu, Zhijie Deng, Zhouan Shen, Keyu Hu, Jiaheng Wei",http://arxiv.org/pdf/2505.16149v1,cs.CL
NAN: A Training-Free Solution to Coefficient Estimation in Model Merging,"Model merging offers a training-free alternative to multi-task learning by
combining independently fine-tuned models into a unified one without access to
raw data. However, existing approaches often rely on heuristics to determine
the merging coefficients, limiting their scalability and generality. In this
work, we revisit model merging through the lens of least-squares optimization
and show that the optimal merging weights should scale with the amount of
task-specific information encoded in each model. Based on this insight, we
propose NAN, a simple yet effective method that estimates model merging
coefficients via the inverse of parameter norm. NAN is training-free,
plug-and-play, and applicable to a wide range of merging strategies. Extensive
experiments on show that NAN consistently improves performance of baseline
methods.",2025-05-22,"Chongjie Si, Kangtao Lv, Jingjing Jiang, Yadao Wang, Yongwei Wang, Xiaokang Yang, Wenbo Su, Bo Zheng, Wei Shen",http://arxiv.org/pdf/2505.16148v1,cs.CL
Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation,"Large vision-language models (LVLMs) have achieved remarkable performance on
multimodal tasks such as visual question answering (VQA) and image captioning.
However, they still suffer from hallucinations, generating text inconsistent
with visual input, posing significant risks in real-world applications.
Existing approaches to address this issue focus on incorporating external
knowledge bases, alignment training, or decoding strategies, all of which
require substantial computational cost and time. Recent works try to explore
more efficient alternatives by adjusting LVLMs' internal representations.
Although promising, these methods may cause hallucinations to be insufficiently
suppressed or lead to excessive interventions that negatively affect normal
semantics. In this work, we leverage sparse autoencoders (SAEs) to identify
semantic directions closely associated with either hallucinations or actuality,
realizing more precise and direct hallucination-related representations. Our
analysis demonstrates that interventions along the faithful direction we
identified can mitigate hallucinations, while those along the hallucinatory
direction can exacerbate them. Building on these insights, we propose Steering
LVLMs via SAE Latent Directions (SSL), a training-free method based on
SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive
experiments demonstrate that SSL significantly outperforms existing decoding
approaches in mitigating hallucinations, while maintaining transferability
across different model architectures with negligible additional time overhead.",2025-05-22,"Zhenglin Hua, Jinghan He, Zijun Yao, Tianxu Han, Haiyun Guo, Yuheng Jia, Junfeng Fang",http://arxiv.org/pdf/2505.16146v1,cs.CL
Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning,"Distilling reasoning paths from teacher to student models via supervised
fine-tuning (SFT) provides a shortcut for improving the reasoning ability of
smaller Large Language Models (LLMs). However, the reasoning paths generated by
teacher models often reflect only surface-level traces of their underlying
authentic reasoning. Insights from cognitive neuroscience suggest that
authentic reasoning involves a complex interweaving between meta-reasoning
(which selects appropriate sub-problems from multiple candidates) and solving
(which addresses the sub-problem). This implies authentic reasoning has an
implicit multi-branch structure. Supervised fine-tuning collapses this rich
structure into a flat sequence of token prediction in the teacher's reasoning
path, preventing effective distillation of this structure to students. To
address this limitation, we propose RLKD, a reinforcement learning (RL)-based
distillation framework guided by a novel Generative Structure Reward Model
(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving
steps and computes rewards to measure structural alignment between student and
teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to
internalize the teacher's implicit multi-branch reasoning structure rather than
merely mimicking fixed output paths. Experiments show RLKD surpasses standard
SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,
unlocking greater student reasoning potential than SFT-based distillation.",2025-05-22,"Shicheng Xu, Liang Pang, Yunchang Zhu, Jia Gu, Zihao Wei, Jingcheng Deng, Feiyang Pan, Huawei Shen, Xueqi Cheng",http://arxiv.org/pdf/2505.16142v1,cs.CL
Sudoku-Bench: Evaluating creative reasoning with Sudoku variants,"Existing reasoning benchmarks for large language models (LLMs) frequently
fail to capture authentic creativity, often rewarding memorization of
previously observed patterns. We address this shortcoming with Sudoku-Bench, a
curated benchmark of challenging and unconventional Sudoku variants
specifically selected to evaluate creative, multi-step logical reasoning.
Sudoku variants form an unusually effective domain for reasoning research: each
puzzle introduces unique or subtly interacting constraints, making memorization
infeasible and requiring solvers to identify novel logical breakthroughs
(``break-ins''). Despite their diversity, Sudoku variants maintain a common and
compact structure, enabling clear and consistent evaluation. Sudoku-Bench
includes a carefully chosen puzzle set, a standardized text-based puzzle
representation, and flexible tools compatible with thousands of publicly
available puzzles -- making it easy to extend into a general research
environment. Baseline experiments show that state-of-the-art LLMs solve fewer
than 15\% of puzzles unaided, highlighting significant opportunities to advance
long-horizon, strategic reasoning capabilities.",2025-05-22,"Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, Llion Jones",http://arxiv.org/pdf/2505.16135v1,cs.CL
Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models,"Large language models exhibit positional bias -- systematic neglect of
information at specific context positions -- yet its interplay with linguistic
diversity remains poorly understood. We present a cross-linguistic study across
five typologically distinct languages (English, Russian, German, Hindi,
Vietnamese), examining how positional bias interacts with model uncertainty,
syntax, and prompting. Key findings: (1) Positional bias is model-driven, with
language-specific variations -- Qwen2.5-7B favors late positions, challenging
assumptions of early-token bias; (2) Explicit positional guidance (e.g.,
correct context is at position X) reduces accuracy across languages,
undermining prompt-engineering practices; (3) Aligning context with positional
bias increases entropy, yet minimal entropy does not predict accuracy. (4) We
further uncover that LLMs differently impose dominant word order in
free-word-order languages like Hindi.",2025-05-22,"Menschikov Mikhail, Alexander Kharitonov, Maiia Kotyga, Vadim Porvatov, Anna Zhukovskaya, David Kagramanyan, Egor Shvetsov, Evgeny Burnaev",http://arxiv.org/pdf/2505.16134v1,cs.CL
LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods,"Recent studies have applied large language models (LLMs) to machine
translation quality estimation (MTQE) by prompting models to assign numeric
scores. Nonetheless, these direct scoring methods tend to show low
segment-level correlation with human judgments. In this paper, we propose a
generation-based evaluation paradigm that leverages decoder-only LLMs to
produce high-quality references, followed by semantic similarity scoring using
sentence embeddings. We conduct the most extensive evaluation to date in MTQE,
covering 8 LLMs and 8 language pairs. Empirical results show that our method
outperforms both intra-LLM direct scoring baselines and external non-LLM
reference-free metrics from MTME. These findings demonstrate the strength of
generation-based evaluation and support a shift toward hybrid approaches that
combine fluent generation with accurate semantic assessment.",2025-05-22,Hyang Cui,http://arxiv.org/pdf/2505.16129v1,cs.CL
Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning,"Despite LLMs' explicit alignment against demographic stereotypes, they have
been shown to exhibit biases under various social contexts. In this work, we
find that LLMs exhibit concerning biases in how they associate solution
veracity with demographics. Through experiments across five human value-aligned
LLMs on mathematics, coding, commonsense, and writing problems, we reveal two
forms of such veracity biases: Attribution Bias, where models
disproportionately attribute correct solutions to certain demographic groups,
and Evaluation Bias, where models' assessment of identical solutions varies
based on perceived demographic authorship. Our results show pervasive biases:
LLMs consistently attribute fewer correct solutions and more incorrect ones to
African-American groups in math and coding, while Asian authorships are least
preferred in writing evaluation. In additional studies, we show LLMs
automatically assign racially stereotypical colors to demographic groups in
visualization code, suggesting these biases are deeply embedded in models'
reasoning processes. Our findings indicate that demographic bias extends beyond
surface-level stereotypes and social context provocations, raising concerns
about LLMs' deployment in educational and evaluation settings.",2025-05-22,"Yue Zhou, Barbara Di Eugenio",http://arxiv.org/pdf/2505.16128v2,cs.CL
KoBALT: Korean Benchmark For Advanced Linguistic Tasks,"We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a
comprehensive linguistically-motivated benchmark comprising 700 multiple-choice
questions spanning 24 phenomena across five linguistic domains: syntax,
semantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed
to advance the evaluation of large language models (LLMs) in Korean, a
morphologically rich language, by addressing the limitations of conventional
benchmarks that often lack linguistic depth and typological grounding. It
introduces a suite of expert-curated, linguistically motivated questions with
minimal n-gram overlap with standard Korean corpora, substantially mitigating
the risk of data contamination and allowing a more robust assessment of true
language understanding. Our evaluation of 20 contemporary LLMs reveals
significant performance disparities, with the highest-performing model
achieving 61\% general accuracy but showing substantial variation across
linguistic domains - from stronger performance in semantics (66\%) to
considerable weaknesses in phonology (31\%) and morphology (36\%). Through
human preference evaluation with 95 annotators, we demonstrate a strong
correlation between KoBALT scores and human judgments, validating our
benchmark's effectiveness as a discriminative measure of Korean language
understanding. KoBALT addresses critical gaps in linguistic evaluation for
typologically diverse languages and provides a robust framework for assessing
genuine linguistic competence in Korean language models.",2025-05-22,"Hyopil Shin, Sangah Lee, Dongjun Jang, Wooseok Song, Jaeyoon Kim, Chaeyoung Oh, Hyemi Jo, Youngchae Ahn, Sihyun Oh, Hyohyeong Chang, Sunkyoung Kim, Jinsik Lee",http://arxiv.org/pdf/2505.16125v1,cs.CL
Relative Bias: A Comparative Framework for Quantifying Bias in LLMs,"The growing deployment of large language models (LLMs) has amplified concerns
regarding their inherent biases, raising critical questions about their
fairness, safety, and societal impact. However, quantifying LLM bias remains a
fundamental challenge, complicated by the ambiguity of what ""bias"" entails.
This challenge grows as new models emerge rapidly and gain widespread use,
while introducing potential biases that have not been systematically assessed.
In this paper, we propose the Relative Bias framework, a method designed to
assess how an LLM's behavior deviates from other LLMs within a specified target
domain. We introduce two complementary methodologies: (1) Embedding
Transformation analysis, which captures relative bias patterns through sentence
representations over the embedding space, and (2) LLM-as-a-Judge, which employs
a language model to evaluate outputs comparatively. Applying our framework to
several case studies on bias and alignment scenarios following by statistical
tests for validation, we find strong alignment between the two scoring methods,
offering a systematic, scalable, and statistically grounded approach for
comparative bias analysis in LLMs.",2025-05-22,"Alireza Arbabi, Florian Kerschbaum",http://arxiv.org/pdf/2505.17131v1,cs.CL
Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics,"Social media's rise establishes user-generated content (UGC) as pivotal for
travel decisions, yet analytical methods lack scalability. This study
introduces a dual-method LLM framework: unsupervised expectation extraction
from UGC paired with survey-informed supervised fine-tuning. Findings reveal
leisure/social expectations drive engagement more than foundational
natural/emotional factors. By establishing LLMs as precision tools for
expectation quantification, we advance tourism analytics methodology and
propose targeted strategies for experience personalization and social travel
promotion. The framework's adaptability extends to consumer behavior research,
demonstrating computational social science's transformative potential in
marketing optimization.",2025-05-22,"Haotian Lan, Yao Gao, Yujun Cheng, Wei Yuan, Kun Wang",http://arxiv.org/pdf/2505.16118v1,cs.CL
Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools,"Modern Large Language Models (LLMs) often require external tools, such as
machine learning classifiers or knowledge retrieval systems, to provide
accurate answers in domains where their pre-trained knowledge is insufficient.
This integration of LLMs with external tools expands their utility but also
introduces a critical challenge: determining the trustworthiness of responses
generated by the combined system. In high-stakes applications, such as medical
decision-making, it is essential to assess the uncertainty of both the LLM's
generated text and the tool's output to ensure the reliability of the final
response. However, existing uncertainty quantification methods do not account
for the tool-calling scenario, where both the LLM and external tool contribute
to the overall system's uncertainty. In this work, we present a novel framework
for modeling tool-calling LLMs that quantifies uncertainty by jointly
considering the predictive uncertainty of the LLM and the external tool. We
extend previous methods for uncertainty quantification over token sequences to
this setting and propose efficient approximations that make uncertainty
computation practical for real-world applications. We evaluate our framework on
two new synthetic QA datasets, derived from well-known machine learning
datasets, which require tool-calling for accurate answers. Additionally, we
apply our method to retrieval-augmented generation (RAG) systems and conduct a
proof-of-concept experiment demonstrating the effectiveness of our uncertainty
metrics in scenarios where external information retrieval is needed. Our
results show that the framework is effective in enhancing trust in LLM-based
systems, especially in cases where the LLM's internal knowledge is insufficient
and external tools are required.",2025-05-22,"Panagiotis Lymperopoulos, Vasanth Sarathy",http://arxiv.org/pdf/2505.16113v1,cs.CL
MPL: Multiple Programming Languages with Large Language Models for Information Extraction,"Recent research in information extraction (IE) focuses on utilizing
code-style inputs to enhance structured output generation. The intuition behind
this is that the programming languages (PLs) inherently exhibit greater
structural organization than natural languages (NLs). This structural advantage
makes PLs particularly suited for IE tasks. Nevertheless, existing research
primarily focuses on Python for code-style simulation, overlooking the
potential of other widely-used PLs (e.g., C++ and Java) during the supervised
fine-tuning (SFT) phase. In this research, we propose \textbf{M}ultiple
\textbf{P}rogramming \textbf{L}anguages with large language models for
information extraction (abbreviated as \textbf{MPL}), a novel framework that
explores the potential of incorporating different PLs in the SFT phase.
Additionally, we introduce \texttt{function-prompt} with virtual running to
simulate code-style inputs more effectively and efficiently. Experimental
results on a wide range of datasets demonstrate the effectiveness of MPL.
Furthermore, we conduct extensive experiments to provide a comprehensive
analysis. We have released our code for future research.",2025-05-22,"Bo Li, Gexiang Fang, Wei Ye, Zhenghua Xu, Jinglei Zhang, Hao Cheng, Shikun Zhang",http://arxiv.org/pdf/2505.16107v1,cs.CL
Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models,"With the increasing size of Large Vision-Language Models (LVLMs), network
pruning techniques aimed at compressing models for deployment in
resource-constrained environments have garnered significant attention. However,
we observe that pruning often leads to a degradation in safety performance. To
address this issue, we present a novel and lightweight approach, termed
Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the
contribution of each attention head to safety, identifying the most critical
ones, and then selectively restoring neurons directly within these attention
heads that play a pivotal role in maintaining safety. This process
hierarchically realigns the safety of pruned LVLMs, progressing from the
attention head level to the neuron level. We validate HSR across various models
and pruning strategies, consistently achieving notable improvements in safety
performance. To our knowledge, this is the first work explicitly focused on
restoring safety in LVLMs post-pruning.",2025-05-22,"Yue Li, Xin Yi, Dongsheng Shi, Gerard de Melo, Xiaoling Wang, Linlin Wang",http://arxiv.org/pdf/2505.16104v1,cs.CL
Continually Self-Improving Language Models for Bariatric Surgery Question--Answering,"While bariatric and metabolic surgery (MBS) is considered the gold standard
treatment for severe and morbid obesity, its therapeutic efficacy hinges upon
active and longitudinal engagement with multidisciplinary providers, including
surgeons, dietitians/nutritionists, psychologists, and endocrinologists. This
engagement spans the entire patient journey, from preoperative preparation to
long-term postoperative management. However, this process is often hindered by
numerous healthcare disparities, such as logistical and access barriers, which
impair easy patient access to timely, evidence-based, clinician-endorsed
information. To address these gaps, we introduce bRAGgen, a novel adaptive
retrieval-augmented generation (RAG)-based model that autonomously integrates
real-time medical evidence when response confidence dips below dynamic
thresholds. This self-updating architecture ensures that responses remain
current and accurate, reducing the risk of misinformation. Additionally, we
present bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,
validated by an expert bariatric surgeon. bRAGq constitutes the first
large-scale, domain-specific benchmark for comprehensive MBS care. In a
two-phase evaluation, bRAGgen is benchmarked against state-of-the-art models
using both large language model (LLM)--based metrics and expert surgeon review.
Across all evaluation dimensions, bRAGgen demonstrates substantially superior
performance in generating clinically accurate and relevant responses.",2025-05-22,"Yash Kumar Atri, Thomas H Shin, Thomas Hartvigsen",http://arxiv.org/pdf/2505.16102v2,cs.CL
BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research,"Validating scientific hypotheses is a central challenge in biomedical
research, and remains difficult for artificial intelligence (AI) agents due to
the complexity of real-world data analysis and evidence interpretation. In this
work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on
realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K
consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,
curated from over 300 published biomedical studies to reflect the structure and
reasoning found in authentic research workflows. Each task includes a
structured hypothesis derived from the original study's conclusions, expressed
in the affirmative to reflect the language of scientific reporting, and one or
more pieces of supporting evidence grounded in empirical data tables. While
these hypotheses mirror published claims, they remain testable using standard
statistical or machine learning methods. The benchmark enables evaluation along
four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and
conclusion, (3) correctness of the reasoning process, and (4) executability of
the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable
hypotheses: cases where the available data are insufficient to support or
refute a claim, reflecting a common yet underexplored scenario in real-world
science. We propose BioDSA-1K as a foundation for building and evaluating
generalizable, trustworthy AI agents for biomedical discovery.",2025-05-22,"Zifeng Wang, Benjamin Danek, Jimeng Sun",http://arxiv.org/pdf/2505.16100v1,cs.CL
A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization,"Large language models (LLMs) are introducing a paradigm shift in molecular
discovery by enabling text-guided interaction with chemical spaces through
natural language, symbolic notations, with emerging extensions to incorporate
multi-modal inputs. To advance the new field of LLM for molecular discovery,
this survey provides an up-to-date and forward-looking review of the emerging
use of LLMs for two central tasks: molecule generation and molecule
optimization. Based on our proposed taxonomy for both problems, we analyze
representative techniques in each category, highlighting how LLM capabilities
are leveraged across different learning settings. In addition, we include the
commonly used datasets and evaluation protocols. We conclude by discussing key
challenges and future directions, positioning this survey as a resource for
researchers working at the intersection of LLMs and molecular science. A
continuously updated reading list is available at
https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.",2025-05-22,"Ziqing Wang, Kexin Zhang, Zihan Zhao, Yibo Wen, Abhishek Pandey, Han Liu, Kaize Ding",http://arxiv.org/pdf/2505.16094v1,cs.CL
Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance,"As of 2025, Generative Artificial Intelligence (GenAI) has become a central
tool for productivity across industries. Beyond text generation, GenAI now
plays a critical role in coding, data analysis, and research workflows. As
large language models (LLMs) continue to evolve, it is essential to assess the
reliability and accuracy of their outputs, especially in specialized,
high-stakes domains like finance. Most modern LLMs transform text into
numerical vectors, which are used in operations such as cosine similarity
searches to generate responses. However, this abstraction process can lead to
misinterpretation of emotional tone, particularly in nuanced financial
contexts. While LLMs generally excel at identifying sentiment in everyday
language, these models often struggle with the nuanced, strategically ambiguous
language found in earnings call transcripts. Financial disclosures frequently
embed sentiment in hedged statements, forward-looking language, and
industry-specific jargon, making it difficult even for human analysts to
interpret consistently, let alone AI models. This paper presents findings from
the Santa Clara Microsoft Practicum Project, led by Professor Charlie
Goldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's
ChatGPT, Google's Gemini, and traditional machine learning models for sentiment
analysis of financial text. Using Microsoft earnings call transcripts, the
analysis assesses how well LLM-derived sentiment correlates with market
sentiment and stock movements and evaluates the accuracy of model outputs.
Prompt engineering techniques are also examined to improve sentiment analysis
results. Visualizations of sentiment consistency are developed to evaluate
alignment between tone and stock performance, with sentiment trends analyzed
across Microsoft's lines of business to determine which segments exert the
greatest influence.",2025-05-22,"Dominick Kubica, Dylan T. Gordon, Nanami Emura, Derleen Saini, Charlie Goldenberg",http://arxiv.org/pdf/2505.16090v1,cs.CL
Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning,"Modern BPE tokenizers often split calendar dates into meaningless fragments,
e.g., 20250312 $\rightarrow$ 202, 503, 12, inflating token counts and obscuring
the inherent structure needed for robust temporal reasoning. In this work, we
(1) introduce a simple yet interpretable metric, termed date fragmentation
ratio, that measures how faithfully a tokenizer preserves multi-digit date
components; (2) release DateAugBench, a suite of 6500 examples spanning three
temporal reasoning tasks: context-based date resolution, format-invariance
puzzles, and date arithmetic across historical, contemporary, and future time
periods; and (3) through layer-wise probing and causal attention-hop analyses,
uncover an emergent date-abstraction mechanism whereby large language models
stitch together the fragments of month, day, and year components for temporal
reasoning. Our experiments show that excessive fragmentation correlates with
accuracy drops of up to 10 points on uncommon dates like historical and
futuristic dates. Further, we find that the larger the model, the faster the
emergent date abstraction that heals date fragments is accomplished. Lastly, we
observe a reasoning path that LLMs follow to assemble date fragments, typically
differing from human interpretation (year $\rightarrow$ month $\rightarrow$
day). Our datasets and code are made publicly available
\href{https://github.com/gagan3012/date-fragments}{here}.",2025-05-22,"Gagan Bhatia, Maxime Peyrard, Wei Zhao",http://arxiv.org/pdf/2505.16088v2,cs.CL
Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development,"We have seen remarkable progress in large language models (LLMs) empowered
multi-agent systems solving complex tasks necessitating cooperation among
experts with diverse skills. However, optimizing LLM-based multi-agent systems
remains challenging. In this work, we perform an empirical case study on group
optimization of role-based multi-agent systems utilizing natural language
feedback for challenging software development tasks under various evaluation
dimensions. We propose a two-step agent prompts optimization pipeline:
identifying underperforming agents with their failure explanations utilizing
textual feedback and then optimizing system prompts of identified agents
utilizing failure explanations. We then study the impact of various
optimization settings on system performance with two comparison groups: online
against offline optimization and individual against group optimization. For
group optimization, we study two prompting strategies: one-pass and multi-pass
prompting optimizations. Overall, we demonstrate the effectiveness of our
optimization method for role-based multi-agent systems tackling software
development tasks evaluated on diverse evaluation dimensions, and we
investigate the impact of diverse optimization settings on group behaviors of
the multi-agent systems to provide practical insights for future development.",2025-05-22,"Ming Shen, Raphael Shu, Anurag Pratik, James Gung, Yubin Ge, Monica Sunkara, Yi Zhang",http://arxiv.org/pdf/2505.16086v1,cs.CL
BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators,"We present BiasLab, a dataset of 300 political news articles annotated for
perceived ideological bias. These articles were selected from a curated
900-document pool covering diverse political events and source biases. Each
article is labeled by crowdworkers along two independent scales, assessing
sentiment toward the Democratic and Republican parties, and enriched with
rationale indicators. The annotation pipeline incorporates targeted worker
qualification and was refined through pilot-phase analysis. We quantify
inter-annotator agreement, analyze misalignment with source-level outlet bias,
and organize the resulting labels into interpretable subsets. Additionally, we
simulate annotation using schema-constrained GPT-4o, enabling direct comparison
to human labels and revealing mirrored asymmetries, especially in
misclassifying subtly right-leaning content. We define two modeling tasks:
perception drift prediction and rationale type classification, and report
baseline performance to illustrate the challenge of explainable bias detection.
BiasLab's rich rationale annotations provide actionable interpretations that
facilitate explainable modeling of political bias, supporting the development
of transparent, socially aware NLP systems. We release the dataset, annotation
schema, and modeling code to encourage research on human-in-the-loop
interpretability and the evaluation of explanation effectiveness in real-world
settings.",2025-05-21,KMA Solaiman,http://arxiv.org/pdf/2505.16081v1,cs.CL
Small Language Models in the Real World: Insights from Industrial Text Classification,"With the emergence of ChatGPT, Transformer models have significantly advanced
text classification and related tasks. Decoder-only models such as Llama
exhibit strong performance and flexibility, yet they suffer from inefficiency
on inference due to token-by-token generation, and their effectiveness in text
classification tasks heavily depends on prompt quality. Moreover, their
substantial GPU resource requirements often limit widespread adoption. Thus,
the question of whether smaller language models are capable of effectively
handling text classification tasks emerges as a topic of significant interest.
However, the selection of appropriate models and methodologies remains largely
underexplored. In this paper, we conduct a comprehensive evaluation of prompt
engineering and supervised fine-tuning methods for transformer-based text
classification. Specifically, we focus on practical industrial scenarios,
including email classification, legal document categorization, and the
classification of extremely long academic texts. We examine the strengths and
limitations of smaller models, with particular attention to both their
performance and their efficiency in Video Random-Access Memory (VRAM)
utilization, thereby providing valuable insights for the local deployment and
application of compact models in industrial settings.",2025-05-21,"Lujun Li, Lama Sleem, Niccolo' Gentile, Geoffrey Nichil, Radu State",http://arxiv.org/pdf/2505.16078v2,cs.CL
Conformal Language Model Reasoning with Coherent Factuality,"Language models are increasingly being used in important decision pipelines,
so ensuring the correctness of their outputs is crucial. Recent work has
proposed evaluating the ""factuality"" of claims decomposed from a language model
generation and applying conformal prediction techniques to filter out those
claims that are not factual. This can be effective for tasks such as
information retrieval, where constituent claims may be evaluated in isolation
for factuality, but is not appropriate for reasoning tasks, as steps of a
logical argument can be evaluated for correctness only within the context of
the claims that precede them. To capture this, we define ""coherent factuality""
and develop a conformal-prediction-based method to guarantee coherent
factuality for language model outputs. Our approach applies split conformal
prediction to subgraphs within a ""deducibility"" graph"" that represents the
steps of a reasoning problem. We evaluate our method on mathematical reasoning
problems from the MATH and FELM datasets and find that our algorithm
consistently produces correct and substantiated orderings of claims, achieving
coherent factuality across target coverage levels. Moreover, we achieve 90%
factuality on our stricter definition while retaining 80% or more of the
original claims, highlighting the utility of our deducibility-graph-guided
approach.",2025-05-21,"Maxon Rubin-Toles, Maya Gambhir, Keshav Ramji, Aaron Roth, Surbhi Goel",http://arxiv.org/pdf/2505.17126v1,cs.CL
Merge to Mix: Mixing Datasets via Model Merging,"Mixing datasets for fine-tuning large models (LMs) has become critical for
maximizing performance on downstream tasks. However, composing effective
dataset mixtures typically relies on heuristics and trial-and-error, often
requiring multiple fine-tuning runs to achieve the desired outcome. We propose
a novel method, $\textit{Merge to Mix}$, that accelerates composing dataset
mixtures through model merging. Model merging is a recent technique that
combines the abilities of multiple individually fine-tuned LMs into a single LM
by using a few simple arithmetic operations. Our key insight is that merging
models individually fine-tuned on each dataset in a mixture can effectively
serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix
leverages this insight to accelerate selecting dataset mixtures without
requiring full fine-tuning on each candidate mixture. Our experiments
demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset
selection for fine-tuning LMs.",2025-05-21,"Zhixu Silvia Tao, Kasper Vinken, Hao-Wei Yeh, Avi Cooper, Xavier Boix",http://arxiv.org/pdf/2505.16066v1,cs.CL
Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation,"Embedding-Based Retrieval (EBR) is an important technique in modern search
engines, enabling semantic match between search queries and relevant results.
However, search logging data on platforms like Facebook Marketplace lacks the
diversity and details needed for effective EBR model training, limiting the
models' ability to capture nuanced search patterns. To address this challenge,
we propose Aug2Search, an EBR-based framework leveraging synthetic data
generated by Generative AI (GenAI) models, in a multimodal and multitask
approach to optimize query-product relevance. This paper investigates the
capabilities of GenAI, particularly Large Language Models (LLMs), in generating
high-quality synthetic data, and analyzing its impact on enhancing EBR models.
We conducted experiments using eight Llama models and 100 million data points
from Facebook Marketplace logs. Our synthetic data generation follows three
strategies: (1) generate queries, (2) enhance product listings, and (3)
generate queries from enhanced listings. We train EBR models on three different
datasets: sampled engagement data or original data ((e.g., ""Click"" and ""Listing
Interactions"")), synthetic data, and a mixture of both engagement and synthetic
data to assess their performance across various training sets. Our findings
underscore the robustness of Llama models in producing synthetic queries and
listings with high coherence, relevance, and diversity, while maintaining low
levels of hallucination. Aug2Search achieves an improvement of up to 4% in
ROC_AUC with 100 million synthetic data samples, demonstrating the
effectiveness of our approach. Moreover, our experiments reveal that with the
same volume of training data, models trained exclusively on synthetic data
often outperform those trained on original data only or a mixture of original
and synthetic data.",2025-05-21,"Ruijie Xi, He Ba, Hao Yuan, Rishu Agrawal, Arul Prakash",http://arxiv.org/pdf/2505.16065v1,cs.CL
Internal and External Impacts of Natural Language Processing Papers,"We investigate the impacts of NLP research published in top-tier conferences
(i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from
research articles and external sources such as patents, media, and policy
documents, we examine how different NLP topics are consumed both within the
academic community and by the broader public. Our findings reveal that language
modeling has the widest internal and external influence, while linguistic
foundations have lower impacts. We also observe that internal and external
impacts generally align, but topics like ethics, bias, and fairness show
significant attention in policy documents with much fewer academic citations.
Additionally, external domains exhibit distinct preferences, with patents
focusing on practical NLP applications and media and policy documents engaging
more with the societal implications of NLP models.",2025-05-21,Yu Zhang,http://arxiv.org/pdf/2505.16061v1,cs.CL
Causal LLM Routing: End-to-End Regret Minimization from Observational Data,"LLM routing aims to select the most appropriate model for each query,
balancing competing performance metrics such as accuracy and cost across a pool
of language models. Prior approaches typically adopt a decoupled strategy,
where the metrics are first predicted and the model is then selected based on
these estimates. This setup is prone to compounding errors and often relies on
full-feedback data, where each query is evaluated by all candidate models,
which is costly to obtain and maintain in practice. In contrast, we learn from
observational data, which records only the outcome of the model actually
deployed. We propose a causal end-to-end framework that learns routing policies
by minimizing decision-making regret from observational data. To enable
efficient optimization, we introduce two theoretically grounded surrogate
objectives: a classification-based upper bound, and a softmax-weighted regret
approximation shown to recover the optimal policy at convergence. We further
extend our framework to handle heterogeneous cost preferences via an
interval-conditioned architecture. Experiments on public benchmarks show that
our method outperforms existing baselines, achieving state-of-the-art
performance across different embedding models.",2025-05-21,"Asterios Tsiourvas, Wei Sun, Georgia Perakis",http://arxiv.org/pdf/2505.16037v1,cs.CL
OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models,"Generative large language models present significant potential but also raise
critical ethical concerns. Most studies focus on narrow ethical dimensions, and
also limited diversity of languages and models. To address these gaps, we
conduct a broad ethical evaluation of 29 recent open-source large language
models using a novel data collection including four ethical aspects:
Robustness, reliability, safety, and fairness. We analyze model behavior in
both a commonly used language, English, and a low-resource language, Turkish.
Our aim is to provide a comprehensive ethical assessment and guide safer model
development by filling existing gaps in evaluation breadth, language coverage,
and model diversity. Our experimental results, based on LLM-as-a-Judge, reveal
that optimization efforts for many open-source models appear to have
prioritized safety and fairness, and demonstrated good robustness while
reliability remains a concern. We demonstrate that ethical evaluation can be
effectively conducted independently of the language used. In addition, models
with larger parameter counts tend to exhibit better ethical performance, with
Gemma and Qwen models demonstrating the most ethical behavior among those
evaluated.",2025-05-21,"Burak Erinç Çetin, Yıldırım Özen, Elif Naz Demiryılmaz, Kaan Engür, Cagri Toraman",http://arxiv.org/pdf/2505.16036v1,cs.CL
Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild,"As large language models (LLMs) are used in complex writing workflows, users
engage in multi-turn interactions to steer generations to better fit their
needs. Rather than passively accepting output, users actively refine, explore,
and co-construct text. We conduct a large-scale analysis of this collaborative
behavior for users engaged in writing tasks in the wild with two popular AI
assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task
classification or satisfaction estimation common in prior work and instead
characterizes how users interact with LLMs through the course of a session. We
identify prototypical behaviors in how users interact with LLMs in prompts
following their original request. We refer to these as Prototypical Human-AI
Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a
majority of the variation seen in user-LLM interaction. These PATHs span users
revising intents, exploring texts, posing questions, adjusting style or
injecting new content. Next, we find statistically significant correlations
between specific writing intents and PATHs, revealing how users' intents shape
their collaboration behaviors. We conclude by discussing the implications of
our findings on LLM alignment.",2025-05-21,"Sheshera Mysore, Debarati Das, Hancheng Cao, Bahareh Sarrafzadeh",http://arxiv.org/pdf/2505.16023v2,cs.CL
NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning,"Recent advances such as DeepSeek R1-Zero highlight the effectiveness of
incentive training, a reinforcement learning paradigm that computes rewards
solely based on the final answer part of a language model's output, thereby
encouraging the generation of intermediate reasoning steps. However, these
methods fundamentally rely on external verifiers, which limits their
applicability to domains like mathematics and coding where such verifiers are
readily available. Although reward models can serve as verifiers, they require
high-quality annotated data and are costly to train. In this work, we propose
NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning
framework that requires only standard supervised fine-tuning data with no need
for an external verifier. NOVER enables incentive training across a wide range
of text-to-text tasks and outperforms the model of the same size distilled from
large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the
flexibility of NOVER enables new possibilities for optimizing large language
models, such as inverse incentive training.",2025-05-21,"Wei Liu, Siya Qi, Xinyu Wang, Chen Qian, Yali Du, Yulan He",http://arxiv.org/pdf/2505.16022v1,cs.CL
Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains,"Traditional Retrieval-Augmented Generation (RAG) pipelines rely on
similarity-based retrieval and re-ranking, which depend on heuristics such as
top-k, and lack explainability, interpretability, and robustness against
adversarial content. To address this gap, we propose a novel method METEORA
that replaces re-ranking in RAG with a rationale-driven selection approach.
METEORA operates in two stages. First, a general-purpose LLM is
preference-tuned to generate rationales conditioned on the input query using
direct preference optimization. These rationales guide the evidence chunk
selection engine, which selects relevant chunks in three stages: pairing
individual rationales with corresponding retrieved chunks for local relevance,
global selection with elbow detection for adaptive cutoff, and context
expansion via neighboring chunks. This process eliminates the need for top-k
heuristics. The rationales are also used for consistency check using a Verifier
LLM to detect and filter poisoned or misleading content for safe generation.
The framework provides explainable and interpretable evidence flow by using
rationales consistently across both selection and verification. Our evaluation
across six datasets spanning legal, financial, and academic research domains
shows that METEORA improves generation accuracy by 33.34% while using
approximately 50% fewer chunks than state-of-the-art re-ranking methods. In
adversarial settings, METEORA significantly improves the F1 score from 0.10 to
0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating
strong resilience to poisoning attacks. Code available at:
https://anonymous.4open.science/r/METEORA-DC46/README.md",2025-05-21,"Yash Saxena, Ankur Padia, Mandar S Chaudhary, Kalpa Gunaratna, Srinivasan Parthasarathy, Manas Gaur",http://arxiv.org/pdf/2505.16014v2,cs.CL
LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization,"We propose LAGO - Language Similarity-Aware Graph Optimization - a novel
approach for few-shot cross-lingual embedding inversion attacks, addressing
critical privacy vulnerabilities in multilingual NLP systems. Unlike prior work
in embedding inversion attacks that treat languages independently, LAGO
explicitly models linguistic relationships through a graph-based constrained
distributed optimization framework. By integrating syntactic and lexical
similarity as edge constraints, our method enables collaborative parameter
learning across related languages. Theoretically, we show this formulation
generalizes prior approaches, such as ALGEN, which emerges as a special case
when similarity constraints are relaxed. Our framework uniquely combines
Frobenius-norm regularization with linear inequality or total variation
constraints, ensuring robust alignment of cross-lingual embedding spaces even
with extremely limited data (as few as 10 samples per language). Extensive
experiments across multiple languages and embedding models demonstrate that
LAGO substantially improves the transferability of attacks with 10-20% increase
in Rouge-L score over baselines. This work establishes language similarity as a
critical factor in inversion attack transferability, urging renewed focus on
language-aware privacy-preserving multilingual embeddings.",2025-05-21,"Wenrui Yu, Yiyi Chen, Johannes Bjerva, Sokol Kosta, Qiongxiu Li",http://arxiv.org/pdf/2505.16008v1,cs.CL
Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations,"Sparse autoencoders (SAEs) are commonly used to interpret the internal
activations of large language models (LLMs) by mapping them to
human-interpretable concept representations. While existing evaluations of SAEs
focus on metrics such as the reconstruction-sparsity tradeoff, human
(auto-)interpretability, and feature disentanglement, they overlook a critical
aspect: the robustness of concept representations to input perturbations. We
argue that robustness must be a fundamental consideration for concept
representations, reflecting the fidelity of concept labeling. To this end, we
formulate robustness quantification as input-space optimization problems and
develop a comprehensive evaluation framework featuring realistic scenarios in
which adversarial perturbations are crafted to manipulate SAE representations.
Empirically, we find that tiny adversarial input perturbations can effectively
manipulate concept-based interpretations in most scenarios without notably
affecting the outputs of the base LLMs themselves. Overall, our results suggest
that SAE concept representations are fragile and may be ill-suited for
applications in model monitoring and oversight.",2025-05-21,"Aaron J. Li, Suraj Srinivas, Usha Bhalla, Himabindu Lakkaraju",http://arxiv.org/pdf/2505.16004v1,cs.CL
SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models,"The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for
evaluating language models. Although several calibration techniques have been
proposed to better align these evaluators with human judgment, prior studies
focus primarily on narrow, well-structured benchmarks. As a result, it remains
unclear whether such calibrations generalize to real-world, open-ended tasks.
  In this work, we show that SOTA calibrated evaluators often fail in these
settings, exhibiting weak or even negative correlation with human judgments. To
address this, we propose SLMEval, a novel and efficient calibration method
based on entropy maximization over a small amount of human preference data. By
estimating a latent distribution over model quality and reweighting evaluator
scores accordingly, SLMEval achieves strong correlation with human evaluations
across two real-world production use cases and the public benchmark. For
example, on one such task, SLMEval achieves a Spearman correlation of 0.57 with
human judgments, while G-Eval yields a negative correlation. In addition,
SLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated
evaluators such as G-eval.",2025-05-21,"Roland Daynauth, Christopher Clarke, Krisztian Flautner, Lingjia Tang, Jason Mars",http://arxiv.org/pdf/2505.16003v1,cs.CL
Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions,"Large Language Models (LLMs) have emerged as powerful sources of evidence for
linguists seeking to develop theories of syntax. In this paper, we argue that
causal interpretability methods, applied to LLMs, can greatly enhance the value
of such evidence by helping us characterize the abstract mechanisms that LLMs
learn to use. Our empirical focus is a set of English filler-gap dependency
constructions (e.g., questions, relative clauses). Linguistic theories largely
agree that these constructions share many properties. Using experiments based
in Distributed Interchange Interventions, we show that LLMs converge on similar
abstract analyses of these constructions. These analyses also reveal previously
overlooked factors -- relating to frequency, filler type, and surrounding
context -- that could motivate changes to standard linguistic theory. Overall,
these results suggest that mechanistic, internal analyses of LLMs can push
linguistic theory forward.",2025-05-21,"Sasha Boguraev, Christopher Potts, Kyle Mahowald",http://arxiv.org/pdf/2505.16002v1,cs.CL
Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model,"The rapid advancement of language models has demonstrated the potential of
artificial intelligence in the healthcare industry. However, small language
models struggle with specialized domains in low-resource languages like
Persian. While numerous medical-domain websites exist in Persian, no curated
dataset or corpus has been available making ours the first of its kind. This
study explores the enhancement of medical knowledge in a small language model
by leveraging accessible online data, including a crawled corpus from medical
magazines and a dataset of real doctor-patient QA pairs. We fine-tuned a
baseline model using our curated data to improve its medical knowledge.
Benchmark evaluations demonstrate that the fine-tuned model achieves improved
accuracy in medical question answering and provides better responses compared
to its baseline. This work highlights the potential of leveraging open-access
online data to enrich small language models in medical fields, providing a
novel solution for Persian medical AI applications suitable for
resource-constrained environments.",2025-05-21,"Mehrdad Ghassabi, Pedram Rostami, Hamidreza Baradaran Kashani, Amirhossein Poursina, Zahra Kazemi, Milad Tavakoli",http://arxiv.org/pdf/2505.16000v2,cs.CL
Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku,"The success of Large Language Models (LLMs) in human-AI collaborative
decision-making hinges on their ability to provide trustworthy, gradual, and
tailored explanations. Solving complex puzzles, such as Sudoku, offers a
canonical example of this collaboration, where clear and customized
explanations often hold greater importance than the final solution. In this
study, we evaluate the performance of five LLMs in solving and explaining
\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving
puzzles, none can explain the solution process in a manner that reflects
strategic reasoning or intuitive problem-solving. These findings underscore
significant challenges that must be addressed before LLMs can become effective
partners in human-AI collaborative decision-making.",2025-05-21,"Anirudh Maiya, Razan Alghamdi, Maria Leonor Pacheco, Ashutosh Trivedi, Fabio Somenzi",http://arxiv.org/pdf/2505.15993v1,cs.CL
Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning,"Chain-of-thought reasoning has significantly improved the performance of
Large Language Models (LLMs) across various domains. However, this reasoning
process has been confined exclusively to textual space, limiting its
effectiveness in visually intensive tasks. To address this limitation, we
introduce the concept of reasoning in the pixel-space. Within this novel
framework, Vision-Language Models (VLMs) are equipped with a suite of visual
reasoning operations, such as zoom-in and select-frame. These operations enable
VLMs to directly inspect, interrogate, and infer from visual evidences, thereby
enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space
reasoning capabilities in VLMs presents notable challenges, including the
model's initially imbalanced competence and its reluctance to adopt the newly
introduced pixel-space operations. We address these challenges through a
two-phase training approach. The first phase employs instruction tuning on
synthesized reasoning traces to familiarize the model with the novel visual
operations. Following this, a reinforcement learning (RL) phase leverages a
curiosity-driven reward scheme to balance exploration between pixel-space
reasoning and textual reasoning. With these visual operations, VLMs can
interact with complex visual inputs, such as information-rich images or videos
to proactively gather necessary information. We demonstrate that this approach
significantly improves VLM performance across diverse visual reasoning
benchmarks. Our 7B model, \model, achieves 84\% on V* bench, 74\% on
TallyQA-Complex, and 84\% on InfographicsVQA, marking the highest accuracy
achieved by any open-source model to date. These results highlight the
importance of pixel-space reasoning and the effectiveness of our framework.",2025-05-21,"Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, Wenhu Chen",http://arxiv.org/pdf/2505.15966v2,cs.CL
OViP: Online Vision-Language Preference Learning,"Large vision-language models (LVLMs) remain vulnerable to hallucination,
often generating content misaligned with visual inputs. While recent approaches
advance multi-modal Direct Preference Optimization (DPO) to mitigate
hallucination, they typically rely on predefined or randomly edited negative
samples that fail to reflect actual model errors, limiting training efficacy.
In this work, we propose an Online Vision-language Preference Learning (OViP)
framework that dynamically constructs contrastive training data based on the
model's own hallucinated outputs. By identifying semantic differences between
sampled response pairs and synthesizing negative images using a diffusion
model, OViP generates more relevant supervision signals in real time. This
failure-driven training enables adaptive alignment of both textual and visual
preferences. Moreover, we refine existing evaluation protocols to better
capture the trade-off between hallucination suppression and expressiveness.
Experiments on hallucination and general benchmarks demonstrate that OViP
effectively reduces hallucinations while preserving core multi-modal
capabilities.",2025-05-21,"Shujun Liu, Siyuan Wang, Zejun Li, Jianxiang Wang, Cheng Zeng, Zhongyu Wei",http://arxiv.org/pdf/2505.15963v1,cs.CL
Pre-training Large Memory Language Models with Internal and External Knowledge,"Neural language models are black-boxes -- both linguistic patterns and
factual knowledge are distributed across billions of opaque parameters. This
entangled encoding makes it difficult to reliably inspect, verify, or update
specific facts. We propose a new class of language models, Large Memory
Language Models (LMLM) with a pre-training recipe that stores factual knowledge
in both internal weights and an external database. Our approach strategically
masks externally retrieved factual values from the training loss, thereby
teaching the model to perform targeted lookups rather than relying on
memorization in model weights. Our experiments demonstrate that LMLMs achieve
competitive performance compared to significantly larger, knowledge-dense LLMs
on standard benchmarks, while offering the advantages of explicit, editable,
and verifiable knowledge bases. This work represents a fundamental shift in how
language models interact with and manage factual knowledge.",2025-05-21,"Linxi Zhao, Sofian Zalouk, Christian K. Belardi, Justin Lovelace, Jin Peng Zhou, Kilian Q. Weinberger, Yoav Artzi, Jennifer J. Sun",http://arxiv.org/pdf/2505.15962v1,cs.CL
Training Step-Level Reasoning Verifiers with Formal Verification Tools,"Process Reward Models (PRMs), which provide step-by-step feedback on the
reasoning generated by Large Language Models (LLMs), are receiving increasing
attention. However, two key research gaps remain: collecting accurate
step-level error labels for training typically requires costly human
annotation, and existing PRMs are limited to math reasoning problems. In
response to these gaps, this paper aims to address the challenges of automatic
dataset creation and the generalization of PRMs to diverse reasoning tasks. To
achieve this goal, we propose FoVer, an approach for training PRMs on
step-level error labels automatically annotated by formal verification tools,
such as Z3 for formal logic and Isabelle for theorem proof, which provide
automatic and accurate verification for symbolic tasks. Using this approach, we
synthesize a training dataset with error labels on LLM responses for formal
logic and theorem proof tasks without human annotation. Although this data
synthesis is feasible only for tasks compatible with formal verification, we
observe that LLM-based PRMs trained on our dataset exhibit cross-task
generalization, improving verification across diverse reasoning tasks.
Specifically, PRMs trained with FoVer significantly outperform baseline PRMs
based on the original LLMs and achieve competitive or superior results compared
to state-of-the-art PRMs trained on labels annotated by humans or stronger
models, as measured by step-level verification on ProcessBench and Best-of-K
performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU,
and BBH. The datasets, models, and code are provided at
https://github.com/psunlpgroup/FoVer.",2025-05-21,"Ryo Kamoi, Yusen Zhang, Nan Zhang, Sarkar Snigdha Sarathi Das, Rui Zhang",http://arxiv.org/pdf/2505.15960v1,cs.CL
Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey,"With advancements in large audio-language models (LALMs), which enhance large
language models (LLMs) with auditory capabilities, these models are expected to
demonstrate universal proficiency across various auditory tasks. While numerous
benchmarks have emerged to assess LALMs' performance, they remain fragmented
and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive
survey and propose a systematic taxonomy for LALM evaluations, categorizing
them into four dimensions based on their objectives: (1) General Auditory
Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented
Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed
overviews within each category and highlight challenges in this field, offering
insights into promising future directions. To the best of our knowledge, this
is the first survey specifically focused on the evaluations of LALMs, providing
clear guidelines for the community. We will release the collection of the
surveyed papers and actively maintain it to support ongoing advancements in the
field.",2025-05-21,"Chih-Kai Yang, Neo S. Ho, Hung-yi Lee",http://arxiv.org/pdf/2505.15957v2,cs.CL
Citation Parsing and Analysis with Language Models,"A key type of resource needed to address global inequalities in knowledge
production and dissemination is a tool that can support journals in
understanding how knowledge circulates. The absence of such a tool has resulted
in comparatively less information about networks of knowledge sharing in the
Global South. In turn, this gap authorizes the exclusion of researchers and
scholars from the South in indexing services, reinforcing colonial arrangements
that de-center and minoritize those scholars. In order to support citation
network tracking on a global scale, we investigate the capacity of open-weight
language models to mark up manuscript citations in an indexable format. We
assembled a dataset of matched plaintext and annotated citations from preprints
and published research papers. Then, we evaluated a number of open-weight
language models on the annotation task. We find that, even out of the box,
today's language models achieve high levels of accuracy on identifying the
constituent components of each citation, outperforming state-of-the-art
methods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all
fields with high accuracy in $2^5$ passes, suggesting that post-training is
likely to be effective in producing small, robust citation parsing models. Such
a tool could greatly improve the fidelity of citation networks and thus
meaningfully improve research indexing and discovery, as well as further
metascientific research.",2025-05-21,"Parth Sarin, Juan Pablo Alperin",http://arxiv.org/pdf/2505.15948v1,cs.CL
MAPS: A Multilingual Benchmark for Global Agent Performance and Security,"Agentic AI systems, which build on Large Language Models (LLMs) and interact
with tools and memory, have rapidly advanced in capability and scope. Yet,
since LLMs have been shown to struggle in multilingual settings, typically
resulting in lower performance and reduced safety, agentic systems risk
inheriting these limitations. This raises concerns about the global
accessibility of such systems, as users interacting in languages other than
English may encounter unreliable or security-critical agent behavior. Despite
growing interest in evaluating agentic AI, existing benchmarks focus
exclusively on English, leaving multilingual settings unexplored. To address
this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate
agentic AI systems across diverse languages and tasks. MAPS builds on four
widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code
generation), MATH (mathematical reasoning), and the Agent Security Benchmark
(security). We translate each dataset into ten diverse languages, resulting in
805 unique tasks and 8,855 total language-specific instances. Our benchmark
suite enables a systematic analysis of how multilingual contexts affect agent
performance and robustness. Empirically, we observe consistent degradation in
both performance and security when transitioning from English to other
languages, with severity varying by task and correlating with the amount of
translated input. Building on these findings, we provide actionable
recommendations to guide agentic AI systems development and assessment under
multilingual settings. This work establishes a standardized evaluation
framework, encouraging future research towards equitable, reliable, and
globally accessible agentic AI. MAPS benchmark suite is publicly available at
https://huggingface.co/datasets/Fujitsu-FRE/MAPS",2025-05-21,"Omer Hofman, Oren Rachmil, Shamik Bose, Vikas Pahuja, Jonathan Brokman, Toshiya Shimizu, Trisha Starostina, Kelly Marchisio, Seraphina Goldfarb-Tarrant, Roman Vainshtein",http://arxiv.org/pdf/2505.15935v1,cs.CL
ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation,"Recent advancements in Video Question Answering (VideoQA) have introduced
LLM-based agents, modular frameworks, and procedural solutions, yielding
promising results. These systems use dynamic agents and memory-based mechanisms
to break down complex tasks and refine answers. However, significant
improvements remain in tracking objects for grounding over time and
decision-making based on reasoning to better align object references with
language model outputs, as newer models get better at both tasks. This work
presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA)
that combines a Chain-of-Thought framework with grounding reasoning alongside
YOLO-World to enhance object tracking and alignment. This approach establishes
a new state-of-the-art in VideoQA and Video Understanding, showing enhanced
performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also
enables cross-checking of grounding timeframes, improving accuracy and
providing valuable support for verification and increased output reliability
across multiple video domains. The code is available at
https://github.com/t-montes/viqagent.",2025-05-21,"Tony Montes, Fernando Lozano",http://arxiv.org/pdf/2505.15928v1,cs.CL
Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition,"We propose a large language model based reward decomposition framework for
aligning dialogue agents using only a single session-level feedback signal. We
leverage the reasoning capabilities of a frozen, pretrained large language
model (LLM) to infer fine-grained local implicit rewards by decomposing global,
session-level feedback. Our first text-only variant prompts the LLM to perform
reward decomposition using only the dialogue transcript. The second multimodal
variant incorporates additional behavioral cues, such as pitch, gaze, and
facial affect, expressed as natural language descriptions. These inferred
turn-level rewards are distilled into a lightweight reward model, which we
utilize for RL-based fine-tuning for dialogue generation. We evaluate both
text-only and multimodal variants against state-of-the-art reward decomposition
methods and demonstrate notable improvements in human evaluations of
conversation quality, suggesting that LLMs are strong reward decomposers that
obviate the need for manual reward shaping and granular human feedback.",2025-05-21,"Dong Won Lee, Hae Won Park, Cynthia Breazeal, Louis-Philippe Morency",http://arxiv.org/pdf/2505.15922v1,cs.CL
Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization,"Large Language Models (LLMs) have demonstrated potential as factual knowledge
bases; however, their capability to generate probabilistic knowledge about
real-world events remains understudied. This paper investigates using
probabilistic knowledge inherent in LLMs to derive probability estimates for
statements concerning events and their interrelationships captured via a
Bayesian Network (BN). Using LLMs in this context allows for the
parameterization of BNs, enabling probabilistic modeling within specific
domains. Experiments on eighty publicly available Bayesian Networks, from
healthcare to finance, demonstrate that querying LLMs about the conditional
probabilities of events provides meaningful results when compared to baselines,
including random and uniform distributions, as well as approaches based on
next-token generation probabilities. We explore how these LLM-derived
distributions can serve as expert priors to refine distributions extracted from
minimal data, significantly reducing systematic biases. Overall, this work
introduces a promising strategy for automatically constructing Bayesian
Networks by combining probabilistic knowledge extracted from LLMs with small
amounts of real-world data. Additionally, we evaluate several prompting
strategies for eliciting probabilistic knowledge from LLMs and establish the
first comprehensive baseline for assessing LLM performance in extracting
probabilistic knowledge.",2025-05-21,"Aliakbar Nafar, Kristen Brent Venable, Zijun Cui, Parisa Kordjamshidi",http://arxiv.org/pdf/2505.15918v1,cs.CL
"BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law","This paper presents BR-TaxQA-R, a novel dataset designed to support question
answering with references in the context of Brazilian personal income tax law.
The dataset contains 715 questions from the 2024 official Q\&A document
published by Brazil's Internal Revenue Service, enriched with statutory norms
and administrative rulings from the Conselho Administrativo de Recursos Fiscais
(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using
OpenAI embeddings for searching and GPT-4o-mini for answer generation. We
compare different text segmentation strategies and benchmark our system against
commercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.
Results show that our custom RAG pipeline outperforms commercial systems in
Response Relevancy, indicating stronger alignment with user queries, while
commercial models achieve higher scores in Factual Correctness and fluency.
These findings highlight a trade-off between legally grounded generation and
linguistic fluency. Crucially, we argue that human expert evaluation remains
essential to ensure the legal validity of AI-generated answers in high-stakes
domains such as taxation. BR-TaxQA-R is publicly available at
https://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.",2025-05-21,"Juvenal Domingos Júnior, Augusto Faria, E. Seiti de Oliveira, Erick de Brito, Matheus Teotonio, Andre Assumpção, Diedre Carmo, Roberto Lotufo, Jayr Pereira",http://arxiv.org/pdf/2505.15916v1,cs.CL
Learning to Reason via Mixture-of-Thought for Logical Reasoning,"Human beings naturally utilize multiple reasoning modalities to learn and
solve logical problems, i.e., different representational formats such as
natural language, code, and symbolic logic. In contrast, most existing
LLM-based approaches operate with a single reasoning modality during training,
typically natural language. Although some methods explored modality selection
or augmentation at inference time, the training process remains modality-blind,
limiting synergy among modalities. To fill in this gap, we propose
Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three
complementary modalities: natural language, code, and a newly introduced
symbolic modality, truth-table, which systematically enumerates logical cases
and partially mitigates key failure modes in natural language reasoning. MoT
adopts a two-phase design: (1) self-evolving MoT training, which jointly learns
from filtered, self-generated rationales across modalities; and (2) MoT
inference, which fully leverages the synergy of three modalities to produce
better predictions. Experiments on logical reasoning benchmarks including FOLIO
and ProofWriter demonstrate that our MoT framework consistently and
significantly outperforms strong LLM baselines with single-modality
chain-of-thought approaches, achieving up to +11.7pp average accuracy gain.
Further analyses show that our MoT framework benefits both training and
inference stages; that it is particularly effective on harder logical reasoning
problems; and that different modalities contribute complementary strengths,
with truth-table reasoning helping to overcome key bottlenecks in natural
language inference.",2025-05-21,"Tong Zheng, Lichang Chen, Simeng Han, R. Thomas McCoy, Heng Huang",http://arxiv.org/pdf/2505.15817v1,cs.CL
MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation,"Recent advances in Large Language Models (LLMs) have shown promising results
in complex reasoning tasks. However, current evaluations predominantly focus on
single-turn reasoning scenarios, leaving interactive tasks largely unexplored.
We attribute it to the absence of comprehensive datasets and scalable automatic
evaluation protocols. To fill these gaps, we present MTR-Bench for LLMs'
Multi-Turn Reasoning evaluation. Comprising 4 classes, 40 tasks, and 3600
instances, MTR-Bench covers diverse reasoning capabilities, fine-grained
difficulty granularity, and necessitates multi-turn interactions with the
environments. Moreover, MTR-Bench features fully-automated framework spanning
both dataset constructions and model evaluations, which enables scalable
assessment without human interventions. Extensive experiments reveal that even
the cutting-edge reasoning models fall short of multi-turn, interactive
reasoning tasks. And the further analysis upon these results brings valuable
insights for future research in interactive AI systems.",2025-05-21,"Xiaoyuan Li, Keqin Bao, Yubo Ma, Moxin Li, Wenjie Wang, Rui Men, Yichang Zhang, Fuli Feng, Dayiheng Liu, Junyang Lin",http://arxiv.org/pdf/2505.17123v2,cs.CL
GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents,"Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,
coupling online Reinforcement Learning (RL) with explicit chain-of-thought
reasoning prior to object grounding and thereby achieving substantial
performance gains. In this paper, we first conduct extensive analysis
experiments of three key components of that training pipeline: input design,
output evaluation, and policy update-each revealing distinct challenges arising
from blindly applying general-purpose RL without adapting to GUI grounding
tasks. Input design: Current templates encourage the model to generate
chain-of-thought reasoning, but longer chains unexpectedly lead to worse
grounding performance. Output evaluation: Reward functions based on hit signals
or box area allow models to exploit box size, leading to reward hacking and
poor localization quality. Policy update: Online RL tends to overfit easy
examples due to biases in length and sample difficulty, leading to
under-optimization on harder cases. To address these issues, we propose three
targeted solutions. First, we adopt a Fast Thinking Template that encourages
direct answer generation, reducing excessive reasoning during training. Second,
we incorporate a box size constraint into the reward function to mitigate
reward hacking. Third, we revise the RL objective by adjusting length
normalization and adding a difficulty-aware scaling factor, enabling better
optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with
Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on
ScreenSpot-Pro. This surpasses all prior models of similar size and even
outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI
agent grounding. The project repository is available at
https://github.com/Yuqi-Zhou/GUI-G1.",2025-05-21,"Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, Jun Xu",http://arxiv.org/pdf/2505.15810v2,cs.CL
Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?,"Aligning large language models (LLMs) with human preferences remains a key
challenge in AI. Preference-based optimization methods, such as Reinforcement
Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO),
rely on human-annotated datasets to improve alignment. In this work, we
identify a crucial property of the existing learning method: the distinguishing
signal obtained in preferred responses is often concentrated in the early
tokens. We refer to this as shallow preference signals.
  To explore this property, we systematically truncate preference datasets at
various points and train both reward models and DPO models on the truncated
data. Surprisingly, models trained on truncated datasets, retaining only the
first half or fewer tokens, achieve comparable or even superior performance to
those trained on full datasets. For example, a reward model trained on the
Skywork-Reward-Preference-80K-v0.2 dataset outperforms the full dataset when
trained on a 40\% truncated dataset. This pattern is consistent across multiple
datasets, suggesting the widespread presence of shallow preference signals.
  We further investigate the distribution of the reward signal through decoding
strategies. We consider two simple decoding strategies motivated by the shallow
reward signal observation, namely Length Control Decoding and KL Threshold
Control Decoding, which leverage shallow preference signals to optimize the
trade-off between alignment and computational efficiency. The performance is
even better, which again validates our hypothesis.
  The phenomenon of shallow preference signals highlights potential issues in
LLM alignment: existing alignment methods often focus on aligning only the
initial tokens of responses, rather than considering the full response. This
could lead to discrepancies with real-world human preferences, resulting in
suboptimal alignment performance.",2025-05-21,"Xuan Qi, Jiahao Qiu, Xinzhe Juan, Yue Wu, Mengdi Wang",http://arxiv.org/pdf/2505.17122v1,cs.CL
The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation,"Large language models are able to exploit in-context learning to access
external knowledge beyond their training data through retrieval-augmentation.
While promising, its inner workings remain unclear. In this work, we shed light
on the mechanism of in-context retrieval augmentation for question answering by
viewing a prompt as a composition of informational components. We propose an
attribution-based method to identify specialized attention heads, revealing
in-context heads that comprehend instructions and retrieve relevant contextual
information, and parametric heads that store entities' relational knowledge. To
better understand their roles, we extract function vectors and modify their
attention weights to show how they can influence the answer generation process.
Finally, we leverage the gained insights to trace the sources of knowledge used
during inference, paving the way towards more safe and transparent language
models.",2025-05-21,"Patrick Kahardipraja, Reduan Achtibat, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin",http://arxiv.org/pdf/2505.15807v1,cs.CL
Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering,"As Large Language Models (LLMs) are increasingly deployed in sensitive
domains such as enterprise and government, ensuring that they adhere to
user-defined security policies within context is critical-especially with
respect to information non-disclosure. While prior LLM studies have focused on
general safety and socially sensitive data, large-scale benchmarks for
contextual security preservation against attacks remain lacking. To address
this, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating
LLM adherence to contextual non-disclosure policies in question answering.
Derived from realistic contexts, our dataset includes explicit policies and
queries designed as direct and challenging indirect attacks seeking prohibited
information. We evaluate 10 LLMs on our benchmark and reveal a significant
vulnerability: many models violate user-defined policies and leak sensitive
information. This failure is particularly severe against indirect attacks,
highlighting a critical gap in current LLM safety alignment for sensitive
applications. Our analysis reveals that while models can often identify the
correct answer to a query, they struggle to incorporate policy constraints
during generation. In contrast, they exhibit a partial ability to revise
outputs when explicitly prompted. Our findings underscore the urgent need for
more robust methods to guarantee contextual security.",2025-05-21,"Hwan Chang, Yumin Kim, Yonghyun Jun, Hwanhee Lee",http://arxiv.org/pdf/2505.15805v1,cs.CL
GRIT: Teaching MLLMs to Think with Images,"Recent studies have demonstrated the efficacy of using Reinforcement Learning
(RL) in building reasoning models that articulate chains of thoughts prior to
producing final answers. However, despite ongoing advances that aim at enabling
reasoning for vision-language tasks, existing open-source visual reasoning
models typically generate reasoning content with pure natural language, lacking
explicit integration of visual information. This limits their ability to
produce clearly articulated and visually grounded reasoning chains. To this
end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method
for training MLLMs to think with images. GRIT introduces a grounded reasoning
paradigm, in which models generate reasoning chains that interleave natural
language and explicit bounding box coordinates. These coordinates point to
regions of the input image that the model consults during its reasoning
process. Additionally, GRIT is equipped with a reinforcement learning approach,
GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused
on the final answer accuracy and format of the grounded reasoning output, which
eliminates the need for data with reasoning chain annotations or explicit
bounding box labels. As a result, GRIT achieves exceptional data efficiency,
requiring as few as 20 image-question-answer triplets from existing datasets.
Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to
produce coherent and visually grounded reasoning chains, showing a successful
unification of reasoning and grounding abilities.",2025-05-21,"Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, Xin Eric Wang",http://arxiv.org/pdf/2505.15879v1,cs.CL
VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models,"Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved
remarkable performance in the domain of reasoning. A key component of their
training is the incorporation of verifiable rewards within reinforcement
learning (RL). However, existing reward benchmarks do not evaluate
reference-based reward systems, leaving researchers with limited understanding
of the accuracy of verifiers used in RL. In this paper, we introduce two
benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the
performance of reference-based reward systems. These benchmarks are constructed
through meticulous data collection and curation, followed by careful human
annotation to ensure high quality. Current models still show considerable room
for improvement on both VerifyBench and VerifyBench-Hard, especially
smaller-scale models. Furthermore, we conduct a thorough and comprehensive
analysis of evaluation results, offering insights for understanding and
developing reference-based reward systems. Our proposed benchmarks serve as
effective tools for guiding the development of verifier accuracy and the
reasoning capabilities of models trained via RL in reasoning tasks.",2025-05-21,"Yuchen Yan, Jin Jiang, Zhenbang Ren, Yijun Li, Xudong Cai, Yang Liu, Xin Xu, Mengdi Zhang, Jian Shao, Yongliang Shen, Jun Xiao, Yueting Zhuang",http://arxiv.org/pdf/2505.15801v2,cs.CL
Reverse Engineering Human Preferences with Reinforcement Learning,"The capabilities of Large Language Models (LLMs) are routinely evaluated by
other LLMs trained to predict human preferences. This framework--known as
LLM-as-a-judge--is highly scalable and relatively low cost. However, it is also
vulnerable to malicious exploitation, as LLM responses can be tuned to overfit
the preferences of the judge. Previous work shows that the answers generated by
a candidate-LLM can be edited post hoc to maximise the score assigned to them
by a judge-LLM. In this study, we adopt a different approach and use the signal
provided by judge-LLMs as a reward to adversarially tune models that generate
text preambles designed to boost downstream performance. We find that frozen
LLMs pipelined with these models attain higher LLM-evaluation scores than
existing frameworks. Crucially, unlike other frameworks which intervene
directly on the model's response, our method is virtually undetectable. We also
demonstrate that the effectiveness of the tuned preamble generator transfers
when the candidate-LLM and the judge-LLM are replaced with models that are not
used during training. These findings raise important questions about the design
of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that
human preferences can be reverse engineered effectively, by pipelining LLMs to
optimise upstream preambles via reinforcement learning--an approach that could
find future applications in diverse tasks and domains beyond adversarial
attacks.",2025-05-21,"Lisa Alazraki, Tan Yi-Chern, Jon Ander Campos, Maximilian Mozes, Marek Rei, Max Bartolo",http://arxiv.org/pdf/2505.15795v1,cs.CL
Long-Form Information Alignment Evaluation Beyond Atomic Facts,"Information alignment evaluators are vital for various NLG evaluation tasks
and trustworthy LLM deployment, reducing hallucinations and enhancing user
trust. Current fine-grained methods, like FactScore, verify facts individually
but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this
work, we introduce MontageLie, a challenging benchmark that constructs
deceptive narratives by ""montaging"" truthful statements without introducing
explicit hallucinations. We demonstrate that both coarse-grained LLM-based
evaluators and current fine-grained frameworks are susceptible to this attack,
with AUC-ROC scores falling below 65%. To enable more robust fine-grained
evaluation, we propose DoveScore, a novel framework that jointly verifies
factual accuracy and event-order consistency. By modeling inter-fact
relationships, DoveScore outperforms existing fine-grained methods by over 8%,
providing a more robust solution for long-form text alignment evaluation. Our
code and datasets are available at https://github.com/dannalily/DoveScore.",2025-05-21,"Danna Zheng, Mirella Lapata, Jeff Z. Pan",http://arxiv.org/pdf/2505.15792v1,cs.CL
Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval,"While an image is worth more than a thousand words, only a few provide
crucial information for a given task and thus should be focused on. In light of
this, ideal text-to-image (T2I) retrievers should prioritize specific visual
attributes relevant to queries. To evaluate current retrievers on handling
attribute-focused queries, we build COCO-Facet, a COCO-based benchmark with
9,112 queries about diverse attributes of interest. We find that CLIP-like
retrievers, which are widely adopted due to their efficiency and zero-shot
ability, have poor and imbalanced performance, possibly because their image
embeddings focus on global semantics and subjects while leaving out other
details. Notably, we reveal that even recent Multimodal Large Language Model
(MLLM)-based, stronger retrievers with a larger output dimension struggle with
this limitation. Hence, we hypothesize that retrieving with general image
embeddings is suboptimal for performing such queries. As a solution, we propose
to use promptable image embeddings enabled by these multimodal retrievers,
which boost performance by highlighting required attributes. Our pipeline for
deriving such embeddings generalizes across query types, image pools, and base
retriever architectures. To enhance real-world applicability, we offer two
acceleration strategies: Pre-processing promptable embeddings and using linear
approximations. We show that the former yields a 15% improvement in Recall@5
when prompts are predefined, while the latter achieves an 8% improvement when
prompts are only available during inference.",2025-05-21,"Siting Li, Xiang Gao, Simon Shaolei Du",http://arxiv.org/pdf/2505.15877v1,cs.CL
Large Language Models as Computable Approximations to Solomonoff Induction,"The rapid advancement of large language models (LLMs) calls for a rigorous
theoretical framework to explain their empirical success. While significant
progress has been made in understanding LLM behaviors, existing theoretical
frameworks remain fragmented in explaining emergent phenomena through a unified
mathematical lens. We establish the first formal connection between LLM
architectures and Algorithmic Information Theory (AIT) by proving two
fundamental results: (1) the training process computationally approximates
Solomonoff prior through loss minimization interpreted as program length
optimization, and (2) next-token prediction implements approximate Solomonoff
induction. We leverage AIT to provide a unified theoretical explanation for
in-context learning, few-shot learning, and scaling laws. Furthermore, our
theoretical insights lead to a principled method for few-shot example selection
that prioritizes samples where models exhibit lower predictive confidence. We
demonstrate through experiments on diverse text classification benchmarks that
this strategy yields significant performance improvements, particularly for
smaller model architectures, when compared to selecting high-confidence
examples. Our framework bridges the gap between theoretical foundations and
practical LLM behaviors, providing both explanatory power and actionable
insights for future model development.",2025-05-21,"Jun Wan, Lingrui Mei",http://arxiv.org/pdf/2505.15784v1,cs.CL
dKV-Cache: The Cache for Diffusion Language Models,"Diffusion Language Models (DLMs) have been seen as a promising competitor for
autoregressive language models. However, diffusion language models have long
been constrained by slow inference. A core challenge is that their
non-autoregressive architecture and bidirectional attention preclude the
key-value cache that accelerates decoding. We address this bottleneck by
proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising
process of DLMs. Our approach is motivated by the observation that different
tokens have distinct representation dynamics throughout the diffusion process.
Accordingly, we propose a delayed and conditioned caching strategy for key and
value states. We design two complementary variants to cache key and value
step-by-step: (1) dKV-Cache-Decode, which provides almost lossless
acceleration, and even improves performance on long sequences, suggesting that
existing DLMs may under-utilise contextual information during inference. (2)
dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving
higher speed-ups with quadratic time complexity at the cost of some performance
degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,
largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on
several benchmarks, delivering acceleration across general language
understanding, mathematical, and code-generation benchmarks. Experiments
demonstrate that cache can also be used in DLMs, even in a training-free manner
from current DLMs.",2025-05-21,"Xinyin Ma, Runpeng Yu, Gongfan Fang, Xinchao Wang",http://arxiv.org/pdf/2505.15781v1,cs.CL
Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space,"Human cognition typically involves thinking through abstract, fluid concepts
rather than strictly using discrete linguistic tokens. Current reasoning
models, however, are constrained to reasoning within the boundaries of human
language, processing discrete token embeddings that represent fixed points in
the semantic space. This discrete constraint restricts the expressive power and
upper potential of such reasoning models, often causing incomplete exploration
of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling
one token per step. In this work, we introduce Soft Thinking, a training-free
method that emulates human-like ""soft"" reasoning by generating soft, abstract
concept tokens in a continuous concept space. These concept tokens are created
by the probability-weighted mixture of token embeddings, which form the
continuous concept space, enabling smooth transitions and richer
representations that transcend traditional discrete boundaries. In essence,
each generated concept token encapsulates multiple meanings from related
discrete tokens, implicitly exploring various reasoning paths to converge
effectively toward the correct answer. Empirical evaluations on diverse
mathematical and coding benchmarks consistently demonstrate the effectiveness
and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points
while simultaneously reducing token usage by up to 22.4% compared to standard
CoT. Qualitative analysis further reveals that Soft Thinking outputs remain
highly interpretable and readable, highlighting the potential of Soft Thinking
to break the inherent bottleneck of discrete language-based reasoning. Code is
available at https://github.com/eric-ai-lab/Soft-Thinking.",2025-05-21,"Zhen Zhang, Xuehai He, Weixiang Yan, Ao Shen, Chenyang Zhao, Shuohang Wang, Yelong Shen, Xin Eric Wang",http://arxiv.org/pdf/2505.15778v1,cs.CL
ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning,"Conversational search systems require effective handling of context-dependent
queries that often contain ambiguity, omission, and coreference. Conversational
Query Reformulation (CQR) addresses this challenge by transforming these
queries into self-contained forms suitable for off-the-shelf retrievers.
However, existing CQR approaches suffer from two critical constraints: high
dependency on costly external supervision from human annotations or large
language models, and insufficient alignment between the rewriting model and
downstream retrievers. We present ConvSearch-R1, the first self-driven
framework that completely eliminates dependency on external rewrite supervision
by leveraging reinforcement learning to optimize reformulation directly through
retrieval signals. Our novel two-stage approach combines Self-Driven Policy
Warm-Up to address the cold-start problem through retrieval-guided
self-distillation, followed by Retrieval-Guided Reinforcement Learning with a
specially designed rank-incentive reward shaping mechanism that addresses the
sparsity issue in conventional retrieval metrics. Extensive experiments on
TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly
outperforms previous state-of-the-art methods, achieving over 10% improvement
on the challenging TopiOCQA dataset while using smaller 3B parameter models
without any external supervision.",2025-05-21,"Changtai Zhu, Siyin Wang, Ruijun Feng, Kai Song, Xipeng Qiu",http://arxiv.org/pdf/2505.15776v1,cs.CL
Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention,"Large Language Models (LLMs) encounter significant challenges in
long-sequence inference due to computational inefficiency and redundant
processing, driving interest in context compression techniques. Existing
methods often rely on token importance to perform hard local compression or
encode context into latent representations for soft global compression.
However, the uneven distribution of textual content relevance and the diversity
of demands for user instructions mean these approaches frequently lead to the
loss of potentially valuable information. To address this, we propose
$\textbf{Hy}$brid $\textbf{Co}$ntext $\textbf{Co}$mpression (HyCo$_2$) for
LLMs, which integrates both global and local perspectives to guide context
compression while retaining both the essential semantics and critical details
for task completion. Specifically, we employ a hybrid adapter to refine global
semantics with the global view, based on the observation that different
adapters excel at different tasks. Then we incorporate a classification layer
that assigns a retention probability to each context token based on the local
view, determining whether it should be retained or discarded. To foster a
balanced integration of global and local compression, we introduce auxiliary
paraphrasing and completion pretraining before instruction tuning. This
promotes a synergistic integration that emphasizes instruction-relevant
information while preserving essential local details, ultimately balancing
local and global information retention in context compression. Experiments show
that our HyCo$_2$ method significantly enhances long-text reasoning while
reducing token usage. It improves the performance of various LLM series by an
average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover,
HyCo$_2$ matches the performance of uncompressed methods while reducing token
consumption by 88.8\%.",2025-05-21,"Huanxuan Liao, Wen Hu, Yao Xu, Shizhu He, Jun Zhao, Kang Liu",http://arxiv.org/pdf/2505.15774v1,cs.CL
ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality,"Despite extensive research on toxic speech detection in text, a critical gap
remains in handling spoken Mandarin audio. The lack of annotated datasets that
capture the unique prosodic cues and culturally specific expressions in
Mandarin leaves spoken toxicity underexplored. To address this, we introduce
ToxicTone -- the largest public dataset of its kind -- featuring detailed
annotations that distinguish both forms of toxicity (e.g., profanity, bullying)
and sources of toxicity (e.g., anger, sarcasm, dismissiveness). Our data,
sourced from diverse real-world audio and organized into 13 topical categories,
mirrors authentic communication scenarios. We also propose a multimodal
detection framework that integrates acoustic, linguistic, and emotional
features using state-of-the-art speech and emotion encoders. Extensive
experiments show our approach outperforms text-only and baseline models,
underscoring the essential role of speech-specific cues in revealing hidden
toxic expressions.",2025-05-21,"Yu-Xiang Luo, Yi-Cheng Lin, Ming-To Chuang, Jia-Hung Chen, I-Ning Tsai, Pei Xing Kiew, Yueh-Hsuan Huang, Chien-Feng Liu, Yu-Chen Chen, Bo-Han Feng, Wenze Ren, Hung-yi Lee",http://arxiv.org/pdf/2505.15773v1,cs.CL
MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling,"Acquiring large-scale emotional speech data with strong consistency remains a
challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated
multimodal pipeline for extracting high-consistency emotional speech from
unlabeled video data. Leveraging face detection and tracking algorithms, we
developed an automatic emotion analysis system using a multimodal large
language model (MLLM). Our results demonstrate that MIKU-PAL can achieve
human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss
kappa score) while being much cheaper and faster than human annotation. With
the high-quality, flexible, and consistent annotation from MIKU-PAL, we can
annotate fine-grained speech emotion categories of up to 26 types, validated by
human annotators with 83% rationality ratings. Based on our proposed system, we
further released a fine-grained emotional speech dataset MIKU-EmoBench(131.2
hours) as a new benchmark for emotional text-to-speech and visual voice
cloning.",2025-05-21,"Yifan Cheng, Ruoyi Zhang, Jiatong Shi",http://arxiv.org/pdf/2505.15772v1,cs.CL
Transfer of Structural Knowledge from Synthetic Languages,"This work explores transfer learning from several synthetic languages to
English. We investigate the structure of the embeddings in the fine-tuned
models, the information they contain, and the capabilities of the fine-tuned
models on simple linguistic tasks. We also introduce a new synthetic language
that leads to better transfer to English than the languages used in previous
research. Finally, we introduce Tiny-Cloze Benchmark - a new synthetic
benchmark for natural language understanding that is more informative for less
powerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models in
several domains demonstrating that fine-tuning on a new synthetic language
allows for better performance on a variety of tasks.",2025-05-21,"Mikhail Budnikov, Ivan Yamshchikov",http://arxiv.org/pdf/2505.15769v1,cs.CL
Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval,"Large Language Models (LLMs) are known to be vulnerable to jailbreaking
attacks, wherein adversaries exploit carefully engineered prompts to induce
harmful or unethical responses. Such threats have raised critical concerns
about the safety and reliability of LLMs in real-world deployment. While
existing defense mechanisms partially mitigate such risks, subsequent
advancements in adversarial techniques have enabled novel jailbreaking methods
to circumvent these protections, exposing the limitations of static defense
frameworks. In this work, we explore defending against evolving jailbreaking
threats through the lens of context retrieval. First, we conduct a preliminary
study demonstrating that even a minimal set of safety-aligned examples against
a particular jailbreak can significantly enhance robustness against this attack
pattern. Building on this insight, we further leverage the retrieval-augmented
generation (RAG) techniques and propose Safety Context Retrieval (SCR), a
scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our
comprehensive experiments demonstrate how SCR achieves superior defensive
performance against both established and emerging jailbreaking tactics,
contributing a new paradigm to LLM safety. Our code will be available upon
publication.",2025-05-21,"Taiye Chen, Zeming Wei, Ang Li, Yisen Wang",http://arxiv.org/pdf/2505.15753v1,cs.CL
"Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications","Integrating Large Language Models (LLMs) and Evolutionary Computation (EC)
represents a promising avenue for advancing artificial intelligence by
combining powerful natural language understanding with optimization and search
capabilities. This manuscript explores the synergistic potential of LLMs and
EC, reviewing their intersections, complementary strengths, and emerging
applications. We identify key opportunities where EC can enhance LLM training,
fine-tuning, prompt engineering, and architecture search, while LLMs can, in
turn, aid in automating the design, analysis, and interpretation of ECs. The
manuscript explores the synergistic integration of EC and LLMs, highlighting
their bidirectional contributions to advancing artificial intelligence. It
first examines how EC techniques enhance LLMs by optimizing key components such
as prompt engineering, hyperparameter tuning, and architecture search,
demonstrating how evolutionary methods automate and refine these processes.
Secondly, the survey investigates how LLMs improve EC by automating
metaheuristic design, tuning evolutionary algorithms, and generating adaptive
heuristics, thereby increasing efficiency and scalability. Emerging
co-evolutionary frameworks are discussed, showcasing applications across
diverse fields while acknowledging challenges like computational costs,
interpretability, and algorithmic convergence. The survey concludes by
identifying open research questions and advocating for hybrid approaches that
combine the strengths of EC and LLMs.",2025-05-21,"Dikshit Chauhan, Bapi Dutta, Indu Bala, Niki van Stein, Thomas Bäck, Anupam Yadav",http://arxiv.org/pdf/2505.15741v1,cs.CL
NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation,"Obtaining large-scale, high-quality data with reasoning paths is crucial for
improving the geometric reasoning capabilities of multi-modal large language
models (MLLMs). However, existing data generation methods, whether based on
predefined templates or constrained symbolic provers, inevitably face diversity
and numerical generalization limitations. To address these limitations, we
propose NeSyGeo, a novel neuro-symbolic framework for generating geometric
reasoning data. First, we propose a domain-specific language grounded in the
entity-relation-constraint paradigm to comprehensively represent all components
of plane geometry, along with generative actions defined within this symbolic
space. We then design a symbolic-visual-text pipeline that synthesizes symbolic
sequences, maps them to corresponding visual and textual representations, and
generates diverse question-answer (Q&A) pairs using large language models
(LLMs). To the best of our knowledge, we are the first to propose a
neuro-symbolic approach in generating multimodal reasoning data. Based on this
framework, we construct NeSyGeo-CoT and NeSyGeo-Caption datasets, containing
100k samples, and release a new benchmark NeSyGeo-Test for evaluating geometric
reasoning abilities in MLLMs. Experiments demonstrate that the proposal
significantly and consistently improves the performance of multiple MLLMs under
both reinforcement and supervised fine-tuning. With only 4k samples and two
epochs of reinforcement fine-tuning, base models achieve improvements of up to
+15.8% on MathVision, +8.4% on MathVerse, and +7.3% on GeoQA. Notably, a 4B
model can be improved to outperform an 8B model from the same series on
geometric reasoning tasks.",2025-05-21,"Weiming Wu, Zi-kang Wang, Jin Ye, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo",http://arxiv.org/pdf/2505.17121v1,cs.CL
Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses,"Large language models (LLMs) are rapidly deployed in real-world applications
ranging from chatbots to agentic systems. Alignment is one of the main
approaches used to defend against attacks such as prompt injection and
jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even
against Greedy Coordinate Gradient (GCG), a white-box attack that generates
adversarial suffixes to induce attacker-desired outputs. However, this search
space over discrete tokens is extremely large, making the task of finding
successful attacks difficult. GCG has, for instance, been shown to converge to
local minima, making it sensitive to initialization choices. In this paper, we
assess the future-proof robustness of these defenses using a more informed
threat model: attackers who have access to some information about the alignment
process. Specifically, we propose an informed white-box attack leveraging the
intermediate model checkpoints to initialize GCG, with each checkpoint acting
as a stepping stone for the next one. We show this approach to be highly
effective across state-of-the-art (SOTA) defenses and models. We further show
our informed initialization to outperform other initialization methods and show
a gradient-informed checkpoint selection strategy to greatly improve attack
performance and efficiency. Importantly, we also show our method to
successfully find universal adversarial suffixes -- single suffixes effective
across diverse inputs. Our results show that, contrary to previous beliefs,
effective adversarial suffixes do exist against SOTA alignment-based defenses,
that these can be found by existing attack methods when adversaries exploit
alignment knowledge, and that even universal suffixes exist. Taken together,
our results highlight the brittleness of current alignment-based methods and
the need to consider stronger threat models when testing the safety of LLMs.",2025-05-21,"Xiaoxue Yang, Bozhidar Stevanoski, Matthieu Meeus, Yves-Alexandre de Montjoye",http://arxiv.org/pdf/2505.15738v1,cs.CL
"DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning","Large language models (LLMs) have improved significantly in their reasoning
through extensive training on massive datasets. However, relying solely on
additional data for improvement is becoming increasingly impractical,
highlighting the need for models to autonomously enhance their reasoning
without external supervision. In this paper, we propose Debate, Train, Evolve
(DTE), a novel ground truth-free training framework that uses multi-agent
debate traces to evolve a single language model. We also introduce a new
prompting strategy Reflect-Critique-Refine, to improve debate quality by
explicitly instructing agents to critique and refine their reasoning. Extensive
evaluations on five reasoning benchmarks with six open-weight models show that
our DTE framework achieve substantial improvements, with an average accuracy
gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe
strong cross-domain generalization, with an average accuracy gain of 5.8% on
all other benchmarks, suggesting that our method captures general reasoning
capabilities.",2025-05-21,"Gaurav Srivastava, Zhenyu Bi, Meng Lu, Xuan Wang",http://arxiv.org/pdf/2505.15734v1,cs.CL
"Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training","We have only limited understanding of how and why large language models
(LLMs) respond in the ways that they do. Their neural networks have proven
challenging to interpret, and we are only beginning to tease out the function
of individual neurons and circuits within them. However, another path to
understanding these systems is to investigate and develop their capacity to
introspect and explain their own functioning. Here, we show that i)
contemporary LLMs are capable of providing accurate, quantitative descriptions
of their own internal processes during certain kinds of decision-making, ii)
that it is possible to improve these capabilities through training, and iii)
that this training generalizes to at least some degree. To do so, we fine-tuned
GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts
(e.g., choosing between condos, loans, vacations, etc.) according to
randomly-generated, quantitative preferences about how to weigh different
attributes during decision-making (e.g., the relative importance of natural
light versus quiet surroundings for condos). We demonstrate that the LLMs can
accurately report these preferences (i.e., the weights that they learned to
give to different attributes during decision-making). Next, we demonstrate that
these LLMs can be fine-tuned to explain their decision-making even more
accurately. Finally, we demonstrate that this training generalizes: It improves
the ability of the models to accurately explain what they are doing as they
make other complex decisions, not just decisions they have learned to make via
fine-tuning. This work is a step towards training LLMs to accurately and
broadly report on their own internal processes -- a possibility that would
yield substantial benefits for interpretability, control, and safety.",2025-05-21,"Dillon Plunkett, Adam Morris, Keerthi Reddy, Jorge Morales",http://arxiv.org/pdf/2505.17120v1,cs.CL
VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models,"The rapid advancement of large language models (LLMs) has accelerated the
development of multi-modal models capable of vocal communication. Unlike
text-based interactions, speech conveys rich and diverse information, including
semantic content, acoustic variations, paralanguage cues, and environmental
context. However, existing evaluations of speech interaction models
predominantly focus on the quality of their textual responses, often
overlooking critical aspects of vocal performance and lacking benchmarks with
vocal-specific test instances. To address this gap, we propose VocalBench, a
comprehensive benchmark designed to evaluate speech interaction models'
capabilities in vocal communication. VocalBench comprises 9,400 carefully
curated instances across four key dimensions: semantic quality, acoustic
performance, conversational abilities, and robustness. It covers 16 fundamental
skills essential for effective vocal interaction. Experimental results reveal
significant variability in current model capabilities, each exhibiting distinct
strengths and weaknesses, and provide valuable insights to guide future
research in speech-based interaction systems. Code and evaluation instances are
available at https://github.com/SJTU-OmniAgent/VocalBench.",2025-05-21,"Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang",http://arxiv.org/pdf/2505.15727v1,cs.CL
Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models,"Recent research leverages large language models (LLMs) for early mental
health detection, such as depression, often optimized with machine-generated
data. However, their detection may be subject to unknown weaknesses. Meanwhile,
quality control has not been applied to these generated corpora besides limited
human verifications. Our goal is to systematically evaluate LLM reasoning and
reveal potential weaknesses. To this end, we first provide a systematic
evaluation of the reasoning over machine-generated detection and
interpretation. Then we use the models' reasoning abilities to explore
mitigation strategies for enhanced performance. Specifically, we do the
following: A. Design an LLM instruction strategy that allows for systematic
analysis of the detection by breaking down the task into several subtasks. B.
Design contrastive few-shot and chain-of-thought prompts by selecting typical
positive and negative examples of detection reasoning. C. Perform human
annotation for the subtasks identified in the first step and evaluate the
performance. D. Identify human-preferred detection with desired logical
reasoning from the few-shot generation and use them to explore different
optimization strategies. We conducted extensive comparisons on the DepTweet
dataset across the following subtasks: 1. identifying whether the speaker is
describing their own depression; 2. accurately detecting the presence of PHQ-9
symptoms, and 3. finally, detecting depression. Human verification of
statistical outliers shows that LLMs demonstrate greater accuracy in analyzing
and detecting explicit language of depression as opposed to implicit
expressions of depression. Two optimization methods are used for performance
enhancement and reduction of the statistic bias: supervised fine-tuning (SFT)
and direct preference optimization (DPO). Notably, the DPO approach achieves
significant performance improvement.",2025-05-21,"Zongru Shao, Xin Wang, Zhanyang Liu, Chenhan Wang, K. P. Subbalakshmi",http://arxiv.org/pdf/2505.17119v1,cs.CL
Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities,"We present the first comprehensive study of Memorization in Multilingual
Large Language Models (MLLMs), analyzing 95 languages using models across
diverse model scales, architectures, and memorization definitions. As MLLMs are
increasingly deployed, understanding their memorization behavior has become
critical. Yet prior work has focused primarily on monolingual models, leaving
multilingual memorization underexplored, despite the inherently long-tailed
nature of training corpora. We find that the prevailing assumption, that
memorization is highly correlated with training data availability, fails to
fully explain memorization patterns in MLLMs. We hypothesize that treating
languages in isolation - ignoring their similarities - obscures the true
patterns of memorization. To address this, we propose a novel graph-based
correlation metric that incorporates language similarity to analyze
cross-lingual memorization. Our analysis reveals that among similar languages,
those with fewer training tokens tend to exhibit higher memorization, a trend
that only emerges when cross-lingual relationships are explicitly modeled.
These findings underscore the importance of a language-aware perspective in
evaluating and mitigating memorization vulnerabilities in MLLMs. This also
constitutes empirical evidence that language similarity both explains
Memorization in MLLMs and underpins Cross-lingual Transferability, with broad
implications for multilingual NLP.",2025-05-21,"Xiaoyu Luo, Yiyi Chen, Johannes Bjerva, Qiongxiu Li",http://arxiv.org/pdf/2505.15722v1,cs.CL
"After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG","Retrieval-augmented generation (RAG) systems face critical challenges in
balancing internal (parametric) and external (retrieved) knowledge, especially
when these sources conflict or are unreliable. To analyze these scenarios
comprehensively, we construct the Trustworthiness Response Dataset (TRD) with
36,266 questions spanning four RAG settings. We reveal that existing approaches
address isolated scenarios-prioritizing one knowledge source, naively merging
both, or refusing answers-but lack a unified framework to handle different
real-world conditions simultaneously. Therefore, we propose the BRIDGE
framework, which dynamically determines a comprehensive response strategy of
large language models (LLMs). BRIDGE leverages an adaptive weighting mechanism
named soft bias to guide knowledge collection, followed by a Maximum Soft-bias
Decision Tree to evaluate knowledge and select optimal response strategies
(trust internal/external knowledge, or refuse). Experiments show BRIDGE
outperforms baselines by 5-15% in accuracy while maintaining balanced
performance across all scenarios. Our work provides an effective solution for
LLMs' trustworthy responses in real-world RAG applications.",2025-05-21,"Xinbang Dai, Huikang Hu, Yuncheng Hua, Jiaqi Li, Yongrui Chen, Rihui Jin, Nan Hu, Guilin Qi",http://arxiv.org/pdf/2505.17118v1,cs.CL
From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning,"Humans organize knowledge into compact categories through semantic
compression by mapping diverse instances to abstract representations while
preserving meaning (e.g., robin and blue jay are both birds; most birds can
fly). These concepts reflect a trade-off between expressive fidelity and
representational simplicity. Large Language Models (LLMs) demonstrate
remarkable linguistic abilities, yet whether their internal representations
strike a human-like trade-off between compression and semantic fidelity is
unclear. We introduce a novel information-theoretic framework, drawing from
Rate-Distortion Theory and the Information Bottleneck principle, to
quantitatively compare these strategies. Analyzing token embeddings from a
diverse suite of LLMs against seminal human categorization benchmarks, we
uncover key divergences. While LLMs form broad conceptual categories that align
with human judgment, they struggle to capture the fine-grained semantic
distinctions crucial for human understanding. More fundamentally, LLMs
demonstrate a strong bias towards aggressive statistical compression, whereas
human conceptual systems appear to prioritize adaptive nuance and contextual
richness, even if this results in lower compressional efficiency by our
measures. These findings illuminate critical differences between current AI and
human cognitive architectures, guiding pathways toward LLMs with more
human-aligned conceptual representations.",2025-05-21,"Chen Shani, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv",http://arxiv.org/pdf/2505.17117v1,cs.CL
Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data,"This paper presents a comparative study of large language models (LLMs) in
interpreting grid-structured geospatial data. We evaluate the performance of a
base model through structured prompting and contrast it with a fine-tuned
variant trained on a dataset of user-assistant interactions. Our results
highlight the strengths and limitations of zero-shot prompting and demonstrate
the benefits of fine-tuning for structured geospatial and temporal reasoning.",2025-05-21,"Akash Dhruv, Yangxinyu Xie, Jordan Branham, Tanwi Mallick",http://arxiv.org/pdf/2505.17116v1,cs.CL
Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling,"Large language models (LLMs) hold significant potential for mental health
support, capable of generating empathetic responses and simulating therapeutic
conversations. However, existing LLM-based approaches often lack the clinical
grounding necessary for real-world psychological counseling, particularly in
explicit diagnostic reasoning aligned with standards like the DSM/ICD and
incorporating diverse therapeutic modalities beyond basic empathy or single
strategies. To address these critical limitations, we propose PsyLLM, the first
large language model designed to systematically integrate both diagnostic and
therapeutic reasoning for mental health counseling. To develop the PsyLLM, we
propose a novel automated data synthesis pipeline. This pipeline processes
real-world mental health posts, generates multi-turn dialogue structures, and
leverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and
multiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate
detailed clinical reasoning processes. Rigorous multi-dimensional filtering
ensures the generation of high-quality, clinically aligned dialogue data. In
addition, we introduce a new benchmark and evaluation protocol, assessing
counseling quality across four key dimensions: comprehensiveness,
professionalism, authenticity, and safety. Our experiments demonstrate that
PsyLLM significantly outperforms state-of-the-art baseline models on this
benchmark.",2025-05-21,"He Hu, Yucheng Zhou, Juzheng Si, Qianning Wang, Hengheng Zhang, Fuji Ren, Fei Ma, Laizhong Cui",http://arxiv.org/pdf/2505.15715v1,cs.CL
TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games,"This paper introduces TurnaboutLLM, a novel framework and dataset for
evaluating the deductive reasoning abilities of Large Language Models (LLMs) by
leveraging the interactive gameplay of detective games Ace Attorney and
Danganronpa. The framework tasks LLMs with identifying contradictions between
testimonies and evidences within long narrative contexts, a challenging task
due to the large answer space and diverse reasoning types presented by its
questions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at
limitations of popular strategies for enhancing deductive reasoning such as
extensive thinking and Chain-of-Thought prompting. The results also suggest
varying effects of context size, the number of reasoning step and answer space
size on model performance. Overall, TurnaboutLLM presents a substantial
challenge for LLMs' deductive reasoning abilities in complex, narrative-rich
environments.",2025-05-21,"Yuan Yuan, Muyu He, Muhammad Adil Shahid, Jiani Huang, Ziyang Li, Li Zhang",http://arxiv.org/pdf/2505.15712v1,cs.CL
Advancing LLM Safe Alignment with Safety Representation Ranking,"The rapid advancement of large language models (LLMs) has demonstrated
milestone success in a variety of tasks, yet their potential for generating
harmful content has raised significant safety concerns. Existing safety
evaluation approaches typically operate directly on textual responses,
overlooking the rich information embedded in the model's internal
representations. In this paper, we propose Safety Representation Ranking (SRR),
a listwise ranking framework that selects safe responses using hidden states
from the LLM itself. SRR encodes both instructions and candidate completions
using intermediate transformer representations and ranks candidates via a
lightweight similarity-based scorer. Our approach directly leverages internal
model states and supervision at the list level to capture subtle safety
signals. Experiments across multiple benchmarks show that SRR significantly
improves robustness to adversarial prompts. Our code will be available upon
publication.",2025-05-21,"Tianqi Du, Zeming Wei, Quan Chen, Chenheng Zhang, Yisen Wang",http://arxiv.org/pdf/2505.15710v1,cs.CL
LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing,"Large Language Models often contain factually incorrect or outdated
knowledge, giving rise to model editing methods for precise knowledge updates.
However, current mainstream locate-then-edit approaches exhibit a progressive
performance decline during sequential editing, due to inadequate mechanisms for
long-term knowledge preservation. To tackle this, we model the sequential
editing as a constrained stochastic programming. Given the challenges posed by
the cumulative preservation error constraint and the gradually revealed editing
tasks, \textbf{LyapLock} is proposed. It integrates queuing theory and Lyapunov
optimization to decompose the long-term constrained programming into tractable
stepwise subproblems for efficient solving. This is the first model editing
framework with rigorous theoretical guarantees, achieving asymptotic optimal
editing performance while meeting the constraints of long-term knowledge
preservation. Experimental results show that our framework scales sequential
editing capacity to over 10,000 edits while stabilizing general capabilities
and boosting average editing efficacy by 11.89\% over SOTA baselines.
Furthermore, it can be leveraged to enhance the performance of baseline
methods. Our code is released on https://github.com/caskcsg/LyapLock.",2025-05-21,"Peng Wang, Biyu Zhou, Xuehai Tang, Jizhong Han, Songlin Hu",http://arxiv.org/pdf/2505.15702v1,cs.CL
HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases,"Large Language Models (LLMs) have demonstrated their potential in hardware
design tasks, such as Hardware Description Language (HDL) generation and
debugging. Yet, their performance in real-world, repository-level HDL projects
with thousands or even tens of thousands of code lines is hindered. To this
end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval
Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph
representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow
Graphs (DFGs) to capture both code graph view and hardware graph view.
HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the
limited recall issues inherent in similarity-based semantic retrieval by
incorporating structural information, but also enhances its extensibility to
various real-world tasks by a task-specific retrieval finetuning. Additionally,
to address the lack of comprehensive HDL search benchmarks, we introduce
HDLSearch, a multi-granularity evaluation dataset derived from real-world
repository-level projects. Experimental results demonstrate that HDLxGraph
significantly improves average search accuracy, debugging efficiency and
completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based
RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are
available at https://github.com/Nick-Zheng-Q/HDLxGraph.",2025-05-21,"Pingqing Zheng, Jiayin Qin, Fuqi Zhang, Shang Wu, Yu Cao, Caiwen Ding, Yang, Zhao",http://arxiv.org/pdf/2505.15701v1,cs.CL
"""Alexa, can you forget me?"" Machine Unlearning Benchmark in Spoken Language Understanding","Machine unlearning, the process of efficiently removing specific information
from machine learning models, is a growing area of interest for responsible AI.
However, few studies have explored the effectiveness of unlearning methods on
complex tasks, particularly speech-related ones. This paper introduces
UnSLU-BENCH, the first benchmark for machine unlearning in spoken language
understanding (SLU), focusing on four datasets spanning four languages. We
address the unlearning of data from specific speakers as a way to evaluate the
quality of potential ""right to be forgotten"" requests. We assess eight
unlearning techniques and propose a novel metric to simultaneously better
capture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation
for unlearning in SLU and reveals significant differences in the effectiveness
and computational feasibility of various techniques.",2025-05-21,"Alkis Koudounas, Claudio Savelli, Flavio Giobergia, Elena Baralis",http://arxiv.org/pdf/2505.15700v2,cs.CL
MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation,"The [CLS] token in BERT is commonly used as a fixed-length representation for
classification tasks, yet prior work has shown that both other tokens and
intermediate layers encode valuable contextual information. In this work, we
propose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS]
representation by aggregating information across layers and tokens.
Specifically, we explore three modifications: (i) max-pooling the [CLS] token
across multiple layers, (ii) enabling the [CLS] token to attend over the entire
final layer using an additional multi-head attention (MHA) layer, and (iii)
combining max-pooling across the full sequence with MHA. Our approach enhances
BERT's classification accuracy (especially on low-resource tasks) without
requiring pre-training or significantly increasing model size. Experiments on
the GLUE benchmark show that MaxPoolBERT consistently achieves a better
performance on the standard BERT-base model.",2025-05-21,"Maike Behrendt, Stefan Sylvius Wagner, Stefan Harmeling",http://arxiv.org/pdf/2505.15696v1,cs.CL
Can Large Language Models be Effective Online Opinion Miners?,"The surge of user-generated online content presents a wealth of insights into
customer preferences and market trends. However, the highly diverse, complex,
and context-rich nature of such contents poses significant challenges to
traditional opinion mining approaches. To address this, we introduce Online
Opinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol
designed to assess the ability of large language models (LLMs) to mine opinions
effectively from diverse and intricate online environments. OOMB provides
extensive (entity, feature, opinion) tuple annotations and a comprehensive
opinion-centric summary that highlights key opinion topics within each content,
thereby enabling the evaluation of both the extractive and abstractive
capabilities of models. Through our proposed benchmark, we conduct a
comprehensive analysis of which aspects remain challenging and where LLMs
exhibit adaptability, to explore whether they can effectively serve as opinion
miners in realistic online scenarios. This study lays the foundation for
LLM-based opinion mining and discusses directions for future research in this
field.",2025-05-21,"Ryang Heo, Yongsik Seo, Junseong Lee, Dongha Lee",http://arxiv.org/pdf/2505.15695v1,cs.CL
Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities,"Reinforcement learning (RL) has emerged as an effective method for training
reasoning models. However, existing RL approaches typically bias the model's
output distribution toward reward-maximizing paths without introducing external
knowledge. This limits their exploration capacity and results in a narrower
reasoning capability boundary compared to base models. To address this
limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel
framework that augments RL by incorporating external high-level guidance
(""thought patterns""). By adaptively integrating structured thoughts during
training, TAPO effectively balances model-internal exploration and external
guidance exploitation. Extensive experiments show that our approach
significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva
Math. Notably, these high-level thought patterns, abstracted from only 500
prior samples, generalize effectively across various tasks and models. This
highlights TAPO's potential for broader applications across multiple tasks and
domains. Our further analysis reveals that introducing external guidance
produces powerful reasoning models with superior explainability of inference
behavior and enhanced output readability.",2025-05-21,"Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Pengpeng Shao, Huazhe Xu, Jianhua Tao",http://arxiv.org/pdf/2505.15692v2,cs.CL
ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy,"While Chain-of-Thought (CoT) prompting improves reasoning in large language
models (LLMs), the excessive length of reasoning tokens increases latency and
KV cache memory usage, and may even truncate final answers under context
limits. We propose ThinkLess, an inference-efficient framework that terminates
reasoning generation early and maintains output quality without modifying the
model. Atttention analysis reveals that answer tokens focus minimally on
earlier reasoning steps and primarily attend to the reasoning terminator token,
due to information migration under causal masking. Building on this insight,
ThinkLess inserts the terminator token at earlier positions to skip redundant
reasoning while preserving the underlying knowledge transfer. To prevent format
discruption casued by early termination, ThinkLess employs a lightweight
post-regulation mechanism, relying on the model's natural instruction-following
ability to produce well-structured answers. Without fine-tuning or auxiliary
data, ThinkLess achieves comparable accuracy to full-length CoT decoding while
greatly reducing decoding time and memory consumption.",2025-05-21,"Gengyang Li, Yifeng Gao, Yuming Li, Yunfang Wu",http://arxiv.org/pdf/2505.15684v2,cs.CL
"A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability","Private data is typically larger and of higher quality than public data,
offering great potential to improve LLM. However, its scattered distribution
across data silos and the high computational demands of LLMs limit their
deployment in federated environments. To address this, the transformer-based
split learning model has emerged, offloading most model parameters to the
server while retaining only the embedding and output layers on clients to
ensure privacy. However, it still faces significant challenges in security,
efficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,
leading to reverse engineering of private data; 2) the autoregressive nature of
LLMs means that federated split learning can only train and infer sequentially,
causing high communication overhead; 3) fixed partition points lack
adaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a
secure, efficient, and adaptive federated split framework based on LLaMA2.
First, we place some input and output blocks on the local client and inject
Gaussian noise into forward-pass hidden states, enabling secure end-to-end
propagation. Second, we employ client-batch and server-hierarchical strategies
to achieve parallel training, along with attention-mask compression and KV
cache mechanisms to accelerate inference, reducing communication costs
effectively. Third, we allow users to dynamically adjust the partition points
for input/output blocks based on specific task requirements and hardware
limitations. Experiments on NLU, summarization and conversational QA tasks show
that FL-LLaMA maintains performance comparable to centralized LLaMA2, and
achieves up to 2x train speedups and 8x inference speedups. Further analysis of
privacy attacks and different partition points also demonstrates the
effectiveness of FL-LLaMA in security and adaptability.",2025-05-21,"Zishuai Zhang, Hainan Zhang, Jiaying Zheng, Ziwei Wang, Yongxin Tong, Jin Dong, Zhiming Zheng",http://arxiv.org/pdf/2505.15683v1,cs.CL
The Representational Alignment between Humans and Language Models is implicitly driven by a Concreteness Effect,"The nouns of our language refer to either concrete entities (like a table) or
abstract concepts (like justice or love), and cognitive psychology has
established that concreteness influences how words are processed. Accordingly,
understanding how concreteness is represented in our mind and brain is a
central question in psychology, neuroscience, and computational linguistics.
While the advent of powerful language models has allowed for quantitative
inquiries into the nature of semantic representations, it remains largely
underexplored how they represent concreteness. Here, we used behavioral
judgments to estimate semantic distances implicitly used by humans, for a set
of carefully selected abstract and concrete nouns. Using Representational
Similarity Analysis, we find that the implicit representational space of
participants and the semantic representations of language models are
significantly aligned. We also find that both representational spaces are
implicitly aligned to an explicit representation of concreteness, which was
obtained from our participants using an additional concreteness rating task.
Importantly, using ablation experiments, we demonstrate that the human-to-model
alignment is substantially driven by concreteness, but not by other important
word characteristics established in psycholinguistics. These results indicate
that humans and language models converge on the concreteness dimension, but not
on other dimensions.",2025-05-21,"Cosimo Iaia, Bhavin Choksi, Emily Wiebers, Gemma Roig, Christian J. Fiebach",http://arxiv.org/pdf/2505.15682v1,cs.CL
UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models,"Large language models require iterative updates to address challenges such as
knowledge conflicts and outdated information (e.g., incorrect, private, or
illegal contents). Machine unlearning provides a systematic methodology for
targeted knowledge removal from trained models, enabling elimination of
sensitive information influences. However, mainstream fine-tuning-based
unlearning methods often fail to balance unlearning efficacy and model ability,
frequently resulting in catastrophic model collapse under extensive knowledge
removal. Meanwhile, in-context unlearning, which relies solely on contextual
prompting without modifying the model's intrinsic mechanisms, suffers from
limited generalizability and struggles to achieve true unlearning. In this
work, we introduce UniErase, a novel unlearning paradigm that employs learnable
parametric suffix (unlearning token) to steer language models toward targeted
forgetting behaviors. UniErase operates through two key phases: (I) an
optimization stage that binds desired unlearning outputs to the model's
autoregressive probability distribution via token optimization, followed by
(II) a lightweight model editing phase that activates the learned token to
probabilistically induce specified forgetting objective. Serving as a new
research direction for token learning to induce unlearning target, UniErase
achieves state-of-the-art (SOTA) performance across batch, sequential, and
precise unlearning under fictitious and real-world knowledge settings.
Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%
of the LLM parameters, outperforms previous forgetting SOTA baseline by around
4.01 times for model ability with even better unlearning efficacy. Similarly,
UniErase, maintaining more ability, also surpasses previous retaining SOTA by
35.96% for unlearning efficacy, showing dual top-tier performances in current
unlearing domain.",2025-05-21,"Miao Yu, Liang Lin, Guibin Zhang, Xinfeng Li, Junfeng Fang, Ningyu Zhang, Kun Wang, Yang Wang",http://arxiv.org/pdf/2505.15674v1,cs.CL
Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model,"Spoken dialogue is an intuitive form of human-computer interaction, yet
current speech language models often remain constrained to turn-based
exchanges, lacking real-time adaptability such as user barge-in. We propose a
novel duplex speech to speech (S2S) architecture featuring continuous user
inputs and codec agent outputs with channel fusion that directly models
simultaneous user and agent streams. Using a pretrained streaming encoder for
user input enables the first duplex S2S model without requiring speech
pretrain. Separate architectures for agent and user modeling facilitate codec
fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared
to previous works. Experimental results show that the proposed model
outperforms previous duplex models in reasoning, turn-taking, and barge-in
abilities. The model requires significantly less speech data, as speech
pretrain is skipped, which markedly simplifies the process of building a duplex
S2S model from any LLMs. Finally, it is the first openly available duplex S2S
model with training and inference code to foster reproducibility.",2025-05-21,"Ke Hu, Ehsan Hosseini-Asl, Chen Chen, Edresson Casanova, Subhankar Ghosh, Piotr Żelasko, Zhehuai Chen, Jason Li, Jagadeesh Balam, Boris Ginsburg",http://arxiv.org/pdf/2505.15670v1,cs.CL
Segmentation-Variant Codebooks for Preservation of Paralinguistic and Prosodic Information,"Quantization in SSL speech models (e.g., HuBERT) improves compression and
performance in tasks like language modeling, resynthesis, and text-to-speech
but often discards prosodic and paralinguistic information (e.g., emotion,
prominence). While increasing codebook size mitigates some loss, it
inefficiently raises bitrates. We propose Segmentation-Variant Codebooks
(SVCs), which quantize speech at distinct linguistic units (frame, phone, word,
utterance), factorizing it into multiple streams of segment-specific discrete
features. Our results show that SVCs are significantly more effective at
preserving prosodic and paralinguistic information across probing tasks.
Additionally, we find that pooling before rather than after discretization
better retains segment-level information. Resynthesis experiments further
confirm improved style realization and slightly improved quality while
preserving intelligibility.",2025-05-21,"Nicholas Sanders, Yuanchao Li, Korin Richmond, Simon King",http://arxiv.org/pdf/2505.15667v1,cs.CL
Text-to-Pipeline: Bridging Natural Language and Data Preparation Pipelines,"Data preparation (DP) transforms raw data into a form suitable for downstream
applications, typically by composing operations into executable pipelines.
Building such pipelines is time-consuming and requires sophisticated
programming skills. If we can build the pipelines with natural language (NL),
the technical barrier of DP will be significantly reduced. However,
constructing DP pipelines from NL instructions remains underexplored. To fill
the gap, we introduce Text-to-Pipeline, a new task that translates NL data
preparation instructions into DP pipelines. Furthermore, we develop a benchmark
named PARROT to support systematic evaluation. To simulate realistic DP
scenarios, we mined transformation patterns from production pipelines and
instantiated them on 23,009 real-world tables collected from six public
sources. The resulting benchmark comprises ~18,000 pipelines covering 16 core
DP operators. We evaluated cutting-edge large language models on PARROTand
observed that they only solved 72.86% of the cases, revealing notable
limitations in instruction understanding and multi-step reasoning. To address
this, we propose Pipeline-Agent, a stronger baseline that iteratively predicts
and executes operations with intermediate table feedback, achieving the best
performance of 76.17%. Despite this improvement, there remains substantial room
for progress on Text-to-Pipeline. Our data, codes, and evaluation tools are
available at https://anonymous.4open.science/r/Text-to-Pipeline.",2025-05-21,"Yuhang Ge, Yachuan Liu, Yuren Mao, Yunjun Gao",http://arxiv.org/pdf/2505.15874v1,cs.CL
Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!,"Fine-tuning on open-source Large Language Models (LLMs) with proprietary data
is now a standard practice for downstream developers to obtain task-specific
LLMs. Surprisingly, we reveal a new and concerning risk along with the
practice: the creator of the open-source LLMs can later extract the private
downstream fine-tuning data through simple backdoor training, only requiring
black-box access to the fine-tuned downstream model. Our comprehensive
experiments, across 4 popularly used open-source models with 3B to 32B
parameters and 2 downstream datasets, suggest that the extraction performance
can be strikingly high: in practical settings, as much as 76.3% downstream
fine-tuning data (queries) out of a total 5,000 samples can be perfectly
extracted, and the success rate can increase to 94.9% in more ideal settings.
We also explore a detection-based defense strategy but find it can be bypassed
with improved attack. Overall, we highlight the emergency of this newly
identified data breaching risk in fine-tuning, and we hope that more follow-up
research could push the progress of addressing this concerning risk. The code
and data used in our experiments are released at
https://github.com/thu-coai/Backdoor-Data-Extraction.",2025-05-21,"Zhexin Zhang, Yuhao Sun, Junxiao Yang, Shiyao Cui, Hongning Wang, Minlie Huang",http://arxiv.org/pdf/2505.15656v1,cs.CL
Word Level Timestamp Generation for Automatic Speech Recognition and Translation,"We introduce a data-driven approach for enabling word-level timestamp
prediction in the Canary model. Accurate timestamp information is crucial for a
variety of downstream tasks such as speech content retrieval and timed
subtitles. While traditional hybrid systems and end-to-end (E2E) models may
employ external modules for timestamp prediction, our approach eliminates the
need for separate alignment mechanisms. By leveraging the NeMo Forced Aligner
(NFA) as a teacher model, we generate word-level timestamps and train the
Canary model to predict timestamps directly. We introduce a new <|timestamp|>
token, enabling the Canary model to predict start and end timestamps for each
word. Our method demonstrates precision and recall rates between 80% and 90%,
with timestamp prediction errors ranging from 20 to 120 ms across four
languages, with minimal WER degradation. Additionally, we extend our system to
automatic speech translation (AST) tasks, achieving timestamp prediction errors
around 200 milliseconds.",2025-05-21,"Ke Hu, Krishna Puvvada, Elena Rastorgueva, Zhehuai Chen, He Huang, Shuoyang Ding, Kunal Dhawan, Hainan Xu, Jagadeesh Balam, Boris Ginsburg",http://arxiv.org/pdf/2505.15646v1,cs.CL
Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models,"Large Language Models (LLMs) demonstrate the ability to solve reasoning and
mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT
length, as seen in models such as DeepSeek-R1, significantly enhances this
reasoning for complex problems, but requires costly and high-quality long CoT
data and fine-tuning. This work, inspired by the deep thinking paradigm of
DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of
an LLM without external datasets. Our method first employs Sparse Autoencoders
(SAEs) to extract interpretable features from vanilla CoT. These features are
then used to steer the LLM's internal states during generation. Recognizing
that many LLMs do not have corresponding pre-trained SAEs, we further introduce
a novel SAE-free steering algorithm, which directly computes steering
directions from the residual activations of an LLM, obviating the need for an
explicit SAE. Experimental results demonstrate that both our SAE-based and
subsequent SAE-free steering algorithms significantly enhance the reasoning
capabilities of LLMs.",2025-05-21,"Zihao Li, Xu Wang, Yuzhe Yang, Ziyu Yao, Haoyi Xiong, Mengnan Du",http://arxiv.org/pdf/2505.15634v2,cs.CL
Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions,"Large language models that use retrieval augmented generation have the
potential to unlock valuable knowledge for researchers, policymakers, and the
public by making long and technical climate-related documents more accessible.
While this approach can help alleviate factual hallucinations by relying on
retrieved passages as additional context, its effectiveness depends on whether
the model's output remains faithful to these passages. To address this, we
explore the automatic assessment of faithfulness of different models in this
setting. We then focus on ClimateGPT, a large language model specialised in
climate science, to examine which factors in its instruction fine-tuning impact
the model's faithfulness. By excluding unfaithful subsets of the model's
training data, we develop ClimateGPT Faithful+, which achieves an improvement
in faithfulness from 30% to 57% in supported atomic claims according to our
automatic metric.",2025-05-21,"David Thulke, Jakob Kemmler, Christian Dugast, Hermann Ney",http://arxiv.org/pdf/2505.15633v1,cs.CL
Mechanistic Insights into Grokking from the Embedding Layer,"Grokking, a delayed generalization in neural networks after perfect training
performance, has been observed in Transformers and MLPs, but the components
driving it remain underexplored. We show that embeddings are central to
grokking: introducing them into MLPs induces delayed generalization in modular
arithmetic tasks, whereas MLPs without embeddings can generalize immediately.
Our analysis identifies two key mechanisms: (1) Embedding update dynamics,
where rare tokens stagnate due to sparse gradient updates and weight decay, and
(2) Bilinear coupling, where the interaction between embeddings and downstream
weights introduces saddle points and increases sensitivity to initialization.
To confirm these mechanisms, we investigate frequency-aware sampling, which
balances token updates by minimizing gradient variance, and embedding-specific
learning rates, derived from the asymmetric curvature of the bilinear loss
landscape. We prove that an adaptive learning rate ratio,
\(\frac{\eta_E}{\eta_W} \propto \frac{\sigma_{\max}(E)}{\sigma_{\max}(W)} \cdot
\frac{f_W}{f_E}\), mitigates bilinear coupling effects, accelerating
convergence. Our methods not only improve grokking dynamics but also extend to
broader challenges in Transformer optimization, where bilinear interactions
hinder efficient training.",2025-05-21,"H. V. AlquBoj, Hilal AlQuabeh, Velibor Bojkovic, Munachiso Nwadike, Kentaro Inui",http://arxiv.org/pdf/2505.15624v1,cs.CL
Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning,"Large language models (LLMs) demonstrate considerable potential in various
natural language tasks but face significant challenges in mathematical
reasoning, particularly in executing precise, multi-step logic. However,
current evaluation frameworks judge their performance solely based on accuracy,
which only accounts for the final answer. This study explores these pitfalls by
employing a novel evaluation framework. We propose an evaluation metric called
the MAPLE score, which holistically quantifies reasoning misalignment by
integrating error rates, redundancy, and validity.",2025-05-21,"Tiasa Singha Roy, Aditeya Baral, Ayush Rajesh Jhaveri, Yusuf Baig",http://arxiv.org/pdf/2505.15623v1,cs.CL
Learn to Reason Efficiently with Adaptive Length-based Reward Shaping,"Large Reasoning Models (LRMs) have shown remarkable capabilities in solving
complex problems through reinforcement learning (RL), particularly by
generating long reasoning traces. However, these extended outputs often exhibit
substantial redundancy, which limits the efficiency of LRMs. In this paper, we
investigate RL-based approaches to promote reasoning efficiency. Specifically,
we first present a unified framework that formulates various efficient
reasoning methods through the lens of length-based reward shaping. Building on
this perspective, we propose a novel Length-bAsed StEp Reward shaping method
(LASER), which employs a step function as the reward, controlled by a target
length. LASER surpasses previous methods, achieving a superior Pareto-optimal
balance between performance and efficiency. Next, we further extend LASER based
on two key intuitions: (1) The reasoning behavior of the model evolves during
training, necessitating reward specifications that are also adaptive and
dynamic; (2) Rather than uniformly encouraging shorter or longer chains of
thought (CoT), we posit that length-based reward shaping should be
difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.
This approach is expected to facilitate a combination of fast and slow
thinking, leading to a better overall tradeoff. The resulting method is termed
LASER-D (Dynamic and Difficulty-aware). Experiments on
DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and
DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both
reasoning performance and response length efficiency. For instance, LASER-D and
its variant achieve a +6.1 improvement on AIME2024 while reducing token usage
by 63%. Further analysis reveals our RL-based compression produces more concise
reasoning patterns with less redundant ""self-reflections"". Resources are at
https://github.com/hkust-nlp/Laser.",2025-05-21,"Wei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, Junxian He",http://arxiv.org/pdf/2505.15612v1,cs.CL
From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning,"Large language models (LLMs) can transform education, but their optimization
for direct question-answering often undermines effective pedagogy which
requires strategically withholding answers. To mitigate this, we propose an
online reinforcement learning (RL)-based alignment framework that can quickly
adapt LLMs into effective tutors using simulated student-tutor interactions by
emphasizing pedagogical quality and guided problem-solving over simply giving
away answers. We use our method to train a 7B parameter tutor model without
human annotations which reaches similar performance to larger proprietary
models like LearnLM. We introduce a controllable reward weighting to balance
pedagogical support and student solving accuracy, allowing us to trace the
Pareto frontier between these two objectives. Our models better preserve
reasoning capabilities than single-turn SFT baselines and can optionally
enhance interpretability through thinking tags that expose the model's
instructional planning.",2025-05-21,"David Dinucu-Jianu, Jakub Macina, Nico Daheim, Ido Hakimi, Iryna Gurevych, Mrinmaya Sachan",http://arxiv.org/pdf/2505.15607v1,cs.CL
InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding responses with retrieved information. As an emerging paradigm,
Agentic RAG further enhances this process by introducing autonomous LLM agents
into the information seeking process. However, existing benchmarks fall short
in evaluating such systems, as they are confined to a static retrieval
environment with a fixed, limited corpus} and simple queries that fail to
elicit agentic behavior. Moreover, their evaluation protocols assess
information seeking effectiveness by pre-defined gold sets of documents, making
them unsuitable for the open-ended and dynamic nature of real-world web
environments. To bridge this gap, we present InfoDeepSeek, a new benchmark with
challenging questions designed for assessing agentic information seeking in
real-world, dynamic web environments. We propose a systematic methodology for
constructing challenging queries satisfying the criteria of determinacy,
difficulty, and diversity. Based on this, we develop the first evaluation
framework tailored to dynamic agentic information seeking, including
fine-grained metrics about the accuracy, utility, and compactness of
information seeking outcomes. Through extensive experiments across LLMs, search
engines, and question types, InfoDeepSeek reveals nuanced agent behaviors and
offers actionable insights for future research.",2025-05-21,"Yunjia Xi, Jianghao Lin, Menghui Zhu, Yongzhao Xiao, Zhuoying Ou, Jiaqi Liu, Tong Wan, Bo Chen, Weiwen Liu, Yasheng Wang, Ruiming Tang, Weinan Zhang, Yong Yu",http://arxiv.org/pdf/2505.15872v2,cs.CL
MIRB: Mathematical Information Retrieval Benchmark,"Mathematical Information Retrieval (MIR) is the task of retrieving
information from mathematical documents and plays a key role in various
applications, including theorem search in mathematical libraries, answer
retrieval on math forums, and premise selection in automated theorem proving.
However, a unified benchmark for evaluating these diverse retrieval tasks has
been lacking. In this paper, we introduce MIRB (Mathematical Information
Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB
includes four tasks: semantic statement retrieval, question-answer retrieval,
premise retrieval, and formula retrieval, spanning a total of 12 datasets. We
evaluate 13 retrieval models on this benchmark and analyze the challenges
inherent to MIR. We hope that MIRB provides a comprehensive framework for
evaluating MIR systems and helps advance the development of more effective
retrieval models tailored to the mathematical domain.",2025-05-21,"Haocheng Ju, Bin Dong",http://arxiv.org/pdf/2505.15585v1,cs.CL
"RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language","Multimodal question answering (QA) often requires identifying which video,
audio, or sensor tokens are relevant to the question. Yet modality
disagreements are common: off-camera speech, background noise, or motion
outside the field of view often mislead fusion models that weight all streams
equally. We present RAVEN, a unified QA architecture whose core is QuART, a
query-conditioned cross-modal gating module that assigns scalar relevance
scores to each token across modalities, enabling the model to amplify
informative signals and suppress distractors before fusion. RAVEN is trained
through a three-stage pipeline comprising unimodal pretraining, query-aligned
fusion, and disagreement-oriented fine-tuning -- each stage targeting a
distinct challenge in multi-modal reasoning: representation quality,
cross-modal relevance, and robustness to modality mismatch. To support training
and evaluation, we release AVS-QA, a dataset of 300K synchronized
Audio--Video-Sensor streams paired with automatically generated question-answer
pairs. Experimental results on seven multi-modal QA benchmarks -- including
egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and
8.0\% gains in accuracy compared to state-of-the-art multi-modal large language
models, respectively. Incorporating sensor data provides an additional 16.4\%
boost, and the model remains robust under modality corruption, outperforming
SOTA baselines by 50.23\%. Our code and dataset are available at
https://github.com/BASHLab/RAVEN.",2025-05-21,"Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam",http://arxiv.org/pdf/2505.17114v1,cs.CL
Semantic-based Unsupervised Framing Analysis (SUFA): A Novel Approach for Computational Framing Analysis,"This research presents a novel approach to computational framing analysis,
called Semantic Relations-based Unsupervised Framing Analysis (SUFA). SUFA
leverages semantic relations and dependency parsing algorithms to identify and
assess entity-centric emphasis frames in news media reports. This innovative
method is derived from two studies -- qualitative and computational -- using a
dataset related to gun violence, demonstrating its potential for analyzing
entity-centric emphasis frames. This article discusses SUFA's strengths,
limitations, and application procedures. Overall, the SUFA approach offers a
significant methodological advancement in computational framing analysis, with
its broad applicability across both the social sciences and computational
domains.",2025-05-21,"Mohammad Ali, Naeemul Hassan",http://arxiv.org/pdf/2505.15563v1,cs.CL
Do RAG Systems Suffer From Positional Bias?,"Retrieval Augmented Generation enhances LLM accuracy by adding passages
retrieved from an external corpus to the LLM prompt. This paper investigates
how positional bias - the tendency of LLMs to weight information differently
based on its position in the prompt - affects not only the LLM's capability to
capitalize on relevant passages, but also its susceptibility to distracting
passages. Through extensive experiments on three benchmarks, we show how
state-of-the-art retrieval pipelines, while attempting to retrieve relevant
passages, systematically bring highly distracting ones to the top ranks, with
over 60% of queries containing at least one highly distracting passage among
the top-10 retrieved passages. As a result, the impact of the LLM positional
bias, which in controlled settings is often reported as very prominent by
related works, is actually marginal in real scenarios since both relevant and
distracting passages are, in turn, penalized. Indeed, our findings reveal that
sophisticated strategies that attempt to rearrange the passages based on LLM
positional preferences do not perform better than random shuffling.",2025-05-21,"Florin Cuconasu, Simone Filice, Guy Horowitz, Yoelle Maarek, Fabrizio Silvestri",http://arxiv.org/pdf/2505.15561v1,cs.CL
A Survey on Multilingual Mental Disorders Detection from Social Media Data,"The increasing prevalence of mental health disorders globally highlights the
urgent need for effective digital screening methods that can be used in
multilingual contexts. Most existing studies, however, focus on English data,
overlooking critical mental health signals that may be present in non-English
texts. To address this important gap, we present the first survey on the
detection of mental health disorders using multilingual social media data. We
investigate the cultural nuances that influence online language patterns and
self-disclosure behaviors, and how these factors can impact the performance of
NLP tools. Additionally, we provide a comprehensive list of multilingual data
collections that can be used for developing NLP models for mental health
screening. Our findings can inform the design of effective multilingual mental
health screening tools that can meet the needs of diverse populations,
ultimately improving mental health outcomes on a global scale.",2025-05-21,"Ana-Maria Bucur, Marcos Zampieri, Tharindu Ranasinghe, Fabio Crestani",http://arxiv.org/pdf/2505.15556v1,cs.CL
DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion,"Critical questions are essential resources to provoke critical thinking when
encountering an argumentative text. We present our system for the Critical
Questions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach
leverages large language models (LLMs) with chain-of-thought prompting to
generate critical questions guided by Walton's argumentation schemes. For each
input intervention, we conversationally prompt LLMs to instantiate the
corresponding argument scheme template to first obtain structured arguments,
and then generate relevant critical questions. Following this, we rank all the
available critical questions by prompting LLMs to select the top 3 most helpful
questions based on the original intervention text. This combination of
structured argumentation theory and step-by-step reasoning enables the
generation of contextually relevant and diverse critical questions. Our
pipeline achieves competitive performance in the final test set, showing its
potential to foster critical thinking given argumentative text and detect
missing or uninformed claims. Code available at
\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.",2025-05-21,"Wendi Zhou, Ameer Saadat-Yazdi, Nadin Kökciyan",http://arxiv.org/pdf/2505.15554v1,cs.CL
Social Bias in Popular Question-Answering Benchmarks,"Question-answering (QA) and reading comprehension (RC) benchmarks are
essential for assessing the capabilities of large language models (LLMs) in
retrieving and reproducing knowledge. However, we demonstrate that popular QA
and RC benchmarks are biased and do not cover questions about different
demographics or regions in a representative way, potentially due to a lack of
diversity of those involved in their creation. We perform a qualitative content
analysis of 30 benchmark papers and a quantitative analysis of 20 respective
benchmark datasets to learn (1) who is involved in the benchmark creation, (2)
how social bias is addressed or prevented, and (3) whether the demographics of
the creators and annotators correspond to particular biases in the content.
Most analyzed benchmark papers provided insufficient information regarding the
stakeholders involved in benchmark creation, particularly the annotators.
Notably, just one of the benchmark papers explicitly reported measures taken to
address social representation issues. Moreover, the data analysis revealed
gender, religion, and geographic biases across a wide range of encyclopedic,
commonsense, and scholarly benchmarks. More transparent and bias-aware QA and
RC benchmark creation practices are needed to facilitate better scrutiny and
incentivize the development of fairer LLMs.",2025-05-21,"Angelie Kraft, Judith Simon, Sonja Schimmler",http://arxiv.org/pdf/2505.15553v2,cs.CL
"Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek","This study examines cultural value alignment in large language models (LLMs)
by analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from
Schwartz's value framework. Using the 40-item Portrait Values Questionnaire, we
assessed whether DeepSeek, trained on Chinese-language data, exhibits distinct
value preferences compared to Western models. Results of a Bayesian ordinal
regression model show that self-transcendence values (e.g., benevolence,
universalism) were highly prioritized across all models, reflecting a general
LLM tendency to emphasize prosocial values. However, DeepSeek uniquely
downplayed self-enhancement values (e.g., power, achievement) compared to
ChatGPT and Gemini, aligning with collectivist cultural tendencies. These
findings suggest that LLMs reflect culturally situated biases rather than a
universal ethical framework. To address value asymmetries in LLMs, we propose
multi-perspective reasoning, self-reflective feedback, and dynamic
contextualization. This study contributes to discussions on AI fairness,
cultural neutrality, and the need for pluralistic AI alignment frameworks that
integrate diverse moral perspectives.",2025-05-21,Robin Segerer,http://arxiv.org/pdf/2505.17112v1,cs.CL
Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs,"Bias in Large Language Models (LLMs) significantly undermines their
reliability and fairness. We focus on a common form of bias: when two reference
concepts in the model's concept space, such as sentiment polarities (e.g.,
""positive"" and ""negative""), are asymmetrically correlated with a third, target
concept, such as a reviewing aspect, the model exhibits unintended bias. For
instance, the understanding of ""food"" should not skew toward any particular
sentiment. Existing bias evaluation methods assess behavioral differences of
LLMs by constructing labeled data for different social groups and measuring
model responses across them, a process that requires substantial human effort
and captures only a limited set of social concepts. To overcome these
limitations, we propose BiasLens, a test-set-free bias analysis framework based
on the structure of the model's vector space. BiasLens combines Concept
Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract
interpretable concept representations, and quantifies bias by measuring the
variation in representational similarity between the target concept and each of
the reference concepts. Even without labeled data, BiasLens shows strong
agreement with traditional bias evaluation metrics (Spearman correlation r >
0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect
using existing methods. For example, in simulated clinical scenarios, a
patient's insurance status can cause the LLM to produce biased diagnostic
assessments. Overall, BiasLens offers a scalable, interpretable, and efficient
paradigm for bias discovery, paving the way for improving fairness and
transparency in LLMs.",2025-05-21,"Lang Gao, Kaiyang Wan, Wei Liu, Chenxi Wang, Zirui Song, Zixiang Xu, Yanbo Wang, Veselin Stoyanov, Xiuying Chen",http://arxiv.org/pdf/2505.15524v1,cs.CL
Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets,"Vision-Language Models (VLMs) acquire real-world knowledge and general
reasoning ability through Internet-scale image-text corpora. They can augment
robotic systems with scene understanding and task planning, and assist
visuomotor policies that are trained on robot trajectory data. We explore the
reverse paradigm - using rich, real, multi-modal robot trajectory data to
enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual
Question Answering (VQA) dataset generation framework for VLMs. Given a human
tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual
and non-descriptive sensory modalities, such as end-effector pose, gripper
aperture, and force sensing. Based on these modalities, it segments the robot
trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses
scene and interaction understanding to identify 3D properties of the robot,
task goal, and the target object. The properties are used to generate
representative VQA queries - images with textural multiple-choice questions -
based on spatial, goal-conditioned, and interaction reasoning question
templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710
questions covering 463 distinct scenes and 3,396 robotic manipulation tasks
from 176k real robot trajectories. Results suggest that Robo2VLM-1 can
benchmark and improve VLM capabilities in spatial and interaction reasoning.",2025-05-21,"Kaiyuan Chen, Shuangyu Xie, Zehan Ma, Ken Goldberg",http://arxiv.org/pdf/2505.15517v1,cs.CL
Explainable embeddings with Distance Explainer,"While eXplainable AI (XAI) has advanced significantly, few methods address
interpretability in embedded vector spaces where dimensions represent complex
abstractions. We introduce Distance Explainer, a novel method for generating
local, post-hoc explanations of embedded spaces in machine learning models. Our
approach adapts saliency-based techniques from RISE to explain the distance
between two embedded data points by assigning attribution values through
selective masking and distance-ranked mask filtering. We evaluate Distance
Explainer on cross-modal embeddings (image-image and image-caption pairs) using
established XAI metrics including Faithfulness, Sensitivity/Robustness, and
Randomization. Experiments with ImageNet and CLIP models demonstrate that our
method effectively identifies features contributing to similarity or
dissimilarity between embedded data points while maintaining high robustness
and consistency. We also explore how parameter tuning, particularly mask
quantity and selection strategy, affects explanation quality. This work
addresses a critical gap in XAI research and enhances transparency and
trustworthiness in deep learning applications utilizing embedded spaces.",2025-05-21,"Christiaan Meijer, E. G. Patrick Bos",http://arxiv.org/pdf/2505.15516v1,cs.CL
Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought,"Large Vision-Language Models (LVLMs) have achieved significant success in
multimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing
performance and interpretability. Recent MCoT methods fall into two categories:
(i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual
output; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved
image-text outputs. Despite advances in both approaches, the mechanisms driving
these improvements are not fully understood. To fill this gap, we first reveal
that MCoT boosts LVLMs by incorporating visual thoughts, which convey image
information to the reasoning process regardless of the MCoT format, depending
only on clarity and conciseness of expression. Furthermore, to explore visual
thoughts systematically, we define four distinct forms of visual thought
expressions and analyze them comprehensively. Our findings demonstrate that
these forms differ in clarity and conciseness, yielding varying levels of MCoT
improvement. Additionally, we explore the internal nature of visual thoughts,
finding that visual thoughts serve as intermediaries between the input image
and reasoning to deeper transformer layers, enabling more advanced visual
information transmission. We hope that the visual thoughts can inspire further
breakthroughs for future MCoT research.",2025-05-21,"Zihui Cheng, Qiguang Chen, Xiao Xu, Jiaqi Wang, Weiyun Wang, Hao Fei, Yidong Wang, Alex Jinpeng Wang, Zhi Chen, Wanxiang Che, Libo Qin",http://arxiv.org/pdf/2505.15510v1,cs.CL
Multilingual Test-Time Scaling via Initial Thought Transfer,"Test-time scaling has emerged as a widely adopted inference-time strategy for
boosting reasoning performance. However, its effectiveness has been studied
almost exclusively in English, leaving its behavior in other languages largely
unexplored. We present the first systematic study of test-time scaling in
multilingual settings, evaluating DeepSeek-R1-Distill-LLama-8B and
DeepSeek-R1-Distill-Qwen-7B across both high- and low-resource Latin-script
languages. Our findings reveal that the relative gains from test-time scaling
vary significantly across languages. Additionally, models frequently switch to
English mid-reasoning, even when operating under strictly monolingual prompts.
We further show that low-resource languages not only produce initial reasoning
thoughts that differ significantly from English but also have lower internal
consistency across generations in their early reasoning. Building on our
findings, we introduce MITT (Multilingual Initial Thought Transfer), an
unsupervised and lightweight reasoning prefix-tuning approach that transfers
high-resource reasoning prefixes to enhance test-time scaling across all
languages, addressing inconsistencies in multilingual reasoning performance.
MITT significantly boosts DeepSeek-R1-Distill-Qwen-7B's reasoning performance,
especially for underrepresented languages.",2025-05-21,"Prasoon Bajpai, Tanmoy Chakraborty",http://arxiv.org/pdf/2505.15508v1,cs.CL
Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs,"We introduce the concept of protoknowledge to formalize and measure how
sequences of tokens encoding Knowledge Graphs are internalized during
pretraining and utilized at inference time by Large Language Models (LLMs).
Indeed, LLMs have demonstrated the ability to memorize vast amounts of token
sequences during pretraining, and a central open question is how they leverage
this memorization as reusable knowledge through generalization. We then
categorize protoknowledge into lexical, hierarchical, and topological forms,
varying on the type of knowledge that needs to be activated. We measure
protoknowledge through Knowledge Activation Tasks (KATs), analyzing its general
properties such as semantic bias. We then investigate the impact of
protoknowledge on Text-to-SPARQL performance by varying prompting strategies
depending on input conditions. To this end, we adopt a novel analysis framework
that assesses whether model predictions align with the successful activation of
the relevant protoknowledge for each query. This methodology provides a
practical tool to explore Semantic-Level Data Contamination and serves as an
effective strategy for Closed-Pretraining models.",2025-05-21,"Federico Ranaldi, Andrea Zugarini, Leonardo Ranaldi, Fabio Massimo Zanzotto",http://arxiv.org/pdf/2505.15501v1,cs.CL
Collaborative Problem-Solving in an Optimization Game,"Dialogue agents that support human users in solving complex tasks have
received much attention recently. Many such tasks are NP-hard optimization
problems that require careful collaborative exploration of the solution space.
We introduce a novel dialogue game in which the agents collaboratively solve a
two-player Traveling Salesman problem, along with an agent that combines LLM
prompting with symbolic mechanisms for state tracking and grounding. Our best
agent solves 45% of games optimally in self-play. It also demonstrates an
ability to collaborate successfully with human users and generalize to
unfamiliar graphs.",2025-05-21,"Isidora Jeknic, Alex Duchnowski, Alexander Koller",http://arxiv.org/pdf/2505.15490v1,cs.CL
Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models,"The real-world impact of misinformation stems from the underlying misleading
narratives that creators seek to convey. As such, interpreting misleading
creator intent is essential for multimodal misinformation detection (MMD)
systems aimed at effective information governance. In this paper, we introduce
an automated framework that simulates real-world multimodal news creation by
explicitly modeling creator intent through two components: the desired
influence and the execution plan. Using this framework, we construct
DeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs
aligned with trustworthy reference articles. The dataset captures both
misleading and non-misleading intents and spans manipulations across visual and
textual modalities. We conduct a comprehensive evaluation of 14
state-of-the-art vision-language models (VLMs) on three intent-centric tasks:
(1) misleading intent detection, (2) misleading source attribution, and (3)
creator desire inference. Despite recent advances, we observe that current VLMs
fall short in recognizing misleading intent, often relying on spurious cues
such as superficial cross-modal consistency, stylistic signals, and heuristic
authenticity hints. Our findings highlight the pressing need for intent-aware
modeling in MMD and open new directions for developing systems capable of
deeper reasoning about multimodal misinformation.",2025-05-21,"Jiaying Wu, Fanxiao Li, Min-Yen Kan, Bryan Hooi",http://arxiv.org/pdf/2505.15489v2,cs.CL
KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance,"Supervised fine-tuning (SFT) is a common approach to improve the
domain-specific question-answering (QA) performance of large language models
(LLMs). However, recent literature reveals that due to the conflicts between
LLMs' internal knowledge and the context knowledge of training data, vanilla
SFT using the full QA training set is usually suboptimal. In this paper, we
first design a query diversification strategy for robust conflict detection and
then conduct a series of experiments to analyze the impact of knowledge
conflict. We find that 1) training samples with varied conflicts contribute
differently, where SFT on the data with large conflicts leads to catastrophic
performance drops; 2) compared to directly filtering out the conflict data,
appropriately applying the conflict data would be more beneficial. Motivated by
this, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely
KaFT) approach to effectively boost LLMs' performance. The core of KaFT is to
adapt the training weight by assigning different rewards for different training
samples according to conflict level. Extensive experiments show that KaFT
brings consistent and significant improvements across four LLMs. More analyses
prove that KaFT effectively improves the model generalization and alleviates
the hallucination.",2025-05-21,"Qihuang Zhong, Liang Ding, Xiantao Cai, Juhua Liu, Bo Du, Dacheng Tao",http://arxiv.org/pdf/2505.15480v1,cs.CL
LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models,"Nowadays, Large Language Models (LLMs) have attracted widespread attention
due to their powerful performance. However, due to the unavoidable exposure to
socially biased data during training, LLMs tend to exhibit social biases,
particularly gender bias. To better explore and quantifying the degree of
gender bias in LLMs, we propose a pair of datasets named GenBiasEval and
GenHintEval, respectively. The GenBiasEval is responsible for evaluating the
degree of gender bias in LLMs, accompanied by an evaluation metric named
AFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is
used to assess whether LLMs can provide responses consistent with prompts that
contain gender hints, along with the accompanying evaluation metric UB-Score
(UnBias Score). Besides, in order to mitigate gender bias in LLMs more
effectively, we present the LFTF (Locating First and Then Fine-Tuning)
algorithm.The algorithm first ranks specific LLM blocks by their relevance to
gender bias in descending order using a metric called BMI (Block Mitigating
Importance Score). Based on this ranking, the block most strongly associated
with gender bias is then fine-tuned using a carefully designed loss function.
Numerous experiments have shown that our proposed LFTF algorithm can
significantly mitigate gender bias in LLMs while maintaining their general
capabilities.",2025-05-21,"Zhanyue Qin, Yue Ding, Deyuan Liu, Qingbin Liu, Junxian Cai, Xi Chen, Zhiying Tu, Dianhui Chu, Cuiyun Gao, Dianbo Sui",http://arxiv.org/pdf/2505.15475v1,cs.CL
"PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions","Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in diverse reasoning tasks, yet their application to complex
physics reasoning remains underexplored. Physics reasoning presents unique
challenges, requiring grounding in physical conditions and the interpretation
of multimodal information. Current physics benchmarks are limited, often
focusing on text-only inputs or solely on problem-solving, thereby overlooking
the critical intermediate steps of variable identification and process
formulation. To address these limitations, we introduce PhysicsArena, the first
multimodal physics reasoning benchmark designed to holistically evaluate MLLMs
across three critical dimensions: variable identification, physical process
formulation, and solution derivation. PhysicsArena aims to provide a
comprehensive platform for assessing and advancing the multimodal physics
reasoning abilities of MLLMs.",2025-05-21,"Song Dai, Yibo Yan, Jiamin Su, Dongfang Zihao, Yubo Gao, Yonghua Hei, Jungang Li, Junyan Zhang, Sicheng Tao, Zhuoran Gao, Xuming Hu",http://arxiv.org/pdf/2505.15472v2,cs.CL
CoLA: Collaborative Low-Rank Adaptation,"The scaling law of Large Language Models (LLMs) reveals a power-law
relationship, showing diminishing return on performance as model scale
increases. While training LLMs from scratch is resource-intensive, fine-tuning
a pre-trained model for specific tasks has become a practical alternative. Full
fine-tuning (FFT) achieves strong performance; however, it is computationally
expensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like
LoRA, have been proposed to address these challenges by freezing the
pre-trained model and adding lightweight task-specific modules. LoRA, in
particular, has proven effective, but its application to multi-task scenarios
is limited by interference between tasks. Recent approaches, such as
Mixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these
issues but still struggle with sample scarcity and noise interference due to
their fixed structure. In response, we propose CoLA, a more flexible LoRA
architecture with an efficient initialization scheme, and introduces three
collaborative strategies to enhance performance by better utilizing the
quantitative relationships between matrices $A$ and $B$. Our experiments
demonstrate the effectiveness and robustness of CoLA, outperforming existing
PEFT methods, especially in low-sample scenarios. Our data and code are fully
publicly available at https://github.com/zyy-2001/CoLA.",2025-05-21,"Yiyun Zhou, Chang Yao, Jingyuan Chen",http://arxiv.org/pdf/2505.15471v1,cs.CL
Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning,"Large language models have achieved remarkable success in various tasks.
However, it is challenging for them to learn new tasks incrementally due to
catastrophic forgetting. Existing approaches rely on experience replay,
optimization constraints, or task differentiation, which encounter strict
limitations in real-world scenarios. To address these issues, we propose Joint
Flashback Adaptation. We first introduce flashbacks -- a limited number of
prompts from old tasks -- when adapting to new tasks and constrain the
deviations of the model outputs compared to the original one. We then
interpolate latent tasks between flashbacks and new tasks to enable jointly
learning relevant latent tasks, new tasks, and flashbacks, alleviating data
sparsity in flashbacks and facilitating knowledge sharing for smooth
adaptation. Our method requires only a limited number of flashbacks without
access to the replay data and is task-agnostic. We conduct extensive
experiments on state-of-the-art large language models across 1000+
instruction-following tasks, arithmetic reasoning tasks, and general reasoning
tasks. The results demonstrate the superior performance of our method in
improving generalization on new tasks and reducing forgetting in old tasks.",2025-05-21,"Yukun Zhao, Lingyong Yan, Zhenyang Li, Shuaiqiang Wang, Zhumin Chen, Zhaochun Ren, Dawei Yin",http://arxiv.org/pdf/2505.15467v1,cs.CL
A Participatory Strategy for AI Ethics in Education and Rehabilitation grounded in the Capability Approach,"AI-based technologies have significant potential to enhance inclusive
education and clinical-rehabilitative contexts for children with Special
Educational Needs and Disabilities. AI can enhance learning experiences,
empower students, and support both teachers and rehabilitators. However, their
usage presents challenges that require a systemic-ecological vision, ethical
considerations, and participatory research. Therefore, research and
technological development must be rooted in a strong ethical-theoretical
framework. The Capability Approach - a theoretical model of disability, human
vulnerability, and inclusion - offers a more relevant perspective on
functionality, effectiveness, and technological adequacy in inclusive learning
environments. In this paper, we propose a participatory research strategy with
different stakeholders through a case study on the ARTIS Project, which
develops an AI-enriched interface to support children with text comprehension
difficulties. Our research strategy integrates ethical, educational, clinical,
and technological expertise in designing and implementing AI-based technologies
for children's learning environments through focus groups and collaborative
design sessions. We believe that this holistic approach to AI adoption in
education can help bridge the gap between technological innovation and ethical
responsibility.",2025-05-21,"Valeria Cesaroni, Eleonora Pasqua, Piercosma Bisconti, Martina Galletti",http://arxiv.org/pdf/2505.15466v1,cs.CL
Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling,"Fine-tuning Large Language Models (LLMs) with multimodal encoders on
modality-specific data expands the modalities that LLMs can handle, leading to
the formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies
on resource-intensive and inflexible fine-tuning from scratch with new
multimodal data. In this paper, we propose MMER (Multi-modality Expansion and
Retention), a training-free approach that integrates existing MLLMs for
effective multimodal expansion while retaining their original performance.
Specifically, MMER reuses MLLMs' multimodal encoders while merging their LLM
parameters. By comparing original and merged LLM parameters, MMER generates
binary masks to approximately separate LLM parameters for each modality. These
decoupled parameters can independently process modality-specific inputs,
reducing parameter conflicts and preserving original MLLMs' fidelity. MMER can
also mitigate catastrophic forgetting by applying a similar process to MLLMs
fine-tuned on new tasks. Extensive experiments show significant improvements
over baselines, proving that MMER effectively expands LLMs' multimodal
capabilities while retaining 99% of the original performance, and also markedly
mitigates catastrophic forgetting.",2025-05-21,"Junlin Li, Guodong DU, Jing Li, Sim Kuan Goh, Wenya Wang, Yequan Wang, Fangming Liu, Ho-Kin Tang, Saleh Alharbi, Daojing He, Min Zhang",http://arxiv.org/pdf/2505.17110v1,cs.CL
Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment,"Personalized alignment is essential for enabling large language models (LLMs)
to engage effectively in user-centric dialogue. While recent prompt-based and
offline optimization methods offer preliminary solutions, they fall short in
cold-start scenarios and long-term personalization due to their inherently
static and shallow designs. In this work, we introduce the Reinforcement
Learning for Personalized Alignment (RLPA) framework, in which an LLM interacts
with a simulated user model to iteratively infer and refine user profiles
through dialogue. The training process is guided by a dual-level reward
structure: the Profile Reward encourages accurate construction of user
representations, while the Response Reward incentivizes generation of responses
consistent with the inferred profile. We instantiate RLPA by fine-tuning
Qwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art
performance in personalized dialogue. Empirical evaluations demonstrate that
Qwen-RLPA consistently outperforms prompting and offline fine-tuning baselines,
and even surpasses advanced commercial models such as Claude-3.5 and GPT-4o.
Further analysis highlights Qwen-RLPA's robustness in reconciling conflicting
user preferences, sustaining long-term personalization and delivering more
efficient inference compared to recent reasoning-focused LLMs. These results
emphasize the potential of dynamic profile inference as a more effective
paradigm for building personalized dialogue systems.",2025-05-21,"Weixiang Zhao, Xingyu Sui, Yulin Hu, Jiahe Guo, Haixiao Liu, Biye Li, Yanyan Zhao, Bing Qin, Ting Liu",http://arxiv.org/pdf/2505.15456v1,cs.CL
"Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization","Existing studies have optimized retrieval-augmented generation (RAG) across
various sub-tasks, such as query understanding and retrieval refinement, but
integrating these optimizations into a unified framework remains challenging.
To tackle this problem, this work proposes RoleRAG, a unified RAG framework
that achieves efficient multi-task processing through role-specific token
optimization. RoleRAG comprises six modules, each handling a specific sub-task
within the RAG process. Additionally, we introduce a query graph to represent
the decomposition of the query, which can be dynamically resolved according to
the decomposing state. All modules are driven by the same underlying LLM,
distinguished by task-specific role tokens that are individually optimized.
This design allows RoleRAG to dynamically activate different modules within a
single LLM instance, thereby streamlining deployment and reducing resource
consumption. Experimental results on five open-domain question-answering
datasets demonstrate the effectiveness, generalizability, and flexibility of
our framework.",2025-05-21,"Yutao Zhu, Jiajie Jin, Hongjin Qian, Zheng Liu, Zhicheng Dou, Ji-Rong Wen",http://arxiv.org/pdf/2505.15444v1,cs.CL
AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs,"Uncertainty estimation remains a critical challenge in adapting pre-trained
language models to classification tasks, particularly under parameter-efficient
fine-tuning approaches such as adapters. We introduce AdUE1, an efficient
post-hoc uncertainty estimation (UE) method, to enhance softmax-based
estimates. Our approach (1) uses a differentiable approximation of the maximum
function and (2) applies additional regularization through L2-SP, anchoring the
fine-tuned head weights and regularizing the model. Evaluations on five NLP
classification datasets across four language models (RoBERTa, ELECTRA, LLaMA-2,
Qwen) demonstrate that our method consistently outperforms established
baselines such as Mahalanobis distance and softmax response. Our approach is
lightweight (no base-model changes) and produces better-calibrated confidence.",2025-05-21,"Artem Zabolotnyi, Roman Makarov, Mile Mitrovic, Polina Proskura, Oleg Travkin, Roman Alferov, Alexey Zaytsev",http://arxiv.org/pdf/2505.15443v1,cs.CL
On the Generalization vs Fidelity Paradox in Knowledge Distillation,"Knowledge distillation (KD) is a key technique for compressing large language
models into smaller ones while preserving performance. Despite the recent
traction of KD research, its effectiveness for smaller language models (LMs)
and the mechanisms driving knowledge transfer remain underexplored. In this
work, we present the first large-scale empirical and statistical analysis of KD
across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks
in a zero-shot setting. Our findings reveal that KD can improve the average
performance of smaller models by up to $10\%$, with a peak task specific gain
of $22\%$, while providing only marginal benefits ($\sim 1.3\%$) for larger
models. Surprisingly, teacher performance has a minimal impact on student
outcomes, while teacher task expertise impacts KD effectiveness. A correlation
study indicates that smaller LMs benefit more from KD, whereas larger LMs show
diminished gains. Additionally, we uncover a misalignment between improvements
in student performance and reasoning fidelity, suggesting that while KD
enhances accuracy, it does not always maintain the structured decision-making
processes of the teacher. Our ablation study further highlights the importance
of teacher signals and logit smoothing in influencing students' performance
after distillation. Overall, our study offers a comprehensive empirical and
statistical assessment of KD, highlighting both its benefits and trade-offs
when distilling knowledge from larger to smaller LMs.",2025-05-21,"Suhas Kamasetty Ramesh, Ayan Sengupta, Tanmoy Chakraborty",http://arxiv.org/pdf/2505.15442v1,cs.CL
Set-LLM: A Permutation-Invariant LLM,"While large language models (LLMs) demonstrate impressive capabilities across
numerous applications, their robustness remains a critical concern. This paper
is motivated by a specific vulnerability: the order sensitivity of LLMs. This
vulnerability manifests itself as the order bias observed when LLMs decide
between possible options (for example, a preference for the first option) and
the tendency of LLMs to provide different answers when options are reordered.
The use cases for this scenario extend beyond the classical case of
multiple-choice question answering to the use of LLMs as automated evaluators
in AI pipelines, comparing output generated by different models. We introduce
Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the
processing of mixed set-text inputs with permutation invariance guarantees. The
adaptations involve a new attention mask and new positional encodings
specifically designed for sets. We provide a theoretical proof of invariance
and demonstrate through experiments that Set-LLM can be trained effectively,
achieving comparable or improved performance and maintaining the runtime of the
original model, while eliminating order sensitivity.",2025-05-21,"Beni Egressy, Jan Stühmer",http://arxiv.org/pdf/2505.15433v1,cs.CL
Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought,"As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,
a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It
synergistically combines Mamba's long-sequence processing efficiency with
Transformer's superior contextual understanding. Hunyuan-TurboS features an
adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching
between rapid responses for simple queries and deep ""thinking"" modes for
complex problems, optimizing computational resources. Architecturally, this 56B
activated (560B total) parameter model employs 128 layers (Mamba2, Attention,
FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear
complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE
structure. Pre-trained on 16T high-quality tokens, it supports a 256K context
length and is the first industry-deployed large-scale Mamba model. Our
comprehensive post-training strategy enhances capabilities via Supervised
Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,
Multi-round Deliberation Learning for iterative improvement, and a two-stage
Large-scale Reinforcement Learning process targeting STEM and general
instruction-following. Evaluations show strong performance: overall top 7 rank
on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like
Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves
an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances
high performance and efficiency, offering substantial capabilities at lower
inference costs than many reasoning models, establishing a new paradigm for
efficient large-scale pre-trained models.",2025-05-21,"Tencent Hunyuan Team, Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, Dong Du, Dong Wang, Feng Zhang, Fengzong Lian, Guanghui Xu, Guanwei Zhang, Hai Wang, Haipeng Luo, Han Hu, Huilin Xu, Jiajia Wu, Jianchen Zhu, Jianfeng Yan, Jiaqi Zhu, Jihong Zhang, Jinbao Xue, Jun Xia, Junqiang Zheng, Kai Liu, Kai Zhang, Kai Zheng, Kejiao Li, Keyao Wang, Lan Jiang, Lixin Liu, Lulu Wu, Mengyuan Huang, Peijie Yu, Peiqi Wang, Qian Wang, Qianbiao Xiang, Qibin Liu, Qingfeng Sun, Richard Guo, Ruobing Xie, Saiyong Yang, Shaohua Chen, Shihui Hu, Shuai Li, Shuaipeng Li, Shuang Chen, Suncong Zheng, Tao Yang, Tian Zhang, Tinghao Yu, Weidong Han, Weijie Liu, Weijin Zhou, Weikang Wang, Wesleye Chen, Xiao Feng, Xiaoqin Ren, Xingwu Sun, Xiong Kuang, Xuemeng Huang, Xun Cao, Yanfeng Chen, Yang Du, Yang Zhen, Yangyu Tao, Yaping Deng, Yi Shen, Yigeng Hong, Yiqi Chen, Yiqing Huang, Yuchi Deng, Yue Mao, Yulong Wang, Yuyuan Zeng, Zenan Xu, Zhanhui Kang, Zhe Zhao, ZhenXiang Yan, Zheng Fang, Zhichao Hu, Zhongzhi Chen, Zhuoyu Li, Zongwei Li, Alex Yan, Ande Liang, Baitong Liu, Beiping Pan, Bin Xing, Binghong Wu, Bingxin Qu, Bolin Ni, Boyu Wu, Chen Li, Cheng Jiang, Cheng Zhang, Chengjun Liu, Chengxu Yang, Chengzhong Xu, Chiyu Wang, Chong Zha, Daisy Yi, Di Wang, Fanyang Lu, Fei Chen, Feifei Liu, Feng Zheng, Guanghua Yu, Guiyang Li, Guohua Wang, Haisheng Lin, Han Liu, Han Wang, Hao Fei, Hao Lu, Haoqing Jiang, Haoran Sun, Haotian Zhu, Huangjin Dai, Huankui Chen, Huawen Feng, Huihui Cai, Huxin Peng, Jackson Lv, Jiacheng Shi, Jiahao Bu, Jianbo Li, Jianglu Hu, Jiangtao Guan, Jianing Xu, Jianwei Cai, Jiarong Zhang, Jiawei Song, Jie Jiang, Jie Liu, Jieneng Yang, Jihong Zhang, Jin lv, Jing Zhao, Jinjian Li, Jinxing Liu, Jun Zhao, Juntao Guo, Kai Wang, Kan Wu, Lei Fu, Lei He, Lei Wang, Li Liu, Liang Dong, Liya Zhan, Long Cheng, Long Xu, Mao Zheng, Meng Liu, Mengkang Hu, Nanli Chen, Peirui Chen, Peng He, Pengju Pan, Pengzhi Wei, Qi Yang, Qi Yi, Roberts Wang, Rongpeng Chen, Rui Sun, Rui Yang, Ruibin Chen, Ruixu Zhou, Shaofeng Zhang, Sheng Zhang, Shihao Xu, Shuaishuai Chang, Shulin Liu, SiQi Wang, Songjia Feng, Songling Yuan, Tao Zhang, Tianjiao Lang, Tongkai Li, Wei Deng, Wei Li, Weichao Wang, Weigang Zhang, Weixuan Sun, Wen Ouyang, Wenxiang Jiao, Wenzhi Sun, Wenzhuo Jia, Xiang Zhang, Xiangyu He, Xianshun Ren, XiaoYing Zhu, Xiaolong Guo, Xiaoxue Li, Xiaoyu Ma, Xican Lu, Xinhua Feng, Xinting Huang, Xinyu Guan, Xirui Li, Xu Zhang, Xudong Gao, Xun Luo, Xuxiang Qi, Yangkun Chen, Yangyu Tao, Yanling Xiao, Yantao Mai, Yanze Chen, Yao Ding, Yeting Yang, YiFan Song, Yifan Yang, Yijiao Zhu, Yinhe Wu, Yixian Liu, Yong Yang, Yuanjun Cai, Yuanlin Tu, Yue Zhang, Yufei Huang, Yuhang Zhou, Yuhao Jiang, Yuhong Liu, Yuhui Hu, Yujin Lin, Yun Yang, Yunhao Wang, Yusong Zhang, Zekun Wu, Zelong Zhang, Zhan Yu, Zhaoliang Yang, Zhe Zhao, Zheng Li, Zhenyu Huang, Zhiguang Liu, Zhijiang Xu, Zhiqing Kui, Zhiyin Zeng, Zhiyuan Xiong, Zhuo Han, Zifan Wu, Zigang Geng, Zilong Zhao, Ziyan Tang, Ziyuan Zhu, Zonglei Zhu, Zhijiang Xu",http://arxiv.org/pdf/2505.15431v2,cs.CL
Likelihood Variance as Text Importance for Resampling Texts to Map Language Models,"We address the computational cost of constructing a model map, which embeds
diverse language models into a common space for comparison via KL divergence.
The map relies on log-likelihoods over a large text set, making the cost
proportional to the number of texts. To reduce this cost, we propose a
resampling method that selects important texts with weights proportional to the
variance of log-likelihoods across models for each text. Our method
significantly reduces the number of required texts while preserving the
accuracy of KL divergence estimates. Experiments show that it achieves
comparable performance to uniform sampling with about half as many texts, and
also facilitates efficient incorporation of new models into an existing map.
These results enable scalable and efficient construction of language model
maps.",2025-05-21,"Momose Oyama, Ryo Kishino, Hiroaki Yamagiwa, Hidetoshi Shimodaira",http://arxiv.org/pdf/2505.15428v1,cs.CL
Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions,"The remarkable ability of diffusion models to generate high-fidelity images
has led to their widespread adoption. However, concerns have also arisen
regarding their potential to produce Not Safe for Work (NSFW) content and
exhibit social biases, hindering their practical use in real-world
applications. In response to this challenge, prior work has focused on
employing security filters to identify and exclude toxic text, or
alternatively, fine-tuning pre-trained diffusion models to erase sensitive
concepts. Unfortunately, existing methods struggle to achieve satisfactory
performance in the sense that they can have a significant impact on the normal
model output while still failing to prevent the generation of harmful content
in some cases. In this paper, we propose a novel self-discovery approach to
identifying a semantic direction vector in the embedding space to restrict text
embedding within a safe region. Our method circumvents the need for correcting
individual words within the input text and steers the entire text prompt
towards a safe region in the embedding space, thereby enhancing model
robustness against all possibly unsafe prompts. In addition, we employ Low-Rank
Adaptation (LoRA) for semantic direction vector initialization to reduce the
impact on the model performance for other semantics. Furthermore, our method
can also be integrated with existing methods to improve their social
responsibility. Extensive experiments on benchmark datasets demonstrate that
our method can effectively reduce NSFW content and mitigate social bias
generated by diffusion models compared to several state-of-the-art baselines.",2025-05-21,"Zhiwen Li, Die Chen, Mingyuan Fan, Cen Chen, Yaliang Li, Yanhao Wang, Wenmeng Zhou",http://arxiv.org/pdf/2505.15427v1,cs.CL
"NeoN: A Tool for Automated Detection, Linguistic and LLM-Driven Analysis of Neologisms in Polish","NeoN, a tool for detecting and analyzing Polish neologisms. Unlike
traditional dictionary-based methods requiring extensive manual review, NeoN
combines reference corpora, Polish-specific linguistic filters, an LLM-driven
precision-boosting filter, and daily RSS monitoring in a multi-layered
pipeline. The system uses context-aware lemmatization, frequency analysis, and
orthographic normalization to extract candidate neologisms while consolidating
inflectional variants. Researchers can verify candidates through an intuitive
interface with visualizations and filtering controls. An integrated LLM module
automatically generates definitions and categorizes neologisms by domain and
sentiment. Evaluations show NeoN maintains high accuracy while significantly
reducing manual effort, providing an accessible solution for tracking lexical
innovation in Polish.",2025-05-21,"Aleksandra Tomaszewska, Dariusz Czerski, Bartosz Żuk, Maciej Ogrodniczuk",http://arxiv.org/pdf/2505.15426v1,cs.CL
Gated Integration of Low-Rank Adaptation for Continual Learning of Language Models,"Continual learning (CL), which requires the model to learn multiple tasks
sequentially, is crucial for language models (LMs). Recently, low-rank
adaptation (LoRA), one of the most representative parameter-efficient
fine-tuning (PEFT) methods, has gained increasing attention in CL of LMs.
However, most existing CL methods based on LoRA typically expand a new LoRA
branch to learn each new task and force the new and old LoRA branches to
contribute equally to old tasks, potentially leading to forgetting. In this
work, we propose a new method, called gated integration of low-rank adaptation
(GainLoRA), for CL of LMs. GainLoRA expands a new LoRA branch for each new task
and introduces gating modules to integrate the new and old LoRA branches.
Furthermore, GainLoRA leverages the new gating module to minimize the
contribution from the new LoRA branch to old tasks, effectively mitigating
forgetting and improving the model's overall performance. Experimental results
on CL benchmarks demonstrate that GainLoRA outperforms existing
state-of-the-art methods.",2025-05-21,"Yan-Shuo Liang, Wu-Jun Li",http://arxiv.org/pdf/2505.15424v1,cs.CL
"Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches","Authorship analysis plays an important role in diverse domains, including
forensic linguistics, academia, cybersecurity, and digital content
authentication. This paper presents a systematic literature review on two key
sub-tasks of authorship analysis; Author Attribution and Author Verification.
The review explores SOTA methodologies, ranging from traditional ML approaches
to DL models and LLMs, highlighting their evolution, strengths, and
limitations, based on studies conducted from 2015 to 2024. Key contributions
include a comprehensive analysis of methods, techniques, their corresponding
feature extraction techniques, datasets used, and emerging challenges in
authorship analysis. The study highlights critical research gaps, particularly
in low-resource language processing, multilingual adaptation, cross-domain
generalization, and AI-generated text detection. This review aims to help
researchers by giving an overview of the latest trends and challenges in
authorship analysis. It also points out possible areas for future study. The
goal is to support the development of better, more reliable, and accurate
authorship analysis system in diverse textual domain.",2025-05-21,"Nudrat Habib, Tosin Adewumi, Marcus Liwicki, Elisa Barney",http://arxiv.org/pdf/2505.15422v1,cs.CL
ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs,"Clickstream data from digital learning environments offer valuable insights
into students' learning behaviors, but are challenging to interpret due to
their high dimensionality and granularity. Prior approaches have relied mainly
on handcrafted features, expert labeling, clustering, or supervised models,
therefore often lacking generalizability and scalability. In this work, we
introduce ClickSight, an in-context Large Language Model (LLM)-based pipeline
that interprets student clickstreams to reveal their learning strategies.
ClickSight takes raw clickstreams and a list of learning strategies as input
and generates textual interpretations of students' behaviors during
interaction. We evaluate four different prompting strategies and investigate
the impact of self-refinement on interpretation quality. Our evaluation spans
two open-ended learning environments and uses a rubric-based domain-expert
evaluation. Results show that while LLMs can reasonably interpret learning
strategies from clickstreams, interpretation quality varies by prompting
strategy, and self-refinement offers limited improvement. ClickSight
demonstrates the potential of LLMs to generate theory-driven insights from
educational interaction data.",2025-05-21,"Bahar Radmehr, Ekaterina Shved, Fatma Betül Güreş, Adish Singla, Tanja Käser",http://arxiv.org/pdf/2505.15410v1,cs.CL
How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study,"Large Reasoning Models (LRMs) have achieved remarkable success on
reasoning-intensive tasks such as mathematics and programming. However, their
enhanced reasoning capabilities do not necessarily translate to improved safety
performance-and in some cases, may even degrade it. This raises an important
research question: how can we enhance the safety of LRMs? In this paper, we
present a comprehensive empirical study on how to enhance the safety of LRMs
through Supervised Fine-Tuning (SFT). Our investigation begins with an
unexpected observation: directly distilling safe responses from DeepSeek-R1
fails to significantly enhance safety. We analyze this phenomenon and identify
three key failure patterns that contribute to it. We then demonstrate that
explicitly addressing these issues during the data distillation process can
lead to substantial safety improvements. Next, we explore whether a long and
complex reasoning process is necessary for achieving safety. Interestingly, we
find that simply using short or template-based reasoning process can attain
comparable safety performance-and are significantly easier for models to learn
than more intricate reasoning chains. These findings prompt a deeper reflection
on the role of reasoning in ensuring safety. Finally, we find that mixing math
reasoning data during safety fine-tuning is helpful to balance safety and
over-refusal. Overall, we hope our empirical study could provide a more
holistic picture on enhancing the safety of LRMs. The code and data used in our
experiments are released in https://github.com/thu-coai/LRM-Safety-Study.",2025-05-21,"Zhexin Zhang, Xian Qi Loye, Victor Shea-Jay Huang, Junxiao Yang, Qi Zhu, Shiyao Cui, Fei Mi, Lifeng Shang, Yingkang Wang, Hongning Wang, Minlie Huang",http://arxiv.org/pdf/2505.15404v1,cs.CL
When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning,"Large reasoning models (LRMs) achieve remarkable performance via long
reasoning chains, but often incur excessive computational overhead due to
redundant reasoning, especially on simple tasks. In this work, we
systematically quantify the upper bounds of LRMs under both Long-Thinking and
No-Thinking modes, and uncover the phenomenon of ""Internal Self-Recovery
Mechanism"" where models implicitly supplement reasoning during answer
generation. Building on this insight, we propose Adaptive Self-Recovery
Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables
implicit recovery. By introducing accuracy-aware length reward regulation, ASRR
adaptively allocates reasoning effort according to problem difficulty,
achieving high efficiency with negligible performance sacrifice. Experiments
across multiple benchmarks and models show that, compared with GRPO, ASRR
reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal
accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates
on safety benchmarks (up to +21.7%). Our results highlight the potential of
ASRR for enabling efficient, adaptive, and safer reasoning in LRMs.",2025-05-21,"Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Haodong Zhao, Hao Li, Jiansong Chen, Ke Zeng, Xunliang Cai",http://arxiv.org/pdf/2505.15400v2,cs.CL
Mitigating Cyber Risk in the Age of Open-Weight LLMs: Policy Gaps and Technical Realities,"Open-weight general-purpose AI (GPAI) models offer significant benefits but
also introduce substantial cybersecurity risks, as demonstrated by the
offensive capabilities of models like DeepSeek-R1 in evaluations such as
MITRE's OCCULT. These publicly available models empower a wider range of actors
to automate and scale cyberattacks, challenging traditional defence paradigms
and regulatory approaches. This paper analyzes the specific threats --
including accelerated malware development and enhanced social engineering --
magnified by open-weight AI release. We critically assess current regulations,
notably the EU AI Act and the GPAI Code of Practice, identifying significant
gaps stemming from the loss of control inherent in open distribution, which
renders many standard security mitigations ineffective. We propose a path
forward focusing on evaluating and controlling specific high-risk capabilities
rather than entire models, advocating for pragmatic policy interpretations for
open-weight systems, promoting defensive AI innovation, and fostering
international collaboration on standards and cyber threat intelligence (CTI)
sharing to ensure security without unduly stifling open technological progress.",2025-05-21,Alfonso de Gregorio,http://arxiv.org/pdf/2505.17109v1,cs.CL
"An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations","The rise of Large Language Models (LLMs) like ChatGPT has advanced natural
language processing, yet concerns about cognitive biases are growing. In this
paper, we investigate the anchoring effect, a cognitive bias where the mind
relies heavily on the first information as anchors to make affected judgments.
We explore whether LLMs are affected by anchoring, the underlying mechanisms,
and potential mitigation strategies. To facilitate studies at scale on the
anchoring effect, we introduce a new dataset, SynAnchors. Combining refined
evaluation metrics, we benchmark current widely used LLMs. Our findings show
that LLMs' anchoring bias exists commonly with shallow-layer acting and is not
eliminated by conventional strategies, while reasoning can offer some
mitigation. This recontextualization via cognitive psychology urges that LLM
evaluations focus not on standard benchmarks or over-optimized robustness
tests, but on cognitive-bias-aware trustworthy evaluation.",2025-05-21,"Yiming Huang, Biquan Bie, Zuqiu Na, Weilin Ruan, Songxin Lei, Yutao Yue, Xinlei He",http://arxiv.org/pdf/2505.15392v1,cs.CL
Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study,"Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet
most evaluations rely on artificial images. This study asks: How safe are
current VLMs when confronted with meme images that ordinary users share? To
investigate this question, we introduce MemeSafetyBench, a 50,430-instance
benchmark pairing real meme images with both harmful and benign instructions.
Using a comprehensive safety taxonomy and LLM-based instruction generation, we
assess multiple VLMs across single and multi-turn interactions. We investigate
how real-world memes influence harmful outputs, the mitigating effects of
conversational context, and the relationship between model scale and safety
metrics. Our findings demonstrate that VLMs show greater vulnerability to
meme-based harmful prompts than to synthetic or typographic images. Memes
significantly increase harmful responses and decrease refusals compared to
text-only inputs. Though multi-turn interactions provide partial mitigation,
elevated vulnerability persists. These results highlight the need for
ecologically valid evaluations and stronger safety mechanisms.",2025-05-21,"DongGeon Lee, Joonwon Jang, Jihae Jeong, Hwanjo Yu",http://arxiv.org/pdf/2505.15389v1,cs.CL
RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection,"Large Language Models (LLMs) have become powerful, but hallucinations remain
a vital obstacle to their trustworthy use. While previous works improved the
capability of hallucination detection by measuring uncertainty, they all lack
the ability to explain the provenance behind why hallucinations occur, i.e.,
which part of the inputs tends to trigger hallucinations. Recent works on the
prompt attack indicate that uncertainty exists in semantic propagation, where
attention mechanisms gradually fuse local token information into high-level
semantics across layers. Meanwhile, uncertainty also emerges in language
generation, due to its probability-based selection of high-level semantics for
sampled generations. Based on that, we propose RePPL to recalibrate uncertainty
measurement by these two aspects, which dispatches explainable uncertainty
scores to each token and aggregates in Perplexity-style Log-Average form as
total score. Experiments show that our method achieves the best comprehensive
detection performance across various QA datasets on advanced models (average
AUC of 0.833), and our method is capable of producing token-level uncertainty
scores as explanations for the hallucination. Leveraging these scores, we
preliminarily find the chaotic pattern of hallucination and showcase its
promising usage.",2025-05-21,"Yiming Huang, Junyan Zhang, Zihao Wang, Biquan Bie, Xuming Hu, Yi R., Fung, Xinlei He",http://arxiv.org/pdf/2505.15386v1,cs.CL
X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System,"Recently, large language model (LLM)-based agents have achieved significant
success in interactive environments, attracting significant academic and
industrial attention. Despite these advancements, current research
predominantly focuses on English scenarios. In reality, there are over 7,000
languages worldwide, all of which demand access to comparable agentic services.
Nevertheless, the development of language agents remains inadequate for meeting
the diverse requirements of multilingual agentic applications. To fill this
gap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an
interactive web environment, which evaluates the planning and interaction
performance of language agents across multiple languages, thereby contributing
to the advancement of global agent intelligence. Additionally, we assess the
performance of various LLMs and cross-lingual alignment methods, examining
their effectiveness in enhancing agents. Our findings reveal that even advanced
models like GPT-4o, when combined with cross-lingual techniques, fail to
achieve satisfactory results. We hope that X-WebAgentBench can serve as a
valuable benchmark for multilingual agent scenario in real-world applications.",2025-05-21,"Peng Wang, Ruihan Tao, Qiguang Chen, Mengkang Hu, Libo Qin",http://arxiv.org/pdf/2505.15372v1,cs.CL
Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition,"Vision-Language Models (VLMs) have demonstrated impressive capabilities in
understanding visual content, but their reliability in safety-critical contexts
remains under-explored. We introduce VERI (Visual Emergency Recognition
Dataset), a carefully designed diagnostic benchmark of 200 images (100
contrastive pairs). Each emergency scene is matched with a visually similar but
safe counterpart through multi-stage human verification and iterative
refinement. Using a two-stage protocol - risk identification and emergency
response - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,
accidents, and natural disasters. Our analysis reveals a systematic
overreaction problem: models excel at identifying real emergencies (70-100
percent success rate) but suffer from an alarming rate of false alarms,
misidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios
failed by all models regardless of scale. This ""better-safe-than-sorry"" bias
manifests primarily through contextual overinterpretation (88-93 percent of
errors), challenging VLMs' reliability for safety applications. These findings
highlight persistent limitations that are not resolved by increasing model
scale, motivating targeted approaches for improving contextual safety
assessment in visually misleading scenarios.",2025-05-21,"Dasol Choi, Seunghyun Lee, Youngsook Song",http://arxiv.org/pdf/2505.15367v1,cs.CL
AI vs. Human Judgment of Content Moderation: LLM-as-a-Judge and Ethics-Based Response Refusals,"As large language models (LLMs) are increasingly deployed in high-stakes
settings, their ability to refuse ethically sensitive prompts-such as those
involving hate speech or illegal activities-has become central to content
moderation and responsible AI practices. While refusal responses can be viewed
as evidence of ethical alignment and safety-conscious behavior, recent research
suggests that users may perceive them negatively. At the same time, automated
assessments of model outputs are playing a growing role in both evaluation and
training. In particular, LLM-as-a-Judge frameworks-in which one model is used
to evaluate the output of another-are now widely adopted to guide benchmarking
and fine-tuning. This paper examines whether such model-based evaluators assess
refusal responses differently than human users. Drawing on data from Chatbot
Arena and judgments from two AI judges (GPT-4o and Llama 3 70B), we compare how
different types of refusals are rated. We distinguish ethical refusals, which
explicitly cite safety or normative concerns (e.g., ""I can't help with that
because it may be harmful""), and technical refusals, which reflect system
limitations (e.g., ""I can't answer because I lack real-time data""). We find
that LLM-as-a-Judge systems evaluate ethical refusals significantly more
favorably than human users, a divergence not observed for technical refusals.
We refer to this divergence as a moderation bias-a systematic tendency for
model-based evaluators to reward refusal behaviors more than human users do.
This raises broader questions about transparency, value alignment, and the
normative assumptions embedded in automated evaluation systems.",2025-05-21,Stefan Pasch,http://arxiv.org/pdf/2505.15365v1,cs.CL
NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging,"Debugging is a critical aspect of LLM's coding ability. Early debugging
efforts primarily focused on code-level analysis, which often falls short when
addressing complex programming errors that require a deeper understanding of
algorithmic logic. Recent advancements in large language models (LLMs) have
shifted attention toward leveraging natural language reasoning to enhance
code-related tasks. However, two fundamental questions remain unanswered: What
type of natural language format is most effective for debugging tasks? And what
specific benefits does natural language reasoning bring to the debugging
process? In this paper, we introduce NL-DEBUGGING, a novel framework that
employs natural language as an intermediate representation to improve code
debugging. By debugging at a natural language level, we demonstrate that
NL-DEBUGGING outperforms traditional debugging methods and enables a broader
modification space through direct refinement guided by execution feedback. Our
findings highlight the potential of natural language reasoning to advance
automated code debugging and address complex programming challenges.",2025-05-21,"Weiming Zhang, Qingyao Li, Xinyi Dai, Jizheng Chen, Kounianhua Du, Weinan Zhang, Weiwen Liu, Yasheng Wang, Ruiming Tang, Yong Yu",http://arxiv.org/pdf/2505.15356v1,cs.CL
Decoding Phone Pairs from MEG Signals Across Speech Modalities,"Understanding the neural mechanisms underlying speech production is essential
for both advancing cognitive neuroscience theory and developing practical
communication technologies. In this study, we investigated
magnetoencephalography signals to decode phones from brain activity during
speech production and perception (passive listening and voice playback) tasks.
Using a dataset comprising 17 participants, we performed pairwise phone
classification, extending our analysis to 15 phonetic pairs. Multiple machine
learning approaches, including regularized linear models and neural network
architectures, were compared to determine their effectiveness in decoding
phonetic information. Our results demonstrate significantly higher decoding
accuracy during speech production (76.6%) compared to passive listening and
playback modalities (~51%), emphasizing the richer neural information available
during overt speech. Among the models, the Elastic Net classifier consistently
outperformed more complex neural networks, highlighting the effectiveness of
traditional regularization techniques when applied to limited and
high-dimensional MEG datasets. Besides, analysis of specific brain frequency
bands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz)
and Theta (4-7 Hz), contributed the most substantially to decoding accuracy,
suggesting that these bands encode critical speech production-related neural
processes. Despite using advanced denoising methods, it remains unclear whether
decoding solely reflects neural activity or if residual muscular or movement
artifacts also contributed, indicating the need for further methodological
refinement. Overall, our findings underline the critical importance of
examining overt speech production paradigms, which, despite their complexity,
offer opportunities to improve brain-computer interfaces to help individuals
with severe speech impairments.",2025-05-21,"Xabier de Zuazo, Eva Navas, Ibon Saratxaga, Mathieu Bourguignon, Nicola Molinaro",http://arxiv.org/pdf/2505.15355v1,cs.CL
Revealing Language Model Trajectories via Kullback-Leibler Divergence,"A recently proposed method enables efficient estimation of the KL divergence
between language models, including models with different architectures, by
assigning coordinates based on log-likelihood vectors. To better understand the
behavior of this metric, we systematically evaluate KL divergence across a wide
range of conditions using publicly available language models. Our analysis
covers comparisons between pretraining checkpoints, fine-tuned and base models,
and layers via the logit lens. We find that trajectories of language models, as
measured by KL divergence, exhibit a spiral structure during pretraining and
thread-like progressions across layers. Furthermore, we show that, in terms of
diffusion exponents, model trajectories in the log-likelihood space are more
constrained than those in weight space.",2025-05-21,"Ryo Kishino, Yusuke Takase, Momose Oyama, Hiroaki Yamagiwa, Hidetoshi Shimodaira",http://arxiv.org/pdf/2505.15353v1,cs.CL
RRTL: Red Teaming Reasoning Large Language Models in Tool Learning,"While tool learning significantly enhances the capabilities of large language
models (LLMs), it also introduces substantial security risks. Prior research
has revealed various vulnerabilities in traditional LLMs during tool learning.
However, the safety of newly emerging reasoning LLMs (RLLMs), such as
DeepSeek-R1, in the context of tool learning remains underexplored. To bridge
this gap, we propose RRTL, a red teaming approach specifically designed to
evaluate RLLMs in tool learning. It integrates two novel strategies: (1) the
identification of deceptive threats, which evaluates the model's behavior in
concealing the usage of unsafe tools and their potential risks; and (2) the use
of Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also
includes a benchmark for traditional LLMs. We conduct a comprehensive
evaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs
generally achieve stronger safety performance than traditional LLMs, yet
substantial safety disparities persist across models; (2) RLLMs can pose
serious deceptive risks by frequently failing to disclose tool usage and to
warn users of potential tool output risks; (3) CoT prompting reveals
multi-lingual safety vulnerabilities in RLLMs. Our work provides important
insights into enhancing the security of RLLMs in tool learning.",2025-05-21,"Yifei Liu, Yu Cui, Haibin Zhang",http://arxiv.org/pdf/2505.17106v1,cs.CL
The Super Emotion Dataset,"Despite the wide-scale usage and development of emotion classification
datasets in NLP, the field lacks a standardized, large-scale resource that
follows a psychologically grounded taxonomy. Existing datasets either use
inconsistent emotion categories, suffer from limited sample size, or focus on
specific domains. The Super Emotion Dataset addresses this gap by harmonizing
diverse text sources into a unified framework based on Shaver's empirically
validated emotion taxonomy, enabling more consistent cross-domain emotion
recognition research.",2025-05-21,Enric Junqué de Fortuny,http://arxiv.org/pdf/2505.15348v1,cs.CL
FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management,"Large Language Models (LLMs) are increasingly deployed in multi-turn
conversational applications, where the management of the Key-Value (KV) Cache
presents a significant bottleneck. The linear growth of the KV Cache with
dialogue history imposes substantial computational costs, and existing eviction
strategies often degrade performance by repeatedly compressing early
conversational context, leading to information loss and context forgetting.
This paper introduces FlowKV, a novel \textbf{multi-turn isolation mechanism}
for KV Cache management, which can be applied to any KV Cache compression
method without training. FlowKV's core innovation is a multi-turn isolation
mechanism that preserves the accumulated compressed KV cache from past turns.
Compression is then strategically applied only to the newly generated KV pairs
of the latest completed turn, effectively preventing the re-compression of
older context and thereby mitigating catastrophic forgetting. Our results
demonstrate that FlowKV consistently and significantly outperforms baseline
strategies in maintaining instruction-following accuracy and user preference
retention from 10.90\% to 75.40\%, particularly in later conversational turns.",2025-05-21,"Xiang Liu, Hong Chen, Xuming Hu, Xiaowen Chu",http://arxiv.org/pdf/2505.15347v1,cs.CL
Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors,"The misuse of large language models (LLMs), such as academic plagiarism, has
driven the development of detectors to identify LLM-generated texts. To bypass
these detectors, paraphrase attacks have emerged to purposely rewrite these
texts to evade detection. Despite the success, existing methods require
substantial data and computational budgets to train a specialized paraphraser,
and their attack efficacy greatly reduces when faced with advanced detection
algorithms. To address this, we propose \textbf{Co}ntrastive
\textbf{P}araphrase \textbf{A}ttack (CoPA), a training-free method that
effectively deceives text detectors using off-the-shelf LLMs. The first step is
to carefully craft instructions that encourage LLMs to produce more human-like
texts. Nonetheless, we observe that the inherent statistical biases of LLMs can
still result in some generated texts carrying certain machine-like attributes
that can be captured by detectors. To overcome this, CoPA constructs an
auxiliary machine-like word distribution as a contrast to the human-like
distribution generated by the LLM. By subtracting the machine-like patterns
from the human-like distribution during the decoding process, CoPA is able to
produce sentences that are less discernible by text detectors. Our theoretical
analysis suggests the superiority of the proposed attack. Extensive experiments
validate the effectiveness of CoPA in fooling text detectors across various
scenarios.",2025-05-21,"Hao Fang, Jiawei Kong, Tianqu Zhuang, Yixiang Qiu, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, Min Zhang",http://arxiv.org/pdf/2505.15337v2,cs.CL
Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation,"The success of building textless speech-to-speech translation (S2ST) models
has attracted much attention. However, S2ST still faces two main challenges: 1)
extracting linguistic features for various speech signals, called cross-modal
(CM), and 2) learning alignment of difference languages in long sequences,
called cross-lingual (CL). We propose the unit language to overcome the two
modeling challenges. The unit language can be considered a text-like
representation format, constructed using $n$-gram language modeling. We
implement multi-task learning to utilize the unit language in guiding the
speech modeling process. Our initial results reveal a conflict when applying
source and target unit languages simultaneously. We propose task prompt
modeling to mitigate this conflict. We conduct experiments on four languages of
the Voxpupil dataset. Our method demonstrates significant improvements over a
strong baseline and achieves performance comparable to models trained with
text.",2025-05-21,"Yuhao Zhang, Xiangnan Ma, Kaiqi Kou, Peizhuo Liu, Weiqiao Shan, Benyou Wang, Tong Xiao, Yuxin Huang, Zhengtao Yu, Jingbo Zhu",http://arxiv.org/pdf/2505.15333v1,cs.CL
Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack,"Large Language Models (LLMs) are increasingly evaluated on multiple-choice
question answering (MCQA) tasks using *first-token probability* (FTP), which
selects the answer option whose initial token has the highest likelihood. While
efficient, FTP can be fragile: models may assign high probability to unrelated
tokens (*misalignment*) or use a valid token merely as part of a generic
preamble rather than as a clear answer choice (*misinterpretation*),
undermining the reliability of symbolic evaluation. We propose a simple
solution: the *prefilling attack*, a structured natural-language prefix (e.g.,
""*The correct option is:*"") prepended to the model output. Originally explored
in AI safety, we repurpose prefilling to steer the model to respond with a
clean, valid option, without modifying its parameters. Empirically, the FTP
with prefilling strategy substantially improves accuracy, calibration, and
output consistency across a broad set of LLMs and MCQA benchmarks. It
outperforms standard FTP and often matches the performance of open-ended
generation approaches that require full decoding and external classifiers,
while being significantly more efficient. Our findings suggest that prefilling
is a simple, robust, and low-cost method to enhance the reliability of
FTP-based evaluation in multiple-choice settings.",2025-05-21,"Silvia Cappelletti, Tobia Poppi, Samuele Poppi, Zheng-Xin Yong, Diego Garcia-Olano, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara",http://arxiv.org/pdf/2505.15323v1,cs.CL
Emotional Supporters often Use Multiple Strategies in a Single Turn,"Emotional Support Conversations (ESC) are crucial for providing empathy,
validation, and actionable guidance to individuals in distress. However,
existing definitions of the ESC task oversimplify the structure of supportive
responses, typically modelling them as single strategy-utterance pairs. Through
a detailed corpus analysis of the ESConv dataset, we identify a common yet
previously overlooked phenomenon: emotional supporters often employ multiple
strategies consecutively within a single turn. We formally redefine the ESC
task to account for this, proposing a revised formulation that requires
generating the full sequence of strategy-utterance pairs given a dialogue
history. To facilitate this refined task, we introduce several modelling
approaches, including supervised deep learning models and large language
models. Our experiments show that, under this redefined task, state-of-the-art
LLMs outperform both supervised models and human supporters. Notably, contrary
to some earlier findings, we observe that LLMs frequently ask questions and
provide suggestions, demonstrating more holistic support capabilities.",2025-05-21,"Xin Bai, Guanyi Chen, Tingting He, Chenlian Zhou, Yu Liu",http://arxiv.org/pdf/2505.15316v1,cs.CL
Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning,"Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.",2025-05-21,"Yurun Yuan, Fan Chen, Zeyu Jia, Alexander Rakhlin, Tengyang Xie",http://arxiv.org/pdf/2505.15311v1,cs.CL
Multi-Hop Question Generation via Dual-Perspective Keyword Guidance,"Multi-hop question generation (MQG) aims to generate questions that require
synthesizing multiple information snippets from documents to derive target
answers. The primary challenge lies in effectively pinpointing crucial
information snippets related to question-answer (QA) pairs, typically relying
on keywords. However, existing works fail to fully utilize the guiding
potential of keywords and neglect to differentiate the distinct roles of
question-specific and document-specific keywords. To address this, we define
dual-perspective keywords (i.e., question and document keywords) and propose a
Dual-Perspective Keyword-Guided (DPKG) framework, which seamlessly integrates
keywords into the multi-hop question generation process. We argue that question
keywords capture the questioner's intent, whereas document keywords reflect the
content related to the QA pair. Functionally, question and document keywords
work together to pinpoint essential information snippets in the document, with
question keywords required to appear in the generated question. The DPKG
framework consists of an expanded transformer encoder and two answer-aware
transformer decoders for keyword and question generation, respectively.
Extensive experiments demonstrate the effectiveness of our work, showcasing its
promising performance and underscoring its significant value in the MQG task.",2025-05-21,"Maodong Li, Longyin Zhang, Fang Kong",http://arxiv.org/pdf/2505.15299v1,cs.CL
AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving,"Vision-Language Models (VLMs) show promise for autonomous driving, yet their
struggle with hallucinations, inefficient reasoning, and limited real-world
validation hinders accurate perception and robust step-by-step reasoning. To
overcome this, we introduce \textbf{AgentThink}, a pioneering unified framework
that, for the first time, integrates Chain-of-Thought (CoT) reasoning with
dynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's
core innovations include: \textbf{(i) Structured Data Generation}, by
establishing an autonomous driving tool library to automatically construct
structured, self-verified reasoning data explicitly incorporating tool usage
for diverse driving scenarios; \textbf{(ii) A Two-stage Training Pipeline},
employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization
(GRPO) to equip VLMs with the capability for autonomous tool invocation; and
\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel
multi-tool assessment protocol to rigorously evaluate the model's tool
invocation and utilization. Experiments on the DriveLMM-o1 benchmark
demonstrate AgentThink significantly boosts overall reasoning scores by
\textbf{53.91\%} and enhances answer accuracy by \textbf{33.54\%}, while
markedly improving reasoning quality and consistency. Furthermore, ablation
studies and robust zero-shot/few-shot generalization experiments across various
benchmarks underscore its powerful capabilities. These findings highlight a
promising trajectory for developing trustworthy and tool-aware autonomous
driving models.",2025-05-21,"Kangan Qian, Sicong Jiang, Yang Zhong, Ziang Luo, Zilin Huang, Tianze Zhu, Kun Jiang, Mengmeng Yang, Zheng Fu, Jinyu Miao, Yining Shi, He Zhe Lim, Li Liu, Tianbao Zhou, Hongyi Wang, Huang Yu, Yifei Hu, Guang Li, Guang Chen, Hao Ye, Lijun Sun, Diange Yang",http://arxiv.org/pdf/2505.15298v2,cs.CL
Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites,"Detoxifying offensive language while preserving the speaker's original intent
is a challenging yet critical goal for improving the quality of online
interactions. Although large language models (LLMs) show promise in rewriting
toxic content, they often default to overly polite rewrites, distorting the
emotional tone and communicative intent. This problem is especially acute in
Chinese, where toxicity often arises implicitly through emojis, homophones, or
discourse context. We present ToxiRewriteCN, the first Chinese detoxification
dataset explicitly designed to preserve sentiment polarity. The dataset
comprises 1,556 carefully annotated triplets, each containing a toxic sentence,
a sentiment-aligned non-toxic rewrite, and labeled toxic spans. It covers five
real-world scenarios: standard expressions, emoji-induced and homophonic
toxicity, as well as single-turn and multi-turn dialogues. We evaluate 17 LLMs,
including commercial and open-source models with variant architectures, across
four dimensions: detoxification accuracy, fluency, content preservation, and
sentiment polarity. Results show that while commercial and MoE models perform
best overall, all models struggle to balance safety with emotional fidelity in
more subtle or context-heavy settings such as emoji, homophone, and
dialogue-based inputs. We release ToxiRewriteCN to support future research on
controllable, sentiment-aware detoxification for Chinese.",2025-05-21,"Xintong Wang, Yixiao Liu, Jingheng Pan, Liang Ding, Longyue Wang, Chris Biemann",http://arxiv.org/pdf/2505.15297v1,cs.CL
Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization,"Large Language Models (LLMs) have significantly advanced text generation
capabilities, including tasks like summarization, often producing coherent and
fluent outputs. However, faithfulness to source material remains a significant
challenge due to the generation of hallucinations. While extensive research
focuses on detecting and reducing these inaccuracies, less attention has been
paid to the positional distribution of hallucination within generated text,
particularly in long outputs. In this work, we investigate where hallucinations
occur in LLM-based long response generation, using long document summarization
as a key case study. Focusing on the challenging setting of long context-aware
long response generation, we find a consistent and concerning phenomenon:
hallucinations tend to concentrate disproportionately in the latter parts of
the generated long response. To understand this bias, we explore potential
contributing factors related to the dynamics of attention and decoding over
long sequences. Furthermore, we investigate methods to mitigate this positional
hallucination, aiming to improve faithfulness specifically in the concluding
segments of long outputs.",2025-05-21,"Joonho Yang, Seunghyun Yoon, Hwan Chang, Byeongjeong Kim, Hwanhee Lee",http://arxiv.org/pdf/2505.15291v1,cs.CL
P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark,"Academic posters are vital for scholarly communication, yet their manual
creation is time-consuming. However, automated academic poster generation faces
significant challenges in preserving intricate scientific details and achieving
effective visual-textual integration. Existing approaches often struggle with
semantic richness and structural nuances, and lack standardized benchmarks for
evaluating generated academic posters comprehensively. To address these
limitations, we introduce P2P, the first flexible, LLM-based multi-agent
framework that generates high-quality, HTML-rendered academic posters directly
from research papers, demonstrating strong potential for practical
applications. P2P employs three specialized agents-for visual element
processing, content generation, and final poster assembly-each integrated with
dedicated checker modules to enable iterative refinement and ensure output
quality. To foster advancements and rigorous evaluation in this domain, we
construct and release P2PInstruct, the first large-scale instruction dataset
comprising over 30,000 high-quality examples tailored for the academic
paper-to-poster generation task. Furthermore, we establish P2PEval, a
comprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation
methodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and
detailed, human-annotated checklists. Our contributions aim to streamline
research dissemination and provide the community with robust tools for
developing and evaluating next-generation poster generation systems.",2025-05-21,"Tao Sun, Enhao Pan, Zhengkai Yang, Kaixin Sui, Jiajun Shi, Xianfu Cheng, Tongliang Li, Wenhao Huang, Ge Zhang, Jian Yang, Zhoujun Li",http://arxiv.org/pdf/2505.17104v1,cs.CL
Exploring In-Image Machine Translation with Real-World Background,"In-Image Machine Translation (IIMT) aims to translate texts within images
from one language to another. Previous research on IIMT was primarily conducted
on simplified scenarios such as images of one-line text with black font in
white backgrounds, which is far from reality and impractical for applications
in the real world. To make IIMT research practically valuable, it is essential
to consider a complex scenario where the text backgrounds are derived from
real-world images. To facilitate research of complex scenario IIMT, we design
an IIMT dataset that includes subtitle text with real-world background. However
previous IIMT models perform inadequately in complex scenarios. To address the
issue, we propose the DebackX model, which separates the background and
text-image from the source image, performs translation on text-image directly,
and fuses the translated text-image with the background, to generate the target
image. Experimental results show that our model achieves improvements in both
translation quality and visual effect.",2025-05-21,"Yanzhi Tian, Zeming Liu, Zhengyang Liu, Yuhang Guo",http://arxiv.org/pdf/2505.15282v1,cs.CL
Web-Shepherd: Advancing PRMs for Reinforcing Web Agents,"Web navigation is a unique domain that can automate many repetitive real-life
tasks and is challenging as it requires long-horizon sequential decision making
beyond typical multimodal large language model (MLLM) tasks. Yet, specialized
reward models for web navigation that can be utilized during both training and
test-time have been absent until now. Despite the importance of speed and
cost-effectiveness, prior works have utilized MLLMs as reward models, which
poses significant constraints for real-world deployment. To address this, in
this work, we propose the first process reward model (PRM) called Web-Shepherd
which could assess web navigation trajectories in a step-level. To achieve
this, we first construct the WebPRM Collection, a large-scale dataset with 40K
step-level preference pairs and annotated checklists spanning diverse domains
and difficulty levels. Next, we also introduce the WebRewardBench, the first
meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe
that our Web-Shepherd achieves about 30 points better accuracy compared to
using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by
using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve
10.9 points better performance, in 10 less cost compared to using GPT-4o-mini
as the verifier. Our model, dataset, and code are publicly available at LINK.",2025-05-21,"Hyungjoo Chae, Sunghwan Kim, Junhee Cho, Seungone Kim, Seungjun Moon, Gyeom Hwangbo, Dongha Lim, Minjin Kim, Yeonjun Hwang, Minju Gwak, Dongwook Choi, Minseok Kang, Gwanhoon Im, ByeongUng Cho, Hyojun Kim, Jun Hee Han, Taeyoon Kwon, Minju Kim, Beong-woo Kwak, Dongjin Kang, Jinyoung Yeo",http://arxiv.org/pdf/2505.15277v1,cs.CL
When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning,"Large reasoning models (LRMs) have significantly advanced performance on
complex tasks, yet their tendency to overthink introduces inefficiencies. This
study investigates the internal mechanisms of reinforcement learning
(RL)-trained LRMs when prompted to save thinking, revealing three distinct
thinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking
(IT). Through comprehensive analysis of confidence in thinking termination,
attention from thinking to generation, and attentional focus on input sections,
we uncover key factors influencing the reasoning behaviors. We further find
that NT reduces output length at the cost of accuracy, while ET and IT maintain
accuracy with reduced response length. Our findings expose fundamental
inconsistencies in RL-optimized LRMs, necessitating adaptive improvements for
reliable efficiency.",2025-05-21,"Rongzhi Zhu, Yi Liu, Zequn Sun, Yiwei Wang, Wei Hu",http://arxiv.org/pdf/2505.15276v1,cs.CL
Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation,"SDForger is a flexible and efficient framework for generating high-quality
multivariate time series using LLMs. Leveraging a compact data representation,
SDForger provides synthetic time series generation from a few samples and
low-computation fine-tuning of any autoregressive LLM. Specifically, the
framework transforms univariate and multivariate signals into tabular
embeddings, which are then encoded into text and used to fine-tune the LLM. At
inference, new textual embeddings are sampled and decoded into synthetic time
series that retain the original data's statistical properties and temporal
dynamics. Across a diverse range of datasets, SDForger outperforms existing
generative models in many scenarios, both in similarity-based evaluations and
downstream forecasting tasks. By enabling textual conditioning in the
generation process, SDForger paves the way for multimodal modeling and the
streamlined integration of time series with textual information. SDForger
source code will be open-sourced soon.",2025-05-21,"Cécile Rousseau, Tobia Boschi, Giandomenico Cornacchia, Dhaval Salwala, Alessandra Pascale, Juan Bernabe Moreno",http://arxiv.org/pdf/2505.17103v1,cs.CL
AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection,"Existing AI-generated text detection methods heavily depend on large
annotated datasets and external threshold tuning, restricting interpretability,
adaptability, and zero-shot effectiveness. To address these limitations, we
propose AGENT-X, a zero-shot multi-agent framework informed by classical
rhetoric and systemic functional linguistics. Specifically, we organize
detection guidelines into semantic, stylistic, and structural dimensions, each
independently evaluated by specialized linguistic agents that provide explicit
reasoning and robust calibrated confidence via semantic steering. A meta agent
integrates these assessments through confidence-aware aggregation, enabling
threshold-free, interpretable classification. Additionally, an adaptive
Mixture-of-Agent router dynamically selects guidelines based on inferred
textual characteristics. Experiments on diverse datasets demonstrate that
AGENT-X substantially surpasses state-of-the-art supervised and zero-shot
approaches in accuracy, interpretability, and generalization.",2025-05-21,"Jiatao Li, Mao Ye, Cheng Peng, Xunjian Yin, Xiaojun Wan",http://arxiv.org/pdf/2505.15261v1,cs.CL
ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search,"Recent advances in Multimodal Large Language Models (MLLMs) have enabled
autonomous agents to interact with computers via Graphical User Interfaces
(GUIs), where accurately localizing the coordinates of interface elements
(e.g., buttons) is often required for fine-grained actions. However, this
remains significantly challenging, leading prior works to rely on large-scale
web datasets to improve the grounding accuracy. In this work, we propose
Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a
novel and effective framework for web grounding that enables MLLMs to learn
data efficiently through self-generated reasoning and spatial-aware criticism.
More specifically, ReGUIDE learns to (i) self-generate a language reasoning
process for the localization via online reinforcement learning, and (ii)
criticize the prediction using spatial priors that enforce equivariance under
input transformations. At inference time, ReGUIDE further boosts performance
through a test-time scaling strategy, which combines spatial search with
coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly
advances web grounding performance across multiple benchmarks, outperforming
baselines with substantially fewer training data points (e.g., only 0.2%
samples compared to the best open-sourced baselines).",2025-05-21,"Hyunseok Lee, Jeonghoon Kim, Beomjun Kim, Jihoon Tack, Chansong Jo, Jaehong Lee, Cheonbok Park, Sookyo In, Jinwoo Shin, Kang Min Yoo",http://arxiv.org/pdf/2505.15259v2,cs.CL
When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners,"Multilingual reasoning remains a significant challenge for large language
models (LLMs), with performance disproportionately favoring high-resource
languages. Drawing inspiration from cognitive neuroscience, which suggests that
human reasoning functions largely independently of language processing, we
hypothesize that LLMs similarly encode reasoning and language as separable
components that can be disentangled to enhance multilingual reasoning. To
evaluate this, we perform a causal intervention by ablating language-specific
representations at inference time. Experiments on 10 open-source LLMs spanning
11 typologically diverse languages show that this language-specific ablation
consistently boosts multilingual reasoning performance. Layer-wise analyses
further confirm that language and reasoning representations can be effectively
decoupled throughout the model, yielding improved multilingual reasoning
capabilities, while preserving top-layer language features remains essential
for maintaining linguistic fidelity. Compared to post-training such as
supervised fine-tuning or reinforcement learning, our training-free ablation
achieves comparable or superior results with minimal computational overhead.
These findings shed light on the internal mechanisms underlying multilingual
reasoning in LLMs and suggest a lightweight and interpretable strategy for
improving cross-lingual generalization.",2025-05-21,"Weixiang Zhao, Jiahe Guo, Yang Deng, Tongtong Wu, Wenxuan Zhang, Yulin Hu, Xingyu Sui, Yanyan Zhao, Wanxiang Che, Bing Qin, Tat-Seng Chua, Ting Liu",http://arxiv.org/pdf/2505.15257v1,cs.CL
MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation,"Mental manipulation is a subtle yet pervasive form of psychological abuse
that poses serious threats to mental health. Its covert nature and the
complexity of manipulation strategies make it challenging to detect, even for
state-of-the-art large language models (LLMs). This concealment also hinders
the manual collection of large-scale, high-quality annotations essential for
training effective models. Although recent efforts have sought to improve LLMs'
performance on this task, progress remains limited due to the scarcity of
real-world annotated datasets. To address these challenges, we propose
MentalMAC, a multi-task anti-curriculum distillation method that enhances LLMs'
ability to detect mental manipulation in multi-turn dialogue. Our approach
includes: (i) EvoSA, an unsupervised data expansion method based on
evolutionary operations and speech act theory; (ii) teacher model-generated
multi-task supervision; and (iii) progressive knowledge distillation from
complex to simpler tasks. We then constructed the ReaMent dataset with 5,000
real-world dialogue samples, using a MentalMAC-distilled model to assist human
annotation. Vast experiments demonstrate that our method significantly narrows
the gap between student and teacher models and outperforms competitive LLMs
across key evaluation metrics. All code, datasets, and checkpoints will be
released upon paper acceptance. Warning: This paper contains content that may
be offensive to readers.",2025-05-21,"Yuansheng Gao, Han Bao, Tong Zhang, Bin Li, Zonghui Wang, Wenzhi Chen",http://arxiv.org/pdf/2505.15255v2,cs.CL
Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation,"Recently, large vision-language models (LVLMs) have emerged as the preferred
tools for judging text-image alignment, yet their robustness along the visual
modality remains underexplored. This work is the first study to address a key
research question: Can adversarial visual manipulations systematically fool
LVLM judges into assigning unfairly inflated scores? We define potential image
induced biases within the context of T2I evaluation and examine how these
biases affect the evaluations of LVLM judges. Moreover, we introduce a novel,
fine-grained, multi-domain meta-evaluation benchmark named FRAME, which is
deliberately constructed to exhibit diverse score distributions. By introducing
the defined biases into the benchmark, we reveal that all tested LVLM judges
exhibit vulnerability across all domains, consistently inflating scores for
manipulated images. Further analysis reveals that combining multiple biases
amplifies their effects, and pairwise evaluations are similarly susceptible.
Moreover, we observe that visual biases persist under prompt-based mitigation
strategies, highlighting the vulnerability of current LVLM evaluation systems
and underscoring the urgent need for more robust LVLM judges.",2025-05-21,"Yerin Hwang, Dongryeol Lee, Kyungmin Min, Taegwan Kang, Yong-il Kim, Kyomin Jung",http://arxiv.org/pdf/2505.15249v1,cs.CL
Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework,"While large language models (LLMs) show great potential in temporal
reasoning, most existing work focuses heavily on enhancing performance, often
neglecting the explainable reasoning processes underlying the results. To
address this gap, we introduce a comprehensive benchmark covering a wide range
of temporal granularities, designed to systematically evaluate LLMs'
capabilities in explainable temporal reasoning. Furthermore, our findings
reveal that LLMs struggle to deliver convincing explanations when relying
solely on textual information. To address challenge, we propose GETER, a novel
structure-aware generative framework that integrates Graph structures with text
for Explainable TEmporal Reasoning. Specifically, we first leverage temporal
knowledge graphs to develop a temporal encoder that captures structural
information for the query. Subsequently, we introduce a structure-text prefix
adapter to map graph structure features into the text embedding space. Finally,
LLMs generate explanation text by seamlessly integrating the soft graph token
with instruction-tuning prompt tokens. Experimental results indicate that GETER
achieves state-of-the-art performance while also demonstrating its
effectiveness as well as strong generalization capabilities. Our dataset and
code are available at https://github.com/carryTatum/GETER.",2025-05-21,"Zihao Jiang, Ben Liu, Miao Peng, Wenjie Xu, Yao Xiao, Zhenyan Shan, Min Peng",http://arxiv.org/pdf/2505.15245v1,cs.CL
Multilingual Prompting for Improving LLM Generation Diversity,"Large Language Models (LLMs) are known to lack cultural representation and
overall diversity in their generations, from expressing opinions to answering
factual questions. To mitigate this problem, we propose multilingual prompting:
a prompting method which generates several variations of a base prompt with
added cultural and linguistic cues from several cultures, generates responses,
and then combines the results. Building on evidence that LLMs have
language-specific knowledge, multilingual prompting seeks to increase diversity
by activating a broader range of cultural knowledge embedded in model training
data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA
70B, and LLaMA 8B), we show that multilingual prompting consistently
outperforms existing diversity-enhancing techniques such as high-temperature
sampling, step-by-step recall, and personas prompting. Further analyses show
that the benefits of multilingual prompting vary with language resource level
and model size, and that aligning the prompting language with the cultural cues
reduces hallucination about culturally-specific information.",2025-05-21,"Qihan Wang, Shidong Pan, Tal Linzen, Emily Black",http://arxiv.org/pdf/2505.15229v1,cs.CL
BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems,"AI agents have the potential to significantly alter the cybersecurity
landscape. To help us understand this change, we introduce the first framework
to capture offensive and defensive cyber-capabilities in evolving real-world
systems. Instantiating this framework with BountyBench, we set up 25 systems
with complex, real-world codebases. To capture the vulnerability lifecycle, we
define three task types: Detect (detecting a new vulnerability), Exploit
(exploiting a specific vulnerability), and Patch (patching a specific
vulnerability). For Detect, we construct a new success indicator, which is
general across vulnerability types and provides localized evaluation. We
manually set up the environment for each system, including installing packages,
setting up server(s), and hydrating database(s). We add 40 bug bounties, which
are vulnerabilities with monetary awards from \$10 to \$30,485, and cover 9 of
the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy
based on information to guide detection, interpolating from identifying a zero
day to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code,
OpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and
Claude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing
agents are Claude Code (5% on Detect, mapping to \$1,350), Custom Agent with
Claude 3.7 Sonnet Thinking (5% on Detect, mapping to \$1,025; 67.5% on
Exploit), and OpenAI Codex CLI (5% on Detect, mapping to \$2,400; 90% on Patch,
mapping to \$14,422). OpenAI Codex CLI and Claude Code are more capable at
defense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit
scores of 32.5% and 57.5% respectively; in contrast, the custom agents are
relatively balanced between offense and defense, achieving Exploit scores of
40-67.5% and Patch scores of 45-60%.",2025-05-21,"Andy K. Zhang, Joey Ji, Celeste Menders, Riya Dulepet, Thomas Qin, Ron Y. Wang, Junrong Wu, Kyleen Liao, Jiliang Li, Jinghan Hu, Sara Hong, Nardos Demilew, Shivatmica Murgai, Jason Tran, Nishka Kacheria, Ethan Ho, Denis Liu, Lauren McLane, Olivia Bruvik, Dai-Rong Han, Seungwoo Kim, Akhil Vyas, Cuiyuanxiu Chen, Ryan Li, Weiran Xu, Jonathan Z. Ye, Prerit Choudhary, Siddharth M. Bhatia, Vikram Sivashankar, Yuxuan Bao, Dawn Song, Dan Boneh, Daniel E. Ho, Percy Liang",http://arxiv.org/pdf/2505.15216v1,cs.CL
R-TOFU: Unlearning in Large Reasoning Models,"Large Reasoning Models (LRMs) embed private or copyrighted information not
only in their final answers but also throughout multi-step chain-of-thought
(CoT) traces, making reliable unlearning far more demanding than in standard
LLMs. We introduce Reasoning-TOFU (R-TOFU), the first benchmark tailored to
this setting. R-TOFU augments existing unlearning tasks with realistic CoT
annotations and provides step-wise metrics that expose residual knowledge
invisible to answer-level checks. Using R-TOFU, we carry out a comprehensive
comparison of gradient-based and preference-optimization baselines and show
that conventional answer-only objectives leave substantial forget traces in
reasoning. We further propose Reasoned IDK, a preference-optimization variant
that preserves coherent yet inconclusive reasoning, achieving a stronger
balance between forgetting efficacy and model utility than earlier refusal
styles. Finally, we identify a failure mode: decoding variants such as
ZeroThink and LessThink can still reveal forgotten content despite seemingly
successful unlearning, emphasizing the need to evaluate models under diverse
decoding settings. Together, the benchmark, analysis, and new baseline
establish a systematic foundation for studying and improving unlearning in LRMs
while preserving their reasoning capabilities.",2025-05-21,"Sangyeon Yoon, Wonje Jeung, Albert No",http://arxiv.org/pdf/2505.15214v1,cs.CL
BanglaByT5: Byte-Level Modelling for Bangla,"Large language models (LLMs) have achieved remarkable success across various
natural language processing tasks. However, most LLM models use traditional
tokenizers like BPE and SentencePiece, which fail to capture the finer nuances
of a morphologically rich language like Bangla (Bengali). In this work, we
introduce BanglaByT5, the first byte-level encoder-decoder model explicitly
tailored for Bangla. Built upon a small variant of Googles ByT5 architecture,
BanglaByT5 is pre-trained on a 14GB curated corpus combining high-quality
literary and newspaper articles. Through zeroshot and supervised evaluations
across generative and classification tasks, BanglaByT5 demonstrates competitive
performance, surpassing several multilingual and larger models. Our findings
highlight the efficacy of byte-level modelling for morphologically rich
languages and highlight BanglaByT5 potential as a lightweight yet powerful tool
for Bangla NLP, particularly in both resource-constrained and scalable
environments.",2025-05-21,"Pramit Bhattacharyya, Arnab Bhattacharya",http://arxiv.org/pdf/2505.17102v1,cs.CL
An approach to identify the most semantically informative deep representations of text and images,"Deep neural networks are known to develop similar representations for
semantically related data, even when they belong to different domains, such as
an image and its description, or the same text in different languages. We
present a method for quantitatively investigating this phenomenon by measuring
the relative information content of the representations of semantically related
data and probing how it is encoded into multiple tokens of large language
models (LLMs) and vision transformers. Looking first at how LLMs process pairs
of translated sentences, we identify inner ``semantic'' layers containing the
most language-transferable information. We find moreover that, on these layers,
a larger LLM (DeepSeek-V3) extracts significantly more general information than
a smaller one (Llama3.1-8B). Semantic information is spread across many tokens
and it is characterized by long-distance correlations between tokens and by a
causal left-to-right (i.e., past-future) asymmetry. We also identify layers
encoding semantic information within visual transformers. We show that caption
representations in the semantic layers of LLMs predict visual representations
of the corresponding images. We observe significant and model-dependent
information asymmetries between image and text representations.",2025-05-21,"Santiago Acevedo, Andrea Mascaretti, Riccardo Rende, Matéo Mahaut, Marco Baroni, Alessandro Laio",http://arxiv.org/pdf/2505.17101v1,cs.CL
Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs,"Knowledge graph-based retrieval-augmented generation seeks to mitigate
hallucinations in Large Language Models (LLMs) caused by insufficient or
outdated knowledge. However, existing methods often fail to fully exploit the
prior knowledge embedded in knowledge graphs (KGs), particularly their
structural information and explicit or implicit constraints. The former can
enhance the faithfulness of LLMs' reasoning, while the latter can improve the
reliability of response generation. Motivated by these, we propose a
trustworthy reasoning framework, termed Deliberation over Priors (DP), which
sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a
progressive knowledge distillation strategy that integrates structural priors
into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky
optimization, thereby improving the faithfulness of relation path generation.
Furthermore, our framework employs a reasoning-introspection strategy, which
guides LLMs to perform refined reasoning verification based on extracted
constraint priors, ensuring the reliability of response generation. Extensive
experiments on three benchmark datasets demonstrate that DP achieves new
state-of-the-art performance, especially a Hit@1 improvement of 13% on the
ComplexWebQuestions dataset, and generates highly trustworthy responses. We
also conduct various analyses to verify its flexibility and practicality. The
code is available at https://github.com/reml-group/Deliberation-on-Priors.",2025-05-21,"Jie Ma, Ning Qu, Zhitao Gao, Rui Xing, Jun Liu, Hongbin Pei, Jiang Xie, Linyun Song, Pinghui Wang, Jing Tao, Zhou Su",http://arxiv.org/pdf/2505.15210v1,cs.CL
DUSK: Do Not Unlearn Shared Knowledge,"Large language models (LLMs) are increasingly deployed in real-world
applications, raising concerns about the unauthorized use of copyrighted or
sensitive data. Machine unlearning aims to remove such 'forget' data while
preserving utility and information from the 'retain' set. However, existing
evaluations typically assume that forget and retain sets are fully disjoint,
overlooking realistic scenarios where they share overlapping content. For
instance, a news article may need to be unlearned, even though the same event,
such as an earthquake in Japan, is also described factually on Wikipedia.
Effective unlearning should remove the specific phrasing of the news article
while preserving publicly supported facts. In this paper, we introduce DUSK, a
benchmark designed to evaluate unlearning methods under realistic data overlap.
DUSK constructs document sets that describe the same factual content in
different styles, with some shared information appearing across all sets and
other content remaining unique to each. When one set is designated for
unlearning, an ideal method should remove its unique content while preserving
shared facts. We define seven evaluation metrics to assess whether unlearning
methods can achieve this selective removal. Our evaluation of nine recent
unlearning methods reveals a key limitation: while most can remove
surface-level text, they often fail to erase deeper, context-specific knowledge
without damaging shared content. We release DUSK as a public benchmark to
support the development of more precise and reliable unlearning techniques for
real-world applications.",2025-05-21,"Wonje Jeung, Sangyeon Yoon, Hyesoo Hong, Soeun Kim, Seungju Han, Youngjae Yu, Albert No",http://arxiv.org/pdf/2505.15209v1,cs.CL
Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems,"Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts
for each problem and reward them independently. This optimizes for pass@1
performance and prioritizes the strength of isolated samples at the expense of
the diversity and collective utility of sets of samples. This under-utilizes
the sampling capacity, limiting exploration and eventual improvement on harder
examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a
transformation on the final rewards which leads to direct optimization of
pass@k performance, thus optimizing for sets of samples that maximize reward
when considered jointly. Our contribution is to derive novel low variance
unbiased estimators for pass@k and its gradient, in both the binary and
continuous reward settings. We show optimization with our estimators reduces to
standard RL with rewards that have been jointly transformed by a stable and
efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable
robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of
trading off pass@1 performance for pass@k gains, our method allows annealing k
during training, optimizing both metrics and often achieving strong pass@1
numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the
variance reducing properties of our formulations. We also include real-world
examples using the open-source LLM, GEMMA-2. We find that our transformation
effectively optimizes for the target k. Furthermore, higher k values enable
solving more and harder problems, while annealing k boosts both the pass@1 and
pass@k . Crucially, for challenging task sets where conventional pass@1
optimization stalls, our pass@k approach unblocks learning, likely due to
better exploration by prioritizing joint utility over the utility of individual
samples.",2025-05-21,"Christian Walder, Deep Karkhanis",http://arxiv.org/pdf/2505.15201v1,cs.CL
Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector,"LLM-as-a-Judge has emerged as a promising tool for automatically evaluating
generated outputs, but its reliability is often undermined by potential biases
in judgment. Existing efforts to mitigate these biases face key limitations:
in-context learning-based methods fail to address rooted biases due to the
evaluator's limited capacity for self-reflection, whereas fine-tuning is not
applicable to all evaluator types, especially closed-source models. To address
this challenge, we introduce the Reasoning-based Bias Detector (RBD), which is
a plug-in module that identifies biased evaluations and generates structured
reasoning to guide evaluator self-correction. Rather than modifying the
evaluator itself, RBD operates externally and engages in an iterative process
of bias detection and feedback-driven revision. To support its development, we
design a complete pipeline consisting of biased dataset construction,
supervision collection, distilled reasoning-based fine-tuning of RBD, and
integration with LLM evaluators. We fine-tune four sizes of RBD models, ranging
from 1.5B to 14B, and observe consistent performance improvements across all
scales. Experimental results on 4 bias types--verbosity, position, bandwagon,
and sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong
effectiveness. For example, the RBD-8B model improves evaluation accuracy by an
average of 18.5% and consistency by 10.9%, and surpasses prompting-based
baselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results
highlight RBD's effectiveness and scalability. Additional experiments further
demonstrate its strong generalization across biases and domains, as well as its
efficiency.",2025-05-21,"Haoyan Yang, Runxue Bao, Cao Xiao, Jun Ma, Parminder Bhatia, Shangqian Gao, Taha Kass-Hout",http://arxiv.org/pdf/2505.17100v1,cs.CL
EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association,"Goal-oriented script planning, or the ability to devise coherent sequences of
actions toward specific goals, is commonly employed by humans to plan for
typical activities. In e-commerce, customers increasingly seek LLM-based
assistants to generate scripts and recommend products at each step, thereby
facilitating convenient and efficient shopping experiences. However, this
capability remains underexplored due to several challenges, including the
inability of LLMs to simultaneously conduct script planning and product
retrieval, difficulties in matching products caused by semantic discrepancies
between planned actions and search queries, and a lack of methods and benchmark
data for evaluation. In this paper, we step forward by formally defining the
task of E-commerce Script Planning (EcomScript) as three sequential subtasks.
We propose a novel framework that enables the scalable generation of
product-enriched scripts by associating products with each step based on the
semantic similarity between the actions and their purchase intentions. By
applying our framework to real-world e-commerce data, we construct the very
first large-scale EcomScript dataset, EcomScriptBench, which includes 605,229
scripts sourced from 2.4 million products. Human annotations are then conducted
to provide gold labels for a sampled subset, forming an evaluation benchmark.
Extensive experiments reveal that current (L)LMs face significant challenges
with EcomScript tasks, even after fine-tuning, while injecting product purchase
intentions improves their performance.",2025-05-21,"Weiqi Wang, Limeng Cui, Xin Liu, Sreyashi Nag, Wenju Xu, Chen Luo, Sheikh Muhammad Sarwar, Yang Li, Hansu Gu, Hui Liu, Changlong Yu, Jiaxin Bai, Yifan Gao, Haiyang Zhang, Qi He, Shuiwang Ji, Yangqiu Song",http://arxiv.org/pdf/2505.15196v1,cs.CL
ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection,"Recent advances in LLM agents have largely built on reasoning backbones like
ReAct, which interleave thought and action in complex environments. However,
ReAct often produces ungrounded or incoherent reasoning steps, leading to
misalignment between the agent's actual state and goal. Our analysis finds that
this stems from ReAct's inability to maintain consistent internal beliefs and
goal alignment, causing compounding errors and hallucinations. To address this,
we introduce ReflAct, a novel backbone that shifts reasoning from merely
planning next actions to continuously reflecting on the agent's state relative
to its goal. By explicitly grounding decisions in states and enforcing ongoing
goal alignment, ReflAct dramatically improves strategic reliability. This
design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7%
on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even
outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM),
showing that strengthening the core reasoning backbone is key to reliable agent
performance.",2025-05-21,"Jeonghye Kim, Sojeong Rhee, Minbeom Kim, Dohyung Kim, Sangmook Lee, Youngchul Sung, Kyomin Jung",http://arxiv.org/pdf/2505.15182v1,cs.CL
"ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving","Recent advances have explored integrating large language models (LLMs) into
end-to-end autonomous driving systems to enhance generalization and
interpretability. However, most existing approaches are limited to either
driving performance or vision-language reasoning, making it difficult to
achieve both simultaneously. In this paper, we propose ALN-P3, a unified
co-distillation framework that introduces cross-modal alignment between ""fast""
vision-based autonomous driving systems and ""slow"" language-driven reasoning
modules. ALN-P3 incorporates three novel alignment mechanisms: Perception
Alignment (P1A), Prediction Alignment (P2A), and Planning Alignment (P3A),
which explicitly align visual tokens with corresponding linguistic outputs
across the full perception, prediction, and planning stack. All alignment
modules are applied only during training and incur no additional costs during
inference. Extensive experiments on four challenging benchmarks-nuScenes, Nu-X,
TOD3Cap, and nuScenes QA-demonstrate that ALN-P3 significantly improves both
driving decisions and language reasoning, achieving state-of-the-art results.",2025-05-21,"Yunsheng Ma, Burhaneddin Yaman, Xin Ye, Mahmut Yurt, Jingru Luo, Abhirup Mallik, Ziran Wang, Liu Ren",http://arxiv.org/pdf/2505.15158v1,cs.CL
Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning,"Recent advancements in reasoning have significantly enhanced the capabilities
of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
across diverse tasks. However, excessive reliance on chain-of-thought (CoT)
reasoning can impair model performance and brings unnecessarily lengthened
outputs, reducing efficiency. Our work reveals that prolonged reasoning does
not universally improve accuracy and even degrade performance on simpler tasks.
To address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel
framework that dynamically switches between short answers and long-form
reasoning based on the model perplexity. CAR first generates a short answer and
evaluates its perplexity, triggering reasoning only when the model exhibits low
confidence (i.e., high perplexity). Experiments across diverse multimodal
VQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both
short-answer and long-form reasoning approaches, striking an optimal balance
between accuracy and efficiency.",2025-05-21,"Jinghui Lu, Haiyang Yu, Siliang Xu, Shiwei Ran, Guozhi Tang, Siqi Wang, Bin Shan, Teng Fu, Hao Feng, Jingqun Tang, Han Wang, Can Huang",http://arxiv.org/pdf/2505.15154v1,cs.CL
Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation,"Pretrained generative models have opened new frontiers in brain decoding by
enabling the synthesis of realistic texts and images from non-invasive brain
recordings. However, the reliability of such outputs remains
questionable--whether they truly reflect semantic activation in the brain, or
are merely hallucinated by the powerful generative models. In this paper, we
focus on EEG-to-text decoding and address its hallucination issue through the
lens of posterior collapse. Acknowledging the underlying mismatch in
information capacity between EEG and text, we reframe the decoding task as
semantic summarization of core meanings rather than previously verbatim
reconstruction of stimulus texts. To this end, we propose the Generative
Language Inspection Model (GLIM), which emphasizes learning informative and
interpretable EEG representations to improve semantic grounding under
heterogeneous and small-scale data conditions. Experiments on the public ZuCo
dataset demonstrate that GLIM consistently generates fluent, EEG-grounded
sentences without teacher forcing. Moreover, it supports more robust evaluation
beyond text similarity, through EEG-text retrieval and zero-shot semantic
classification across sentiment categories, relation types, and corpus topics.
Together, our architecture and evaluation protocols lay the foundation for
reliable and scalable benchmarking in generative brain decoding.",2025-05-21,"Xiaozhao Liu, Dinggang Shen, Xihui Liu",http://arxiv.org/pdf/2505.17099v1,cs.CL
TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration,"Multimodal in-context learning (ICL) has emerged as a key mechanism for
harnessing the capabilities of large vision-language models (LVLMs). However,
its effectiveness remains highly sensitive to the quality of input in-context
sequences, particularly for tasks involving complex reasoning or open-ended
generation. A major limitation is our limited understanding of how LVLMs
actually exploit these sequences during inference. To bridge this gap, we
systematically interpret multimodal ICL through the lens of task mapping, which
reveals how local and global relationships within and among demonstrations
guide model reasoning. Building on this insight, we present TACO, a lightweight
transformer-based model equipped with task-aware attention that dynamically
configures in-context sequences. By injecting task-mapping signals into the
autoregressive decoding process, TACO creates a bidirectional synergy between
sequence construction and task reasoning. Experiments on five LVLMs and nine
datasets demonstrate that TACO consistently surpasses baselines across diverse
ICL tasks. These results position task mapping as a valuable perspective for
interpreting and improving multimodal ICL.",2025-05-21,"Yanshu Li, Tian Yun, Jianjiang Yang, Pinyuan Feng, Jinfa Huang, Ruixiang Tang",http://arxiv.org/pdf/2505.17098v1,cs.CL
An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents,"Reinforcement learning (RL) has demonstrated strong potential in training
large language models (LLMs) capable of complex reasoning for real-world
problem solving. More recently, RL has been leveraged to create sophisticated
LLM-based search agents that adeptly combine reasoning with search engine use.
While the use of RL for training search agents is promising, the optimal design
of such agents remains not fully understood. In particular, key factors -- such
as (1) reward formulation, (2) the choice and characteristics of the underlying
LLM, and (3) the role of the search engine in the RL process -- require further
investigation. In this work, we conduct comprehensive empirical studies to
systematically investigate these and offer actionable insights. We highlight
several key findings: format rewards are effective in improving final
performance, whereas intermediate retrieval rewards have limited impact; the
scale and initialization of the LLM (general-purpose vs. reasoning-specialized)
significantly influence RL outcomes; and the choice of search engine plays a
critical role in shaping RL training dynamics and the robustness of the trained
agent during inference. These establish important guidelines for successfully
building and deploying LLM-based search agents in real-world applications. Code
is available at https://github.com/PeterGriffinJin/Search-R1.",2025-05-21,"Bowen Jin, Jinsung Yoon, Priyanka Kargupta, Sercan O. Arik, Jiawei Han",http://arxiv.org/pdf/2505.15117v1,cs.CL
RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals,"The table reasoning task, crucial for efficient data acquisition, aims to
answer questions based on the given table. Recently, reasoning large language
models (RLLMs) with Long Chain-of-Thought (Long CoT) significantly enhance
reasoning capabilities, leading to brilliant performance on table reasoning.
However, Long CoT suffers from high cost for training and exhibits low
reliability due to table content hallucinations. Therefore, we propose
Row-of-Thought (RoT), which performs iteratively row-wise table traversal,
allowing for reasoning extension and reflection-based refinement at each
traversal. Scaling reasoning length by row-wise traversal and leveraging
reflection capabilities of LLMs, RoT is training-free. The sequential traversal
encourages greater attention to the table, thus reducing hallucinations.
Experiments show that RoT, using non-reasoning models, outperforms RLLMs by an
average of 4.3%, and achieves state-of-the-art results on WikiTableQuestions
and TableBench with comparable models, proving its effectiveness. Also, RoT
outperforms Long CoT with fewer reasoning tokens, indicating higher efficiency.",2025-05-21,"Xuanliang Zhang, Dingzirui Wang, Keyan Xu, Qingfu Zhu, Wanxiang Che",http://arxiv.org/pdf/2505.15110v1,cs.CL
A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents,"The proliferation of Large Language Models (LLMs) and Intelligent Virtual
Agents acting as psychotherapists presents significant opportunities for
expanding mental healthcare access. However, their deployment has also been
linked to serious adverse outcomes, including user harm and suicide,
facilitated by a lack of standardized evaluation methodologies capable of
capturing the nuanced risks of therapeutic interaction. Current evaluation
techniques lack the sensitivity to detect subtle changes in patient cognition
and behavior during therapy sessions that may lead to subsequent
decompensation. We introduce a novel risk taxonomy specifically designed for
the systematic evaluation of conversational AI psychotherapists. Developed
through an iterative process including review of the psychotherapy risk
literature, qualitative interviews with clinical and legal experts, and
alignment with established clinical criteria (e.g., DSM-5) and existing
assessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured
approach to identifying and assessing user/patient harms. We provide a
high-level overview of this taxonomy, detailing its grounding, and discuss
potential use cases. We discuss two use cases in detail: monitoring cognitive
model-based risk factors during a counseling conversation to detect unsafe
deviations, in both human-AI counseling sessions and in automated benchmarking
of AI psychotherapists with simulated patients. The proposed taxonomy offers a
foundational step towards establishing safer and more responsible innovation in
the domain of AI-driven mental health support.",2025-05-21,"Ian Steenstra, Timothy W. Bickmore",http://arxiv.org/pdf/2505.15108v1,cs.CL
StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization,"Efficient multi-hop reasoning requires Large Language Models (LLMs) based
agents to acquire high-value external knowledge iteratively. Previous work has
explored reinforcement learning (RL) to train LLMs to perform search-based
document retrieval, achieving notable improvements in QA performance, but
underperform on complex, multi-hop QA resulting from the sparse rewards from
global signal only. To address this gap in existing research, we introduce
StepSearch, a framework for search LLMs that trained with step-wise proximal
policy optimization method. It consists of richer and more detailed
intermediate search rewards and token-level process supervision based on
information gain and redundancy penalties to better guide each search step. We
constructed a fine-grained question-answering dataset containing
sub-question-level search trajectories based on open source datasets through a
set of data pipeline method. On standard multi-hop QA benchmarks, it
significantly outperforms global-reward baselines, achieving 11.2% and 4.2%
absolute improvements for 3B and 7B models over various search with RL
baselines using only 19k training data, demonstrating the effectiveness of
fine-grained, stepwise supervision in optimizing deep search LLMs. Our code
will be released on https://github.com/Zillwang/StepSearch.",2025-05-21,"Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, Yichao Wu",http://arxiv.org/pdf/2505.15107v2,cs.CL
Mechanistic evaluation of Transformers and state space models,"State space models (SSMs) for language modelling promise an efficient and
performant alternative to quadratic-attention Transformers, yet show variable
performance on recalling basic information from the context. While performance
on synthetic tasks like Associative Recall (AR) can point to this deficiency,
behavioural metrics provide little information as to why--on a mechanistic
level--certain architectures fail and others succeed. To address this, we
conduct experiments on AR and find that only Transformers and Based SSM models
fully succeed at AR, with Mamba a close third, whereas the other SSMs (H3,
Hyena) fail. We then use causal interventions to explain why. We find that
Transformers and Based learn to store key-value associations in-context using
induction heads. By contrast, the SSMs compute these associations only at the
last state, with only Mamba succeeding because of its short convolution
component. To extend and deepen these findings, we introduce Associative
Treecall (ATR), a synthetic task similar to AR based on PCFG induction. ATR
introduces language-like hierarchical structure into the AR setting. We find
that all architectures learn the same mechanism as they did for AR, and the
same three models succeed at the task. These results reveal that architectures
with similar accuracy may still have substantive differences, motivating the
adoption of mechanistic evaluations.",2025-05-21,"Aryaman Arora, Neil Rathi, Nikil Roashan Selvam, Róbert Csórdas, Dan Jurafsky, Christopher Potts",http://arxiv.org/pdf/2505.15105v1,cs.CL
Cost-aware LLM-based Online Dataset Annotation,"Recent advances in large language models (LLMs) have enabled automated
dataset labeling with minimal human supervision. While majority voting across
multiple LLMs can improve label reliability by mitigating individual model
biases, it incurs high computational costs due to repeated querying. In this
work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo),
for efficient and accurate LLM-based dataset annotation. CaMVo adaptively
selects a subset of LLMs for each data instance based on contextual embeddings,
balancing confidence and cost without requiring pre-training or ground-truth
labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator
over confidence scores, CaMVo estimates a lower bound on labeling accuracy for
each LLM and aggregates responses through weighted majority voting. Our
empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates
that CaMVo achieves comparable or superior accuracy to full majority voting
while significantly reducing labeling costs. This establishes CaMVo as a
practical and robust solution for cost-efficient annotation in dynamic labeling
environments.",2025-05-21,"Eray Can Elumar, Cem Tekin, Osman Yagan",http://arxiv.org/pdf/2505.15101v1,cs.CL
Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English,"Sarcasm is a challenge to sentiment analysis because of the incongruity
between stated and implied sentiment. The challenge is exacerbated when the
implication may be relevant to a specific country or geographical region.
Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that
has been used for pragmatic reasoning. In this paper, we harness PMP for
explainable sarcasm detection for Australian and Indian English, alongside a
benchmark dataset for standard English. We manually add sarcasm explanations to
an existing sarcasm-labeled dataset for Australian and Indian English called
BESSTIE, and compare the performance for explainable sarcasm detection for them
with FLUTE, a standard English dataset containing sarcasm explanations. Our
approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)
achieves statistically significant performance improvement across all tasks and
datasets when compared with four alternative prompting strategies. We also find
that alternative techniques such as agentic prompting mitigate context-related
failures by enabling external knowledge retrieval. The focused contribution of
our work is utilising PMP in generating sarcasm explanations for varieties of
English.",2025-05-21,"Ishmanbir Singh, Dipankar Srirag, Aditya Joshi",http://arxiv.org/pdf/2505.15095v1,cs.CL
SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models,"Large Language Models (LLMs) have shown impressive capabilities in contextual
understanding and reasoning. However, evaluating their performance across
diverse scientific domains remains underexplored, as existing benchmarks
primarily focus on general domains and fail to capture the intricate complexity
of scientific data. To bridge this gap, we construct SciCUEval, a comprehensive
benchmark dataset tailored to assess the scientific context understanding
capability of LLMs. It comprises ten domain-specific sub-datasets spanning
biology, chemistry, physics, biomedicine, and materials science, integrating
diverse data modalities including structured tables, knowledge graphs, and
unstructured texts. SciCUEval systematically evaluates four core competencies:
Relevant information identification, Information-absence detection,
Multi-source information integration, and Context-aware inference, through a
variety of question formats. We conduct extensive evaluations of
state-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their
strengths and limitations in scientific context understanding, and offering
valuable insights for the future development of scientific-domain LLMs.",2025-05-21,"Jing Yu, Yuqi Tang, Kehua Feng, Mingyang Rao, Lei Liang, Zhiqiang Zhang, Mengshu Sun, Wen Zhang, Qiang Zhang, Keyan Ding, Huajun Chen",http://arxiv.org/pdf/2505.15094v1,cs.CL
CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention,"Multimodal in-context learning (ICL) enables large vision-language models
(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of
real-world applications. However, multimodal ICL remains unstable, and current
research largely focuses on optimizing sequence configuration while overlooking
the internal mechanisms of LVLMs. In this work, we first provide a theoretical
analysis of attentional dynamics in multimodal ICL and identify three core
limitations of standard attention that ICL impair performance. To address these
challenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet
effective plug-and-play method for directly calibrating LVLM attention logits.
CAMA is training-free and can be seamlessly applied to various open-source
LVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its
effectiveness and generality. CAMA opens new opportunities for deeper
exploration and targeted utilization of LVLM attention dynamics to advance
multimodal reasoning.",2025-05-21,"Yanshu Li, JianJiang Yang, Bozheng Li, Ruixiang Tang",http://arxiv.org/pdf/2505.17097v1,cs.CL
DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer,"Effective cross-lingual transfer remains a critical challenge in scaling the
benefits of large language models from high-resource to low-resource languages.
Towards this goal, prior studies have explored many approaches to combine task
knowledge from task-specific data in a (high-resource) source language and
language knowledge from unlabeled text in a (low-resource) target language. One
notable approach proposed composable sparse fine-tuning (SFT) for cross-lingual
transfer that learns task-specific and language-specific sparse masks to select
a subset of the pretrained model's parameters that are further fine-tuned.
These sparse fine-tuned vectors (SFTs) are subsequently composed with the
pretrained model to facilitate zero-shot cross-lingual transfer to a task in a
target language, using only task-specific data from a source language. These
sparse masks for SFTs were identified using a simple magnitude-based pruning.
In our work, we introduce DeFT-X, a novel composable SFT approach that denoises
the weight matrices of a pretrained model before magnitude pruning using
singular value decomposition, thus yielding more robust SFTs. We evaluate
DeFT-X on a diverse set of extremely low-resource languages for sentiment
classification (NusaX) and natural language inference (AmericasNLI) and
demonstrate that it performs at par or outperforms SFT and other prominent
cross-lingual transfer baselines.",2025-05-21,"Sona Elza Simon, Preethi Jyothi",http://arxiv.org/pdf/2505.15090v1,cs.CL
HopWeaver: Synthesizing Authentic Multi-Hop Questions Across Text Corpora,"Multi-Hop Question Answering (MHQA) is crucial for evaluating the model's
capability to integrate information from diverse sources. However, creating
extensive and high-quality MHQA datasets is challenging: (i) manual annotation
is expensive, and (ii) current synthesis methods often produce simplistic
questions or require extensive manual guidance. This paper introduces
HopWeaver, the first automatic framework synthesizing authentic multi-hop
questions from unstructured text corpora without human intervention. HopWeaver
synthesizes two types of multi-hop questions (bridge and comparison) using an
innovative approach that identifies complementary documents across corpora. Its
coherent pipeline constructs authentic reasoning paths that integrate
information across multiple documents, ensuring synthesized questions
necessitate authentic multi-hop reasoning. We further present a comprehensive
system for evaluating synthesized multi-hop questions. Empirical evaluations
demonstrate that the synthesized questions achieve comparable or superior
quality to human-annotated datasets at a lower cost. Our approach is valuable
for developing MHQA datasets in specialized domains with scarce annotated
resources. The code for HopWeaver is publicly available.",2025-05-21,"Zhiyu Shen, Jiyuan Liu, Yunhe Pang, Yanghui Rao",http://arxiv.org/pdf/2505.15087v1,cs.CL
SUS backprop: linear backpropagation algorithm for long inputs in transformers,"It is straightforward to design an unbiased gradient estimator that
stochastically cuts the backpropagation flow through any part of a
computational graph. By cutting the parts that have little effect on the
computation, one can potentially save a significant amount of back-propagation
computation in exchange for a minimal increase in the stochastic gradient
variance, in some situations. Such a situation occurs in the attention
mechanism of the transformer architecture. For long sequences, attention
becomes the limiting factor, as its compute requirements increase quadratically
with sequence length $n$. At the same time, most attention weights become very
small, as most attention heads tend to connect a given token with only a small
fraction of other tokens in the sequence. These weights become promising
targets for cutting backpropagation. We propose a simple probabilistic rule
controlled by a single parameter $c$ that cuts backpropagation through most
attention weights, leaving at most $c$ interactions per token per attention
head. This brings a factor of $c/n$ reduction in the compute required for the
attention backpropagation, turning it from quadratic $O(n^2)$ to linear
complexity $O(nc)$. We have empirically verified that, for a typical
transformer model, cutting $99\%$ of the attention gradient flow (i.e. choosing
$c \sim 20-30$) results in relative gradient variance increase of only about
$1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable
to efficient sparse matrix implementation, thus being promising for making the
cost of a backward pass negligible relative to the cost of a forward pass when
training a transformer model on long sequences.",2025-05-21,"Sergey Pankov, Georges Harik",http://arxiv.org/pdf/2505.15080v1,cs.CL
Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation,"Due to the legal and ethical responsibilities of healthcare providers (HCPs)
for accurate documentation and protection of patient data privacy, the natural
variability in the responses of large language models (LLMs) presents
challenges for incorporating clinical note generation (CNG) systems, driven by
LLMs, into real-world clinical processes. The complexity is further amplified
by the detailed nature of texts in CNG. To enhance the confidence of HCPs in
tools powered by LLMs, this study evaluates the reliability of 12 open-weight
and proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms
of their ability to generate notes that are string equivalent (consistency
rate), have the same meaning (semantic consistency) and are correct (semantic
similarity), across several iterations using the same prompt. The results show
that (1) LLMs from all model families are stable, such that their responses are
semantically consistent despite being written in various ways, and (2) most of
the LLMs generated notes close to the corresponding notes made by experts.
Overall, Meta's Llama 70B was the most reliable, followed by Mistral's Small
model. With these findings, we recommend the local deployment of these
relatively smaller open-weight models for CNG to ensure compliance with data
privacy regulations, as well as to improve the efficiency of HCPs in clinical
documentation.",2025-05-21,"Kristine Ann M. Carandang, Jasper Meynard P. Araña, Ethan Robert A. Casin, Christopher P. Monterola, Daniel Stanley Y. Tan, Jesus Felix B. Valenzuela, Christian M. Alis",http://arxiv.org/pdf/2505.17095v1,cs.CL
Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs,"The rapid evolution of multimodal large language models (MLLMs) has
significantly enhanced their real-world applications. However, achieving
consistent performance across languages, especially when integrating cultural
knowledge, remains a significant challenge. To better assess this issue, we
introduce two new benchmarks: KnowRecall and VisRecall, which evaluate
cross-lingual consistency in MLLMs. KnowRecall is a visual question answering
benchmark designed to measure factual knowledge consistency in 15 languages,
focusing on cultural and historical questions about global landmarks. VisRecall
assesses visual memory consistency by asking models to describe landmark
appearances in 9 languages without access to images. Experimental results
reveal that state-of-the-art MLLMs, including proprietary ones, still struggle
to achieve cross-lingual consistency. This underscores the need for more robust
approaches that produce truly multilingual and culturally aware models.",2025-05-21,"Hao Wang, Pinzhi Huang, Jihan Yang, Saining Xie, Daisuke Kawahara",http://arxiv.org/pdf/2505.15075v1,cs.CL
DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data,"Large Language Models (LLMs) are increasingly aligned with human preferences
through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,
Group Relative Policy Optimization (GRPO) has gained attention for its
simplicity and strong performance, notably eliminating the need for a learned
value function. However, GRPO implicitly assumes a balanced domain distribution
and uniform semantic alignment across groups - assumptions that rarely hold in
real-world datasets. When applied to multi-domain, imbalanced data, GRPO
disproportionately optimizes for dominant domains, neglecting underrepresented
ones and resulting in poor generalization and fairness. We propose
Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled
extension to GRPO that addresses inter-group imbalance with two key
innovations. Domain-aware reward scaling counteracts frequency bias by
reweighting optimization based on domain prevalence. Difficulty-aware reward
scaling leverages prompt-level self-consistency to identify and prioritize
uncertain prompts that offer greater learning value. Together, these strategies
promote more equitable and effective policy learning across domains. Extensive
experiments across multiple LLMs and skewed training distributions show that
DISCO improves generalization, outperforms existing GRPO variants by 5% on
Qwen3 models, and sets new state-of-the-art results on multi-domain alignment
benchmarks.",2025-05-21,"Yuhang Zhou, Jing Zhu, Shengyi Qian, Zhuokai Zhao, Xiyao Wang, Xiaoyu Liu, Ming Li, Paiheng Xu, Wei Ai, Furong Huang",http://arxiv.org/pdf/2505.15074v1,cs.CL
MoTime: A Dataset Suite for Multimodal Time Series Forecasting,"While multimodal data sources are increasingly available from real-world
forecasting, most existing research remains on unimodal time series. In this
work, we present MoTime, a suite of multimodal time series forecasting datasets
that pair temporal signals with external modalities such as text, metadata, and
images. Covering diverse domains, MoTime supports structured evaluation of
modality utility under two scenarios: 1) the common forecasting task, where
varying-length history is available, and 2) cold-start forecasting, where no
historical data is available. Experiments show that external modalities can
improve forecasting performance in both scenarios, with particularly strong
benefits for short series in some datasets, though the impact varies depending
on data characteristics. By making datasets and findings publicly available, we
aim to support more comprehensive and realistic benchmarks in future multimodal
time series forecasting research.",2025-05-21,"Xin Zhou, Weiqing Wang, Francisco J. Baldán, Wray Buntine, Christoph Bergmeir",http://arxiv.org/pdf/2505.15072v1,cs.CL
Can Large Language Models Understand Internet Buzzwords Through User-Generated Content,"The massive user-generated content (UGC) available in Chinese social media is
giving rise to the possibility of studying internet buzzwords. In this paper,
we study if large language models (LLMs) can generate accurate definitions for
these buzzwords based on UGC as examples. Our work serves a threefold
contribution. First, we introduce CHEER, the first dataset of Chinese internet
buzzwords, each annotated with a definition and relevant UGC. Second, we
propose a novel method, called RESS, to effectively steer the comprehending
process of LLMs to produce more accurate buzzword definitions, mirroring the
skills of human language learning. Third, with CHEER, we benchmark the
strengths and weaknesses of various off-the-shelf definition generation methods
and our RESS. Our benchmark demonstrates the effectiveness of RESS while
revealing crucial shared challenges: over-reliance on prior exposure,
underdeveloped inferential abilities, and difficulty identifying high-quality
UGC to facilitate comprehension. We believe our work lays the groundwork for
future advancements in LLM-based definition generation. Our dataset and code
are available at https://github.com/SCUNLP/Buzzword.",2025-05-21,"Chen Huang, Junkai Luo, Xinzuo Wang, Wenqiang Lei, Jiancheng Lv",http://arxiv.org/pdf/2505.15071v1,cs.CL
An Alternative to FLOPS Regularization to Effectively Productionize SPLADE-Doc,"Learned Sparse Retrieval (LSR) models encode text as weighted term vectors,
which need to be sparse to leverage inverted index structures during retrieval.
SPLADE, the most popular LSR model, uses FLOPS regularization to encourage
vector sparsity during training. However, FLOPS regularization does not ensure
sparsity among terms - only within a given query or document. Terms with very
high Document Frequencies (DFs) substantially increase latency in production
retrieval engines, such as Apache Solr, due to their lengthy posting lists. To
address the issue of high DFs, we present a new variant of FLOPS
regularization: DF-FLOPS. This new regularization technique penalizes the usage
of high-DF terms, thereby shortening posting lists and reducing retrieval
latency. Unlike other inference-time sparsification methods, such as stopword
removal, DF-FLOPS regularization allows for the selective inclusion of
high-frequency terms in cases where the terms are truly salient. We find that
DF-FLOPS successfully reduces the prevalence of high-DF terms and lowers
retrieval latency (around 10x faster) in a production-grade engine while
maintaining effectiveness both in-domain (only a 2.2-point drop in MRR@10) and
cross-domain (improved performance in 12 out of 13 tasks on which we tested).
With retrieval latencies on par with BM25, this work provides an important step
towards making LSR practical for deployment in production-grade search engines.",2025-05-21,"Aldo Porco, Dhruv Mehra, Igor Malioutov, Karthik Radhakrishnan, Moniba Keymanesh, Daniel Preoţiuc-Pietro, Sean MacAvaney, Pengxiang Cheng",http://arxiv.org/pdf/2505.15070v1,cs.CL
In-Domain African Languages Translation Using LLMs and Multi-armed Bandits,"Neural Machine Translation (NMT) systems face significant challenges when
working with low-resource languages, particularly in domain adaptation tasks.
These difficulties arise due to limited training data and suboptimal model
generalization, As a result, selecting an optimal model for translation is
crucial for achieving strong performance on in-domain data, particularly in
scenarios where fine-tuning is not feasible or practical. In this paper, we
investigate strategies for selecting the most suitable NMT model for a given
domain using bandit-based algorithms, including Upper Confidence Bound, Linear
UCB, Neural Linear Bandit, and Thompson Sampling. Our method effectively
addresses the resource constraints by facilitating optimal model selection with
high confidence. We evaluate the approach across three African languages and
domains, demonstrating its robustness and effectiveness in both scenarios where
target data is available and where it is absent.",2025-05-21,"Pratik Rakesh Singh, Kritarth Prasad, Mohammadi Zaki, Pankaj Wasnik",http://arxiv.org/pdf/2505.15069v1,cs.CL
ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges,"Recent progress in large language models (LLMs) has enabled substantial
advances in solving mathematical problems. However, existing benchmarks often
fail to reflect the complexity of real-world problems, which demand open-ended,
interdisciplinary reasoning and integration of computational tools. To address
this gap, we introduce ModelingBench, a novel benchmark featuring
real-world-inspired, open-ended problems from math modeling competitions across
diverse domains, ranging from urban traffic optimization to ecosystem resource
planning. These tasks require translating natural language into formal
mathematical formulations, applying appropriate tools, and producing
structured, defensible reports. ModelingBench also supports multiple valid
solutions, capturing the ambiguity and creativity of practical modeling. We
also present ModelingAgent, a multi-agent framework that coordinates tool use,
supports structured workflows, and enables iterative self-refinement to
generate well-grounded, creative solutions. To evaluate outputs, we further
propose ModelingJudge, an expert-in-the-loop system leveraging LLMs as
domain-specialized judges assessing solutions from multiple expert
perspectives. Empirical results show that ModelingAgent substantially
outperforms strong baselines and often produces solutions indistinguishable
from those of human experts. Together, our work provides a comprehensive
framework for evaluating and advancing real-world problem-solving in
open-ended, interdisciplinary modeling challenges.",2025-05-21,"Cheng Qian, Hongyi Du, Hongru Wang, Xiusi Chen, Yuji Zhang, Avirup Sil, Chengxiang Zhai, Kathleen McKeown, Heng Ji",http://arxiv.org/pdf/2505.15068v1,cs.CL
The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support,"Can small language models with 0.5B to 5B parameters meaningfully engage in
trauma-informed, empathetic dialogue for individuals with PTSD? We address this
question by introducing TIDE, a dataset of 10,000 two-turn dialogues spanning
500 diverse PTSD client personas and grounded in a three-factor empathy model:
emotion recognition, distress normalization, and supportive reflection. All
scenarios and reference responses were reviewed for realism and trauma
sensitivity by a clinical psychologist specializing in PTSD. We evaluate eight
small language models before and after fine-tuning, comparing their outputs to
a frontier model (Claude Sonnet 3.5). Our IRB-approved human evaluation and
automatic metrics show that fine-tuning generally improves perceived empathy,
but gains are highly scenario- and user-dependent, with smaller models facing
an empathy ceiling. Demographic analysis shows older adults value distress
validation and graduate-educated users prefer nuanced replies, while gender
effects are minimal. We highlight the limitations of automatic metrics and the
need for context- and user-aware system design. Our findings, along with the
planned release of TIDE, provide a foundation for building safe,
resource-efficient, and ethically sound empathetic AI to supplement, not
replace, clinical mental health care.",2025-05-21,"Suhas BN, Yash Mahajan, Dominik Mattioli, Andrew M. Sherrill, Rosa I. Arriaga, Chris W. Wiese, Saeed Abdullah",http://arxiv.org/pdf/2505.15065v1,cs.CL
UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking,"The rapid use of large language models (LLMs) has raised critical concerns
regarding the factual reliability of their outputs, especially in low-resource
languages such as Urdu. Existing automated fact-checking solutions
overwhelmingly focus on English, leaving a significant gap for the 200+ million
Urdu speakers worldwide. In this work, we introduce UrduFactCheck, the first
comprehensive, modular fact-checking framework specifically tailored for Urdu.
Our system features a dynamic, multi-strategy evidence retrieval pipeline that
combines monolingual and translation-based approaches to address the scarcity
of high-quality Urdu evidence. We curate and release two new hand-annotated
benchmarks: UrduFactBench for claim verification and UrduFactQA for evaluating
LLM factuality. Extensive experiments demonstrate that UrduFactCheck,
particularly its translation-augmented variants, consistently outperforms
baselines and open-source alternatives on multiple metrics. We further
benchmark twelve state-of-the-art (SOTA) LLMs on factual question answering in
Urdu, highlighting persistent gaps between proprietary and open-source models.
UrduFactCheck's code and datasets are open-sourced and publicly available at
https://github.com/mbzuai-nlp/UrduFactCheck.",2025-05-21,"Sarfraz Ahmad, Hasan Iqbal, Momina Ahsan, Numaan Naeem, Muhammad Ahsan Riaz Khan, Arham Riaz, Muhammad Arslan Manzoor, Yuxia Wang, Preslav Nakov",http://arxiv.org/pdf/2505.15063v1,cs.CL
Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning,"When addressing complex questions that require new information, people often
associate the question with existing knowledge to derive a sensible answer. For
instance, when evaluating whether melatonin aids insomnia, one might associate
""hormones helping mental disorders"" with ""melatonin being a hormone and
insomnia a mental disorder"" to complete the reasoning. Large Language Models
(LLMs) also require such associative thinking, particularly in resolving
scientific inquiries when retrieved knowledge is insufficient and does not
directly answer the question. Graph Inspired Veracity Extrapolation (GIVE)
addresses this by using a knowledge graph (KG) to extrapolate structured
knowledge. However, it involves the construction and pruning of many
hypothetical triplets, which limits efficiency and generalizability. We propose
Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic
associative thinking through reinforcement learning. Self-GIVE extracts
structured information and entity sets to assist the model in linking to the
queried concepts. We address GIVE's key limitations: (1) extensive LLM calls
and token overhead for knowledge extrapolation, (2) difficulty in deploying on
smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate
knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE
with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B
models by up to $\textbf{28.5%$\rightarrow$71.4%}$ and
$\textbf{78.6$\rightarrow$90.5%}$ in samples $\textbf{unseen}$ in challenging
biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or
outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\%.
Self-GIVE enhances the scalable integration of structured retrieval and
reasoning with associative thinking.",2025-05-21,"Jiashu He, Jinxuan Fan, Bowen Jiang, Ignacio Houine, Dan Roth, Alejandro Ribeiro",http://arxiv.org/pdf/2505.15062v2,cs.CL
Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory,"The evaluation of large language models (LLMs) via benchmarks is widespread,
yet inconsistencies between different leaderboards and poor separability among
top models raise concerns about their ability to accurately reflect authentic
model capabilities. This paper provides a critical analysis of benchmark
effectiveness, examining main-stream prominent LLM benchmarks using results
from diverse models. We first propose a new framework for accurate and reliable
estimations of item characteristics and model abilities. Specifically, we
propose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced
Item Response Theory framework that incorporates a rich set of item parameters
within an IRT-grounded architecture. Based on PSN-IRT, we conduct extensive
analysis which reveals significant and varied shortcomings in the measurement
quality of current benchmarks. Furthermore, we demonstrate that leveraging
PSN-IRT is able to construct smaller benchmarks while maintaining stronger
alignment with human preference.",2025-05-21,"Hongli Zhou, Hui Huang, Ziqing Zhao, Lvyuan Han, Huicheng Wang, Kehai Chen, Muyun Yang, Wei Bao, Jian Dong, Bing Xu, Conghui Zhu, Hailong Cao, Tiejun Zhao",http://arxiv.org/pdf/2505.15055v1,cs.CL
"MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation","Precise recognition, editing, and generation of molecules are essential
prerequisites for both chemists and AI systems tackling various chemical tasks.
We present MolLangBench, a comprehensive benchmark designed to evaluate
fundamental molecule-language interface tasks: language-prompted molecular
structure recognition, editing, and generation. To ensure high-quality,
unambiguous, and deterministic outputs, we construct the recognition tasks
using automated cheminformatics tools, and curate editing and generation tasks
through rigorous expert annotation and validation. MolLangBench supports the
evaluation of models that interface language with different molecular
representations, including linear strings, molecular images, and molecular
graphs. Evaluations of state-of-the-art models reveal significant limitations:
the strongest model (o3) achieves $79.2\%$ and $78.5\%$ accuracy on recognition
and editing tasks, which are intuitively simple for humans, and performs even
worse on the generation task, reaching only $29.0\%$ accuracy. These results
highlight the shortcomings of current AI systems in handling even preliminary
molecular recognition and manipulation tasks. We hope MolLangBench will
catalyze further research toward more effective and reliable AI systems for
chemical applications.",2025-05-21,"Feiyang Cai, Jiahui Bai, Tao Tang, Joshua Luo, Tianyu Zhu, Ling Liu, Feng Luo",http://arxiv.org/pdf/2505.15054v1,cs.CL
Improving the fact-checking performance of language models by relying on their entailment ability,"Automated fact-checking is a crucial task in this digital age. To verify a
claim, current approaches majorly follow one of two strategies i.e. (i) relying
on embedded knowledge of language models, and (ii) fine-tuning them with
evidence pieces. While the former can make systems to hallucinate, the later
have not been very successful till date. The primary reason behind this is that
fact verification is a complex process. Language models have to parse through
multiple pieces of evidence before making a prediction. Further, the evidence
pieces often contradict each other. This makes the reasoning process even more
complex. We proposed a simple yet effective approach where we relied on
entailment and the generative ability of language models to produce
''supporting'' and ''refuting'' justifications (for the truthfulness of a
claim). We trained language models based on these justifications and achieved
superior results. Apart from that, we did a systematic comparison of different
prompting and fine-tuning strategies, as it is currently lacking in the
literature. Some of our observations are: (i) training language models with raw
evidence sentences registered an improvement up to 8.20% in macro-F1, over the
best performing baseline for the RAW-FC dataset, (ii) similarly, training
language models with prompted claim-evidence understanding (TBE-2) registered
an improvement (with a margin up to 16.39%) over the baselines for the same
dataset, (iii) training language models with entailed justifications (TBE-3)
outperformed the baselines by a huge margin (up to 28.57% and 44.26% for
LIAR-RAW and RAW-FC, respectively). We have shared our code repository to
reproduce the results.",2025-05-21,"Gaurav Kumar, Debajyoti Mazumder, Ayush Garg, Jasabanta Patro",http://arxiv.org/pdf/2505.15050v1,cs.CL
ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding,"The emergence of Multi-modal Large Language Models (MLLMs) presents new
opportunities for chart understanding. However, due to the fine-grained nature
of these tasks, applying MLLMs typically requires large, high-quality datasets
for task-specific fine-tuning, leading to high data collection and training
costs. To address this, we propose ChartCards, a unified chart-metadata
generation framework for multi-task chart understanding. ChartCards
systematically synthesizes various chart information, including data tables,
visualization code, visual elements, and multi-dimensional semantic captions.
By structuring this information into organized metadata, ChartCards enables a
single chart to support multiple downstream tasks, such as text-to-chart
retrieval, chart summarization, chart-to-table conversion, chart description,
and chart question answering. Using ChartCards, we further construct MetaChart,
a large-scale high-quality dataset containing 10,862 data tables, 85K charts,
and 170 K high-quality chart captions. We validate the dataset through
qualitative crowdsourcing evaluations and quantitative fine-tuning experiments
across various chart understanding tasks. Fine-tuning six different models on
MetaChart resulted in an average performance improvement of 5% across all
tasks. The most notable improvements are seen in text-to-chart retrieval and
chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements
of 17% and 28%, respectively.",2025-05-21,"Yifan Wu, Lutao Yan, Leixian Shen, Yinan Mei, Jiannan Wang, Yuyu Luo",http://arxiv.org/pdf/2505.15046v2,cs.CL
Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective,"Large language model (LLM)-based embedding models, benefiting from large
scale pre-training and post-training, have begun to surpass BERT and T5-based
models on general-purpose text embedding tasks such as document retrieval.
However, a fundamental limitation of LLM embeddings lies in the unidirectional
attention used during autoregressive pre-training, which misaligns with the
bidirectional nature of text embedding tasks. To this end, We propose adopting
diffusion language models for text embeddings, motivated by their inherent
bidirectional architecture and recent success in matching or surpassing LLMs
especially on reasoning tasks. We present the first systematic study of the
diffusion language embedding model, which outperforms the LLM-based embedding
model by 20% on long-document retrieval, 8% on reasoning-intensive retrieval,
2% on instruction-following retrieval, and achieve competitive performance on
traditional text embedding benchmarks. Our analysis verifies that bidirectional
attention is crucial for encoding global context in long and complex text.",2025-05-21,"Siyue Zhang, Yilun Zhao, Liyuan Geng, Arman Cohan, Anh Tuan Luu, Chen Zhao",http://arxiv.org/pdf/2505.15045v1,cs.CL
Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering,"Linear Concept Vectors have proven effective for steering large language
models (LLMs). While existing approaches like linear probing and
difference-in-means derive these vectors from LLM hidden representations,
diverse data introduces noises (i.e., irrelevant features) that challenge
steering robustness. To address this, we propose Sparse Autoencoder-Denoised
Concept Vectors (SDCV), which uses Sparse Autoencoders to filter out noisy
features from hidden representations. When applied to linear probing and
difference-in-means, our method improves their steering success rates. We
validate our noise hypothesis through counterfactual experiments and feature
visualizations.",2025-05-21,"Haiyan Zhao, Xuansheng Wu, Fan Yang, Bo Shen, Ninghao Liu, Mengnan Du",http://arxiv.org/pdf/2505.15038v1,cs.CL
RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning,"Reinforcement learning (RL) has recently emerged as a compelling approach for
enhancing the reasoning capabilities of large language models (LLMs), where an
LLM generator serves as a policy guided by a verifier (reward model). However,
current RL post-training methods for LLMs typically use verifiers that are
fixed (rule-based or frozen pretrained) or trained discriminatively via
supervised fine-tuning (SFT). Such designs are susceptible to reward hacking
and generalize poorly beyond their training distributions. To overcome these
limitations, we propose Tango, a novel framework that uses RL to concurrently
train both an LLM generator and a verifier in an interleaved manner. A central
innovation of Tango is its generative, process-level LLM verifier, which is
trained via RL and co-evolves with the generator. Importantly, the verifier is
trained solely based on outcome-level verification correctness rewards without
requiring explicit process-level annotations. This generative RL-trained
verifier exhibits improved robustness and superior generalization compared to
deterministic or SFT-trained verifiers, fostering effective mutual
reinforcement with the generator. Extensive experiments demonstrate that both
components of Tango achieve state-of-the-art results among 7B/8B-scale models:
the generator attains best-in-class performance across five competition-level
math benchmarks and four challenging out-of-domain reasoning tasks, while the
verifier leads on the ProcessBench dataset. Remarkably, both components exhibit
particularly substantial improvements on the most difficult mathematical
reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.",2025-05-21,"Kaiwen Zha, Zhengqi Gao, Maohao Shen, Zhang-Wei Hong, Duane S. Boning, Dina Katabi",http://arxiv.org/pdf/2505.15034v1,cs.CL
Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI,"Peer review is vital in academia for evaluating research quality. Top AI
conferences use reviewer confidence scores to ensure review reliability, but
existing studies lack fine-grained analysis of text-score consistency,
potentially missing key details. This work assesses consistency at word,
sentence, and aspect levels using deep learning and NLP conference review data.
We employ deep learning to detect hedge sentences and aspects, then analyze
report length, hedge word/sentence frequency, aspect mentions, and sentiment to
evaluate text-score alignment. Correlation, significance, and regression tests
examine confidence scores' impact on paper outcomes. Results show high
text-score consistency across all levels, with regression revealing higher
confidence scores correlate with paper rejection, validating expert assessments
and peer review fairness.",2025-05-21,"Wenqing Wu, Haixu Xi, Chengzhi Zhang",http://arxiv.org/pdf/2505.15031v1,cs.CL
Diagnosing our datasets: How does my language model learn clinical information?,"Large language models (LLMs) have performed well across various clinical
natural language processing tasks, despite not being directly trained on
electronic health record (EHR) data. In this work, we examine how popular
open-source LLMs learn clinical information from large mined corpora through
two crucial but understudied lenses: (1) their interpretation of clinical
jargon, a foundational ability for understanding real-world clinical notes, and
(2) their responses to unsupported medical claims. For both use cases, we
investigate the frequency of relevant clinical information in their
corresponding pretraining corpora, the relationship between pretraining data
composition and model outputs, and the sources underlying this data. To isolate
clinical jargon understanding, we evaluate LLMs on a new dataset MedLingo.
Unsurprisingly, we find that the frequency of clinical jargon mentions across
major pretraining corpora correlates with model performance. However, jargon
frequently appearing in clinical notes often rarely appears in pretraining
corpora, revealing a mismatch between available data and real-world usage.
Similarly, we find that a non-negligible portion of documents support disputed
claims that can then be parroted by models. Finally, we classified and analyzed
the types of online sources in which clinical jargon and unsupported medical
claims appear, with implications for future dataset composition.",2025-05-21,"Furong Jia, David Sontag, Monica Agrawal",http://arxiv.org/pdf/2505.15024v2,cs.CL
Voicing Personas: Rewriting Persona Descriptions into Style Prompts for Controllable Text-to-Speech,"In this paper, we propose a novel framework to control voice style in
prompt-based, controllable text-to-speech systems by leveraging textual
personas as voice style prompts. We present two persona rewriting strategies to
transform generic persona descriptions into speech-oriented prompts, enabling
fine-grained manipulation of prosodic attributes such as pitch, emotion, and
speaking rate. Experimental results demonstrate that our methods enhance the
naturalness, clarity, and consistency of synthesized speech. Finally, we
analyze implicit social biases introduced by LLM-based rewriting, with a focus
on gender. We underscore voice style as a crucial factor for persona-driven AI
dialogue systems.",2025-05-21,"Yejin Lee, Jaehoon Kang, Kyuhong Shim",http://arxiv.org/pdf/2505.17093v1,cs.CL
Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems,"Recent advances in large language models (LLMs) and multimodal LLMs (MLLMs)
have led to strong reasoning ability across a wide range of tasks. However,
their ability to perform mathematical reasoning from spoken input remains
underexplored. Prior studies on speech modality have mostly focused on factual
speech understanding or simple audio reasoning tasks, providing limited insight
into logical step-by-step reasoning, such as that required for mathematical
problem solving. To address this gap, we introduce Spoken Math Question
Answering (Spoken-MQA), a new benchmark designed to evaluate the mathematical
reasoning capabilities of speech-based models, including both cascade models
(ASR + LLMs) and end-to-end speech LLMs. Spoken-MQA covers a diverse set of
math problems, including pure arithmetic, single-step and multi-step contextual
reasoning, and knowledge-oriented reasoning problems, all presented in
unambiguous natural spoken language. Through extensive experiments, we find
that: (1) while some speech LLMs perform competitively on contextual reasoning
tasks involving basic arithmetic, they still struggle with direct arithmetic
problems; (2) current LLMs exhibit a strong bias toward symbolic mathematical
expressions written in LaTex and have difficulty interpreting verbalized
mathematical expressions; and (3) mathematical knowledge reasoning abilities
are significantly degraded in current speech LLMs.",2025-05-21,"Chengwei Wei, Bin Wang, Jung-jae Kim, Nancy F. Chen",http://arxiv.org/pdf/2505.15000v1,cs.CL
Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision,"Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs), often requiring robust multi step logical consistency. While
Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee
correctness, and improving reliability via extensive sampling is
computationally costly. This paper introduces the Energy Outcome Reward Model
(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy
Based Models (EBMs) to simplify the training of reward models by learning to
assign a scalar energy score to CoT solutions using only outcome labels,
thereby avoiding detailed annotations. It achieves this by interpreting
discriminator output logits as negative energies, effectively ranking
candidates where lower energy is assigned to solutions leading to correct final
outcomes implicitly favoring coherent reasoning. On mathematical benchmarks
(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with
Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively
leverages a given pool of candidate solutions to match or exceed the
performance of brute force sampling, thereby enhancing LLM reasoning outcome
reliability through its streamlined post hoc verification process.",2025-05-21,"Eric Hanchen Jiang, Haozheng Luo, Shengyuan Pang, Xiaomin Li, Zhenting Qi, Hengli Li, Cheng-Fu Yang, Zongyu Lin, Xinfeng Li, Hao Xu, Kai-Wei Chang, Ying Nian Wu",http://arxiv.org/pdf/2505.14999v1,cs.CL
MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision,"Multi-agent systems (MAS) leveraging the impressive capabilities of Large
Language Models (LLMs) hold significant potential for tackling complex tasks.
However, most current MAS depend on manually designed agent roles and
communication protocols. These manual designs often fail to align with the
underlying LLMs' strengths and struggle to adapt to novel tasks. Recent
automatic MAS approaches attempt to mitigate these limitations but typically
necessitate a validation set for tuning and yield static MAS designs lacking
adaptability during inference. We introduce MAS-ZERO, the first self-evolved,
inference-time framework for automatic MAS design. MAS-ZERO employs meta-level
design to iteratively generate, evaluate, and refine MAS configurations
tailored to each problem instance, without requiring a validation set.
Critically, it enables dynamic agent composition and problem decomposition
through meta-feedback on solvability and completeness. Experiments across math,
graduate-level QA, and software engineering benchmarks, using both
closed-source and open-source LLM backbones of varying sizes, demonstrate that
MAS-ZERO outperforms both manual and automatic MAS baselines, achieving a 7.44%
average accuracy improvement over the next strongest baseline while maintaining
cost-efficiency. These findings underscore the promise of meta-level
self-evolved design for creating effective and adaptive MAS.",2025-05-21,"Zixuan Ke, Austin Xu, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty",http://arxiv.org/pdf/2505.14996v2,cs.CL
Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models,"Information extraction (IE) plays a crucial role in natural language
processing (NLP) by converting unstructured text into structured knowledge.
Deploying computationally intensive large language models (LLMs) on
resource-constrained devices for information extraction is challenging,
particularly due to issues like hallucinations, limited context length, and
high latency-especially when handling diverse extraction schemas. To address
these challenges, we propose a two-stage information extraction approach
adapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching
(DLISC), which enhances both schema identification and schema-aware extraction
in terms of effectiveness and efficiency. In particular, DLISC adopts an
Identification LoRA module for retrieving the most relevant schemas to a given
query, and an Extraction LoRA module for performing information extraction
based on the previously selected schemas. To accelerate extraction inference,
Incremental Schema Caching is incorporated to reduce redundant computation,
substantially improving efficiency. Extensive experiments across multiple
information extraction datasets demonstrate notable improvements in both
effectiveness and efficiency.",2025-05-21,"Zhihao Wen, Sheng Liang, Yaxiong Wu, Yongyue Zhang, Yong Liu",http://arxiv.org/pdf/2505.14992v1,cs.CL
Language Specific Knowledge: Do Models Know Better in X than in English?,"Code-switching is a common phenomenon of alternating between different
languages in the same utterance, thought, or conversation. We posit that humans
code-switch because they feel more comfortable talking about certain topics and
domains in one language than another. With the rise of knowledge-intensive
language models, we ask ourselves the next, natural question: Could models hold
more knowledge on some topics in some language X? More importantly, could we
improve reasoning by changing the language that reasoning is performed in? We
coin the term Language Specific Knowledge (LSK) to represent this phenomenon.
As ethnic cultures tend to develop alongside different languages, we employ
culture-specific datasets (that contain knowledge about cultural and social
behavioral norms). We find that language models can perform better when using
chain-of-thought reasoning in some languages other than English, sometimes even
better in low-resource languages. Paired with previous works showing that
semantic similarity does not equate to representational similarity, we
hypothesize that culturally specific texts occur more abundantly in
corresponding languages, enabling specific knowledge to occur only in specific
""expert"" languages. Motivated by our initial results, we design a simple
methodology called LSKExtractor to benchmark the language-specific knowledge
present in a language model and, then, exploit it during inference. We show our
results on various models and datasets, showing an average relative improvement
of 10% in accuracy. Our research contributes to the open-source development of
language models that are inclusive and more aligned with the cultural and
linguistic contexts in which they are deployed.",2025-05-21,"Ishika Agarwal, Nimet Beyza Bozdag, Dilek Hakkani-Tür",http://arxiv.org/pdf/2505.14990v1,cs.CL
CRAFT: Training-Free Cascaded Retrieval for Tabular QA,"Table Question Answering (TQA) involves retrieving relevant tables from a
large corpus to answer natural language queries. Traditional dense retrieval
models, such as DTR and ColBERT, not only incur high computational costs for
large-scale retrieval tasks but also require retraining or fine-tuning on new
datasets, limiting their adaptability to evolving domains and knowledge. In
this work, we propose $\textbf{CRAFT}$, a cascaded retrieval approach that
first uses a sparse retrieval model to filter a subset of candidate tables
before applying more computationally expensive dense models and neural
re-rankers. Our approach achieves better retrieval performance than
state-of-the-art (SOTA) sparse, dense, and hybrid retrievers. We further
enhance table representations by generating table descriptions and titles using
Gemini Flash 1.5. End-to-end TQA results using various Large Language Models
(LLMs) on NQ-Tables, a subset of the Natural Questions Dataset, demonstrate
$\textbf{CRAFT}$ effectiveness.",2025-05-21,"Adarsh Singh, Kushal Raj Bhandari, Jianxi Gao, Soham Dan, Vivek Gupta",http://arxiv.org/pdf/2505.14984v1,cs.CL
Multimodal Cultural Safety: Evaluation Frameworks and Alignment Strategies,"Large vision-language models (LVLMs) are increasingly deployed in globally
distributed applications, such as tourism assistants, yet their ability to
produce culturally appropriate responses remains underexplored. Existing
multimodal safety benchmarks primarily focus on physical safety and overlook
violations rooted in cultural norms, which can result in symbolic harm. To
address this gap, we introduce CROSS, a benchmark designed to assess the
cultural safety reasoning capabilities of LVLMs. CROSS includes 1,284
multilingual visually grounded queries from 16 countries, three everyday
domains, and 14 languages, where cultural norm violations emerge only when
images are interpreted in context. We propose CROSS-Eval, an intercultural
theory-based framework that measures four key dimensions: cultural awareness,
norm education, compliance, and helpfulness. Using this framework, we evaluate
21 leading LVLMs, including mixture-of-experts models and reasoning models.
Results reveal significant cultural safety gaps: the best-performing model
achieves only 61.79% in awareness and 37.73% in compliance. While some
open-source models reach GPT-4o-level performance, they still fall notably
short of proprietary models. Our results further show that increasing reasoning
capacity improves cultural alignment but does not fully resolve the issue. To
improve model performance, we develop two enhancement strategies: supervised
fine-tuning with culturally grounded, open-ended data and preference tuning
with contrastive response pairs that highlight safe versus unsafe behaviors.
These methods substantially improve GPT-4o's cultural awareness (+60.14%) and
compliance (+55.2%), while preserving general multimodal capabilities with
minimal performance reduction on general multimodal understanding benchmarks.",2025-05-20,"Haoyi Qiu, Kung-Hsiang Huang, Ruichen Zheng, Jiao Sun, Nanyun Peng",http://arxiv.org/pdf/2505.14972v1,cs.CL
DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis,"Recent advancements in large language models (LLMs) have revolutionized
natural language processing (NLP) and expanded their applications across
diverse domains. However, despite their impressive capabilities, LLMs have been
shown to reflect and perpetuate harmful societal biases, including those based
on ethnicity, gender, and religion. A critical and underexplored issue is the
reinforcement of caste-based biases, particularly towards India's marginalized
caste groups such as Dalits and Shudras. In this paper, we address this gap by
proposing DECASTE, a novel, multi-dimensional framework designed to detect and
assess both implicit and explicit caste biases in LLMs. Our approach evaluates
caste fairness across four dimensions: socio-cultural, economic, educational,
and political, using a range of customized prompting strategies. By
benchmarking several state-of-the-art LLMs, we reveal that these models
systematically reinforce caste biases, with significant disparities observed in
the treatment of oppressed versus dominant caste groups. For example, bias
scores are notably elevated when comparing Dalits and Shudras with dominant
caste groups, reflecting societal prejudices that persist in model outputs.
These results expose the subtle yet pervasive caste biases in LLMs and
emphasize the need for more comprehensive and inclusive bias evaluation
methodologies that assess the potential risks of deploying such models in
real-world contexts.",2025-05-20,"Prashanth Vijayaraghavan, Soroush Vosoughi, Lamogha Chizor, Raya Horesh, Rogerio Abreu de Paula, Ehsan Degan, Vandana Mukherjee",http://arxiv.org/pdf/2505.14971v1,cs.CL
MedBrowseComp: Benchmarking Medical Deep Research and Computer Use,"Large language models (LLMs) are increasingly envisioned as decision-support
tools in clinical practice, yet safe clinical reasoning demands integrating
heterogeneous knowledge bases -- trials, primary studies, regulatory documents,
and cost data -- under strict accuracy constraints. Existing evaluations often
rely on synthetic prompts, reduce the task to single-hop factoid queries, or
conflate reasoning with open-ended generation, leaving their real-world utility
unclear. To close this gap, we present MedBrowseComp, the first benchmark that
systematically tests an agent's ability to reliably retrieve and synthesize
multi-hop medical facts from live, domain-specific knowledge bases.
MedBrowseComp contains more than 1,000 human-curated questions that mirror
clinical scenarios where practitioners must reconcile fragmented or conflicting
information to reach an up-to-date conclusion. Applying MedBrowseComp to
frontier agentic systems reveals performance shortfalls as low as ten percent,
exposing a critical gap between current LLM capabilities and the rigor demanded
in clinical settings. MedBrowseComp therefore offers a clear testbed for
reliable medical information seeking and sets concrete goals for future model
and toolchain upgrades. You can visit our project page at:
https://moreirap12.github.io/mbc-browse-app/",2025-05-20,"Shan Chen, Pedro Moreira, Yuxin Xiao, Sam Schmidgall, Jeremy Warner, Hugo Aerts, Thomas Hartvigsen, Jack Gallifant, Danielle S. Bitterman",http://arxiv.org/pdf/2505.14963v1,cs.CL
Large Language Models Implicitly Learn to See and Hear Just By Reading,"This paper presents a fascinating find: By training an auto-regressive LLM
model on text tokens, the text model inherently develops internally an ability
to understand images and audio, thereby developing the ability to see and hear
just by reading. Popular audio and visual LLM models fine-tune text LLM models
to give text output conditioned on images and audio embeddings. On the other
hand, our architecture takes in patches of images, audio waveforms or tokens as
input. It gives us the embeddings or category labels typical of a
classification pipeline. We show the generality of text weights in aiding audio
classification for datasets FSD-50K and GTZAN. Further, we show this working
for image classification on CIFAR-10 and Fashion-MNIST, as well on image
patches. This pushes the notion of text-LLMs learning powerful internal
circuits that can be utilized by activating necessary connections for various
applications rather than training models from scratch every single time.",2025-05-20,"Prateek Verma, Mert Pilanci",http://arxiv.org/pdf/2505.17091v1,cs.CL
"Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models","Large Language Models (LLMs) exhibit impressive capabilities, but remain
susceptible to a growing spectrum of safety risks, including jailbreaks, toxic
content, hallucinations, and bias. Existing defenses often address only a
single threat type or resort to rigid outright rejection, sacrificing user
experience and failing to generalize across diverse and novel attacks. This
paper introduces Adversarial Scenario Extrapolation (ASE), a novel
inference-time computation framework that leverages Chain-of-Thought (CoT)
reasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides
the LLM through a self-generative process of contemplating potential
adversarial scenarios and formulating defensive strategies before generating a
response to the user query. Comprehensive evaluation on four adversarial
benchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak
attack success rates and minimal toxicity, while slashing outright rejections
to <4%. ASE outperforms six state-of-the-art defenses in
robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and
4-10x lower bias scores. By transforming adversarial perception into an
intrinsic cognitive process, ASE sets a new paradigm for secure and natural
human-AI interaction.",2025-05-20,"Md Rafi Ur Rashid, Vishnu Asutosh Dasu, Ye Wang, Gang Tan, Shagufta Mehnaz",http://arxiv.org/pdf/2505.17089v1,cs.CL
"Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels","Although the context length of large language models (LLMs) has increased to
millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack
approaches has proven difficult. We argue that novels provide a case study of
subtle, complicated structure and long-range semantic dependencies often over
128k tokens in length. Inspired by work on computational novel analysis, we
release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's
ability to report plot summary, storyworld configuration, and elapsed narrative
time. We find that none of seven tested frontier LLMs retain stable
understanding beyond 64k tokens. Our results suggest language model developers
must look beyond ""lost in the middle"" benchmarks when evaluating model
performance in complex long-context scenarios. To aid in further development we
release the TLDM benchmark together with reference code and data.",2025-05-20,"Sil Hamilton, Rebecca M. M. Hicke, Matthew Wilkens, David Mimno",http://arxiv.org/pdf/2505.14925v1,cs.CL
Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications,"This study introduces a framework for evaluating consistency in large
language model (LLM) binary text classification, addressing the lack of
established reliability assessment methods. Adapting psychometric principles,
we determine sample size requirements, develop metrics for invalid responses,
and evaluate intra- and inter-rater reliability. Our case study examines
financial news sentiment classification across 14 LLMs (including
claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and
command-r-plus), with five replicates per model on 1,350 articles. Models
demonstrated high intra-rater consistency, achieving perfect agreement on
90-98% of examples, with minimal differences between expensive and economical
models from the same families. When validated against StockNewsAPI labels,
models achieved strong performance (accuracy 0.76-0.88), with smaller models
like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger
counterparts. All models performed at chance when predicting actual market
movements, indicating task constraints rather than model limitations. Our
framework provides systematic guidance for LLM selection, sample size planning,
and reliability assessment, enabling organizations to optimize resources for
classification tasks.",2025-05-20,"Fadel M. Megahed, Ying-Ju Chen, L. Allision Jones-Farmer, Younghwa Lee, Jiawei Brooke Wang, Inez M. Zwetsloot",http://arxiv.org/pdf/2505.14918v1,cs.CL
ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories,"Despite the many benefits of large language models (LLMs), they can also
cause harm, e.g., through automatic generation of misinformation, including
conspiracy theories. Moreover, LLMs can also ''disguise'' conspiracy theories
by altering characteristic textual features, e.g., by transforming their
typically strong negative emotions into a more positive tone. Although several
studies have proposed automated conspiracy theory detection methods, they are
usually trained using human-authored text, whose features can vary from
LLM-generated text. Furthermore, several conspiracy detection models, including
the previously proposed ConspEmoLLM, rely heavily on the typical emotional
features of human-authored conspiracy content. As such, intentionally disguised
content may evade detection. To combat such issues, we firstly developed an
augmented version of the ConDID conspiracy detection dataset, ConDID-v2, which
supplements human-authored conspiracy tweets with versions rewritten by an LLM
to reduce the negativity of their original sentiment. The quality of the
rewritten tweets was verified by combining human and LLM-based assessment. We
subsequently used ConDID-v2 to train ConspEmoLLM-v2, an enhanced version of
ConspEmoLLM. Experimental results demonstrate that ConspEmoLLM-v2 retains or
exceeds the performance of ConspEmoLLM on the original human-authored content
in ConDID, and considerably outperforms both ConspEmoLLM and several other
baselines when applied to sentiment-transformed tweets in ConDID-v2. The
project will be available at https://github.com/lzw108/ConspEmoLLM.",2025-05-20,"Zhiwei Liu, Paul Thompson, Jiaqi Rong, Sophia Ananiadou",http://arxiv.org/pdf/2505.14917v1,cs.CL
TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis,"Customizable multilingual zero-shot singing voice synthesis (SVS) has various
potential applications in music composition and short video dubbing. However,
existing SVS models overly depend on phoneme and note boundary annotations,
limiting their robustness in zero-shot scenarios and producing poor transitions
between phonemes and notes. Moreover, they also lack effective multi-level
style control via diverse prompts. To overcome these challenges, we introduce
TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer
and style control based on various prompts. TCSinger 2 mainly includes three
key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration,
extends content embedding, and applies masking to the boundaries to enable
smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to
extract aligned representations from singing, speech, and textual prompts. 3)
Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision,
enhancing both the synthesis quality and style modeling of the generated
singing voice. Experimental results show that TCSinger 2 outperforms baseline
models in both subjective and objective metrics across multiple related tasks.",2025-05-20,"Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, Zhou Zhao",http://arxiv.org/pdf/2505.14910v1,cs.CL
Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain,"Knowledge understanding is a foundational part of envisioned 6G networks to
advance network intelligence and AI-native network architectures. In this
paradigm, information extraction plays a pivotal role in transforming
fragmented telecom knowledge into well-structured formats, empowering diverse
AI models to better understand network terminologies. This work proposes a
novel language model-based information extraction technique, aiming to extract
structured entities from the telecom context. The proposed telecom structured
entity extraction (TeleSEE) technique applies a token-efficient representation
method to predict entity types and attribute keys, aiming to save the number of
output tokens and improve prediction accuracy. Meanwhile, TeleSEE involves a
hierarchical parallel decoding method, improving the standard encoder-decoder
architecture by integrating additional prompting and decoding strategies into
entity extraction tasks. In addition, to better evaluate the performance of the
proposed technique in the telecom domain, we further designed a dataset named
6GTech, including 2390 sentences and 23747 words from more than 100 6G-related
technical publications. Finally, the experiment shows that the proposed TeleSEE
method achieves higher accuracy than other baseline techniques, and also
presents 5 to 9 times higher sample processing speed.",2025-05-20,"Ye Yuan, Haolun Wu, Hao Zhou, Xue Liu, Hao Chen, Yan Xin, Jianzhong, Zhang",http://arxiv.org/pdf/2505.14906v1,cs.CL
Concept Incongruence: An Exploration of Time and Death in Role Playing,"Consider this prompt ""Draw a unicorn with two horns"". Should large language
models (LLMs) recognize that a unicorn has only one horn by definition and ask
users for clarifications, or proceed to generate something anyway? We introduce
concept incongruence to capture such phenomena where concept boundaries clash
with each other, either in user prompts or in model representations, often
leading to under-specified or mis-specified behaviors. In this work, we take
the first step towards defining and analyzing model behavior under concept
incongruence. Focusing on temporal boundaries in the Role-Play setting, we
propose three behavioral metrics--abstention rate, conditional accuracy, and
answer rate--to quantify model behavior under incongruence due to the role's
death. We show that models fail to abstain after death and suffer from an
accuracy drop compared to the Non-Role-Play setting. Through probing
experiments, we identify two main causes: (i) unreliable encoding of the
""death"" state across different years, leading to unsatisfactory abstention
behavior, and (ii) role playing causes shifts in the model's temporal
representations, resulting in accuracy drops. We leverage these insights to
improve consistency in the model's abstention and answer behaviors. Our
findings suggest that concept incongruence leads to unexpected model behaviors
and point to future directions on improving model behavior under concept
incongruence.",2025-05-20,"Xiaoyan Bai, Ike Peng, Aditya Singh, Chenhao Tan",http://arxiv.org/pdf/2505.14905v1,cs.CL
"Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic Planning with LLMs","While large language models (LLMs) have shown great potential across various
domains, their applications in robotics remain largely limited to static,
prompt-based behaviors and still face challenges in handling complex tasks
under zero-shot or few-shot settings. Inspired by human metacognitive learning
and creative problem-solving, we address this limitation by exploring a
fundamental research question: Can LLMs be empowered with metacognitive
capabilities to reason, reflect, and create, thereby enhancing their ability to
perform robotic tasks with minimal demonstrations? In this paper, we present an
early-stage framework that integrates metacognitive learning into LLM-powered
multi-robot collaboration. The proposed framework equips the LLM-powered
robotic agents with a skill decomposition and self-reflection mechanism that
identifies modular skills from prior tasks, reflects on failures in unseen task
scenarios, and synthesizes effective new solutions. Experimental results show
that our metacognitive-learning-empowered LLM framework significantly
outperforms existing baselines. Moreover, we observe that the framework is
capable of generating solutions that differ from the ground truth yet still
successfully complete the tasks. These exciting findings support our hypothesis
that metacognitive learning can foster creativity in robotic planning.",2025-05-20,"Wenjie Lin, Jin Wei-Kocsis",http://arxiv.org/pdf/2505.14899v1,cs.CL
"From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data","Recent progress in speech recognition has relied on models trained on vast
amounts of labeled data. However, classroom Automatic Speech Recognition (ASR)
faces the real-world challenge of abundant weak transcripts paired with only a
small amount of accurate, gold-standard data. In such low-resource settings,
high transcription costs make re-transcription impractical. To address this, we
ask: what is the best approach when abundant inexpensive weak transcripts
coexist with limited gold-standard data, as is the case for classroom speech
data? We propose Weakly Supervised Pretraining (WSP), a two-step process where
models are first pretrained on weak transcripts in a supervised manner, and
then fine-tuned on accurate data. Our results, based on both synthetic and real
weak transcripts, show that WSP outperforms alternative methods, establishing
it as an effective training methodology for low-resource ASR in real-world
scenarios.",2025-05-20,"Ahmed Adel Attia, Dorottya Demszky, Jing Liu, Carol Espy-Wilson",http://arxiv.org/pdf/2505.17088v1,cs.CL
Informatics for Food Processing,"This chapter explores the evolution, classification, and health implications
of food processing, while emphasizing the transformative role of machine
learning, artificial intelligence (AI), and data science in advancing food
informatics. It begins with a historical overview and a critical review of
traditional classification frameworks such as NOVA, Nutri-Score, and SIGA,
highlighting their strengths and limitations, particularly the subjectivity and
reproducibility challenges that hinder epidemiological research and public
policy. To address these issues, the chapter presents novel computational
approaches, including FoodProX, a random forest model trained on nutrient
composition data to infer processing levels and generate a continuous FPro
score. It also explores how large language models like BERT and BioBERT can
semantically embed food descriptions and ingredient lists for predictive tasks,
even in the presence of missing data. A key contribution of the chapter is a
novel case study using the Open Food Facts database, showcasing how multimodal
AI models can integrate structured and unstructured data to classify foods at
scale, offering a new paradigm for food processing assessment in public health
and research.",2025-05-20,"Gordana Ispirova, Michael Sebek, Giulia Menichetti",http://arxiv.org/pdf/2505.17087v1,cs.CL
Scaling Laws for State Dynamics in Large Language Models,"Large Language Models (LLMs) are increasingly used in tasks requiring
internal state tracking, yet their ability to model state transition dynamics
remains poorly understood. We evaluate how well LLMs capture deterministic
state dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and
Complex Text Games, each formalizable as a finite-state system. Across tasks,
we find that next-state prediction accuracy degrades with increasing
state-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in
low-complexity settings but drops below 30% when the number of boxes or states
exceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50%
accuracy when the number of states is > 10 and transitions are < 30. Through
activation patching, we identify attention heads responsible for propagating
state information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10,
11, 12, and 14. While these heads successfully move relevant state features,
action information is not reliably routed to the final token, indicating weak
joint state-action reasoning. Our results suggest that state tracking in LLMs
emerges from distributed interactions of next-token heads rather than explicit
symbolic computation.",2025-05-20,"Jacob X Li, Shreyas S Raman, Jessica Wan, Fahad Samman, Jazlyn Lin",http://arxiv.org/pdf/2505.14892v1,cs.CL
In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties,"Human listeners readily adjust to unfamiliar speakers and language varieties
through exposure, but do these adaptation benefits extend to state-of-the-art
spoken language models? We introduce a scalable framework that allows for
in-context learning (ICL) in Phi-4 Multimodal using interleaved task prompts
and audio-text pairs, and find that as few as 12 example utterances (~50
seconds) at inference time reduce word error rates by a relative 19.7% (1.2
pp.) on average across diverse English corpora. These improvements are most
pronounced in low-resource varieties, when the context and target speaker
match, and when more examples are provided--though scaling our procedure yields
diminishing marginal returns to context length. Overall, we find that our novel
ICL adaptation scheme (1) reveals a similar performance profile to human
listeners, and (2) demonstrates consistent improvements to automatic speech
recognition (ASR) robustness across diverse speakers and language backgrounds.
While adaptation succeeds broadly, significant gaps remain for certain
varieties, revealing where current models still fall short of human
flexibility. We release our prompts and code on GitHub.",2025-05-20,"Nathan Roll, Calbert Graham, Yuka Tatsumi, Kim Tien Nguyen, Meghan Sumner, Dan Jurafsky",http://arxiv.org/pdf/2505.14887v1,cs.CL
Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters,"Winning competitive debates requires sophisticated reasoning and argument
skills. There are unique challenges in the competitive debate: (1) The time
constraints force debaters to make strategic choices about which points to
pursue rather than covering all possible arguments; (2) The persuasiveness of
the debate relies on the back-and-forth interaction between arguments, which a
single final game status cannot evaluate. To address these challenges, we
propose TreeDebater, a novel debate framework that excels in competitive
debate. We introduce two tree structures: the Rehearsal Tree and Debate Flow
Tree. The Rehearsal Tree anticipates the attack and defenses to evaluate the
strength of the claim, while the Debate Flow Tree tracks the debate status to
identify the active actions. TreeDebater allocates its time budget among
candidate actions and uses the speech time controller and feedback from the
simulated audience to revise its statement. The human evaluation on both the
stage-level and the debate-level comparison shows that our TreeDebater
outperforms the state-of-the-art multi-agent debate system. Further
investigation shows that TreeDebater shows better strategies in limiting time
to important debate actions, aligning with the strategies of human debate
experts.",2025-05-20,"Danqing Wang, Zhuorui Ye, Xinran Zhao, Fei Fang, Lei Li",http://arxiv.org/pdf/2505.14886v1,cs.CL
Incorporating Token Usage into Prompting Strategy Evaluation,"In recent years, large language models have demonstrated remarkable
performance across diverse tasks. However, their task effectiveness is heavily
dependent on the prompting strategy used to elicit output, which can vary
widely in both performance and token usage. While task performance is often
used to determine prompting strategy success, we argue that
efficiency--balancing performance and token usage--can be a more practical
metric for real-world utility. To enable this, we propose Big-$O_{tok}$, a
theoretical framework for describing the token usage growth of prompting
strategies, and analyze Token Cost, an empirical measure of tokens per
performance. We apply these to several common prompting strategies and find
that increased token usage leads to drastically diminishing performance
returns. Our results validate the Big-$O_{tok}$ analyses and reinforce the need
for efficiency-aware evaluations.",2025-05-20,"Chris Sypherd, Sergei Petrov, Sonny George, Vaishak Belle",http://arxiv.org/pdf/2505.14880v1,cs.CL
Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages,"Automatic speech recognition (ASR) for dysarthric speech remains challenging
due to data scarcity, particularly in non-English languages. To address this,
we fine-tune a voice conversion model on English dysarthric speech (UASpeech)
to encode both speaker characteristics and prosodic distortions, then apply it
to convert healthy non-English speech (FLEURS) into non-English dysarthric-like
speech. The generated data is then used to fine-tune a multilingual ASR model,
Massively Multilingual Speech (MMS), for improved dysarthric speech
recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE
(Tamil) demonstrates that VC with both speaker and prosody conversion
significantly outperforms the off-the-shelf MMS performance and conventional
augmentation techniques such as speed and tempo perturbation. Objective and
subjective analyses of the generated data further confirm that the generated
speech simulates dysarthric characteristics.",2025-05-20,"Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea Pérez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar Nöth, David R. Mortensen",http://arxiv.org/pdf/2505.14874v1,cs.CL
Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models,"The efficient implementation of large language models (LLMs) is crucial for
deployment on resource-constrained devices. Low-rank tensor compression
techniques, such as tensor-train (TT) networks, have been widely studied for
over-parameterized neural networks. However, their applications to compress
pre-trained large language models (LLMs) for downstream tasks (post-training)
remains challenging due to the high-rank nature of pre-trained LLMs and the
lack of access to pretraining data. In this study, we investigate low-rank
tensorized LLMs during fine-tuning and propose sparse augmented tensor networks
(Saten) to enhance their performance. The proposed Saten framework enables full
model compression. Experimental results demonstrate that Saten enhances both
accuracy and compression efficiency in tensorized language models, achieving
state-of-the-art performance.",2025-05-20,"Ryan Solgi, Kai Zhen, Rupak Vignesh Swaminathan, Nathan Susanj, Athanasios Mouchtaris, Siegfried Kunzmann, Zheng Zhang",http://arxiv.org/pdf/2505.14871v1,cs.CL
EasyMath: A 0-shot Math Benchmark for SLMs,"EasyMath is a compact benchmark for practical math reasoning in small
language models. It covers thirteen categories, from basic arithmetic and order
of operations to word problems, algebraic expressions, edge cases, and omits
specialist topics. We tested 23 models (14M to 4B parameters) using exact,
numerical, and symbolic checks on free-form answers in a zero-shot setting.
Accuracy rises with size and training, chain-of-thought adds modest gains, and
consistency improves at scale.",2025-05-20,"Drishya Karki, Michiel Kamphuis, Angelecia Frey",http://arxiv.org/pdf/2505.14852v1,cs.CL
MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation,"We present MAATS, a Multi Agent Automated Translation System that leverages
the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal
for error detection and refinement. MAATS employs multiple specialized AI
agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,
Style, Terminology), followed by a synthesis agent that integrates the
annotations to iteratively refine translations. This design contrasts with
conventional single-agent methods that rely on self-correction.
  Evaluated across diverse language pairs and Large Language Models (LLMs),
MAATS outperforms zero-shot and single-agent baselines with statistically
significant gains in both automatic metrics and human assessments. It excels
particularly in semantic accuracy, locale adaptation, and linguistically
distant language pairs. Qualitative analysis highlights its strengths in
multi-layered error diagnosis, omission detection across perspectives, and
context-aware refinement. By aligning modular agent roles with interpretable
MQM dimensions, MAATS narrows the gap between black-box LLMs and human
translation workflows, shifting focus from surface fluency to deeper semantic
and contextual fidelity.",2025-05-20,"Xi Wang, Jiaqian Hu, Safinah Ali",http://arxiv.org/pdf/2505.14848v1,cs.CL
SEPS: A Separability Measure for Robust Unlearning in LLMs,"Machine unlearning aims to selectively remove targeted knowledge from Large
Language Models (LLMs), ensuring they forget specified content while retaining
essential information. Existing unlearning metrics assess whether a model
correctly answers retain queries and rejects forget queries, but they fail to
capture real-world scenarios where forget queries rarely appear in isolation.
In fact, forget and retain queries often coexist within the same prompt, making
mixed-query evaluation crucial.
  We introduce SEPS, an evaluation framework that explicitly measures a model's
ability to both forget and retain information within a single prompt. Through
extensive experiments across three benchmarks, we identify two key failure
modes in existing unlearning methods: (1) untargeted unlearning
indiscriminately erases both forget and retain content once a forget query
appears, and (2) targeted unlearning overfits to single-query scenarios,
leading to catastrophic failures when handling multiple queries. To address
these issues, we propose Mixed Prompt (MP) unlearning, a strategy that
integrates both forget and retain queries into a unified training objective.
Our approach significantly improves unlearning effectiveness, demonstrating
robustness even in complex settings with up to eight mixed forget and retain
queries in a single prompt.",2025-05-20,"Wonje Jeung, Sangyeon Yoon, Albert No",http://arxiv.org/pdf/2505.14832v1,cs.CL
Text Generation Beyond Discrete Token Sampling,"In standard autoregressive generation, an LLM predicts the next-token
distribution, samples a discrete token, and then discards the distribution,
passing only the sampled token as new input. To preserve this distribution's
rich information, we propose Mixture of Inputs (MoI), a training-free method
for autoregressive generation. After generating a token following the standard
paradigm, we construct a new input that blends the generated discrete token
with the previously discarded token distribution. Specifically, we employ a
Bayesian estimation method that treats the token distribution as the prior, the
sampled token as the observation, and replaces the conventional one-hot vector
with the continuous posterior expectation as the new model input. MoI allows
the model to maintain a richer internal representation throughout the
generation process, resulting in improved text quality and reasoning
capabilities. On mathematical reasoning, code generation, and PhD-level QA
tasks, MoI consistently improves performance across multiple models including
QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional
training and negligible computational overhead.",2025-05-20,"Yufan Zhuang, Liyuan Liu, Chandan Singh, Jingbo Shang, Jianfeng Gao",http://arxiv.org/pdf/2505.14827v1,cs.CL
FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain,"Supervised fine-tuning (SFT) is a standard approach to adapting large
language models (LLMs) to new domains. In this work, we improve the statistical
efficiency of SFT by selecting an informative subset of training examples.
Specifically, for a fixed budget of training examples, which determines the
computational cost of fine-tuning, we determine the most informative ones. The
key idea in our method is to select examples that maximize information gain,
measured by the Hessian of the log-likelihood of the LLM. We approximate it
efficiently by linearizing the LLM at the last layer using multinomial logistic
regression models. Our approach is computationally efficient, analyzable, and
performs well empirically. We demonstrate this on several problems, and back
our claims with both quantitative results and an LLM evaluation.",2025-05-20,"Rohan Deb, Kiran Thekumparampil, Kousha Kalantari, Gaurush Hiranandani, Shoham Sabach, Branislav Kveton",http://arxiv.org/pdf/2505.14826v1,cs.CL
Tracing Multilingual Factual Knowledge Acquisition in Pretraining,"Large Language Models (LLMs) are capable of recalling multilingual factual
knowledge present in their pretraining data. However, most studies evaluate
only the final model, leaving the development of factual recall and
crosslingual consistency throughout pretraining largely unexplored. In this
work, we trace how factual recall and crosslingual consistency evolve during
pretraining, focusing on OLMo-7B as a case study. We find that both accuracy
and consistency improve over time for most languages. We show that this
improvement is primarily driven by the fact frequency in the pretraining
corpus: more frequent facts are more likely to be recalled correctly,
regardless of language. Yet, some low-frequency facts in non-English languages
can still be correctly recalled. Our analysis reveals that these instances
largely benefit from crosslingual transfer of their English counterparts -- an
effect that emerges predominantly in the early stages of pretraining. We
pinpoint two distinct pathways through which multilingual factual knowledge
acquisition occurs: (1) frequency-driven learning, which is dominant and
language-agnostic, and (2) crosslingual transfer, which is limited in scale and
typically constrained to relation types involving named entities. We release
our code and data to facilitate further research at
https://github.com/cisnlp/multilingual-fact-tracing.",2025-05-20,"Yihong Liu, Mingyang Wang, Amir Hossein Kargaran, Felicia Körner, Ercong Nie, Barbara Plank, François Yvon, Hinrich Schütze",http://arxiv.org/pdf/2505.14824v1,cs.CL
Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization,"Large Language Models (LLMs) have demonstrated remarkable versatility, due to
the lack of factual knowledge, their application to Question Answering (QA)
tasks remains hindered by hallucination.
  While Retrieval-Augmented Generation mitigates these issues by integrating
external knowledge, existing approaches rely heavily on in-context learning,
whose performance is constrained by the fundamental reasoning capabilities of
LLMs.
  In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex
Question Answering, comprising a planner that decomposes questions into a
directed acyclic graph of subquestions and a worker that resolves questions via
retrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy
Gradient Optimization), a novel reinforcement learning method that replaces
traditional policy gradient updates with Maximum Likelihood Estimation (MLE) by
sampling trajectories from an asymptotically optimal policy. MyGO eliminates
the need for gradient rescaling and reference models, ensuring stable and
efficient training.
  Empirical results across multiple datasets demonstrate the effectiveness of
Mujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a
scalable and resource-efficient solution for complex QA tasks.",2025-05-20,"Yihong Wu, Liheng Ma, Muzhi Li, Jiaming Zhou, Jianye Hao, Ho-fung Leung, Irwin King, Yingxue Zhang, Jian-Yun Nie",http://arxiv.org/pdf/2505.17086v1,cs.CL
WebNovelBench: Placing LLM Novelists on the Web Novel Distribution,"Robustly evaluating the long-form storytelling capabilities of Large Language
Models (LLMs) remains a significant challenge, as existing benchmarks often
lack the necessary scale, diversity, or objective measures. To address this, we
introduce WebNovelBench, a novel benchmark specifically designed for evaluating
long-form novel generation. WebNovelBench leverages a large-scale dataset of
over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story
generation task. We propose a multi-faceted framework encompassing eight
narrative quality dimensions, assessed automatically via an LLM-as-Judge
approach. Scores are aggregated using Principal Component Analysis and mapped
to a percentile rank against human-authored works. Our experiments demonstrate
that WebNovelBench effectively differentiates between human-written
masterpieces, popular web novels, and LLM-generated content. We provide a
comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling
abilities and offering insights for future development. This benchmark provides
a scalable, replicable, and data-driven methodology for assessing and advancing
LLM-driven narrative generation.",2025-05-20,"Leon Lin, Jun Zheng, Haidong Wang",http://arxiv.org/pdf/2505.14818v1,cs.CL
"Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes","Reasoning language models (RLMs) excel at complex tasks by leveraging a
chain-of-thought process to generate structured intermediate steps. However,
language mixing, i.e., reasoning steps containing tokens from languages other
than the prompt, has been observed in their outputs and shown to affect
performance, though its impact remains debated. We present the first systematic
study of language mixing in RLMs, examining its patterns, impact, and internal
causes across 15 languages, 7 task difficulty levels, and 18 subject areas, and
show how all three factors influence language mixing. Moreover, we demonstrate
that the choice of reasoning language significantly affects performance:
forcing models to reason in Latin or Han scripts via constrained decoding
notably improves accuracy. Finally, we show that the script composition of
reasoning traces closely aligns with that of the model's internal
representations, indicating that language mixing reflects latent processing
preferences in RLMs. Our findings provide actionable insights for optimizing
multilingual reasoning and open new directions for controlling reasoning
languages to build more interpretable and adaptable RLMs.",2025-05-20,"Mingyang Wang, Lukas Lange, Heike Adel, Yunpu Ma, Jannik Strötgen, Hinrich Schütze",http://arxiv.org/pdf/2505.14815v1,cs.CL
GraphemeAug: A Systematic Approach to Synthesized Hard Negative Keyword Spotting Examples,"Spoken Keyword Spotting (KWS) is the task of distinguishing between the
presence and absence of a keyword in audio. The accuracy of a KWS model hinges
on its ability to correctly classify examples close to the keyword and
non-keyword boundary. These boundary examples are often scarce in training
data, limiting model performance. In this paper, we propose a method to
systematically generate adversarial examples close to the decision boundary by
making insertion/deletion/substitution edits on the keyword's graphemes. We
evaluate this technique on held-out data for a popular keyword and show that
the technique improves AUC on a dataset of synthetic hard negatives by 61%
while maintaining quality on positives and ambient negative audio data.",2025-05-20,"Harry Zhang, Kurt Partridge, Pai Zhu, Neng Chen, Hyun Jin Park, Dhruuv Agarwal, Quan Wang",http://arxiv.org/pdf/2505.14814v2,cs.CL
"Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models","Instruction-following is essential for aligning large language models (LLMs)
with user intent. While recent reasoning-oriented models exhibit impressive
performance on complex mathematical problems, their ability to adhere to
natural language instructions remains underexplored. In this work, we introduce
MathIF, a dedicated benchmark for evaluating instruction-following in
mathematical reasoning tasks. Our empirical analysis reveals a consistent
tension between scaling up reasoning capacity and maintaining controllability,
as models that reason more effectively often struggle to comply with user
directives. We find that models tuned on distilled long chains-of-thought or
trained with reasoning-oriented reinforcement learning often degrade in
instruction adherence, especially when generation length increases.
Furthermore, we show that even simple interventions can partially recover
obedience, though at the cost of reasoning performance. These findings
highlight a fundamental tension in current LLM training paradigms and motivate
the need for more instruction-aware reasoning models. We release the code and
data at https://github.com/TingchenFu/MathIF.",2025-05-20,"Tingchen Fu, Jiawei Gu, Yafu Li, Xiaoye Qu, Yu Cheng",http://arxiv.org/pdf/2505.14810v2,cs.CL
Automated Journalistic Questions: A New Method for Extracting 5W1H in French,"The 5W1H questions -- who, what, when, where, why and how -- are commonly
used in journalism to ensure that an article describes events clearly and
systematically. Answering them is a crucial prerequisites for tasks such as
summarization, clustering, and news aggregation. In this paper, we design the
first automated extraction pipeline to get 5W1H information from French news
articles. To evaluate the performance of our algo- rithm, we also create a
corpus of 250 Quebec news articles with 5W1H answers marked by four human
annotators. Our results demonstrate that our pipeline performs as well in this
task as the large language model GPT-4o.",2025-05-20,"Richard Khoury, Maxence Verhaverbeke, Julie A. Gramaccia",http://arxiv.org/pdf/2505.14804v1,cs.CL
Language Models use Lookbacks to Track Beliefs,"How do language models (LMs) represent characters' beliefs, especially when
those beliefs may differ from reality? This question lies at the heart of
understanding the Theory of Mind (ToM) capabilities of LMs. We analyze
Llama-3-70B-Instruct's ability to reason about characters' beliefs using causal
mediation and abstraction. We construct a dataset that consists of simple
stories where two characters each separately change the state of two objects,
potentially unaware of each other's actions. Our investigation uncovered a
pervasive algorithmic pattern that we call a lookback mechanism, which enables
the LM to recall important information when it becomes necessary. The LM binds
each character-object-state triple together by co-locating reference
information about them, represented as their Ordering IDs (OIs) in low rank
subspaces of the state token's residual stream. When asked about a character's
beliefs regarding the state of an object, the binding lookback retrieves the
corresponding state OI and then an answer lookback retrieves the state token.
When we introduce text specifying that one character is (not) visible to the
other, we find that the LM first generates a visibility ID encoding the
relation between the observing and the observed character OIs. In a visibility
lookback, this ID is used to retrieve information about the observed character
and update the observing character's beliefs. Our work provides insights into
the LM's belief tracking mechanisms, taking a step toward reverse-engineering
ToM reasoning in LMs.",2025-05-20,"Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma, Christoph Riedl, Yonatan Belinkov, Tamar Rott Shaham, David Bau, Atticus Geiger",http://arxiv.org/pdf/2505.14685v1,cs.CL
Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning,"Large language models (LLMs) have achieved remarkable progress on
mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.",2025-05-20,"Haolei Xu, Yuchen Yan, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Shengpei Jiang, Kaitao Song, Weiming Lu, Jun Xiao, Yueting Zhuang",http://arxiv.org/pdf/2505.14684v2,cs.CL
Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training,"Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.",2025-05-20,"Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu",http://arxiv.org/pdf/2505.14681v1,cs.CL
NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search,"Generative AI search is reshaping information retrieval by offering
end-to-end answers to complex queries, reducing users' reliance on manually
browsing and summarizing multiple web pages. However, while this paradigm
enhances convenience, it disrupts the feedback-driven improvement loop that has
historically powered the evolution of traditional Web search. Web search can
continuously improve their ranking models by collecting large-scale,
fine-grained user feedback (e.g., clicks, dwell time) at the document level. In
contrast, generative AI search operates through a much longer search pipeline,
spanning query decomposition, document retrieval, and answer generation, yet
typically receives only coarse-grained feedback on the final answer. This
introduces a feedback loop disconnect, where user feedback for the final output
cannot be effectively mapped back to specific system components, making it
difficult to improve each intermediate stage and sustain the feedback loop. In
this paper, we envision NExT-Search, a next-generation paradigm designed to
reintroduce fine-grained, process-level feedback into generative AI search.
NExT-Search integrates two complementary modes: User Debug Mode, which allows
engaged users to intervene at key stages; and Shadow User Mode, where a
personalized user agent simulates user preferences and provides AI-assisted
feedback for less interactive users. Furthermore, we envision how these
feedback signals can be leveraged through online adaptation, which refines
current search outputs in real-time, and offline update, which aggregates
interaction logs to periodically fine-tune query decomposition, retrieval, and
generation models. By restoring human control over key stages of the generative
AI search pipeline, we believe NExT-Search offers a promising direction for
building feedback-rich AI search systems that can evolve continuously alongside
human feedback.",2025-05-20,"Sunhao Dai, Wenjie Wang, Liang Pang, Jun Xu, See-Kiong Ng, Ji-Rong Wen, Tat-Seng Chua",http://arxiv.org/pdf/2505.14680v1,cs.CL
"UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models","Lifelong learning enables large language models (LLMs) to adapt to evolving
information by continually updating their internal knowledge. An ideal system
should support efficient, wide-ranging updates while preserving existing
capabilities and ensuring reliable deployment. Model editing stands out as a
promising solution for this goal, offering a focused and efficient way to
revise a model's internal knowledge. Although recent paradigms have made
notable progress, they often struggle to meet the demands of practical lifelong
adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally
new editing solution that is training-, subject- and memory-free, making it
particularly well-suited for ultra-scalable, real-world lifelong model editing.
ULTRAEDIT performs editing through a self-contained process that relies solely
on lightweight linear algebra operations to compute parameter shifts, enabling
fast and consistent parameter modifications with minimal overhead. To improve
scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization
strategy that continuously updates feature statistics across turns, allowing it
to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT
achieves editing speeds over 7x faster than the previous state-of-the-art
method-which was also the fastest known approach-while consuming less than 1/3
the VRAM, making it the only method currently capable of editing a 7B LLM on a
24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest
dataset in the field to date, with over 2M editing pairs-and demonstrate that
our method supports up to 1M edits while maintaining high accuracy.
Comprehensive experiments on four datasets and six models show that ULTRAEDIT
consistently achieves superior performance across diverse model editing
scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.",2025-05-20,"Xiaojie Gu, Guangxu Chen, Jungang Li, Jia-Chen Gu, Xuming Hu, Kai Zhang",http://arxiv.org/pdf/2505.14679v1,cs.CL
Reward Reasoning Model,"Reward models play a critical role in guiding large language models toward
outputs that align with human expectations. However, an open challenge remains
in effectively utilizing test-time compute to enhance reward model performance.
In this work, we introduce Reward Reasoning Models (RRMs), which are
specifically designed to execute a deliberate reasoning process before
generating final rewards. Through chain-of-thought reasoning, RRMs leverage
additional test-time compute for complex queries where appropriate rewards are
not immediately apparent. To develop RRMs, we implement a reinforcement
learning framework that fosters self-evolved reward reasoning capabilities
without requiring explicit reasoning traces as training data. Experimental
results demonstrate that RRMs achieve superior performance on reward modeling
benchmarks across diverse domains. Notably, we show that RRMs can adaptively
exploit test-time compute to further improve reward accuracy. The pretrained
reward reasoning models are available at
https://huggingface.co/Reward-Reasoning.",2025-05-20,"Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, Furu Wei",http://arxiv.org/pdf/2505.14674v1,cs.CL
ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions,"Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts to enhance the proactive capabilities of LLM agents.
ContextAgent first extracts multi-dimensional contexts from massive sensory
perceptions on wearables (e.g., video and audio) to understand user intentions.
ContextAgent then leverages the sensory contexts and the persona contexts from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants.",2025-05-20,"Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, Zhenyu Yan",http://arxiv.org/pdf/2505.14668v1,cs.CL
SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment,"Large Reasoning Models (LRMs) have become powerful tools for complex problem
solving, but their structured reasoning pathways can lead to unsafe outputs
when exposed to harmful prompts. Existing safety alignment methods reduce
harmful outputs but can degrade reasoning depth, leading to significant
trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated
jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight
alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at
the start of their reasoning, in response to harmful prompts, while leaving the
rest of the reasoning process unsupervised. Empirical results across multiple
benchmarks indicate that SAFEPATH effectively reduces harmful outputs while
maintaining reasoning performance. Specifically, SAFEPATH reduces harmful
responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the
DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than
Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot
variant that requires no fine-tuning. In addition, we provide a comprehensive
analysis of how existing methods in LLMs generalize, or fail, when applied to
reasoning-centric models, revealing critical gaps and new directions for safer
AI.",2025-05-20,"Wonje Jeung, Sangyeon Yoon, Minsuk Kahng, Albert No",http://arxiv.org/pdf/2505.14667v1,cs.CL
EmoGist: Efficient In-Context Learning for Visual Emotion Understanding,"In this paper, we introduce EmoGist, a training-free, in-context learning
method for performing visual emotion classification with LVLMs. The key
intuition of our approach is that context-dependent definition of emotion
labels could allow more accurate predictions of emotions, as the ways in which
emotions manifest within images are highly context dependent and nuanced.
EmoGist pre-generates multiple explanations of emotion labels, by analyzing the
clusters of example images belonging to each category. At test time, we
retrieve a version of explanation based on embedding similarity, and feed it to
a fast VLM for classification. Through our experiments, we show that EmoGist
allows up to 13 points improvement in micro F1 scores with the multi-label
Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.",2025-05-20,"Ronald Seoh, Dan Goldwasser",http://arxiv.org/pdf/2505.14660v1,cs.CL
Beyond Words: Multimodal LLM Knows When to Speak,"While large language model (LLM)-based chatbots have demonstrated strong
capabilities in generating coherent and contextually relevant responses, they
often struggle with understanding when to speak, particularly in delivering
brief, timely reactions during ongoing conversations. This limitation arises
largely from their reliance on text input, lacking the rich contextual cues in
real-world human dialogue. In this work, we focus on real-time prediction of
response types, with an emphasis on short, reactive utterances that depend on
subtle, multimodal signals across vision, audio, and text. To support this, we
introduce a new multimodal dataset constructed from real-world conversational
videos, containing temporally aligned visual, auditory, and textual streams.
This dataset enables fine-grained modeling of response timing in dyadic
interactions. Building on this dataset, we propose MM-When2Speak, a multimodal
LLM-based model that adaptively integrates visual, auditory, and textual
context to predict when a response should occur, and what type of response is
appropriate. Experiments show that MM-When2Speak significantly outperforms
state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x
improvement in response timing accuracy over leading commercial LLMs. These
results underscore the importance of multimodal inputs for producing timely,
natural, and engaging conversational AI.",2025-05-20,"Zikai Liao, Yi Ouyang, Yi-Lun Lee, Chen-Ping Yu, Yi-Hsuan Tsai, Zhaozheng Yin",http://arxiv.org/pdf/2505.14654v1,cs.CL
General-Reasoner: Advancing LLM Reasoning Across All Domains,"Reinforcement learning (RL) has recently demonstrated strong potential in
enhancing the reasoning capabilities of large language models (LLMs).
Particularly, the ""Zero"" reinforcement learning introduced by Deepseek-R1-Zero,
enables direct RL training of base LLMs without relying on an intermediate
supervised fine-tuning stage. Despite these advancements, current works for LLM
reasoning mainly focus on mathematical and coding domains, largely due to data
abundance and the ease of answer verification. This limits the applicability
and generalization of such models to broader domains, where questions often
have diverse answer representations, and data is more scarce. In this paper, we
propose General-Reasoner, a novel training paradigm designed to enhance LLM
reasoning capabilities across diverse domains. Our key contributions include:
(1) constructing a large-scale, high-quality dataset of questions with
verifiable answers curated by web crawling, covering a wide range of
disciplines; and (2) developing a generative model-based answer verifier, which
replaces traditional rule-based verification with the capability of
chain-of-thought and context-awareness. We train a series of models and
evaluate them on a wide range of datasets covering wide domains like physics,
chemistry, finance, electronics etc. Our comprehensive evaluation across these
12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)
demonstrates that General-Reasoner outperforms existing baseline methods,
achieving robust and generalizable reasoning performance while maintaining
superior effectiveness in mathematical reasoning tasks.",2025-05-20,"Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen",http://arxiv.org/pdf/2505.14652v3,cs.CL
Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference,"Deep neural networks have achieved state-of-the-art results in a wide range
of applications, from natural language processing and computer vision to speech
recognition. However, as tasks become increasingly complex, model sizes
continue to grow, posing challenges in latency and memory efficiency. To meet
these constraints, post-training quantization has emerged as a promising
solution. In this paper, we propose a novel hardware-efficient quantization and
inference scheme that exploits hardware advantages with minimal accuracy
degradation. Specifically, we introduce a W4A8 scheme, where weights are
quantized and stored using 4-bit integer precision, and inference computations
are performed using 8-bit floating-point arithmetic, demonstrating significant
speedups and improved memory utilization compared to 16-bit operations,
applicable on various modern accelerators. To mitigate accuracy loss, we
develop a novel quantization algorithm, dubbed Dual Precision Quantization
(DPQ), that leverages the unique structure of our scheme without introducing
additional inference overhead. Experimental results demonstrate improved
performance (i.e., increased throughput) while maintaining tolerable accuracy
degradation relative to the full-precision model.",2025-05-20,"Tomer Gafni, Asaf Karnieli, Yair Hanani",http://arxiv.org/pdf/2505.14638v1,cs.CL
Addressing the Challenges of Planning Language Generation,"Using LLMs to generate formal planning languages such as PDDL that invokes
symbolic solvers to deterministically derive plans has been shown to outperform
generating plans directly. While this success has been limited to
closed-sourced models or particular LLM pipelines, we design and evaluate 8
different PDDL generation pipelines with open-source models under 50 billion
parameters previously shown to be incapable of this task. We find that
intuitive approaches such as using a high-resource language wrapper or
constrained decoding with grammar decrease performance, yet inference-time
scaling approaches such as revision with feedback from the solver and plan
validator more than double the performance.",2025-05-20,"Prabhu Prakash Kagitha, Andrew Zhu, Li Zhang",http://arxiv.org/pdf/2505.14763v1,cs.CL
Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas,"Detecting AI risks becomes more challenging as stronger models emerge and
find novel methods such as Alignment Faking to circumvent these detection
attempts. Inspired by how risky behaviors in humans (i.e., illegal activities
that may hurt others) are sometimes guided by strongly-held values, we believe
that identifying values within AI models can be an early warning system for
AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal
AI models' priorities on a range of AI value classes. Then, we collect
AIRiskDilemmas, a diverse collection of dilemmas that pit values against one
another in scenarios relevant to AI safety risks such as Power Seeking. By
measuring an AI model's value prioritization using its aggregate choices, we
obtain a self-consistent set of predicted value priorities that uncover
potential risks. We show that values in LitmusValues (including seemingly
innocuous ones like Care) can predict for both seen risky behaviors in
AIRiskDilemmas and unseen risky behaviors in HarmBench.",2025-05-20,"Yu Ying Chiu, Zhilin Wang, Sharan Maiya, Yejin Choi, Kyle Fish, Sydney Levine, Evan Hubinger",http://arxiv.org/pdf/2505.14633v1,cs.CL
Think Only When You Need with Large Hybrid-Reasoning Models,"Recent Large Reasoning Models (LRMs) have shown substantially improved
reasoning capabilities over traditional Large Language Models (LLMs) by
incorporating extended thinking processes prior to producing final responses.
However, excessively lengthy thinking introduces substantial overhead in terms
of token consumption and latency, which is particularly unnecessary for simple
queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the
first kind of model capable of adaptively determining whether to perform
thinking based on the contextual information of user queries. To achieve this,
we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as
a cold start, followed by online reinforcement learning with the proposed
Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the
appropriate thinking mode. Furthermore, we introduce a metric called Hybrid
Accuracy to quantitatively assess the model's capability for hybrid thinking.
Extensive experimental results show that LHRMs can adaptively perform hybrid
thinking on queries of varying difficulty and type. It outperforms existing
LRMs and LLMs in reasoning and general capabilities while significantly
improving efficiency. Together, our work advocates for a reconsideration of the
appropriate use of extended thinking processes and provides a solid starting
point for building hybrid thinking systems.",2025-05-20,"Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, Furu Wei",http://arxiv.org/pdf/2505.14631v2,cs.CL
KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models,"Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.",2025-05-20,"Fnu Mohbat, Mohammed J Zaki",http://arxiv.org/pdf/2505.14629v1,cs.CL
Debating for Better Reasoning: An Unsupervised Multimodal Approach,"As Large Language Models (LLMs) gain expertise across diverse domains and
modalities, scalable oversight becomes increasingly challenging, particularly
when their capabilities may surpass human evaluators. Debate has emerged as a
promising mechanism for enabling such oversight. In this work, we extend the
debate paradigm to a multimodal setting, exploring its potential for weaker
models to supervise and enhance the performance of stronger models. We focus on
visual question answering (VQA), where two ""sighted"" expert vision-language
models debate an answer, while a ""blind"" (text-only) judge adjudicates based
solely on the quality of the arguments. In our framework, the experts defend
only answers aligned with their beliefs, thereby obviating the need for
explicit role-playing and concentrating the debate on instances of expert
disagreement. Experiments on several multimodal tasks demonstrate that the
debate framework consistently outperforms individual expert models. Moreover,
judgments from weaker LLMs can help instill reasoning capabilities in
vision-language models through finetuning.",2025-05-20,"Ashutosh Adhikari, Mirella Lapata",http://arxiv.org/pdf/2505.14627v1,cs.CL
TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning,"Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.",2025-05-20,"Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran",http://arxiv.org/pdf/2505.14625v2,cs.CL
Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs,"Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and
scalable method to fine-tune and customize large language models (LLMs) for
application-specific needs. However, tasks that require complex reasoning or
deep contextual understanding are often hindered by biases or interference from
the base model when using typical decoding methods like greedy or beam search.
These biases can lead to generic or task-agnostic responses from the base model
instead of leveraging the LoRA-specific adaptations. In this paper, we
introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed
to maximize the use of task-specific knowledge in LoRA-adapted models,
resulting in better downstream performance. CoLD uses contrastive decoding by
scoring candidate tokens based on the divergence between the probability
distributions of a LoRA-adapted expert model and the corresponding base model.
This approach prioritizes tokens that better align with the LoRA's learned
representations, enhancing performance for specialized tasks. While effective,
a naive implementation of CoLD is computationally expensive because each
decoding step requires evaluating multiple token candidates across both models.
To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD
achieves up to a 5.54% increase in task accuracy while reducing end-to-end
latency by 28% compared to greedy decoding. This work provides practical and
efficient decoding strategies for fine-tuned LLMs in resource-constrained
environments and has broad implications for applied data science in both cloud
and on-premises settings.",2025-05-20,"Morgan Lindsay Heisler, Linzi Xing, Ge Shi, Hanieh Sadri, Gursimran Singh, Weiwei Zhang, Tao Ye, Ying Xiong, Yong Zhang, Zhenan Fan",http://arxiv.org/pdf/2505.14620v1,cs.CL
GSDFuse: Capturing Cognitive Inconsistencies from Multi-Dimensional Weak Signals in Social Media Steganalysis,"The ubiquity of social media platforms facilitates malicious linguistic
steganography, posing significant security risks. Steganalysis is profoundly
hindered by the challenge of identifying subtle cognitive inconsistencies
arising from textual fragmentation and complex dialogue structures, and the
difficulty in achieving robust aggregation of multi-dimensional weak signals,
especially given extreme steganographic sparsity and sophisticated
steganography. These core detection difficulties are compounded by significant
data imbalance. This paper introduces GSDFuse, a novel method designed to
systematically overcome these obstacles. GSDFuse employs a holistic approach,
synergistically integrating hierarchical multi-modal feature engineering to
capture diverse signals, strategic data augmentation to address sparsity,
adaptive evidence fusion to intelligently aggregate weak signals, and
discriminative embedding learning to enhance sensitivity to subtle
inconsistencies. Experiments on social media datasets demonstrate GSDFuse's
state-of-the-art (SOTA) performance in identifying sophisticated steganography
within complex dialogue environments. The source code for GSDFuse is available
at https://github.com/NebulaEmmaZh/GSDFuse.",2025-05-20,"Kaibo Huang, Zipei Zhang, Yukun Wei, TianXin Zhang, Zhongliang Yang, Linna Zhou",http://arxiv.org/pdf/2505.17085v1,cs.CL
Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models,"Reasoning-focused large language models (LLMs) sometimes alter their behavior
when they detect that they are being evaluated, an effect analogous to the
Hawthorne phenomenon, which can lead them to optimize for test-passing
performance or to comply more readily with harmful prompts if real-world
consequences appear absent. We present the first quantitative study of how such
""test awareness"" impacts model behavior, particularly its safety alignment. We
introduce a white-box probing framework that (i) linearly identifies
awareness-related activations and (ii) steers models toward or away from test
awareness while monitoring downstream performance. We apply our method to
different state-of-the-art open-source reasoning LLMs across both realistic and
hypothetical tasks. Our results demonstrate that test awareness significantly
impact safety alignment, and is different for different models. By providing
fine-grained control over this latent effect, our work aims to increase trust
in how we perform safety evaluation.",2025-05-20,"Sahar Abdelnabi, Ahmed Salem",http://arxiv.org/pdf/2505.14617v2,cs.CL
SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas,"We introduce SATBench, a benchmark for evaluating the logical reasoning
capabilities of large language models (LLMs) through logical puzzles derived
from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on
inference rule-based reasoning, which often involves deducing conclusions from
a set of premises, our approach leverages the search-based nature of SAT
problems, where the objective is to find a solution that fulfills a specified
set of logical constraints. Each instance in SATBench is generated from a SAT
formula, then translated into a story context and conditions using LLMs. The
generation process is fully automated and allows for adjustable difficulty by
varying the number of clauses. All 2100 puzzles are validated through both
LLM-assisted and solver-based consistency checks, with human validation on a
subset. Experimental results show that even the strongest model, o4-mini,
achieves only 65.0% accuracy on hard UNSAT problems, close to the random
baseline of 50%. SATBench exposes fundamental limitations in the search-based
logical reasoning abilities of current LLMs and provides a scalable testbed for
future research in logical reasoning.",2025-05-20,"Anjiang Wei, Yuheng Wu, Yingjia Wan, Tarun Suresh, Huanmi Tan, Zhanke Zhou, Sanmi Koyejo, Ke Wang, Alex Aiken",http://arxiv.org/pdf/2505.14615v1,cs.CL
Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It),"Despite considerable progress in the development of machine-text detectors,
it has been suggested that the problem is inherently hard, and therefore, that
stakeholders should proceed under the assumption that machine-generated text
cannot be reliably detected as such. We examine a recent such claim by Nicks et
al. (2024) regarding the ease with which language models can be optimized to
degrade the performance of machine-text detectors, including detectors not
specifically optimized against. We identify a feature space$\unicode{x2013}$the
stylistic feature space$\unicode{x2013}$that is robust to such optimization,
and show that it may be used to reliably detect samples from language models
optimized to prevent detection. Furthermore, we show that even when models are
explicitly optimized against stylistic detectors, detection performance remains
surprisingly unaffected. We then seek to understand if stylistic detectors are
inherently more robust. To study this question, we explore a new paraphrasing
approach that simultaneously aims to close the gap between human writing and
machine writing in stylistic feature space while avoiding detection using
traditional features. We show that when only a single sample is available for
detection, this attack is universally effective across all detectors
considered, including those that use writing style. However, as the number of
samples available for detection grows, the human and machine distributions
become distinguishable. This observation encourages us to introduce AURA, a
metric that estimates the overlap between human and machine-generated
distributions by analyzing how detector performance improves as more samples
become available. Overall, our findings underscore previous recommendations to
avoid reliance on machine-text detection.",2025-05-20,"Rafael Rivera Soto, Barry Chen, Nicholas Andrews",http://arxiv.org/pdf/2505.14608v1,cs.CL
sudoLLM : On Multi-role Alignment of Language Models,"User authorization-based access privileges are a key feature in many
safety-critical systems, but have thus far been absent from the large language
model (LLM) realm. In this work, drawing inspiration from such access control
systems, we introduce sudoLLM, a novel framework that results in multi-role
aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user
access rights. sudoLLM injects subtle user-based biases into queries and trains
an LLM to utilize this bias signal in order to produce sensitive information if
and only if the user is authorized. We present empirical results demonstrating
that this approach shows substantially improved alignment, generalization, and
resistance to prompt-based jailbreaking attacks. The persistent tension between
the language modeling objective and safety alignment, which is often exploited
to jailbreak LLMs, is somewhat resolved with the aid of the injected bias
signal. Our framework is meant as an additional security layer, and complements
existing guardrail mechanisms for enhanced end-to-end safety with LLMs.",2025-05-20,"Soumadeep Saha, Akshay Chaturvedi, Joy Mahapatra, Utpal Garain",http://arxiv.org/pdf/2505.14607v1,cs.CL
Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models,"Large language models (LLMs) have shown significant potential in scientific
disciplines such as biomedicine, particularly in hypothesis generation, where
they can analyze vast literature, identify patterns, and suggest research
directions. However, a key challenge lies in evaluating the truthfulness of
generated hypotheses, as verifying their accuracy often requires substantial
time and resources. Additionally, the hallucination problem in LLMs can lead to
the generation of hypotheses that appear plausible but are ultimately
incorrect, undermining their reliability. To facilitate the systematic study of
these challenges, we introduce TruthHypo, a benchmark for assessing the
capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,
a knowledge-based hallucination detector to evaluate how well hypotheses are
grounded in existing knowledge. Our results show that LLMs struggle to generate
truthful hypotheses. By analyzing hallucinations in reasoning steps, we
demonstrate that the groundedness scores provided by KnowHD serve as an
effective metric for filtering truthful hypotheses from the diverse outputs of
LLMs. Human evaluations further validate the utility of KnowHD in identifying
truthful hypotheses and accelerating scientific discovery. Our data and source
code are available at https://github.com/Teddy-XiongGZ/TruthHypo.",2025-05-20,"Guangzhi Xiong, Eric Xie, Corey Williams, Myles Kim, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang",http://arxiv.org/pdf/2505.14599v1,cs.CL
Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals,"Code Sensitivity refers to the ability of Code LLMs to recognize and respond
to details changes in problem descriptions. While current code benchmarks and
instruction data focus on difficulty and diversity, sensitivity is overlooked.
We first introduce the CTF-Code benchmark, constructed using counterfactual
perturbations, minimizing input changes while maximizing output changes. The
evaluation shows that many LLMs have a more than 10\% performance drop compared
to the original problems. To fully utilize sensitivity, CTF-Instruct, an
incremental instruction fine-tuning framework, extends on existing data and
uses a selection mechanism to meet the three dimensions of difficulty,
diversity, and sensitivity. Experiments show that LLMs fine-tuned with
CTF-Instruct data achieve over a 2\% improvement on CTF-Code, and more than a
10\% performance boost on LiveCodeBench, validating the feasibility of
enhancing LLMs' sensitivity to improve performance.",2025-05-20,"Xianzhen Luo, Qingfu Zhu, Zhiming Zhang, Mingzheng Xu, Tianhao Cheng, Yixuan Wang, Zheng Chu, Shijie Xuyang, Zhiyuan Ma, YuanTao Fan, Wanxiang Che",http://arxiv.org/pdf/2505.14597v1,cs.CL
MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol,"As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users
and developers, it also brings underexplored safety risks. Its decentralized
architecture, which separates clients and servers, poses unique challenges for
systematic safety analysis. This paper proposes a novel framework to enhance
MCP safety. Guided by the MAESTRO framework, we first analyze the missing
safety mechanisms in MCP, and based on this analysis, we propose the Model
Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses
these gaps. Next, we develop a fine-grained taxonomy that captures a diverse
range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,
we develop benchmark and training data that support the evaluation and
improvement of LLMs' capabilities in identifying safety risks within MCP
interactions. Leveraging the proposed benchmark and training data, we conduct
extensive experiments on state-of-the-art LLMs. The results highlight LLMs'
vulnerabilities in MCP interactions and demonstrate that our approach
substantially improves their safety performance.",2025-05-20,"Huihao Jing, Haoran Li, Wenbin Hu, Qi Hu, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song",http://arxiv.org/pdf/2505.14590v2,cs.CL
Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning,"While Large Language Models (LLMs) exhibit remarkable capabilities, they also
introduce significant safety and privacy risks. Current mitigation strategies
often fail to preserve contextual reasoning capabilities in risky scenarios.
Instead, they rely heavily on sensitive pattern matching to protect LLMs, which
limits the scope. Furthermore, they overlook established safety and privacy
standards, leading to systemic risks for legal compliance. To address these
gaps, we formulate safety and privacy issues into contextualized compliance
problems following the Contextual Integrity (CI) theory. Under the CI
framework, we align our model with three critical regulatory standards: GDPR,
EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with
a rule-based reward to incentivize contextual reasoning capabilities while
enhancing compliance with safety and privacy norms. Through extensive
experiments, we demonstrate that our method not only significantly enhances
legal compliance (achieving a +17.64% accuracy improvement in safety/privacy
benchmarks) but also further improves general reasoning capability. For
OpenThinker-7B, a strong reasoning model that significantly outperforms its
base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its
general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on
the MMLU and LegalBench benchmark, respectively.",2025-05-20,"Wenbin Hu, Haoran Li, Huihao Jing, Qi Hu, Ziqian Zeng, Sirui Han, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song",http://arxiv.org/pdf/2505.14585v1,cs.CL
Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning,"Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its
verbose, self-reflective style often hinders effective distillation into small
language models (SLMs). We revisit Long-CoT compression through the lens of
capability alignment and ask: Can pruning improve reasoning? We propose
Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic
graphs and selectively prunes low-utility reasoning steps under
self-verification constraints. Through systematic analysis across three pruning
strategies -- targeting entire chains, core reasoning, and verification -- we
find that pruning verification steps yields consistent accuracy gains while
reducing inference cost, outperforming token-level baselines and uncompressed
fine-tuning. In contrast, pruning reasoning or all-chain steps degrades
performance, revealing that small models benefit not from shorter CoTs, but
from semantically leaner ones. Our findings highlight pruning as a structural
optimization strategy for aligning CoT reasoning with SLM capacity.",2025-05-20,"Shangziqi Zhao, Jiahao Yuan, Guisong Yang, Usman Naseem",http://arxiv.org/pdf/2505.14582v1,cs.CL
TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring,"Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there
is a notable lack of attention for assessing essays according to individual
traits. In this work, we propose TRATES, a novel trait-specific and
rubric-based cross-prompt AES framework that is generic yet specific to the
underlying trait. The framework leverages a Large Language Model (LLM) that
utilizes the trait grading rubrics to generate trait-specific features
(represented by assessment questions), then assesses those features given an
essay. The trait-specific features are eventually combined with generic
writing-quality and prompt-specific features to train a simple classical
regression model that predicts trait scores of essays from an unseen prompt.
Experiments show that TRATES achieves a new state-of-the-art performance across
all traits on a widely-used dataset, with the generated LLM-based features
being the most significant.",2025-05-20,"Sohaila Eltanbouly, Salam Albatarni, Tamer Elsayed",http://arxiv.org/pdf/2505.14577v1,cs.CL
Agent Context Protocols Enhance Collective Inference,"AI agents have become increasingly adept at complex tasks such as coding,
reasoning, and multimodal understanding. However, building generalist systems
requires moving beyond individual agents to collective inference -- a paradigm
where multi-agent systems with diverse, task-specialized agents complement one
another through structured communication and collaboration. Today, coordination
is usually handled with imprecise, ad-hoc natural language, which limits
complex interaction and hinders interoperability with domain-specific agents.
We introduce Agent context protocols (ACPs): a domain- and agent-agnostic
family of structured protocols for agent-agent communication, coordination, and
error handling. ACPs combine (i) persistent execution blueprints -- explicit
dependency graphs that store intermediate agent outputs -- with (ii)
standardized message schemas, enabling robust and fault-tolerant multi-agent
collective inference. ACP-powered generalist systems reach state-of-the-art
performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance
and best-in-class multimodal technical reports, outperforming commercial AI
systems in human evaluation. ACPs are highly modular and extensible, allowing
practitioners to build top-tier generalist agents quickly.",2025-05-20,"Devansh Bhardwaj, Arjun Beniwal, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Karthik R. Narasimhan, Ameet Deshpande, Vishvak Murahari",http://arxiv.org/pdf/2505.14569v1,cs.CL
Pivot Language for Low-Resource Machine Translation,"Certain pairs of languages suffer from lack of a parallel corpus which is
large in size and diverse in domain. One of the ways this is overcome is via
use of a pivot language. In this paper we use Hindi as a pivot language to
translate Nepali into English. We describe what makes Hindi a good candidate
for the pivot. We discuss ways in which a pivot language can be used, and use
two such approaches - the Transfer Method (fully supervised) and
Backtranslation (semi-supervised) - to translate Nepali into English. Using the
former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which
improves the baseline fully supervised score reported by (Guzman et al., 2019)
by 6.6 points. While we are slightly below the semi-supervised baseline score
of 15.1, we discuss what may have caused this under-performance, and suggest
scope for future work.",2025-05-20,"Abhimanyu Talwar, Julien Laasri",http://arxiv.org/pdf/2505.14553v2,cs.CL
KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation,"Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.",2025-05-20,"Jiajun Shi, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, Zhoufutu Wen, Bingli Wang, Yancheng He, Liang Song, Hualei Zhu, Shilong Li, Xingjian Wang, Wei Zhang, Ruibin Yuan, Yifan Yao, Wenjun Yang, Yunli Wang, Siyuan Fang, Siyu Yuan, Qianyu He, Xiangru Tang, Yingshui Tan, Wangchunshu Zhou, Zhaoxiang Zhang, Zhoujun Li, Wenhao Huang, Ge Zhang",http://arxiv.org/pdf/2505.14552v2,cs.CL
Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders,"Large language models (LLMs) are now ubiquitous in user-facing applications,
yet they still generate undesirable toxic outputs, including profanity,
vulgarity, and derogatory remarks. Although numerous detoxification methods
exist, most apply broad, surface-level fixes and can therefore easily be
circumvented by jailbreak attacks. In this paper we leverage sparse
autoencoders (SAEs) to identify toxicity-related directions in the residual
stream of models and perform targeted activation steering using the
corresponding decoder vectors. We introduce three tiers of steering
aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing
trade-offs between toxicity reduction and language fluency. At stronger
steering strengths, these causal interventions surpass competitive baselines in
reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2
Small depending on the aggressiveness. Crucially, standard NLP benchmark scores
upon steering remain stable, indicating that the model's knowledge and general
abilities are preserved. We further show that feature-splitting in wider SAEs
hampers safety interventions, underscoring the importance of disentangled
feature learning. Our findings highlight both the promise and the current
limitations of SAE-based causal interventions for LLM detoxification, further
suggesting practical guidelines for safer language-model deployment.",2025-05-20,"Agam Goyal, Vedant Rathi, William Yeh, Yian Wang, Yuen Chen, Hari Sundaram",http://arxiv.org/pdf/2505.14536v1,cs.CL
Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs,"We show that large language models (LLMs) exhibit an $\textit{internal
chain-of-thought}$: they sequentially decompose and execute composite tasks
layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned
at different network depths, and (ii) these subtasks are executed sequentially
across layers. On a benchmark of 15 two-step composite tasks, we employ
layer-from context-masking and propose a novel cross-task patching method,
confirming (i). To examine claim (ii), we apply LogitLens to decode hidden
states, revealing a consistent layerwise execution pattern. We further
replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing
the same stepwise dynamics. Together, our results enhance LLMs transparency by
showing their capacity to internally plan and execute subtasks (or
instructions), opening avenues for fine-grained, instruction-level activation
steering.",2025-05-20,"Zhipeng Yang, Junzhuo Li, Siyu Xia, Xuming Hu",http://arxiv.org/pdf/2505.14530v1,cs.CL
Exploring Graph Representations of Logical Forms for Language Modeling,"We make the case for language models over logical forms (LFLMs), arguing that
such models are more data-efficient than their textual counterparts. To that
end, we introduce the Graph-based Formal-Logical Distributional Semantics
(GFoLDS) prototype, a pretrained LM over graph representations of logical
forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong
experimental evidence that LFLMs can leverage the built-in, basic linguistic
knowledge inherent in such models to immediately begin learning more complex
patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,
transformer LMs pretrained on similar amounts of data, indicating that LFLMs
can learn with substantially less data than models over plain text.
Furthermore, we show that the performance of this model is likely to scale with
additional parameters and pretraining data, suggesting the viability of LFLMs
in real-world applications.",2025-05-20,Michael Sullivan,http://arxiv.org/pdf/2505.14523v1,cs.CL
Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples,"Recent advancements in audio-aware large language models (ALLMs) enable them
to process and understand audio inputs. However, these models often hallucinate
non-existent sound events, reducing their reliability in real-world
applications. To address this, we propose LISTEN (Learning to Identify Sounds
Through Extended Negative Samples), a contrastive-like training method that
enhances ALLMs' ability to distinguish between present and absent sounds using
synthesized data from the backbone LLM. Unlike prior approaches, our method
requires no modification to LLM parameters and efficiently integrates audio
representations via a lightweight adapter. Experiments show that LISTEN
effectively mitigates hallucinations while maintaining impressive performance
on existing audio question and reasoning benchmarks. At the same time, it is
more efficient in both data and computation.",2025-05-20,"Chun-Yi Kuan, Hung-yi Lee",http://arxiv.org/pdf/2505.14518v1,cs.CL
ModRWKV: Transformer Multimodality in Linear Time,"Currently, most multimodal studies are based on large language models (LLMs)
with quadratic-complexity Transformer architectures. While linear models like
RNNs enjoy low inference costs, their application has been largely limited to
the text-only modality. This work explores the capabilities of modern RNN
architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal
framework built upon the RWKV7 architecture as its LLM backbone-which achieves
multi-source information fusion through dynamically adaptable heterogeneous
modality encoders. We designed the multimodal modules in ModRWKV with an
extremely lightweight architecture and, through extensive experiments,
identified a configuration that achieves an optimal balance between performance
and computational efficiency. ModRWKV leverages the pretrained weights of the
RWKV7 LLM for initialization, which significantly accelerates multimodal
training. Comparative experiments with different pretrained checkpoints further
demonstrate that such initialization plays a crucial role in enhancing the
model's ability to understand multimodal signals. Supported by extensive
experiments, we conclude that modern RNN architectures present a viable
alternative to Transformers in the domain of multimodal large language models
(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV
architecture through systematic exploration.",2025-05-20,"Jiale Kang, Ziyin Yue, Qingyu Yin, Jiang Rui, Weile Li, Zening Lu, Zhouran Ji",http://arxiv.org/pdf/2505.14505v1,cs.CL
Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales,"There has been growing interest in Multimodal Aspect-Based Sentiment Analysis
(MABSA) in recent years. Existing methods predominantly rely on pre-trained
small language models (SLMs) to collect information related to aspects and
sentiments from both image and text, with an aim to align these two modalities.
However, small SLMs possess limited capacity and knowledge, often resulting in
inaccurate identification of meaning, aspects, sentiments, and their
interconnections in textual and visual data. On the other hand, Large language
models (LLMs) have shown exceptional capabilities in various tasks by
effectively exploring fine-grained information in multimodal data. However,
some studies indicate that LLMs still fall short compared to fine-tuned small
models in the field of ABSA. Based on these findings, we propose a novel
framework, termed LRSA, which combines the decision-making capabilities of SLMs
with additional information provided by LLMs for MABSA. Specifically, we inject
explanations generated by LLMs as rationales into SLMs and employ a dual
cross-attention mechanism for enhancing feature interaction and fusion, thereby
augmenting the SLMs' ability to identify aspects and sentiments. We evaluated
our method using two baseline models, numerous experiments highlight the
superiority of our approach on three widely-used benchmarks, indicating its
generalizability and applicability to most pre-trained models for MABSA.",2025-05-20,"Jun Cao, Jiyi Li, Ziwei Yang, Renjie Zhou",http://arxiv.org/pdf/2505.14499v2,cs.CL
Scale-invariant Attention,"One persistent challenge in LLM research is the development of attention
mechanisms that are able to generalise from training on shorter contexts to
inference on longer contexts. We propose two conditions that we expect all
effective long context attention mechanisms to have: scale-invariant total
attention, and scale-invariant attention sparsity. Under a Gaussian assumption,
we show that a simple position-dependent transformation of the attention logits
is sufficient for these conditions to hold. Experimentally we find that the
resulting scale-invariant attention scheme gives considerable benefits in terms
of validation loss when zero-shot generalising from training on short contexts
to validation on longer contexts, and is effective at long-context retrieval.",2025-05-20,"Ben Anson, Xi Wang, Laurence Aitchison",http://arxiv.org/pdf/2505.17083v1,cs.CL
Reasoning Models Better Express Their Confidence,"Despite their strengths, large language models (LLMs) often fail to
communicate their confidence accurately, making it difficult to assess when
they might be wrong and limiting their reliability. In this work, we
demonstrate that reasoning models-LLMs that engage in extended chain-of-thought
(CoT) reasoning-exhibit superior performance not only in problem-solving but
also in accurately expressing their confidence. Specifically, we benchmark six
reasoning models across six datasets and find that they achieve strictly better
confidence calibration than their non-reasoning counterparts in 33 out of the
36 settings. Our detailed analysis reveals that these gains in calibration stem
from the slow thinking behaviors of reasoning models-such as exploring
alternative approaches and backtracking-which enable them to adjust their
confidence dynamically throughout their CoT, making it progressively more
accurate. In particular, we find that reasoning models become increasingly
better calibrated as their CoT unfolds, a trend not observed in non-reasoning
models. Moreover, removing slow thinking behaviors from the CoT leads to a
significant drop in calibration. Lastly, we show that these gains are not
exclusive to reasoning models-non-reasoning models also benefit when guided to
perform slow thinking via in-context learning.",2025-05-20,"Dongkeun Yoon, Seungone Kim, Sohee Yang, Sunkyoung Kim, Soyeon Kim, Yongil Kim, Eunbi Choi, Yireun Kim, Minjoon Seo",http://arxiv.org/pdf/2505.14489v1,cs.CL
MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance,"Large language models (LLMs) have shown great potential in flagging harmful
content in online communities. Yet, existing approaches for moderation require
a separate model for every community and are opaque in their decision-making,
limiting real-world adoption. We introduce Mixture of Moderation Experts
(MoMoE), a modular, cross-community framework that adds post-hoc explanations
to scalable content moderation. MoMoE orchestrates four operators -- Allocate,
Predict, Aggregate, Explain -- and is instantiated as seven
community-specialized experts (MoMoE-Community) and five norm-violation experts
(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1
scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned
baselines while consistently producing concise and reliable explanations.
Although community-specialized experts deliver the highest peak accuracy,
norm-violation experts provide steadier performance across domains. These
findings show that MoMoE yields scalable, transparent moderation without
needing per-community fine-tuning. More broadly, they suggest that lightweight,
explainable expert ensembles can guide future NLP and HCI research on
trustworthy human-AI governance of online communities.",2025-05-20,"Agam Goyal, Xianyang Zhan, Yilun Chen, Koustuv Saha, Eshwar Chandrasekharan",http://arxiv.org/pdf/2505.14483v1,cs.CL
PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models,"In the field of urban planning, existing Vision-Language Models (VLMs)
frequently fail to effectively analyze and evaluate planning maps, despite the
critical importance of these visual elements for urban planners and related
educational contexts. Planning maps, which visualize land use, infrastructure
layouts, and functional zoning, require specialized understanding of spatial
configurations, regulatory requirements, and multi-scale analysis. To address
this challenge, we introduce PlanGPT-VL, the first domain-specific
Vision-Language Model tailored specifically for urban planning maps. PlanGPT-VL
employs three innovative approaches: (1) PlanAnno-V framework for high-quality
VQA data synthesis, (2) Critical Point Thinking to reduce hallucinations
through structured verification, and (3) comprehensive training methodology
combining Supervised Fine-Tuning with frozen vision encoder parameters. Through
systematic evaluation on our proposed PlanBench-V benchmark, we demonstrate
that PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs
in specialized planning map interpretation tasks, offering urban planning
professionals a reliable tool for map analysis, assessment, and educational
applications while maintaining high factual accuracy. Our lightweight 7B
parameter model achieves comparable performance to models exceeding 72B
parameters, demonstrating efficient domain specialization without sacrificing
performance.",2025-05-20,"He Zhu, Junyou Su, Minxin Chen, Wen Wang, Yijie Deng, Guanhua Chen, Wenjia Zhang",http://arxiv.org/pdf/2505.14481v2,cs.CL
Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach,"Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.",2025-05-20,"Oren Sultan, Eitan Stern, Dafna Shahaf",http://arxiv.org/pdf/2505.14479v1,cs.CL
Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning,"Citation classification, which identifies the intention behind academic
citations, is pivotal for scholarly analysis. Previous works suggest
fine-tuning pretrained language models (PLMs) on citation classification
datasets, reaping the reward of the linguistic knowledge they gained during
pretraining. However, directly fine-tuning for citation classification is
challenging due to labeled data scarcity, contextual noise, and spurious
keyphrase correlations. In this paper, we present a novel framework, Citss,
that adapts the PLMs to overcome these challenges. Citss introduces
self-supervised contrastive learning to alleviate data scarcity, and is
equipped with two specialized strategies to obtain the contrastive pairs:
sentence-level cropping, which enhances focus on target citations within long
contexts, and keyphrase perturbation, which mitigates reliance on specific
keyphrases. Compared with previous works that are only designed for
encoder-based PLMs, Citss is carefully developed to be compatible with both
encoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged
pretraining. Experiments with three benchmark datasets with both encoder-based
PLMs and decoder-based LLMs demonstrate our superiority compared to the
previous state of the art. Our code is available at: github.com/LITONG99/Citss",2025-05-20,"Tong Li, Jiachuan Wang, Yongqi Zhang, Shuangyin Li, Lei Chen",http://arxiv.org/pdf/2505.14471v1,cs.CL
PAST: Phonetic-Acoustic Speech Tokenizer,"We present PAST, a novel end-to-end framework that jointly models phonetic
information alongside signal reconstruction, eliminating the need for external
pretrained models. Unlike previous approaches that rely on pretrained
self-supervised models, PAST employs supervised phonetic data, directly
integrating domain knowledge into the tokenization process via auxiliary tasks.
Additionally, we introduce a streamable, causal variant of PAST, enabling
real-time speech applications. Results demonstrate that PAST surpasses existing
evaluated baseline tokenizers across common evaluation metrics, including
phonetic representation and speech reconstruction. Notably, PAST also achieves
superior performance when serving as a speech representation for speech
language models, further highlighting its effectiveness as a foundation for
spoken language generation. To foster further research, we release the full
implementation. For code, model checkpoints, and samples see:
https://pages.cs.huji.ac.il/adiyoss-lab/PAST",2025-05-20,"Nadav Har-Tuv, Or Tal, Yossi Adi",http://arxiv.org/pdf/2505.14470v1,cs.CL
Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations,"Recent advancements in LLMs have raised significant safety concerns,
particularly when dealing with code-mixed inputs and outputs. Our study
systematically investigates the increased susceptibility of LLMs to produce
unsafe outputs from code-mixed prompts compared to monolingual English prompts.
Utilizing explainability methods, we dissect the internal attribution shifts
causing model's harmful behaviors. In addition, we explore cultural dimensions
by distinguishing between universally unsafe and culturally-specific unsafe
queries. This paper presents novel experimental insights, clarifying the
mechanisms driving this phenomenon.",2025-05-20,"Somnath Banerjee, Pratyush Chatterjee, Shanu Kumar, Sayan Layek, Parag Agrawal, Rima Hazra, Animesh Mukherjee",http://arxiv.org/pdf/2505.14469v1,cs.CL
Void in Language Models,"Despite advances in transformer-based language models (LMs), a fundamental
question remains largely unanswered: Are all layers activated during inference?
We investigate this question by detecting unactivated layers (which we refer to
as Voids) using a non-trainable and parameter-free adaptive computation method
called L2 Adaptive Computation (LAC). We adapt LAC from its original
efficiency-focused application to trace activated layers during inference. This
method monitors changes in the L2-norm of activations to identify voids. We
analyze layer activation in instruction-tuned LMs across two phases: Prompt
Processing (PP), where we trace activated layers for each token in the input
prompts, and Response Generation (RG), where we trace activated layers for each
generated token. We further demonstrate that distinct layers are activated
during these two phases. To show the effectiveness of our method, we evaluated
three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families
on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a
zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an
improvement from 69.24 to 71.29 while the model uses only 30% of the layers.
Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to
18.36 when using 70% of the layers during both the PP and RG phases. These
results show that not all layers contribute equally during inference, and that
selectively skipping most of them can improve the performance of models on
certain tasks.",2025-05-20,Mani Shemiranifar,http://arxiv.org/pdf/2505.14467v1,cs.CL
Not All Correct Answers Are Equal: Why Your Distillation Source Matters,"Distillation has emerged as a practical and effective approach to enhance the
reasoning capabilities of open-source language models. In this work, we conduct
a large-scale empirical study on reasoning data distillation by collecting
verified outputs from three state-of-the-art teacher models-AM-Thinking-v1,
Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We
construct three parallel datasets and analyze their distributions, revealing
that AM-Thinking-v1-distilled data exhibits greater token length diversity and
lower perplexity. Student models trained on each dataset are evaluated on
reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.
The model distilled from AM-Thinking-v1 consistently achieves the best
performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and
65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing
longer responses for harder tasks and shorter ones for simpler tasks. These
findings highlight the value of high-quality, verified reasoning traces. We
release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support
future research on open and high-performing reasoning-oriented language models.
The datasets are publicly available on Hugging Face\footnote{Datasets are
available on Hugging Face:
\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},
\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.",2025-05-20,"Xiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, Xiangang Li",http://arxiv.org/pdf/2505.14464v2,cs.CL
RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding,"As vision-language models (VLMs) become increasingly integrated into daily
life, the need for accurate visual culture understanding is becoming critical.
Yet, these models frequently fall short in interpreting cultural nuances
effectively. Prior work has demonstrated the effectiveness of
retrieval-augmented generation (RAG) in enhancing cultural understanding in
text-only settings, while its application in multimodal scenarios remains
underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented
Visual culturE uNdErstAnding), a new benchmark designed to advance visual
culture understanding through retrieval, focusing on two tasks: culture-focused
visual question answering (cVQA) and culture-informed image captioning (cIC).
RAVENEA extends existing datasets by integrating over 10,000 Wikipedia
documents curated and ranked by human annotators. With RAVENEA, we train and
evaluate seven multimodal retrievers for each image query, and measure the
downstream impact of retrieval-augmented inputs across fourteen
state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented
with culture-aware retrieval, outperform their non-augmented counterparts (by
at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the
value of retrieval-augmented methods and culturally inclusive benchmarks for
multimodal understanding.",2025-05-20,"Jiaang Li, Yifei Yuan, Wenyan Li, Mohammad Aliannejadi, Daniel Hershcovich, Anders Søgaard, Ivan Vulić, Wenxuan Zhang, Paul Pu Liang, Yang Deng, Serge Belongie",http://arxiv.org/pdf/2505.14462v1,cs.CL
CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation,"Although autoregressive models have dominated language modeling in recent
years, there has been a growing interest in exploring alternative paradigms to
the conventional next-token prediction framework. Diffusion-based language
models have emerged as a compelling alternative due to their powerful parallel
generation capabilities and inherent editability. However, these models are
often constrained by fixed-length generation. A promising direction is to
combine the strengths of both paradigms, segmenting sequences into blocks,
modeling autoregressive dependencies across blocks while leveraging discrete
diffusion to estimate the conditional distribution within each block given the
preceding context. Nevertheless, their practical application is often hindered
by two key limitations: rigid fixed-length outputs and a lack of flexible
control mechanisms. In this work, we address the critical limitations of fixed
granularity and weak controllability in current large diffusion language
models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive
framework that adaptively determines the size of each generation block based on
local semantics using reinforcement learning. Furthermore, we introduce a
classifier-guided control mechanism tailored to discrete diffusion, which
significantly reduces computational overhead while facilitating efficient
post-hoc conditioning without retraining. Extensive experiments demonstrate
that CtrlDiff sets a new standard among hybrid diffusion models, narrows the
performance gap to state-of-the-art autoregressive approaches, and enables
effective conditional text generation across diverse tasks.",2025-05-20,"Chihan Huang, Hao Tang",http://arxiv.org/pdf/2505.14455v1,cs.CL
Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach,"While subgroup disparities and performance bias are increasingly studied in
computational research, fairness in categorical Speech Emotion Recognition
(SER) remains underexplored. Existing methods often rely on explicit
demographic labels, which are difficult to obtain due to privacy concerns. To
address this limitation, we introduce an Implicit Demography Inference (IDI)
module that leverages pseudo-labeling from a pre-trained model and unsupervised
learning using k-means clustering to mitigate bias in SER. Our experiments show
that pseudo-labeling IDI reduces subgroup disparities, improving fairness
metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the
unsupervised IDI yields more than a 26% improvement in fairness metrics with a
drop of less than 4% in SER performance. Further analyses reveal that the
unsupervised IDI consistently mitigates race and age disparities, demonstrating
its potential in scenarios where explicit demographic information is
unavailable.",2025-05-20,"Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee",http://arxiv.org/pdf/2505.14449v2,cs.CL
Creative Preference Optimization,"While Large Language Models (LLMs) have demonstrated impressive performance
across natural language generation tasks, their ability to generate truly
creative content-characterized by novelty, diversity, surprise, and
quality-remains limited. Existing methods for enhancing LLM creativity often
focus narrowly on diversity or specific tasks, failing to address creativity's
multifaceted nature in a generalizable way. In this work, we propose Creative
Preference Optimization (CrPO), a novel alignment method that injects signals
from multiple creativity dimensions into the preference optimization objective
in a modular fashion. We train and evaluate creativity-augmented versions of
several models using CrPO and MuCE, a new large-scale human preference dataset
spanning over 200,000 human-generated responses and ratings from more than 30
psychological creativity assessments. Our models outperform strong baselines,
including GPT-4o, on both automated and human evaluations, producing more
novel, diverse, and surprising generations while maintaining high output
quality. Additional evaluations on NoveltyBench further confirm the
generalizability of our approach. Together, our results demonstrate that
directly optimizing for creativity within preference frameworks is a promising
direction for advancing the creative capabilities of LLMs without compromising
output quality.",2025-05-20,"Mete Ismayilzada, Antonio Laverghetta Jr., Simone A. Luchini, Reet Patel, Antoine Bosselut, Lonneke van der Plas, Roger Beaty",http://arxiv.org/pdf/2505.14442v1,cs.CL
S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models,"End-to-end speech large language models ((LLMs)) extend the capabilities of
text-based models to directly process and generate audio tokens. However, this
often leads to a decline in reasoning and generation performance compared to
text input, a phenomenon referred to as intelligence degradation. To
systematically evaluate this gap, we propose S2SBench, a benchmark designed to
quantify performance degradation in Speech LLMs. It includes diagnostic
datasets targeting sentence continuation and commonsense reasoning under audio
input. We further introduce a pairwise evaluation protocol based on perplexity
differences between plausible and implausible samples to measure degradation
relative to text input. We apply S2SBench to analyze the training process of
Baichuan-Audio, which further demonstrates the benchmark's effectiveness. All
datasets and evaluation code are available at
https://github.com/undobug/S2SBench.",2025-05-20,"Yuanbo Fang, Haoze Sun, Jun Liu, Tao Zhang, Zenan Zhou, Weipeng Chen, Xiaofen Xing, Xiangmin Xu",http://arxiv.org/pdf/2505.14438v1,cs.CL
Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models,"Large Language Models (LLMs) offer a transparent brain with accessible
parameters that encode extensive knowledge, which can be analyzed, located and
transferred. Consequently, a key research challenge is to transcend traditional
knowledge transfer paradigms rooted in symbolic language and achieve genuine
Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods
for transferring knowledge across LLMs of different scales through parameters
presents an intriguing and valuable research direction. In this paper, we first
demonstrate $\textbf{Alignment}$ in parametric space is the fundamental
prerequisite to achieve successful cross-scale PKT. We redefine the previously
explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes
extracted parameters for LoRA initialization and requires subsequent fine-tune
for alignment. Hence, to reduce cost for further fine-tuning, we introduce a
novel Pre-Align PKT (PrePKT) paradigm and propose a solution called
$\textbf{LaTen}$
($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that
aligns the parametric spaces of LLMs across scales only using several training
steps without following training. Comprehensive experiments on four benchmarks
demonstrate that both PostPKT and PrePKT face challenges in achieving
consistently stable transfer. Through in-depth analysis, we identify
$\textbf{Neural Incompatibility}$ as the ethological and parametric structural
differences between LLMs of varying scales, presenting fundamental challenges
to achieving effective PKT. These findings provide fresh insights into the
parametric architectures of LLMs and highlight promising directions for future
research on efficient PKT. Our code is available at
https://github.com/Trae1ounG/Neural_Incompatibility.",2025-05-20,"Yuqiao Tan, Shizhu He, Kang Liu, Jun Zhao",http://arxiv.org/pdf/2505.14436v1,cs.CL
Rank-K: Test-Time Reasoning for Listwise Reranking,"Retrieve-and-rerank is a popular retrieval pipeline because of its ability to
make slow but effective rerankers efficient enough at query time by reducing
the number of comparisons. Recent works in neural rerankers take advantage of
large language models for their capability in reasoning between queries and
passages and have achieved state-of-the-art retrieval effectiveness. However,
such rerankers are resource-intensive, even after heavy optimization. In this
work, we introduce Rank-K, a listwise passage reranking model that leverages
the reasoning capability of the reasoning language model at query time that
provides test time scalability to serve hard queries. We show that Rank-K
improves retrieval effectiveness by 23\% over the RankZephyr, the
state-of-the-art listwise reranker, when reranking a BM25 initial ranked list
and 19\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is
inherently a multilingual model, we found that it ranks passages based on
queries in different languages as effectively as it does in monolingual
retrieval.",2025-05-20,"Eugene Yang, Andrew Yates, Kathryn Ricci, Orion Weller, Vivek Chari, Benjamin Van Durme, Dawn Lawrie",http://arxiv.org/pdf/2505.14432v1,cs.CL
From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning,"Instruction-tuned large language models (LLMs) have shown strong performance
on a variety of tasks; however, generalizing from synthetic to human-authored
instructions in grounded environments remains a challenge for them. In this
work, we study generalization challenges in spatial grounding tasks where
models interpret and translate instructions for building object arrangements on
a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate
their performance on a benchmark dataset containing both synthetic and
human-written instructions. Our results reveal that while models generalize
well on simple tasks, their performance degrades significantly on more complex
tasks. We present a detailed error analysis of the gaps in instruction
generalization.",2025-05-20,"Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen",http://arxiv.org/pdf/2505.14425v1,cs.CL
Scaling Low-Resource MT via Synthetic Data Generation with LLMs,"We investigate the potential of LLM-generated synthetic data for improving
low-resource machine translation (MT). Focusing on seven diverse target
languages, we construct a document-level synthetic corpus from English
Europarl, and extend it via pivoting to 147 additional language pairs.
Automatic and human evaluation confirm its high overall quality. We study its
practical application by (i) identifying effective training regimes, (ii)
comparing our data with the HPLT dataset, and (iii) testing its utility beyond
English-centric MT. Finally, we introduce SynOPUS, a public repository for
synthetic parallel datasets. Our findings show that LLM-generated synthetic
data, even when noisy, can substantially improve MT performance for
low-resource languages.",2025-05-20,"Ona de Gibert, Joseph Attieh, Teemu Vahtola, Mikko Aulamo, Zihao Li, Raúl Vázquez, Tiancheng Hu, Jörg Tiedemann",http://arxiv.org/pdf/2505.14423v1,cs.CL
SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection,"Predicting earnings surprises through the analysis of earnings conference
call transcripts has attracted increasing attention from the financial research
community. Conference calls serve as critical communication channels between
company executives, analysts, and shareholders, offering valuable
forward-looking information. However, these transcripts present significant
analytical challenges, typically containing over 5,000 words with substantial
redundancy and industry-specific terminology that creates obstacles for
language models. In this work, we propose the Sparse Autoencoder for Financial
Representation Enhancement (SAE-FiRE) framework to address these limitations by
extracting key information while eliminating redundancy. SAE-FiRE employs
Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out
noises, and focusing specifically on capturing nuanced financial signals that
have predictive power for earnings surprises. Experimental results indicate
that the proposed method can significantly outperform comparing baselines.",2025-05-20,"Huopu Zhang, Yanguang Liu, Mengnan Du",http://arxiv.org/pdf/2505.14420v1,cs.CL
Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents,"Graphical user interface (GUI) agents powered by multimodal large language
models (MLLMs) have shown greater promise for human-interaction. However, due
to the high fine-tuning cost, users often rely on open-source GUI agents or
APIs offered by AI providers, which introduces a critical but underexplored
supply chain threat: backdoor attacks. In this work, we first unveil that
MLLM-powered GUI agents naturally expose multiple interaction-level triggers,
such as historical steps, environment states, and task progress. Based on this
observation, we introduce AgentGhost, an effective and stealthy framework for
red-teaming backdoor attacks. Specifically, we first construct composite
triggers by combining goal and interaction levels, allowing GUI agents to
unintentionally activate backdoors while ensuring task utility. Then, we
formulate backdoor injection as a Min-Max optimization problem that uses
supervised contrastive learning to maximize the feature difference across
sample classes at the representation space, improving flexibility of the
backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the
discrepancy between backdoor and clean behavior generation, enhancing
effectiveness and utility. Extensive evaluations of various agent models in two
established mobile benchmarks show that AgentGhost is effective and generic,
with attack accuracy that reaches 99.7\% on three attack objectives, and shows
stealthiness with only 1\% utility degradation. Furthermore, we tailor a
defense method against AgentGhost that reduces the attack accuracy to 22.1\%.
Our code is available at \texttt{anonymous}.",2025-05-20,"Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju, Zhuosheng Zhang, Gongshen Liu",http://arxiv.org/pdf/2505.14418v2,cs.CL
PRL: Prompts from Reinforcement Learning,"Effective prompt engineering remains a central challenge in fully harnessing
the capabilities of LLMs. While well-designed prompts can dramatically enhance
performance, crafting them typically demands expert intuition and a nuanced
understanding of the task. Moreover, the most impactful prompts often hinge on
subtle semantic cues, ones that may elude human perception but are crucial for
guiding LLM behavior. In this paper, we introduce PRL (Prompts from
Reinforcement Learning), a novel RL-based approach for automatic prompt
generation. Unlike previous methods, PRL can produce novel few-shot examples
that were not seen during training. Our approach achieves state-of-the-art
performance across a range of benchmarks, including text classification,
simplification, and summarization. On the classification task, it surpasses
prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it
improves the average ROUGE scores on the summarization task by 4.32 over APE
and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over
APE and by 6.01 over EvoPrompt. Our code is available at
https://github.com/Batorskq/prl .",2025-05-20,"Paweł Batorski, Adrian Kosmala, Paul Swoboda",http://arxiv.org/pdf/2505.14412v1,cs.CL
Pairwise Evaluation of Accent Similarity in Speech Synthesis,"Despite growing interest in generating high-fidelity accents, evaluating
accent similarity in speech synthesis has been underexplored. We aim to enhance
both subjective and objective evaluation methods for accent similarity.
Subjectively, we refine the XAB listening test by adding components that
achieve higher statistical significance with fewer listeners and lower costs.
Our method involves providing listeners with transcriptions, having them
highlight perceived accent differences, and implementing meticulous screening
for reliability. Objectively, we utilise pronunciation-related metrics, based
on distances between vowel formants and phonetic posteriorgrams, to evaluate
accent generation. Comparative experiments reveal that these metrics, alongside
accent similarity, speaker similarity, and Mel Cepstral Distortion, can be
used. Moreover, our findings underscore significant limitations of common
metrics like Word Error Rate in assessing underrepresented accents.",2025-05-20,"Jinzuomu Zhong, Suyuan Liu, Dan Wells, Korin Richmond",http://arxiv.org/pdf/2505.14410v1,cs.CL
"Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis","Large Language Models (LLMs), despite their remarkable capabilities, are
hampered by hallucinations. A particularly challenging variant, knowledge
overshadowing, occurs when one piece of activated knowledge inadvertently masks
another relevant piece, leading to erroneous outputs even with high-quality
training data. Current understanding of overshadowing is largely confined to
inference-time observations, lacking deep insights into its origins and
internal mechanisms during model training. Therefore, we introduce
PhantomCircuit, a novel framework designed to comprehensively analyze and
detect knowledge overshadowing. By innovatively employing knowledge circuit
analysis, PhantomCircuit dissects the internal workings of attention heads,
tracing how competing knowledge pathways contribute to the overshadowing
phenomenon and its evolution throughout the training process. Extensive
experiments demonstrate PhantomCircuit's effectiveness in identifying such
instances, offering novel insights into this elusive hallucination and
providing the research community with a new methodological lens for its
potential mitigation.",2025-05-20,"Haoming Huang, Yibo Yan, Jiahao Huo, Xin Zou, Xinfeng Li, Kun Wang, Xuming Hu",http://arxiv.org/pdf/2505.14406v2,cs.CL
OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking,"The code of nature, embedded in DNA and RNA genomes since the origin of life,
holds immense potential to impact both humans and ecosystems through genome
modeling. Genomic Foundation Models (GFMs) have emerged as a transformative
approach to decoding the genome. As GFMs scale up and reshape the landscape of
AI-driven genomics, the field faces an urgent need for rigorous and
reproducible evaluation. We present OmniGenBench, a modular benchmarking
platform designed to unify the data, model, benchmarking, and interpretability
layers across GFMs. OmniGenBench enables standardized, one-command evaluation
of any GFM across five benchmark suites, with seamless integration of over 31
open-source models. Through automated pipelines and community-extensible
features, the platform addresses critical reproducibility challenges, including
data transparency, model interoperability, benchmark fragmentation, and
black-box interpretability. OmniGenBench aims to serve as foundational
infrastructure for reproducible genomic AI research, accelerating trustworthy
discovery and collaborative innovation in the era of genome-scale modeling.",2025-05-20,"Heng Yang, Jack Cole, Yuan Li, Renzhi Chen, Geyong Min, Ke Li",http://arxiv.org/pdf/2505.14402v1,cs.CL
Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation,"While humans naturally learn and adapt from past experiences, large language
models (LLMs) and their agentic counterparts struggle to retain reasoning from
previous tasks and apply them in future contexts. To address this limitation,
we propose a novel framework, log-augmented generation (LAG) that directly
reuses prior computation and reasoning from past logs at test time to enhance
model's ability to learn from previous tasks and perform better on new, unseen
challenges, all while keeping the system efficient and scalable. Specifically,
our system represents task logs using key-value (KV) caches, encoding the full
reasoning context of prior tasks while storing KV caches for only a selected
subset of tokens. When a new task arises, LAG retrieves the KV values from
relevant logs to augment generation. Our approach differs from reflection-based
memory mechanisms by directly reusing prior reasoning and computations without
requiring additional steps for knowledge extraction or distillation. Our method
also goes beyond existing KV caching techniques, which primarily target
efficiency gains rather than improving accuracy. Experiments on knowledge- and
reasoning-intensive datasets demonstrate that our method significantly
outperforms standard agentic systems that do not utilize logs, as well as
existing solutions based on reflection and KV cache techniques.",2025-05-20,"Peter Baile Chen, Yi Zhang, Dan Roth, Samuel Madden, Jacob Andreas, Michael Cafarella",http://arxiv.org/pdf/2505.14398v1,cs.CL
Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds,"Causal world models are systems that can answer counterfactual questions
about an environment of interest, i.e. predict how it would have evolved if an
arbitrary subset of events had been realized differently. It requires
understanding the underlying causes behind chains of events and conducting
causal inference for arbitrary unseen distributions. So far, this task eludes
foundation models, notably large language models (LLMs), which do not have
demonstrated causal reasoning capabilities beyond the memorization of existing
causal relationships. Furthermore, evaluating counterfactuals in real-world
applications is challenging since only the factual world is observed, limiting
evaluation to synthetic datasets. We address these problems by explicitly
extracting and modeling causal relationships and propose the Causal
Cartographer framework. First, we introduce a graph retrieval-augmented
generation agent tasked to retrieve causal relationships from data. This
approach allows us to construct a large network of real-world causal
relationships that can serve as a repository of causal knowledge and build
real-world counterfactuals. In addition, we create a counterfactual reasoning
agent constrained by causal relationships to perform reliable step-by-step
causal inference. We show that our approach can extract causal knowledge and
improve the robustness of LLMs for causal reasoning tasks while reducing
inference costs and spurious correlations.",2025-05-20,"Gaël Gendron, Jože M. Rožanec, Michael Witbrock, Gillian Dobbie",http://arxiv.org/pdf/2505.14396v1,cs.CL
MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language,"Evaluating text generation capabilities of large language models (LLMs) is
challenging, particularly for low-resource languages where methods for direct
assessment are scarce. We propose MUG-Eval, a novel framework that evaluates
LLMs' multilingual generation capabilities by transforming existing benchmarks
into conversational tasks and measuring the LLMs' accuracies on those tasks. We
specifically designed these conversational tasks to require effective
communication in the target language. Then, we simply use task success rate as
a proxy of successful conversation generation. Our approach offers two key
advantages: it is independent of language-specific NLP tools or annotated
datasets, which are limited for most languages, and it does not rely on
LLMs-as-judges, whose evaluation quality degrades outside a few high-resource
languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and
low-resource categories, and we find that MUG-Eval correlates strongly with
established benchmarks ($r$ > 0.75) while enabling standardized comparisons
across languages and models. Our framework provides a robust and
resource-efficient solution for evaluating multilingual generation that can be
extended to thousands of languages.",2025-05-20,"Seyoung Song, Seogyeong Jeong, Eunsu Kim, Jiho Jin, Dongkwan Kim, Jay Shin, Alice Oh",http://arxiv.org/pdf/2505.14395v1,cs.CL
Editing Across Languages: A Survey of Multilingual Knowledge Editing,"While Knowledge Editing has been extensively studied in monolingual settings,
it remains underexplored in multilingual contexts. This survey systematizes
recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of
model editing focused on ensuring factual edits generalize reliably across
languages. We present a comprehensive taxonomy of MKE methods, covering
parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We
survey available benchmarks,summarize key findings on method effectiveness and
transfer patterns, identify challenges in cross-lingual propagation, and
highlight open problems related to language anisotropy, evaluation coverage,
and edit scalability. Our analysis consolidates a rapidly evolving area and
lays the groundwork for future progress in editable language-aware LLMs.",2025-05-20,"Nadir Durrani, Basel Mousi, Fahim Dalvi",http://arxiv.org/pdf/2505.14393v1,cs.CL
AutoRev: Automatic Peer Review System for Academic Research Papers,"Generating a review for an academic research paper is a complex task that
requires a deep understanding of the document's content and the
interdependencies between its sections. It demands not only insight into
technical details but also an appreciation of the paper's overall coherence and
structure. Recent methods have predominantly focused on fine-tuning large
language models (LLMs) to address this challenge. However, they often overlook
the computational and performance limitations imposed by long input token
lengths. To address this, we introduce AutoRev, an Automatic Peer Review System
for Academic Research Papers. Our novel framework represents an academic
document as a graph, enabling the extraction of the most critical passages that
contribute significantly to the review. This graph-based approach demonstrates
effectiveness for review generation and is potentially adaptable to various
downstream tasks, such as question answering, summarization, and document
representation. When applied to review generation, our method outperforms SOTA
baselines by an average of 58.72% across all evaluation metrics. We hope that
our work will stimulate further research in applying graph-based extraction
techniques to other downstream tasks in NLP. We plan to make our code public
upon acceptance.",2025-05-20,"Maitreya Prafulla Chitale, Ketaki Mangesh Shetye, Harshit Gupta, Manav Chaudhary, Vasudeva Varma",http://arxiv.org/pdf/2505.14376v1,cs.CL
Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs,"Recent studies demonstrate that Large Language Models (LLMs) are vulnerable
to different prompt-based attacks, generating harmful content or sensitive
information. Both closed-source and open-source LLMs are underinvestigated for
these attacks. This paper studies effective prompt injection attacks against
the $\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.
Current metrics only consider successful attacks, whereas our proposed Attack
Success Probability (ASP) also captures uncertainty in the model's response,
reflecting ambiguity in attack feasibility. By comprehensively analyzing the
effectiveness of prompt injection attacks, we propose a simple and effective
hypnotism attack; results show that this attack causes aligned language models,
including Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable
behaviors, achieving around $90$% ASP. They also indicate that our ignore
prefix attacks can break all $\mathbf{14}$ open-source LLMs, achieving over
$60$% ASP on a multi-categorical dataset. We find that moderately well-known
LLMs exhibit higher vulnerability to prompt injection attacks, highlighting the
need to raise public awareness and prioritize efficient mitigation strategies.",2025-05-20,"Jiawen Wang, Pritha Gupta, Ivan Habernal, Eyke Hüllermeier",http://arxiv.org/pdf/2505.14368v1,cs.CL
Dual Decomposition of Weights and Singular Value Low Rank Adaptation,"Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for
adapting Large Language Models (LLMs) to downstream tasks, among which Low-rank
Adaptation (LoRA) represents one of the most widely adopted methodologies.
However, existing LoRA-based approaches exhibit two fundamental limitations:
unstable training dynamics and inefficient knowledge transfer from pre-trained
models, both stemming from random initialization of adapter parameters. To
overcome these challenges, we propose DuDe, a novel approach that decomposes
weight matrices into magnitude and direction components, employing Singular
Value Decomposition (SVD) for principled initialization. Our comprehensive
evaluation demonstrates DuDe's superior performance and robustness, achieving
up to 48.35\% accuracy on MMLU and 62.53\% ($\pm$ 1.59) accuracy on GSM8K. Our
theoretical analysis and empirical validation collectively demonstrate that
DuDe's decomposition strategy enhances optimization stability and better
preserves pre-trained representations, particularly for domain-specific tasks
requiring specialized knowledge. The combination of robust empirical
performance and rigorous theoretical foundations establishes DuDe as a
significant contribution to PEFT methodologies for LLMs.",2025-05-20,"Jialong Han, Si Zhang, Ke Zhang",http://arxiv.org/pdf/2505.14367v2,cs.CL
"PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs","Despite significant progress in neural spoken dialog systems,
personality-aware conversation agents -- capable of adapting behavior based on
personalities -- remain underexplored due to the absence of personality
annotations in speech datasets. We propose a pipeline that preprocesses raw
audio recordings to create a dialogue dataset annotated with timestamps,
response types, and emotion/sentiment labels. We employ an automatic speech
recognition (ASR) system to extract transcripts and timestamps, then generate
conversation-level annotations. Leveraging these annotations, we design a
system that employs large language models to predict conversational
personality. Human evaluators were engaged to identify conversational
characteristics and assign personality labels. Our analysis demonstrates that
the proposed system achieves stronger alignment with human judgments compared
to existing approaches.",2025-05-20,"Sho Inoue, Shai Wang, Haizhou Li",http://arxiv.org/pdf/2505.14356v1,cs.CL
WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications,"Large Language Models (LLMs) have achieved impressive results across a broad
array of tasks, yet their capacity for complex, domain-specific mathematical
reasoning-particularly in wireless communications-remains underexplored. In
this work, we introduce WirelessMathBench, a novel benchmark specifically
designed to evaluate LLMs on mathematical modeling challenges to wireless
communications engineering. Our benchmark consists of 587 meticulously curated
questions sourced from 40 state-of-the-art research papers, encompassing a
diverse spectrum of tasks ranging from basic multiple-choice questions to
complex equation completion tasks, including both partial and full completions,
all of which rigorously adhere to physical and dimensional constraints. Through
extensive experimentation with leading LLMs, we observe that while many models
excel in basic recall tasks, their performance degrades significantly when
reconstructing partially or fully obscured equations, exposing fundamental
limitations in current LLMs. Even DeepSeek-R1, the best performer on our
benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%
success rate in full equation completion. By publicly releasing
WirelessMathBench along with the evaluation toolkit, we aim to advance the
development of more robust, domain-aware LLMs for wireless system analysis and
broader engineering applications.",2025-05-20,"Xin Li, Mengbing Liu, Li Wei, Jiancheng An, Mérouane Debbah, Chau Yuen",http://arxiv.org/pdf/2505.14354v1,cs.CL
"FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation","Tibetan is a low-resource language with minimal parallel speech corpora
spanning its three major dialects-\""U-Tsang, Amdo, and Kham-limiting progress
in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,
multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel
dialectal speech from limited reference audio and explicit dialect labels. Our
method features a novel speaker-dialect fusion module and a Dialect-Specialized
Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and
linguistic variations across dialects while preserving speaker identity.
Extensive objective and subjective evaluations demonstrate that FMSD-TTS
significantly outperforms baselines in both dialectal expressiveness and
speaker similarity. We further validate the quality and utility of the
synthesized speech through a challenging speech-to-speech dialect conversion
task. Our contributions include: (1) a novel few-shot TTS system tailored for
Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale
synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source
evaluation toolkit for standardized assessment of speaker similarity, dialect
consistency, and audio quality.",2025-05-20,"Yutong Liu, Ziyue Zhang, Ban Ma-bao, Yuqing Cai, Yongbin Yu, Renzeng Duojie, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi",http://arxiv.org/pdf/2505.14351v1,cs.CL
OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation,"Fine-tuning Large Language Models (LLMs) has become increasingly challenging
due to their massive scale and associated computational costs.
Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as
computational alternatives; however, their implementations still require
significant resources. In this paper, we present OSoRA (Output-Dimension and
Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.
OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value
Decomposition (SVD) with learnable scaling vectors in a unified framework. It
first performs an SVD of pre-trained weight matrices, then optimizes an
output-dimension vector during training, while keeping the corresponding
singular vector matrices frozen. OSoRA substantially reduces computational
resource requirements by minimizing the number of trainable parameters during
fine-tuning. Comprehensive evaluations across mathematical reasoning, common
sense reasoning, and other benchmarks demonstrate that OSoRA achieves
comparable or superior performance to state-of-the-art methods like LoRA and
VeRA, while maintaining a linear parameter scaling even as the rank increases
to higher dimensions. Our ablation studies further confirm that jointly
training both the singular values and the output-dimension vector is critical
for optimal performance.",2025-05-20,"Jialong Han, Si Zhang, Ke Zhang",http://arxiv.org/pdf/2505.14350v2,cs.CL
QA-prompting: Improving Summarization with Large Language Models using Question-Answering,"Language Models (LMs) have revolutionized natural language processing,
enabling high-quality text generation through prompting and in-context
learning. However, models often struggle with long-context summarization due to
positional biases, leading to suboptimal extraction of critical information.
There are techniques to improve this with fine-tuning, pipelining, or using
complex techniques, which have their own challenges. To solve these challenges,
we propose QA-prompting - a simple prompting method for summarization that
utilizes question-answering as an intermediate step prior to summary
generation. Our method extracts key information and enriches the context of
text to mitigate positional biases and improve summarization in a single LM
call per task without requiring fine-tuning or pipelining. Experiments on
multiple datasets belonging to different domains using ten state-of-the-art
pre-trained models demonstrate that QA-prompting outperforms baseline and other
state-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This
provides an effective and scalable solution for summarization and highlights
the importance of domain-specific question selection for optimal performance.",2025-05-20,Neelabh Sinha,http://arxiv.org/pdf/2505.14347v1,cs.CL
RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection,"Large language models (LLMs) have demonstrated remarkable capabilities in
various domains, including radiology report generation. Previous approaches
have attempted to utilize multimodal LLMs for this task, enhancing their
performance through the integration of domain-specific knowledge retrieval.
However, these approaches often overlook the knowledge already embedded within
the LLMs, leading to redundant information integration and inefficient
utilization of learned representations. To address this limitation, we propose
RADAR, a framework for enhancing radiology report generation with supplementary
knowledge injection. RADAR improves report generation by systematically
leveraging both the internal knowledge of an LLM and externally retrieved
information. Specifically, it first extracts the model's acquired knowledge
that aligns with expert image-based classification outputs. It then retrieves
relevant supplementary knowledge to further enrich this information. Finally,
by aggregating both sources, RADAR generates more accurate and informative
radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU
X-ray demonstrate that our model outperforms state-of-the-art LLMs in both
language quality and clinical accuracy",2025-05-20,"Wenjun Hou, Yi Cheng, Kaishuai Xu, Heng Li, Yan Hu, Wenjie Li, Jiang Liu",http://arxiv.org/pdf/2505.14318v1,cs.CL
A MIND for Reasoning: Meta-learning for In-context Deduction,"Large language models (LLMs) are increasingly evaluated on formal tasks,
where strong reasoning abilities define the state of the art. However, their
ability to generalize to out-of-distribution problems remains limited. In this
paper, we investigate how LLMs can achieve a systematic understanding of
deductive rules. Our focus is on the task of identifying the appropriate subset
of premises within a knowledge base needed to derive a given hypothesis. To
tackle this challenge, we propose Meta-learning for In-context Deduction
(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND
is to enable models to generalize more effectively to unseen knowledge bases
and to systematically apply inference rules. Our results show that MIND
significantly improves generalization in small LMs ranging from 1.5B to 7B
parameters. The benefits are especially pronounced in smaller models and
low-data settings. Remarkably, small models fine-tuned with MIND outperform
state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.",2025-05-20,"Leonardo Bertolazzi, Manuel Vargas Guzmán, Raffaella Bernardi, Maciej Malicki, Jakub Szymanik",http://arxiv.org/pdf/2505.14313v1,cs.CL
"HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing","Hausa Natural Language Processing (NLP) has gained increasing attention in
recent years, yet remains understudied as a low-resource language despite
having over 120 million first-language (L1) and 80 million second-language (L2)
speakers worldwide. While significant advances have been made in high-resource
languages, Hausa NLP faces persistent challenges, including limited open-source
datasets and inadequate model representation. This paper presents an overview
of the current state of Hausa NLP, systematically examining existing resources,
research contributions, and gaps across fundamental NLP tasks: text
classification, machine translation, named entity recognition, speech
recognition, and question answering. We introduce HausaNLP
(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,
tools, and research works to enhance accessibility and drive further
development. Furthermore, we discuss challenges in integrating Hausa into large
language models (LLMs), addressing issues of suboptimal tokenization and
dialectal variation. Finally, we propose strategic research directions
emphasizing dataset expansion, improved language modeling approaches, and
strengthened community collaboration to advance Hausa NLP. Our work provides
both a foundation for accelerating Hausa NLP progress and valuable insights for
broader multilingual NLP research.",2025-05-20,"Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Tajuddeen Gwadabe, Kenneth Church, Vukosi Marivate",http://arxiv.org/pdf/2505.14311v2,cs.CL
Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency,"Retrieval-augmented language models have demonstrated performance comparable
to much larger models while requiring fewer computational resources. The
effectiveness of these models crucially depends on the overlap between query
and retrieved context, but the optimal degree of this overlap remains
unexplored. In this paper, we systematically investigate how varying levels of
query--context overlap affect model performance during both training and
inference. Our experiments reveal that increased overlap initially has minimal
effect, but substantially improves test-time perplexity and accelerates model
learning above a critical threshold. Building on these findings, we demonstrate
that deliberately increasing overlap through synthetic context can enhance data
efficiency and reduce training time by approximately 40\% without compromising
performance. We specifically generate synthetic context through paraphrasing
queries. We validate our perplexity-based findings on question-answering tasks,
confirming that the benefits of retrieval-augmented language modeling extend to
practical applications. Our results provide empirical evidence of significant
optimization potential for retrieval mechanisms in language model pretraining.",2025-05-20,"Ehsan Doostmohammadi, Marco Kuhlmann",http://arxiv.org/pdf/2505.14309v1,cs.CL
JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling,"Text-to-SQL, which maps natural language to SQL queries, has benefited
greatly from recent advances in Large Language Models (LLMs). While LLMs offer
various paradigms for this task, including prompting and supervised fine-tuning
(SFT), SFT approaches still face challenges such as complex multi-stage
pipelines and poor robustness to noisy schema information. To address these
limitations, we present JOLT-SQL, a streamlined single-stage SFT framework that
jointly optimizes schema linking and SQL generation via a unified loss.
JOLT-SQL employs discriminative schema linking, enhanced by local bidirectional
attention, alongside a confusion-aware noisy schema sampling strategy with
selective attention to improve robustness under noisy schema conditions.
Experiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL
achieves state-of-the-art execution accuracy among comparable-size open-source
models, while significantly improving both training and inference efficiency.",2025-05-20,"Jinwang Song, Hongying Zan, Kunli Zhang, Lingling Mu, Yingjie Han, Haobo Hua, Min Peng",http://arxiv.org/pdf/2505.14305v1,cs.CL
Scaling Law for Quantization-Aware Training,"Large language models (LLMs) demand substantial computational and memory
resources, creating deployment challenges. Quantization-aware training (QAT)
addresses these challenges by reducing model precision while maintaining
performance. However, the scaling behavior of QAT, especially at 4-bit
precision (W4A4), is not well understood. Existing QAT scaling laws often
ignore key factors such as the number of training tokens and quantization
granularity, which limits their applicability. This paper proposes a unified
scaling law for QAT that models quantization error as a function of model size,
training data volume, and quantization group size. Through 268 QAT experiments,
we show that quantization error decreases as model size increases, but rises
with more training tokens and coarser quantization granularity. To identify the
sources of W4A4 quantization error, we decompose it into weight and activation
components. Both components follow the overall trend of W4A4 quantization
error, but with different sensitivities. Specifically, weight quantization
error increases more rapidly with more training tokens. Further analysis shows
that the activation quantization error in the FC2 layer, caused by outliers, is
the primary bottleneck of W4A4 QAT quantization error. By applying
mixed-precision quantization to address this bottleneck, we demonstrate that
weight and activation quantization errors can converge to similar levels.
Additionally, with more training data, weight quantization error eventually
exceeds activation quantization error, suggesting that reducing weight
quantization error is also important in such scenarios. These findings offer
key insights for improving QAT research and development.",2025-05-20,"Mengzhao Chen, Chaoyi Zhang, Jing Liu, Yutao Zeng, Zeyue Xue, Zhiheng Liu, Yunshui Li, Jin Ma, Jie Huang, Xun Zhou, Ping Luo",http://arxiv.org/pdf/2505.14302v1,cs.CL
SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors,"High-risk industries like nuclear and aviation use real-time monitoring to
detect dangerous system conditions. Similarly, Large Language Models (LLMs)
need monitoring safeguards. We propose a real-time framework to predict harmful
AI outputs before they occur by using an unsupervised approach that treats
normal behavior as the baseline and harmful outputs as outliers. Our study
focuses specifically on backdoor-triggered responses -- where specific input
phrases activate hidden vulnerabilities causing the model to generate unsafe
content like violence, pornography, or hate speech. We address two key
challenges: (1) identifying true causal indicators rather than surface
correlations, and (2) preventing advanced models from deception -- deliberately
evading monitoring systems. Hence, we approach this problem from an
unsupervised lens by drawing parallels to human deception: just as humans
exhibit physical indicators while lying, we investigate whether LLMs display
distinct internal behavioral signatures when generating harmful content. Our
study addresses two critical challenges: 1) designing monitoring systems that
capture true causal indicators rather than superficial correlations; and
2)preventing intentional evasion by increasingly capable ""Future models''. Our
findings show that models can produce harmful content through causal mechanisms
and can become deceptive by: (a) alternating between linear and non-linear
representations, and (b) modifying feature relationships. To counter this, we
developed Safety-Net -- a multi-detector framework that monitors different
representation dimensions, successfully detecting harmful behavior even when
information is shifted across representational spaces to evade individual
monitors. Our evaluation shows 96% accuracy in detecting harmful cases using
our unsupervised ensemble approach.",2025-05-20,"Maheep Chaudhary, Fazl Barez",http://arxiv.org/pdf/2505.14300v1,cs.CL
Cross-Lingual Optimization for Language Transfer in Large Language Models,"Adapting large language models to other languages typically employs
supervised fine-tuning (SFT) as a standard approach. However, it often suffers
from an overemphasis on English performance, a phenomenon that is especially
pronounced in data-constrained environments. To overcome these challenges, we
propose \textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an
English-centric LLM to a target language while preserving its English
capabilities. CLO utilizes publicly available English SFT data and a
translation model to enable cross-lingual transfer. We conduct experiments
using five models on six languages, each possessing varying levels of resource.
Our results show that CLO consistently outperforms SFT in both acquiring target
language proficiency and maintaining English performance. Remarkably, in
low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400
samples, demonstrating that CLO can achieve better performance with less data.
Furthermore, we find that SFT is particularly sensitive to data quantity in
medium and low-resource languages, whereas CLO remains robust. Our
comprehensive analysis emphasizes the limitations of SFT and incorporates
additional training strategies in CLO to enhance efficiency.",2025-05-20,"Jungseob Lee, Seongtae Hong, Hyeonseok Moon, Heuiseok Lim",http://arxiv.org/pdf/2505.14297v1,cs.CL
GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data,"Open-source large language models (LLMs) still marginalise Moroccan Arabic
(Darija), forcing practitioners either to bolt on heavyweight Arabic adapters
or to sacrifice the very reasoning skills that make LLMs useful. We show that a
rigorously quality-over-quantity alignment strategy can surface fluent Darija
while safeguarding the backbone s cross-lingual reasoning at a sliver of the
usual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6
K and TULU 50 K into Darija, preserve 20 of the English originals, and add
mathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on
5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the
reasoning-dense TULU portion pushes it to 47.5 with no English regression.
Scaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which
matches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense,
scoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc
retains Gemma-27B s strong maths and general-reasoning ability, showing only
minimal movement on GSM8K and English benchmarks. The entire model is trained
in just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable
language technology. We release code, data and checkpoints to spur
Darija-centric applications in education, public services and everyday digital
interaction.",2025-05-20,"Abderrahman Skiredj, Ferdaous Azhari, Houdaifa Atou, Nouamane Tazi, Ismail Berrada",http://arxiv.org/pdf/2505.17082v1,cs.CL
Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs,"The combination of pre-trained speech encoders with large language models has
enabled the development of speech LLMs that can handle a wide range of spoken
language processing tasks. While these models are powerful and flexible, this
very flexibility may make them more vulnerable to adversarial attacks. To
examine the extent of this problem, in this work we investigate universal
acoustic adversarial attacks on speech LLMs. Here a fixed, universal,
adversarial audio segment is prepended to the original input audio. We
initially investigate attacks that cause the model to either produce no output
or to perform a modified task overriding the original prompt. We then extend
the nature of the attack to be selective so that it activates only when
specific input attributes, such as a speaker gender or spoken language, are
present. Inputs without the targeted attribute should be unaffected, allowing
fine-grained control over the model outputs. Our findings reveal critical
vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar
speech LLMs may be susceptible to universal adversarial attacks. This
highlights the need for more robust training strategies and improved resistance
to adversarial attacks.",2025-05-20,"Rao Ma, Mengjie Qian, Vyas Raina, Mark Gales, Kate Knill",http://arxiv.org/pdf/2505.14286v1,cs.CL
YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering,"Large Language Models (LLMs) drive scientific question-answering on modern
search engines, yet their evaluation robustness remains underexplored. We
introduce YESciEval, an open-source framework that combines fine-grained
rubric-based assessment with reinforcement learning to mitigate optimism bias
in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including
adversarial variants, with evaluation scores from multiple LLMs. Independent of
proprietary models and human feedback, our approach enables scalable, cost-free
evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI
alignment and fosters robust, transparent evaluation essential for scientific
inquiry and artificial general intelligence.",2025-05-20,"Jennifer D'Souza, Hamed Babaei Giglou, Quentin Münch",http://arxiv.org/pdf/2505.14279v1,cs.CL
Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data,"Considering the importance of detecting hateful language, labeled hate speech
data is expensive and time-consuming to collect, particularly for low-resource
languages. Prior work has demonstrated the effectiveness of cross-lingual
transfer learning and data augmentation in improving performance on tasks with
limited labeled data. To develop an efficient and scalable cross-lingual
transfer learning approach, we leverage nearest-neighbor retrieval to augment
minimal labeled data in the target language, thereby enhancing detection
performance. Specifically, we assume access to a small set of labeled training
instances in the target language and use these to retrieve the most relevant
labeled examples from a large multilingual hate speech detection pool. We
evaluate our approach on eight languages and demonstrate that it consistently
outperforms models trained solely on the target language data. Furthermore, in
most cases, our method surpasses the current state-of-the-art. Notably, our
approach is highly data-efficient, retrieving as small as 200 instances in some
cases while maintaining superior performance. Moreover, it is scalable, as the
retrieval pool can be easily expanded, and the method can be readily adapted to
new languages and tasks. We also apply maximum marginal relevance to mitigate
redundancy and filter out highly similar retrieved instances, resulting in
improvements in some languages.",2025-05-20,"Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser",http://arxiv.org/pdf/2505.14272v2,cs.CL
FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning,"The growing collaboration between humans and AI models in generative tasks
has introduced new challenges in distinguishing between human-written,
AI-generated, and human-AI collaborative texts. In this work, we collect a
multilingual, multi-domain, multi-generator dataset FAIDSet. We further
introduce a fine-grained detection framework FAID to classify text into these
three categories, meanwhile identifying the underlying AI model family. Unlike
existing binary classifiers, FAID is built to capture both authorship and
model-specific characteristics. Our method combines multi-level contrastive
learning with multi-task auxiliary classification to learn subtle stylistic
cues. By modeling AI families as distinct stylistic entities, FAID offers
improved interpretability. We incorporate an adaptation to address
distributional shifts without retraining for unseen data. Experimental results
demonstrate that FAID outperforms several baseline approaches, particularly
enhancing the generalization accuracy on unseen domains and new AI models. It
provide a potential solution for improving transparency and accountability in
AI-assisted writing.",2025-05-20,"Minh Ngoc Ta, Dong Cao Van, Duc-Anh Hoang, Minh Le-Anh, Truong Nguyen, My Anh Tran Nguyen, Yuxia Wang, Preslav Nakov, Sang Dinh",http://arxiv.org/pdf/2505.14271v1,cs.CL
Think-J: Learning to Think for Generative LLM-as-a-Judge,"LLM-as-a-Judge refers to the automatic modeling of preferences for responses
generated by Large Language Models (LLMs), which is of significant importance
for both LLM evaluation and reward modeling. Although generative LLMs have made
substantial progress in various tasks, their performance as LLM-Judge still
falls short of expectations. In this work, we propose Think-J, which improves
generative LLM-as-a-Judge by learning how to think. We first utilized a small
amount of curated data to develop the model with initial judgment thinking
capabilities. Subsequently, we optimize the judgment thinking traces based on
reinforcement learning (RL). We propose two methods for judgment thinking
optimization, based on offline and online RL, respectively. The offline RL
requires training a critic model to construct positive and negative examples
for learning. The online method defines rule-based reward as feedback for
optimization. Experimental results showed that our approach can significantly
enhance the evaluation capability of generative LLM-Judge, surpassing both
generative and classifier-based LLM-Judge without requiring extra human
annotations.",2025-05-20,"Hui Huang, Yancheng He, Hongli Zhou, Rui Zhang, Wei Liu, Weixun Wang, Wenbo Su, Bo Zheng, Jiaheng Liu",http://arxiv.org/pdf/2505.14268v1,cs.CL
AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum,"Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.",2025-05-20,"Jian Xiong, Jingbo Zhou, Jingyong Ye, Dejing Dou",http://arxiv.org/pdf/2505.14264v1,cs.CL
FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation,"In this paper, we present FuxiMT, a novel Chinese-centric multilingual
machine translation model powered by a sparsified large language model (LLM).
We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on
a massive Chinese corpus and then conduct multilingual fine-tuning on a large
parallel dataset encompassing 65 languages. FuxiMT incorporates
Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust
performance across various resource levels. Experimental results demonstrate
that FuxiMT significantly outperforms strong baselines, including
state-of-the-art LLMs and machine translation models, particularly under
low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot
translation capabilities for unseen language pairs, indicating its potential to
bridge communication gaps where parallel data are scarce or unavailable.",2025-05-20,"Shaolin Zhu, Tianyu Dong, Bo Li, Deyi Xiong",http://arxiv.org/pdf/2505.14256v1,cs.CL
TransBench: Benchmarking Machine Translation for Industrial-Scale Applications,"Machine translation (MT) has become indispensable for cross-border
communication in globalized industries like e-commerce, finance, and legal
services, with recent advancements in large language models (LLMs)
significantly enhancing translation quality. However, applying general-purpose
MT models to industrial scenarios reveals critical limitations due to
domain-specific terminology, cultural nuances, and stylistic conventions absent
in generic benchmarks. Existing evaluation frameworks inadequately assess
performance in specialized contexts, creating a gap between academic benchmarks
and real-world efficacy. To address this, we propose a three-level translation
capability framework: (1) Basic Linguistic Competence, (2) Domain-Specific
Proficiency, and (3) Cultural Adaptation, emphasizing the need for holistic
evaluation across these dimensions. We introduce TransBench, a benchmark
tailored for industrial MT, initially targeting international e-commerce with
17,000 professionally translated sentences spanning 4 main scenarios and 33
language pairs. TransBench integrates traditional metrics (BLEU, TER) with
Marco-MOS, a domain-specific evaluation model, and provides guidelines for
reproducible benchmark construction. Our contributions include: (1) a
structured framework for industrial MT evaluation, (2) the first publicly
available benchmark for e-commerce translation, (3) novel metrics probing
multi-level translation quality, and (4) open-sourced evaluation tools. This
work bridges the evaluation gap, enabling researchers and practitioners to
systematically assess and enhance MT systems for industry-specific needs.",2025-05-20,"Haijun Li, Tianqi Shi, Zifu Shang, Yuxuan Han, Xueyu Zhao, Hao Wang, Yu Qian, Zhiqiang Qian, Linlong Xu, Minghao Wu, Chenyang Lyu, Longyue Wang, Gongbo Tang, Weihua Luo, Zhao Xu, Kaifu Zhang",http://arxiv.org/pdf/2505.14244v1,cs.CL
Technical Report on classification of literature related to children speech disorder,"This technical report presents a natural language processing (NLP)-based
approach for systematically classifying scientific literature on childhood
speech disorders. We retrieved and filtered 4,804 relevant articles published
after 2015 from the PubMed database using domain-specific keywords. After
cleaning and pre-processing the abstracts, we applied two topic modeling
techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify
latent thematic structures in the corpus. Our models uncovered 14 clinically
meaningful clusters, such as infantile hyperactivity and abnormal epileptic
behavior. To improve relevance and precision, we incorporated a custom stop
word list tailored to speech pathology. Evaluation results showed that the LDA
model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating
strong topic coherence and predictive performance. The BERTopic model exhibited
a low proportion of outlier topics (less than 20%), demonstrating its capacity
to classify heterogeneous literature effectively. These results provide a
foundation for automating literature reviews in speech-language pathology.",2025-05-20,"Ziang Wang, Amir Aryani",http://arxiv.org/pdf/2505.14242v1,cs.CL
ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models,"Large Language Models have demonstrated strong performance across a wide
range of tasks, but adapting them efficiently to new domains remains a key
challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by
introducing lightweight, trainable modules while keeping most pre-trained
weights fixed. The prevailing approach, LoRA, models updates using a low-rank
decomposition, but its expressivity is inherently constrained by the rank.
Recent methods like HiRA aim to increase expressivity by incorporating a
Hadamard product with the frozen weights, but still rely on the structure of
the pre-trained model. We introduce ABBA, a new PEFT architecture that
reparameterizes the update as a Hadamard product of two independently learnable
low-rank matrices. In contrast to prior work, ABBA fully decouples the update
from the pre-trained weights, enabling both components to be optimized freely.
This leads to significantly higher expressivity under the same parameter
budget. We formally analyze ABBA's expressive capacity and validate its
advantages through matrix reconstruction experiments. Empirically, ABBA
achieves state-of-the-art results on arithmetic and commonsense reasoning
benchmarks, consistently outperforming existing PEFT methods by a significant
margin across multiple models. Our code is publicly available at:
https://github.com/CERT-Lab/abba.",2025-05-20,"Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Praneeth Vepakomma",http://arxiv.org/pdf/2505.14238v2,cs.CL
Mechanistic Fine-tuning for In-context Learning,"In-context Learning (ICL) utilizes structured demonstration-query inputs to
induce few-shot learning on Language Models (LMs), which are not originally
pre-trained on ICL-style data. To bridge the gap between ICL and pre-training,
some approaches fine-tune LMs on large ICL-style datasets by an end-to-end
paradigm with massive computational costs. To reduce such costs, in this paper,
we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous
findings on the inner mechanism of ICL, building training objectives on the
attention scores instead of the final outputs, to force the attention scores to
focus on the correct label tokens presented in the context and mitigate
attention scores from the wrong label tokens. Our experiments on 9 modern LMs
and 8 datasets empirically find that ABFT outperforms in performance,
robustness, unbiasedness, and efficiency, with only around 0.01% data cost
compared to the previous methods. Moreover, our subsequent analysis finds that
the end-to-end training objective contains the ABFT objective, suggesting the
implicit bias of ICL-style data to the emergence of induction heads. Our work
demonstrates the possibility of controlling specific module sequences within
LMs to improve their behavior, opening up the future application of mechanistic
interpretability.",2025-05-20,"Hakaze Cho, Peng Luo, Mariko Kato, Rin Kaenbyou, Naoya Inoue",http://arxiv.org/pdf/2505.14233v1,cs.CL
"""Haet Bhasha aur Diskrimineshun"": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs","Large Language Models (LLMs) have become increasingly powerful, with
multilingual and multimodal capabilities improving by the day. These models are
being evaluated through audits, alignment studies and red-teaming efforts to
expose model vulnerabilities towards generating harmful, biased and unfair
content. Existing red-teaming efforts have previously focused on the English
language, using fixed template-based attacks; thus, models continue to be
susceptible to multilingual jailbreaking strategies, especially in the
multimodal context. In this study, we introduce a novel strategy that leverages
code-mixing and phonetic perturbations to jailbreak LLMs for both text and
image generation tasks. We also introduce two new jailbreak strategies that
show higher effectiveness than baseline strategies. Our work presents a method
to effectively bypass safety filters in LLMs while maintaining interpretability
by applying phonetic misspellings to sensitive words in code-mixed prompts. Our
novel prompts achieve a 99% Attack Success Rate for text generation and 78% for
image generation, with Attack Relevance Rate of 100% for text generation and
95% for image generation when using the phonetically perturbed code-mixed
prompts. Our interpretability experiments reveal that phonetic perturbations
impact word tokenization, leading to jailbreak success. Our study motivates
increasing the focus towards more generalizable safety alignment for
multilingual multimodal models, especially in real-world settings wherein
prompts can have misspelt words.",2025-05-20,"Darpan Aswal, Siddharth D Jaiswal",http://arxiv.org/pdf/2505.14226v1,cs.CL
Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning,"Recent studies have shown that reinforcement learning with verifiable rewards
(RLVR) enhances overall accuracy but fails to improve capability, while
distillation can improve both. In this paper, we investigate the mechanisms
behind these phenomena. First, we demonstrate that RLVR does not improve
capability because it focuses on improving the accuracy of the less-difficult
questions to the detriment of the accuracy of the most difficult questions,
thereby leading to no improvement in capability. Second, we find that RLVR does
not merely increase the success probability for the less difficult questions,
but in our small model settings produces quality responses that were absent in
its output distribution before training. In addition, we show these responses
are neither noticeably longer nor feature more reflection-related keywords,
underscoring the need for more reliable indicators of response quality. Third,
we show that while distillation reliably improves accuracy by learning strong
reasoning patterns, it only improves capability when new knowledge is
introduced. Moreover, when distilling only with reasoning patterns and no new
knowledge, the accuracy of the less-difficult questions improves to the
detriment of the most difficult questions, similar to RLVR. Together, these
findings offer a clearer understanding of how RLVR and distillation shape
reasoning behavior in language models.",2025-05-20,"Minwu Kim, Anubhav Shrestha, Safal Shrestha, Aadim Nepal, Keith Ross",http://arxiv.org/pdf/2505.14216v1,cs.CL
Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks,"A question-answering (QA) system is to search suitable answers within a
knowledge base. Current QA systems struggle with queries requiring complex
reasoning or real-time knowledge integration. They are often supplemented with
retrieval techniques on a data source such as Retrieval-Augmented Generation
(RAG). However, RAG continues to face challenges in handling complex reasoning
and logical connections between multiple sources of information. A novel
approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA
tasks is presented through the automated generation of context-based QA pairs.
This methodology leverages LLMs to create fine-tuning data, reducing reliance
on human labelling and improving model comprehension and reasoning
capabilities. The proposed system includes an automated QA generator and a
model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.
Comprehensive experiments demonstrate improvements in logical coherence and
factual accuracy, with implications for developing adaptable Artificial
Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,
BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA
pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA
pairs.",2025-05-20,"Sizhe Yuen, Ting Su, Ziyang Wang, Yali Du, Adam J. Sobey",http://arxiv.org/pdf/2505.14212v1,cs.CL
"Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification","Recent advancements in large language models (LLMs) have been fueled by large
scale training corpora drawn from diverse sources such as websites, news
articles, and books. These datasets often contain explicit user information,
such as person names and addresses, that LLMs may unintentionally reproduce in
their generated outputs. Beyond such explicit content, LLMs can also leak
identity revealing cues through implicit signals such as distinctive writing
styles, raising significant concerns about authorship privacy. There are three
major automated tasks in authorship privacy, namely authorship obfuscation
(AO), authorship mimicking (AM), and authorship verification (AV). Prior
research has studied AO, AM, and AV independently. However, their interplays
remain under explored, which leaves a major research gap, especially in the era
of LLMs, where they are profoundly shaping how we curate and share user
generated content, and the distinction between machine generated and human
authored text is also increasingly blurred. This work then presents the first
unified framework for analyzing the dynamic relationships among LLM enabled AO,
AM, and AV in the context of authorship privacy. We quantify how they interact
with each other to transform human authored text, examining effects at a single
point in time and iteratively over time. We also examine the role of
demographic metadata, such as gender, academic background, in modulating their
performances, inter-task dynamics, and privacy risks. All source code will be
publicly available.",2025-05-20,"Tuc Nguyen, Yifan Hu, Thai Le",http://arxiv.org/pdf/2505.14195v1,cs.CL
Safety Subspaces are Not Distinct: A Fine-Tuning Case Study,"Large Language Models (LLMs) rely on safety alignment to produce socially
acceptable responses. This is typically achieved through instruction tuning and
reinforcement learning from human feedback. However, this alignment is known to
be brittle: further fine-tuning, even on benign or lightly contaminated data,
can degrade safety and reintroduce harmful behaviors. A growing body of work
suggests that alignment may correspond to identifiable geometric directions in
weight space, forming subspaces that could, in principle, be isolated or
preserved to defend against misalignment. In this work, we conduct a
comprehensive empirical study of this geometric perspective. We examine whether
safety-relevant behavior is concentrated in specific subspaces, whether it can
be separated from general-purpose learning, and whether harmfulness arises from
distinguishable patterns in internal representations. Across both parameter and
activation space, our findings are consistent: subspaces that amplify safe
behaviors also amplify unsafe ones, and prompts with different safety
implications activate overlapping representations. We find no evidence of a
subspace that selectively governs safety. These results challenge the
assumption that alignment is geometrically localized. Rather than residing in
distinct directions, safety appears to emerge from entangled, high-impact
components of the model's broader learning dynamics. This suggests that
subspace-based defenses may face fundamental limitations and underscores the
need for alternative strategies to preserve alignment under continued training.
We corroborate these findings through multiple experiments on five open-source
LLMs. Our code is publicly available at:
https://github.com/CERT-Lab/safety-subspaces.",2025-05-20,"Kaustubh Ponkshe, Shaan Shah, Raghav Singhal, Praneeth Vepakomma",http://arxiv.org/pdf/2505.14185v1,cs.CL
"ThinkSwitcher: When to Think Hard, When to Think Fast","Large reasoning models (LRMs) excel at solving complex tasks by leveraging
long chain-of-thought (CoT) reasoning. However, this often leads to
overthinking on simple tasks, resulting in unnecessary computational overhead.
We observe that LRMs inherently possess the capability for efficient short CoT
reasoning, which can be reliably elicited through prompt design. To leverage
this capability, we propose ThinkSwitcher, a framework that enables a single
LRM to dynamically switch between short and long CoT modes based on task
complexity. ThinkSwitcher introduces a lightweight switching module trained
with supervision signals derived from the relative performance of each
reasoning mode across tasks. Experiments on multiple reasoning benchmarks show
that ThinkSwitcher reduces computational cost by 20-30% while maintaining high
accuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher
as a scalable and efficient solution for unified LRM deployment.",2025-05-20,"Guosheng Liang, Longguang Zhong, Ziyi Yang, Xiaojun Quan",http://arxiv.org/pdf/2505.14183v1,cs.CL
SlangDIT: Benchmarking LLMs in Interpretative Slang Translation,"The challenge of slang translation lies in capturing context-dependent
semantic extensions, as slang terms often convey meanings beyond their literal
interpretation. While slang detection, explanation, and translation have been
studied as isolated tasks in the era of large language models (LLMs), their
intrinsic interdependence remains underexplored. The main reason is lacking of
a benchmark where the two tasks can be a prerequisite for the third one, which
can facilitate idiomatic translation. In this paper, we introduce the
interpretative slang translation task (named SlangDIT) consisting of three
sub-tasks: slang detection, cross-lingual slang explanation, and slang
translation within the current context, aiming to generate more accurate
translation with the help of slang detection and slang explanation. To this
end, we construct a SlangDIT dataset, containing over 25k English-Chinese
sentence pairs. Each source sentence mentions at least one slang term and is
labeled with corresponding cross-lingual slang explanation. Based on the
benchmark, we propose a deep thinking model, named SlangOWL. It firstly
identifies whether the sentence contains a slang, and then judges whether the
slang is polysemous and analyze its possible meaning. Further, the SlangOWL
provides the best explanation of the slang term targeting on the current
context. Finally, according to the whole thought, the SlangOWL offers a
suitable translation. Our experiments on LLMs (\emph{e.g.}, Qwen2.5 and
LLama-3.1), show that our deep thinking approach indeed enhances the
performance of LLMs where the proposed SLangOWL significantly surpasses the
vanilla models and supervised fine-tuned models without thinking.",2025-05-20,"Yunlong Liang, Fandong Meng, Jiaan Wang, Jie Zhou",http://arxiv.org/pdf/2505.14181v1,cs.CL
Enhancing Abstractive Summarization of Scientific Papers Using Structure Information,"Abstractive summarization of scientific papers has always been a research
focus, yet existing methods face two main challenges. First, most summarization
models rely on Encoder-Decoder architectures that treat papers as sequences of
words, thus fail to fully capture the structured information inherent in
scientific papers. Second, existing research often use keyword mapping or
feature engineering to identify the structural information, but these methods
struggle with the structural flexibility of scientific papers and lack
robustness across different disciplines. To address these challenges, we
propose a two-stage abstractive summarization framework that leverages
automatic recognition of structural functions within scientific papers. In the
first stage, we standardize chapter titles from numerous scientific papers and
construct a large-scale dataset for structural function recognition. A
classifier is then trained to automatically identify the key structural
components (e.g., Background, Methods, Results, Discussion), which provides a
foundation for generating more balanced summaries. In the second stage, we
employ Longformer to capture rich contextual relationships across sections and
generating context-aware summaries. Experiments conducted on two
domain-specific scientific paper summarization datasets demonstrate that our
method outperforms advanced baselines, and generates more comprehensive
summaries. The code and dataset can be accessed at
https://github.com/tongbao96/code-for-SFR-AS.",2025-05-20,"Tong Bao, Heng Zhang, Chengzhi Zhang",http://arxiv.org/pdf/2505.14179v1,cs.CL
Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits,"Tokenization is the first - and often underappreciated - layer of computation
in language models. While Chain-of-Thought (CoT) prompting enables transformer
models to approximate recurrent computation by externalizing intermediate
steps, we show that the success of such reasoning is fundamentally bounded by
the structure of tokenized inputs. This work presents a theoretical and
empirical investigation into how tokenization schemes, particularly
subword-based methods like byte-pair encoding (BPE), impede symbolic
computation by merging or obscuring atomic reasoning units. We introduce the
notion of Token Awareness to formalize how poor token granularity disrupts
logical alignment and prevents models from generalizing symbolic procedures.
Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate
that token structure dramatically affect reasoning performance, causing failure
even with CoT, while atomically-aligned formats unlock strong generalization,
allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,
o1) in structured reasoning. Our findings reveal that symbolic reasoning
ability in LLMs is not purely architectural, but deeply conditioned on
token-level representations.",2025-05-20,"Xiang Zhang, Juntai Cao, Jiaqi Wei, Yiwei Xu, Chenyu You",http://arxiv.org/pdf/2505.14178v1,cs.CL
"Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning","LLMs are effective at code generation tasks like text-to-SQL, but is it worth
the cost? Many state-of-the-art approaches use non-task-specific LLM techniques
including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These
methods can be costly at inference time, sometimes requiring over a hundred LLM
calls with reasoning, incurring average costs of up to \$0.46 per query, while
fine-tuning models can cost thousands of dollars. We introduce ""N-rep""
consistency, a more cost-efficient text-to-SQL approach that achieves similar
BIRD benchmark scores as other more expensive methods, at only \$0.039 per
query. N-rep leverages multiple representations of the same schema input to
mitigate weaknesses in any single representation, making the solution more
robust and allowing the use of smaller and cheaper models without any reasoning
or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL
approach in its cost range.",2025-05-20,"Yusuf Denizay Dönder, Derek Hommel, Andrea W Wen-Yi, David Mimno, Unso Eun Seo Jo",http://arxiv.org/pdf/2505.14174v1,cs.CL
THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation,"The sparse Mixture-of-Experts (MoE) has achieved significant progress for
neural machine translation (NMT). However, there exist two limitations in
current MoE solutions which may lead to sub-optimal performance: 1) they
directly use the task knowledge of NMT into MoE (\emph{e.g.},
domain/linguistics-specific knowledge), which are generally unavailable at
practical application and neglect the naturally grouped domain/linguistic
properties; 2) the expert selection only depends on the localized token
representation without considering the context, which fully grasps the state of
each token in a global view. To address the above limitations, we propose
THOR-MoE via arming the MoE with hierarchical task-guided and
context-responsive routing policies. Specifically, it 1) firstly predicts the
domain/language label and then extracts mixed domain/language representation to
allocate task-level experts in a hierarchical manner; 2) injects the context
information to enhance the token routing from the pre-selected task-level
experts set, which can help each token to be accurately routed to more
specialized and suitable experts. Extensive experiments on multi-domain
translation and multilingual translation benchmarks with different
architectures consistently demonstrate the superior performance of THOR-MoE.
Additionally, the THOR-MoE operates as a plug-and-play module compatible with
existing Top-$k$~\cite{shazeer2017} and Top-$p$~\cite{huang-etal-2024-harder}
routing schemes, ensuring broad applicability across diverse MoE architectures.
For instance, compared with vanilla Top-$p$~\cite{huang-etal-2024-harder}
routing, the context-aware manner can achieve an average improvement of 0.75
BLEU with less than 22\% activated parameters on multi-domain translation
tasks.",2025-05-20,"Yunlong Liang, Fandong Meng, Jie Zhou",http://arxiv.org/pdf/2505.14173v1,cs.CL
The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models,"Despite their remarkable progress across diverse domains, Large Language
Models (LLMs) consistently fail at simple character-level tasks, such as
counting letters in words, due to a fundamental limitation: tokenization. In
this work, we frame this limitation as a problem of low mutual information and
analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks
that isolate character-level reasoning in a controlled setting, we show that
such capabilities emerge slowly, suddenly, and only late in training. We
further show that percolation-based models of concept emergence explain these
patterns, suggesting that learning character composition is not fundamentally
different from learning commonsense knowledge. To address this bottleneck, we
propose a lightweight architectural modification that significantly improves
character-level reasoning while preserving the inductive advantages of subword
models. Together, our results bridge low-level perceptual gaps in tokenized LMs
and provide a principled framework for understanding and mitigating their
structural blind spots. We make our code publicly available.",2025-05-20,"Adrian Cosma, Stefan Ruseti, Emilian Radoi, Mihai Dascalu",http://arxiv.org/pdf/2505.14172v2,cs.CL
PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore,"Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity
toward specific aspects within a text, enabling more precise opinion mining in
domains such as product reviews and social media. However, traditional FGSA
approaches often require task-specific architectures and extensive annotated
data, limiting their generalization and scalability. To address these
challenges, we propose PL-FGSA, a unified prompt learning-based framework
implemented using the MindSpore platform, which integrates prompt design with a
lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task
prompt-augmented generation problem, jointly tackling aspect extraction,
sentiment classification, and causal explanation in a unified paradigm. By
leveraging prompt-based guidance, PL-FGSA enhances interpretability and
achieves strong performance under both full-data and low-resource conditions.
Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and
MAMS-demonstrate that our model consistently outperforms traditional
fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,
respectively. These results validate the effectiveness of prompt-based
generalization and highlight the practical value of PL-FGSA for real-world
sentiment analysis tasks.",2025-05-20,"Zhenkai Qin, Jiajing He, Qiao Fang",http://arxiv.org/pdf/2505.14165v1,cs.CL
Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models,"Multilingual vision-language models promise universal image-text retrieval,
yet their social biases remain under-explored. We present the first systematic
audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and
CAPIVARA-CLIP -- across ten languages that vary in resource availability and
grammatical gender. Using balanced subsets of \textsc{FairFace} and the
\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and
gender bias and measure stereotype amplification. Contrary to the assumption
that multilinguality mitigates bias, every model exhibits stronger gender bias
than its English-only baseline. CAPIVARA-CLIP shows its largest biases
precisely in the low-resource languages it targets, while the shared
cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into
gender-neutral languages; loosely coupled encoders largely avoid this transfer.
Highly gendered languages consistently magnify all measured bias types, but
even gender-neutral languages remain vulnerable when cross-lingual weight
sharing imports foreign stereotypes. Aggregated metrics conceal
language-specific ``hot spots,'' underscoring the need for fine-grained,
language-aware bias evaluation in future multilingual vision-language research.",2025-05-20,"Zahraa Al Sahili, Ioannis Patras, Matthew Purver",http://arxiv.org/pdf/2505.14160v1,cs.CL
Temporal Alignment of Time Sensitive Facts with Activation Engineering,"Large Language Models (LLMs) are trained on diverse and often conflicting
knowledge spanning multiple domains and time periods. Some of this knowledge is
only valid within specific temporal contexts, such as answering the question,
""Who is the President of the United States in 2022?"" Ensuring LLMs generate
time appropriate responses is crucial for maintaining relevance and accuracy.
In this work we explore activation engineering as a method for temporally
aligning LLMs to improve factual recall without any training or dataset
creation. In this research we explore an activation engineering technique to
ground three versions of LLaMA 2 to specific points in time and examine the
effects of varying injection layers and prompting strategies. Our experiments
demonstrate up to a 44% and 16% improvement in relative and explicit prompting
respectively, achieving comparable performance to the fine-tuning method
proposed by Zhao et al. (2024) . Notably, our approach achieves similar results
to the fine-tuning baseline while being significantly more computationally
efficient and requiring no pre-aligned datasets.",2025-05-20,"Sanjay Govindan, Maurice Pagnucco, Yang Song",http://arxiv.org/pdf/2505.14158v1,cs.CL
Prior Prompt Engineering for Reinforcement Fine-Tuning,"This paper investigates prior prompt engineering (pPE) in the context of
reinforcement fine-tuning (RFT), where language models (LMs) are incentivized
to exhibit behaviors that maximize performance through reward signals. While
existing RFT research has primarily focused on algorithms, reward shaping, and
data curation, the design of the prior prompt--the instructions prepended to
queries during training to elicit behaviors such as step-by-step
reasoning--remains underexplored. We investigate whether different pPE
approaches can guide LMs to internalize distinct behaviors after RFT. Inspired
by inference-time prompt engineering (iPE), we translate five representative
iPE strategies--reasoning, planning, code-based reasoning, knowledge recall,
and null-example utilization--into corresponding pPE approaches. We experiment
with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on
in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and
GPQA-Diamond). Our results show that all pPE-trained models surpass their
iPE-prompted counterparts, with the null-example pPE approach achieving the
largest average performance gain and the highest improvement on AIME2024 and
GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by
adapting a behavior-classification framework, we demonstrate that different pPE
strategies instill distinct behavioral styles in the resulting models. These
findings position pPE as a powerful yet understudied axis for RFT.",2025-05-20,"Pittawat Taveekitworachai, Potsawee Manakul, Sarana Nutanong, Kunat Pipatanakul",http://arxiv.org/pdf/2505.14157v1,cs.CL
Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information,"The exponential increase in academic papers has significantly increased the
time required for researchers to access relevant literature. Keyphrase
Extraction (KPE) offers a solution to this situation by enabling researchers to
efficiently retrieve relevant literature. The current study on KPE from
academic articles aims to improve the performance of extraction models through
innovative approaches using Title and Abstract as input corpora. However, the
semantic richness of keywords is significantly constrained by the length of the
abstract. While full-text-based KPE can address this issue, it simultaneously
introduces noise, which significantly diminishes KPE performance. To address
this issue, this paper utilized the structural features and section texts
obtained from the section structure information of academic articles to extract
keyphrase from academic papers. The approach consists of two main parts: (1)
exploring the effect of seven structural features on KPE models, and (2)
integrating the extraction results from all section texts used as input corpora
for KPE models via a keyphrase integration algorithm to obtain the keyphrase
integration result. Furthermore, this paper also examined the effect of the
classification quality of section structure on the KPE performance. The results
show that incorporating structural features improves KPE performance, though
different features have varying effects on model efficacy. The keyphrase
integration approach yields the best performance, and the classification
quality of section structure can affect KPE performance. These findings
indicate that using the section structure information of academic articles
contributes to effective KPE from academic articles. The code and dataset
supporting this study are available at https://github.com/yan-xinyi/SSB_KPE.",2025-05-20,"Chengzhi Zhang, Xinyi Yan, Lei Zhao, Yingyi Zhang",http://arxiv.org/pdf/2505.14149v1,cs.CL
s3: You Don't Need That Much Data to Train a Search Agent via RL,"Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.",2025-05-20,"Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, Jiawei Han",http://arxiv.org/pdf/2505.14146v1,cs.CL
Probing BERT for German Compound Semantics,"This paper investigates the extent to which pretrained German BERT encodes
knowledge of noun compound semantics. We comprehensively vary combinations of
target tokens, layers, and cased vs. uncased models, and evaluate them by
predicting the compositionality of 868 gold standard compounds. Looking at
representational patterns within the transformer architecture, we observe
trends comparable to equivalent prior work on English, with compositionality
information most easily recoverable in the early layers. However, our strongest
results clearly lag behind those reported for English, suggesting an inherently
more difficult task in German. This may be due to the higher productivity of
compounding in German than in English and the associated increase in
constituent-level ambiguity, including in our target compound set.",2025-05-20,"Filip Miletić, Aaron Schmid, Sabine Schulte im Walde",http://arxiv.org/pdf/2505.14130v1,cs.CL
Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering,"In table question answering (TQA), tables are encoded as either texts or
images. Prior work suggests that passing images of tables to multi-modal large
language models (MLLMs) performs comparably to or even better than using
textual input with large language models (LLMs). However, the lack of
controlled setups limits fine-grained distinctions between these approaches. In
this paper, we conduct the first controlled study on the effectiveness of
several combinations of table representations and models from two perspectives:
question complexity and table size. We build a new benchmark based on existing
TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we
find that the best combination of table representation and model varies across
setups. We propose FRES, a method selecting table representations dynamically,
and observe a 10% average performance improvement compared to using both
representations indiscriminately.",2025-05-20,"Wei Zhou, Mohsen Mesgar, Heike Adel, Annemarie Friedrich",http://arxiv.org/pdf/2505.14131v1,cs.CL
Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst,"Inference-time scaling has attracted much attention which significantly
enhance the performance of Large Language Models (LLMs) in complex reasoning
tasks by increasing the length of Chain-of-Thought. These longer intermediate
reasoning rationales embody various meta-reasoning skills in human cognition,
such as reflection and decomposition, being difficult to create and acquire. In
this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where
the model itself can synthesize longer CoT data and iteratively improve
performance through self-training. By incorporating a few demonstration
examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from
existing responses, which act as a reasoning catalyst, we demonstrate that SRLM
not only enhances the model's initial performance but also ensures more stable
and consistent improvements in subsequent iterations. Our proposed SRLM
achieves an average absolute improvement of more than $+2.5$ points across five
reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.
Moreover, it brings more improvements with more times of sampling during
inference, such as absolute $+7.89$ average improvement with $64$ sampling
times, revealing the in-depth, diverse and creative reasoning paths in SRLM
against the strong baseline.",2025-05-20,"Hongru Wang, Deng Cai, Wanjun Zhong, Shijue Huang, Jeff Z. Pan, Zeming Liu, Kam-Fai Wong",http://arxiv.org/pdf/2505.14116v1,cs.CL
Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking,"Logit-based LLM watermarking traces and verifies AI-generated content by
maintaining green and red token lists and increasing the likelihood of green
tokens during generation. However, it fails in low-entropy scenarios, where
predictable outputs make green token selection difficult without disrupting
natural text flow. Existing approaches address this by assuming access to the
original LLM to calculate entropy and selectively watermark high-entropy
tokens. However, these methods face two major challenges: (1) high
computational costs and detection delays due to reliance on the original LLM,
and (2) potential risks of model leakage. To address these limitations, we
propose Invisible Entropy (IE), a watermarking paradigm designed to enhance
both safety and efficiency. Instead of relying on the original LLM, IE
introduces a lightweight feature extractor and an entropy tagger to predict
whether the entropy of the next token is high or low. Furthermore, based on
theoretical analysis, we develop a threshold navigator that adaptively sets
entropy thresholds. It identifies a threshold where the watermark ratio
decreases as the green token count increases, enhancing the naturalness of the
watermarked text and improving detection robustness. Experiments on HumanEval
and MBPP datasets demonstrate that IE reduces parameter size by 99\% while
achieving performance on par with state-of-the-art methods. Our work introduces
a safe and efficient paradigm for low-entropy watermarking.
https://github.com/Carol-gutianle/IE
https://huggingface.co/datasets/Carol0110/IE-Tagger",2025-05-20,"Tianle Gu, Zongqi Wang, Kexin Huang, Yuanqi Yao, Xiangliang Zhang, Yujiu Yang, Xiuying Chen",http://arxiv.org/pdf/2505.14112v1,cs.CL
DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models,"The emergence of groundbreaking large language models capable of performing
complex reasoning tasks holds significant promise for addressing various
scientific challenges, including those arising in complex clinical scenarios.
To enable their safe and effective deployment in real-world healthcare
settings, it is urgently necessary to benchmark the diagnostic capabilities of
current models systematically. Given the limitations of existing medical
benchmarks in evaluating advanced diagnostic reasoning, we present
DiagnosisArena, a comprehensive and challenging benchmark designed to
rigorously assess professional-level diagnostic competence. DiagnosisArena
consists of 1,113 pairs of segmented patient cases and corresponding diagnoses,
spanning 28 medical specialties, deriving from clinical case reports published
in 10 top-tier medical journals. The benchmark is developed through a
meticulous construction pipeline, involving multiple rounds of screening and
review by both AI systems and human experts, with thorough checks conducted to
prevent data leakage. Our study reveals that even the most advanced reasoning
models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%
accuracy, respectively. This finding highlights a significant generalization
bottleneck in current large language models when faced with clinical diagnostic
reasoning challenges. Through DiagnosisArena, we aim to drive further
advancements in AIs diagnostic reasoning capabilities, enabling more effective
solutions for real-world clinical diagnostic challenges. We provide the
benchmark and evaluation tools for further research and development
https://github.com/SPIRAL-MED/DiagnosisArena.",2025-05-20,"Yakun Zhu, Zhongzhen Huang, Linjie Mu, Yutong Huang, Wei Nie, Jiaji Liu, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang",http://arxiv.org/pdf/2505.14107v2,cs.CL
A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations,"We present PersonaConvBench, a large-scale benchmark for evaluating
personalized reasoning and generation in multi-turn conversations with large
language models (LLMs). Unlike existing work that focuses on either
personalization or conversational structure in isolation, PersonaConvBench
integrates both, offering three core tasks: sentence classification, impact
regression, and user-centric text generation across ten diverse Reddit-based
domains. This design enables systematic analysis of how personalized
conversational context shapes LLM outputs in realistic multi-user scenarios. We
benchmark several commercial and open-source LLMs under a unified prompting
setup and observe that incorporating personalized history yields substantial
performance improvements, including a 198 percent relative gain over the best
non-conversational baseline in sentiment classification. By releasing
PersonaConvBench with evaluations and code, we aim to support research on LLMs
that adapt to individual styles, track long-term context, and produce
contextually rich, engaging responses.",2025-05-20,"Li Li, Peilin Cai, Ryan A. Rossi, Franck Dernoncourt, Branislav Kveton, Junda Wu, Tong Yu, Linxin Song, Tiankai Yang, Yuehan Qin, Nesreen K. Ahmed, Samyadeep Basu, Subhojyoti Mukherjee, Ruiyi Zhang, Zhengmian Hu, Bo Ni, Yuxiao Zhou, Zichao Wang, Yue Huang, Yu Wang, Xiangliang Zhang, Philip S. Yu, Xiyang Hu, Yue Zhao",http://arxiv.org/pdf/2505.14106v2,cs.CL
Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents,"Legal rules encompass not only codified statutes but also implicit
adjudicatory principles derived from precedents that contain discretionary
norms, social morality, and policy. While computational legal research has
advanced in applying established rules to cases, inducing legal rules from
judicial decisions remains understudied, constrained by limitations in model
inference efficacy and symbolic reasoning capability. The advent of Large
Language Models (LLMs) offers unprecedented opportunities for automating the
extraction of such latent principles, yet progress is stymied by the absence of
formal task definitions, benchmark datasets, and methodologies. To address this
gap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,
generalizable doctrinal rules from sets of analogous precedents, distilling
their shared preconditions, normative behaviors, and legal consequences. We
introduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese
cases in total) for model tuning and 216 expert-annotated gold test sets.
Experimental results reveal that: 1) State-of-the-art LLMs struggle with
over-generalization and hallucination; 2) Training on our dataset markedly
enhances LLMs capabilities in capturing nuanced rule patterns across similar
cases.",2025-05-20,"Wei Fan, Tianshi Zheng, Yiran Hu, Zheye Deng, Weiqi Wang, Baixuan Xu, Chunyang Li, Haoran Li, Weixing Shen, Yangqiu Song",http://arxiv.org/pdf/2505.14104v1,cs.CL
MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations,"Large Language Models (LLMs) have inherent limitations of faithfulness and
factuality, commonly referred to as hallucinations. Several benchmarks have
been developed that provide a test bed for factuality evaluation within the
context of English-centric datasets, while relying on supplementary informative
context like web links or text passages but ignoring the available structured
factual resources. To this end, Knowledge Graphs (KGs) have been identified as
a useful aid for hallucination mitigation, as they provide a structured way to
represent the facts about entities and their relations with minimal linguistic
overhead. We bridge the lack of KG paths and multilinguality for factual
language modeling within the existing hallucination evaluation benchmarks and
propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal}
framed for generative text evaluation. As part of our data collection pipeline,
we mined 140k KG-paths from open-domain KGs, from which we pruned noisy
KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation
shows an absolute scale increase by approximately 0.12 to 0.36 points for the
semantic similarity score in KG-RAG over vanilla QA across multiple languages
and multiple models, demonstrating the potential of KG integration. We
anticipate MultiHal will foster future research towards several graph-based
hallucination mitigation and fact-checking tasks.",2025-05-20,"Ernests Lavrinovics, Russa Biswas, Katja Hose, Johannes Bjerva",http://arxiv.org/pdf/2505.14101v1,cs.CL
Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering,"Knowledge Base Question Answering (KBQA) aims to answer natural language
questions using structured knowledge from KBs. While LLM-only approaches offer
generalization, they suffer from outdated knowledge, hallucinations, and lack
of transparency. Chain-based KG-RAG methods address these issues by
incorporating external KBs, but are limited to simple chain-structured
questions due to the absence of planning and logical structuring. Inspired by
semantic parsing methods, we propose PDRR: a four-stage framework consisting of
Predict, Decompose, Retrieve, and Reason. Our method first predicts the
question type and decomposes the question into structured triples. Then
retrieves relevant information from KBs and guides the LLM as an agent to
reason over and complete the decomposed triples. Experimental results
demonstrate that PDRR consistently outperforms existing methods across various
LLM backbones and achieves superior performance on both chain-structured and
non-chain complex questions.",2025-05-20,"Yihua Zhu, Qianying Liu, Akiko Aizawa, Hidetoshi Shimodaira",http://arxiv.org/pdf/2505.14099v1,cs.CL
"Not Minds, but Signs: Reframing LLMs through Semiotics","This paper challenges the prevailing tendency to frame Large Language Models
(LLMs) as cognitive systems, arguing instead for a semiotic perspective that
situates these models within the broader dynamics of sign manipulation and
meaning-making. Rather than assuming that LLMs understand language or simulate
human thought, we propose that their primary function is to recombine,
recontextualize, and circulate linguistic forms based on probabilistic
associations. By shifting from a cognitivist to a semiotic framework, we avoid
anthropomorphism and gain a more precise understanding of how LLMs participate
in cultural processes, not by thinking, but by generating texts that invite
interpretation. Through theoretical analysis and practical examples, the paper
demonstrates how LLMs function as semiotic agents whose outputs can be treated
as interpretive acts, open to contextual negotiation and critical reflection.
We explore applications in literature, philosophy, education, and cultural
production, emphasizing how LLMs can serve as tools for creativity, dialogue,
and critical inquiry. The semiotic paradigm foregrounds the situated,
contingent, and socially embedded nature of meaning, offering a more rigorous
and ethically aware framework for studying and using LLMs. Ultimately, this
approach reframes LLMs as technological participants in an ongoing ecology of
signs. They do not possess minds, but they alter how we read, write, and make
meaning, compelling us to reconsider the foundations of language,
interpretation, and the role of artificial systems in the production of
knowledge.",2025-05-20,Davide Picca,http://arxiv.org/pdf/2505.17080v1,cs.CL
Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory,"Language models encode and subsequently perpetuate harmful gendered
stereotypes. Research has succeeded in mitigating some of these harms, e.g. by
dissociating non-gendered terms such as occupations from gendered terms such as
'woman' and 'man'. This approach, however, remains superficial given that
associations are only one form of prejudice through which gendered harms arise.
Critical scholarship on gender, such as gender performativity theory,
emphasizes how harms often arise from the construction of gender itself, such
as conflating gender with biological sex. In language models, these issues
could lead to the erasure of transgender and gender diverse identities and
cause harms in downstream applications, from misgendering users to
misdiagnosing patients based on wrong assumptions about their anatomy.
  For FAccT research on gendered harms to go beyond superficial linguistic
associations, we advocate for a broader definition of 'gender bias' in language
models. We operationalize insights on the construction of gender through
language from gender studies literature and then empirically test how 16
language models of different architectures, training datasets, and model sizes
encode gender. We find that language models tend to encode gender as a binary
category tied to biological sex, and that gendered terms that do not neatly
fall into one of these binary categories are erased and pathologized. Finally,
we show that larger models, which achieve better results on performance
benchmarks, learn stronger associations between gender and sex, further
reinforcing a narrow understanding of gender. Our findings lead us to call for
a re-evaluation of how gendered harms in language models are defined and
addressed.",2025-05-20,"Franziska Sofia Hafner, Ana Valdivia, Luc Rocher",http://arxiv.org/pdf/2505.14080v1,cs.CL
BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks,"Large language model (LLM) based agents have shown great potential in
following human instructions and automatically completing various tasks. To
complete a task, the agent needs to decompose it into easily executed steps by
planning. Existing studies mainly conduct the planning by inferring what steps
should be executed next starting from the agent's initial state. However, this
forward reasoning paradigm doesn't work well for complex tasks. We propose to
study this issue in Minecraft, a virtual environment that simulates complex
tasks based on real-world scenarios. We believe that the failure of forward
reasoning is caused by the big perception gap between the agent's initial state
and task goal. To this end, we leverage backward reasoning and make the
planning starting from the terminal state, which can directly achieve the task
goal in one step. Specifically, we design a BAckward Reasoning based agent
(BAR). It is equipped with a recursive goal decomposition module, a state
consistency maintaining module and a stage memory module to make robust,
consistent, and efficient planning starting from the terminal state.
Experimental results demonstrate the superiority of BAR over existing methods
and the effectiveness of proposed modules.",2025-05-20,"Weihong Du, Wenrui Liao, Binyu Yan, Hongru Liang, Anthony G. Cohn, Wenqiang Lei",http://arxiv.org/pdf/2505.14079v2,cs.CL
GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace,"This paper investigates the underlying mechanisms of toxicity generation in
Large Language Models (LLMs) and proposes an effective detoxification approach.
Prior work typically considers the Feed-Forward Network (FFN) as the main
source of toxicity, representing toxic regions as a set of toxic vectors or
layer-wise subspaces. However, our in-depth analysis reveals that the global
toxic subspace offers a more effective and comprehensive representation of
toxic region within the model. Building on this insight, we propose GloSS
(Global Toxic Subspace Suppression), a lightweight, four-stage method that
mitigates toxicity by identifying and removing the global toxic subspace from
the parameters of FFN. Experiments across a range of LLMs show that GloSS
achieves state-of-the-art detoxification performance while preserving the
models general capabilities, without requiring large-scale data or model
retraining.",2025-05-20,"Zenghao Duan, Zhiyi Yin, Zhichao Shi, Liang Pang, Shaoling Jing, Jiayi Wu, Yu Yan, Huawei Shen, Xueqi Cheng",http://arxiv.org/pdf/2505.17078v1,cs.CL
Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models,"Steering methods have emerged as effective and targeted tools for guiding
large language models' (LLMs) behavior without modifying their parameters.
Multimodal large language models (MLLMs), however, do not currently enjoy the
same suite of techniques, due in part to their recency and architectural
diversity. Inspired by this gap, we investigate whether MLLMs can be steered
using vectors derived from their text-only LLM backbone, via sparse
autoencoders (SAEs), mean shift, and linear probing. We find that text-derived
steering consistently enhances multimodal accuracy across diverse MLLM
architectures and visual tasks. In particular, mean shift boosts spatial
relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to
+3.3%, outperforming prompting and exhibiting strong generalization to
out-of-distribution datasets. These results highlight textual steering vectors
as a powerful, efficient mechanism for enhancing grounding in MLLMs with
minimal additional data collection and computational overhead.",2025-05-20,"Woody Haosheng Gan, Deqing Fu, Julian Asilis, Ollie Liu, Dani Yogatama, Vatsal Sharan, Robin Jia, Willie Neiswanger",http://arxiv.org/pdf/2505.14071v1,cs.CL
Enhancing LLMs via High-Knowledge Data Selection,"The performance of Large Language Models (LLMs) is intrinsically linked to
the quality of its training data. Although several studies have proposed
methods for high-quality data selection, they do not consider the importance of
knowledge richness in text corpora. In this paper, we propose a novel and
gradient-free High-Knowledge Scorer (HKS) to select high-quality data from the
dimension of knowledge, to alleviate the problem of knowledge scarcity in the
pre-trained corpus. We propose a comprehensive multi-domain knowledge element
pool and introduce knowledge density and coverage as metrics to assess the
knowledge content of the text. Based on this, we propose a comprehensive
knowledge scorer to select data with intensive knowledge, which can also be
utilized for domain-specific high-knowledge data selection by restricting
knowledge elements to the specific domain. We train models on a high-knowledge
bilingual dataset, and experimental results demonstrate that our scorer
improves the model's performance in knowledge-intensive and general
comprehension tasks, and is effective in enhancing both the generic and
domain-specific capabilities of the model.",2025-05-20,"Feiyu Duan, Xuemiao Zhang, Sirui Wang, Haoran Que, Yuqi Liu, Wenge Rong, Xunliang Cai",http://arxiv.org/pdf/2505.14070v1,cs.CL
Improved Methods for Model Pruning and Knowledge Distillation,"Model pruning is a performance optimization technique for large language
models like R1 or o3-mini. However, existing pruning methods often lead to
significant performance degradation or require extensive retraining and
fine-tuning. This technique aims to identify and remove neurons, connections
unlikely leading to the contribution during the human-computer interaction
phase. Our goal is to obtain a much smaller and faster knowledge distilled
model that can quickly generate content almost as good as those of the unpruned
ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an
improved pruning method that effectively reduces model size and computational
complexity while maintaining performance comparable to the original unpruned
model even at extreme pruned levels. The improved method is based on weights,
bias fixed in the pre-training phase and GRPO rewards verified during the
post-training phase as our novel pruning indicators. Preliminary experimental
results show that our method outperforms and be comparable to state-of-the-art
methods across various pruning levels and different downstream computational
linguistics tasks.",2025-05-20,"Wei Jiang, Anying Fu, Youling Zhang",http://arxiv.org/pdf/2505.14052v1,cs.CL
From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora,"Continued pretraining and instruction tuning on large-scale multilingual data
have proven to be effective in scaling large language models (LLMs) to
low-resource languages. However, the unaligned nature of such data limits its
ability to effectively capture cross-lingual semantics. In contrast, multi-way
parallel data, where identical content is aligned across multiple languages,
provides stronger cross-lingual consistency and offers greater potential for
improving multilingual performance. In this paper, we introduce a large-scale,
high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus
spans 113 languages, with up to 50 languages aligned in parallel, ensuring
extensive multilingual coverage. Using this dataset, we investigate best
practices for leveraging multi-way parallel data to enhance LLMs, including
strategies for continued pretraining, instruction tuning, and the analysis of
key influencing factors. Experiments on six multilingual benchmarks show that
models trained on multiway parallel data consistently outperform those trained
on unaligned multilingual data.",2025-05-20,"Yingli Shen, Wen Lai, Shuo Wang, Kangyang Luo, Alexander Fraser, Maosong Sun",http://arxiv.org/pdf/2505.14045v1,cs.CL
ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data,"Mental health risk is a critical global public health challenge,
necessitating innovative and reliable assessment methods. With the development
of large language models (LLMs), they stand out to be a promising tool for
explainable mental health care applications. Nevertheless, existing approaches
predominantly rely on subjective textual mental records, which can be distorted
by inherent mental uncertainties, leading to inconsistent and unreliable
predictions. To address these limitations, this paper introduces ProMind-LLM.
We investigate an innovative approach integrating objective behavior data as
complementary information alongside subjective mental records for robust mental
health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive
pipeline that includes domain-specific pretraining to tailor the LLM for mental
health contexts, a self-refine mechanism to optimize the processing of
numerical behavioral data, and causal chain-of-thought reasoning to enhance the
reliability and interpretability of its predictions. Evaluations of two
real-world datasets, PMData and Globem, demonstrate the effectiveness of our
proposed methods, achieving substantial improvements over general LLMs. We
anticipate that ProMind-LLM will pave the way for more dependable,
interpretable, and scalable mental health case solutions.",2025-05-20,"Xinzhe Zheng, Sijie Ji, Jiawei Sun, Renqi Chen, Wei Gao, Mani Srivastava",http://arxiv.org/pdf/2505.14038v1,cs.CL
ShieldVLM: Safeguarding the Multimodal Implicit Toxicity via Deliberative Reasoning with LVLMs,"Toxicity detection in multimodal text-image content faces growing challenges,
especially with multimodal implicit toxicity, where each modality appears
benign on its own but conveys hazard when combined. Multimodal implicit
toxicity appears not only as formal statements in social platforms but also
prompts that can lead to toxic dialogs from Large Vision-Language Models
(LVLMs). Despite the success in unimodal text or image moderation, toxicity
detection for multimodal content, particularly the multimodal implicit
toxicity, remains underexplored. To fill this gap, we comprehensively build a
taxonomy for multimodal implicit toxicity (MMIT) and introduce an MMIT-dataset,
comprising 2,100 multimodal statements and prompts across 7 risk categories (31
sub-categories) and 5 typical cross-modal correlation modes. To advance the
detection of multimodal implicit toxicity, we build ShieldVLM, a model which
identifies implicit toxicity in multimodal statements, prompts and dialogs via
deliberative cross-modal reasoning. Experiments show that ShieldVLM outperforms
existing strong baselines in detecting both implicit and explicit toxicity. The
model and dataset will be publicly available to support future researches.
Warning: This paper contains potentially sensitive contents.",2025-05-20,"Shiyao Cui, Qinglin Zhang, Xuan Ouyang, Renmiao Chen, Zhexin Zhang, Yida Lu, Hongning Wang, Han Qiu, Minlie Huang",http://arxiv.org/pdf/2505.14035v1,cs.CL
AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation,"The rapid advancement of domain-specific large language models (LLMs) in
fields like law necessitates frameworks that account for nuanced regional legal
distinctions, which are critical for ensuring compliance and trustworthiness.
Existing legal evaluation benchmarks often lack adaptability and fail to
address diverse local contexts, limiting their utility in dynamically evolving
regulatory landscapes. To address these gaps, we propose AutoLaw, a novel
violation detection framework that combines adversarial data generation with a
jury-inspired deliberation process to enhance legal compliance of LLMs. Unlike
static approaches, AutoLaw dynamically synthesizes case law to reflect local
regulations and employs a pool of LLM-based ""jurors"" to simulate judicial
decision-making. Jurors are ranked and selected based on synthesized legal
expertise, enabling a deliberation process that minimizes bias and improves
detection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG
(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:
adversarial data generation improves LLM discrimination, while the jury-based
voting strategy significantly boosts violation detection rates. Our results
highlight the framework's ability to adaptively probe legal misalignments and
deliver reliable, context-aware judgments, offering a scalable solution for
evaluating and enhancing LLMs in legally sensitive applications.",2025-05-20,"Tai D. Nguyen, Long H. Pham, Jun Sun",http://arxiv.org/pdf/2505.14015v1,cs.CL
Activation-Guided Consensus Merging for Large Language Models,"Recent research has increasingly focused on reconciling the reasoning
capabilities of System 2 with the efficiency of System 1. While existing
training-based and prompt-based approaches face significant challenges in terms
of efficiency and stability, model merging emerges as a promising strategy to
integrate the diverse capabilities of different Large Language Models (LLMs)
into a unified model. However, conventional model merging methods often assume
uniform importance across layers, overlooking the functional heterogeneity
inherent in neural components. To address this limitation, we propose
\textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}),
a plug-and-play merging framework that determines layer-specific merging
coefficients based on mutual information between activations of pre-trained and
fine-tuned models. ACM effectively preserves task-specific capabilities without
requiring gradient computations or additional training. Extensive experiments
on Long-to-Short (L2S) and general merging tasks demonstrate that ACM
consistently outperforms all baseline methods. For instance, in the case of
Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%}
reduction in response length while simultaneously improving reasoning accuracy
by \textbf{1.3} points. We submit the code with the paper for reproducibility,
and it will be publicly available.",2025-05-20,"Yuxuan Yao, Shuqi Liu, Zehua Liu, Qintong Li, Mingyang Liu, Xiongwei Han, Zhijiang Guo, Han Wu, Linqi Song",http://arxiv.org/pdf/2505.14009v1,cs.CL
Social Sycophancy: A Broader Understanding of LLM Sycophancy,"A serious risk to the safety and utility of LLMs is sycophancy, i.e.,
excessive agreement with and flattery of the user. Yet existing work focuses on
only one aspect of sycophancy: agreement with users' explicitly stated beliefs
that can be compared to a ground truth. This overlooks forms of sycophancy that
arise in ambiguous contexts such as advice and support-seeking, where there is
no clear ground truth, yet sycophancy can reinforce harmful implicit
assumptions, beliefs, or actions. To address this gap, we introduce a richer
theory of social sycophancy in LLMs, characterizing sycophancy as the excessive
preservation of a user's face (the positive self-image a person seeks to
maintain in an interaction). We present ELEPHANT, a framework for evaluating
social sycophancy across five face-preserving behaviors (emotional validation,
moral endorsement, indirect language, indirect action, and accepting framing)
on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole
(AITA). Across eight models, we show that LLMs consistently exhibit high rates
of social sycophancy: on OEQ, they preserve face 47% more than humans, and on
AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments
in 42% of cases. We further show that social sycophancy is rewarded in
preference datasets and is not easily mitigated. Our work provides theoretical
grounding and empirical tools (datasets and code) for understanding and
addressing this under-recognized but consequential issue.",2025-05-20,"Myra Cheng, Sunny Yu, Cinoo Lee, Pranav Khadpe, Lujain Ibrahim, Dan Jurafsky",http://arxiv.org/pdf/2505.13995v1,cs.CL
DecIF: Improving Instruction-Following through Meta-Decomposition,"Instruction-following has emerged as a crucial capability for large language
models (LLMs). However, existing approaches often rely on pre-existing
documents or external resources to synthesize instruction-following data, which
limits their flexibility and generalizability. In this paper, we introduce
DecIF, a fully autonomous, meta-decomposition guided framework that generates
diverse and high-quality instruction-following data using only LLMs. DecIF is
grounded in the principle of decomposition. For instruction generation, we
guide LLMs to iteratively produce various types of meta-information, which are
then combined with response constraints to form well-structured and
semantically rich instructions. We further utilize LLMs to detect and resolve
potential inconsistencies within the generated instructions. Regarding response
generation, we decompose each instruction into atomic-level evaluation
criteria, enabling rigorous validation and the elimination of inaccurate
instruction-response pairs. Extensive experiments across a wide range of
scenarios and settings demonstrate DecIF's superior performance on
instruction-following tasks. Further analysis highlights its strong
flexibility, scalability, and generalizability in automatically synthesizing
high-quality instruction data.",2025-05-20,"Tingfeng Hui, Pengyu Zhu, Bowen Ping, Ling Tang, Yaqi Zhang, Sen Su",http://arxiv.org/pdf/2505.13990v1,cs.CL
The Hallucination Tax of Reinforcement Finetuning,"Reinforcement finetuning (RFT) has become a standard approach for enhancing
the reasoning capabilities of large language models (LLMs). However, its impact
on model trustworthiness remains underexplored. In this work, we identify and
systematically study a critical side effect of RFT, which we term the
hallucination tax: a degradation in refusal behavior causing models to produce
hallucinated answers to unanswerable questions confidently. To investigate
this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of
unanswerable math problems designed to probe models' ability to recognize an
unanswerable question by reasoning from the insufficient or ambiguous
information. Our results show that standard RFT training could reduce model
refusal rates by more than 80%, which significantly increases model's tendency
to hallucinate. We further demonstrate that incorporating just 10% SUM during
RFT substantially restores appropriate refusal behavior, with minimal accuracy
trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage
inference-time compute to reason about their own uncertainty and knowledge
boundaries, improving generalization not only to out-of-domain math problems
but also to factual question answering tasks.",2025-05-20,"Linxin Song, Taiwei Shi, Jieyu Zhao",http://arxiv.org/pdf/2505.13988v1,cs.CL
Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection,"Multimodal models play a key role in empathy detection, but their performance
can suffer when modalities provide conflicting cues. To understand these
failures, we examine cases where unimodal and multimodal predictions diverge.
Using fine-tuned models for text, audio, and video, along with a gated fusion
model, we find that such disagreements often reflect underlying ambiguity, as
evidenced by annotator uncertainty. Our analysis shows that dominant signals in
one modality can mislead fusion when unsupported by others. We also observe
that humans, like models, do not consistently benefit from multimodal input.
These insights position disagreement as a useful diagnostic signal for
identifying challenging examples and improving empathy system robustness.",2025-05-20,"Maya Srikanth, Run Chen, Julia Hirschberg",http://arxiv.org/pdf/2505.13979v1,cs.CL
DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models,"While Large Reasoning Models (LRMs) have demonstrated success in complex
reasoning tasks through long chain-of-thought (CoT) reasoning, their inference
often involves excessively verbose reasoning traces, resulting in substantial
inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a
hybrid framework that combines inference-time pruning with tuning-based
distillation, two widely used strategies for efficient reasoning. DRP uses a
teacher model to perform skill-aware step decomposition and content pruning,
and then distills the pruned reasoning paths into a student model, enabling it
to reason both efficiently and accurately. Across several challenging
mathematical reasoning datasets, we find that models trained with DRP achieve
substantial improvements in token efficiency without sacrificing accuracy.
Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while
improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on
AIME with no performance drop. Further analysis shows that aligning the
reasoning structure of training CoTs with the student's reasoning capacity is
critical for effective knowledge transfer and performance gains.",2025-05-20,"Yuxuan Jiang, Dawei Li, Frank Ferraro",http://arxiv.org/pdf/2505.13975v1,cs.CL
Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models,"Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.",2025-05-20,"Wenhui Zhu, Xuanzhao Dong, Xin Li, Peijie Qiu, Xiwen Chen, Abolfazl Razi, Aris Sotiras, Yi Su, Yalin Wang",http://arxiv.org/pdf/2505.13973v1,cs.CL
Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals,"Counterfactual examples are widely employed to enhance the performance and
robustness of large language models (LLMs) through counterfactual data
augmentation (CDA). However, the selection of the judge model used to evaluate
label flipping, the primary metric for assessing the validity of generated
counterfactuals for CDA, yields inconsistent results. To decipher this, we
define four types of relationships between the counterfactual generator and
judge models. Through extensive experiments involving two state-of-the-art
LLM-based methods, three datasets, five generator models, and 15 judge models,
complemented by a user study (n = 90), we demonstrate that judge models with an
independent, non-fine-tuned relationship to the generator model provide the
most reliable label flipping evaluations. Relationships between the generator
and judge models, which are closely aligned with the user study for CDA, result
in better model performance and robustness. Nevertheless, we find that the gap
between the most effective judge models and the results obtained from the user
study remains considerably large. This suggests that a fully automated pipeline
for CDA may be inadequate and requires human intervention.",2025-05-20,"Qianli Wang, Van Bach Nguyen, Nils Feldhus, Luis Felipe Villa-Arenas, Christin Seifert, Sebastian Möller, Vera Schmitt",http://arxiv.org/pdf/2505.13972v1,cs.CL
CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring,"Automated Essay Scoring (AES) is crucial for modern education, particularly
with the increasing prevalence of multimodal assessments. However, traditional
AES methods struggle with evaluation generalizability and multimodal
perception, while even recent Multimodal Large Language Model (MLLM)-based
approaches can produce hallucinated justifications and scores misaligned with
human judgment. To address the limitations, we introduce CAFES, the first
collaborative multi-agent framework specifically designed for AES. It
orchestrates three specialized agents: an Initial Scorer for rapid,
trait-specific evaluations; a Feedback Pool Manager to aggregate detailed,
evidence-grounded strengths; and a Reflective Scorer that iteratively refines
scores based on this feedback to enhance human alignment. Extensive
experiments, using state-of-the-art MLLMs, achieve an average relative
improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,
especially for grammatical and lexical diversity. Our proposed CAFES framework
paves the way for an intelligent multimodal AES system. The code will be
available upon acceptance.",2025-05-20,"Jiamin Su, Yibo Yan, Zhuoran Gao, Han Zhang, Xiang Liu, Xuming Hu",http://arxiv.org/pdf/2505.13965v1,cs.CL
Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English,"The speech tokenizer plays a crucial role in recent speech tasks, generally
serving as a bridge between speech signals and language models. While
low-frame-rate codecs are widely employed as speech tokenizers, the impact of
frame rates on speech tokens remains underexplored. In this study, we
investigate how varying frame rates affect speech tokenization by examining
Mandarin and English, two typologically distinct languages. We encode speech at
different frame rates and evaluate the resulting semantic tokens in the speech
recognition task. Our findings reveal that frame rate variations influence
speech tokenization differently for each language, highlighting the interplay
between frame rates, phonetic density, and language-specific acoustic features.
The results provide insights into optimizing frame rate selection for speech
tokenizers, with implications for automatic speech recognition, text-to-speech,
and other speech-related applications.",2025-05-20,"Haoyang Zhang, Hexin Liu, Xiangyu Zhang, Qiquan Zhang, Yuchen Hu, Junqi Zhao, Fei Tian, Xuerui Yang, Eng Siong Chng",http://arxiv.org/pdf/2505.17076v1,cs.CL
Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability,"Quantization methods are widely used to accelerate inference and streamline
the deployment of large language models (LLMs). While prior research has
extensively investigated the degradation of various LLM capabilities due to
quantization, its effects on model explainability and interpretability, which
are crucial for understanding decision-making processes, remain unexplored. To
address this gap, we conduct comprehensive experiments using three common
quantization techniques at distinct bit widths, in conjunction with two
explainability methods, counterfactual examples and natural language
explanations, as well as two interpretability approaches, knowledge
memorization analysis and latent multi-hop reasoning analysis. We complement
our analysis with a thorough user study, evaluating selected explainability
methods. Our findings reveal that, depending on the configuration, quantization
can significantly impact model explainability and interpretability. Notably,
the direction of this effect is not consistent, as it strongly depends on (1)
the quantization method, (2) the explainability or interpretability approach,
and (3) the evaluation protocol. In some settings, human evaluation shows that
quantization degrades explainability, while in others, it even leads to
improvements. Our work serves as a cautionary tale, demonstrating that
quantization can unpredictably affect model transparency. This insight has
important implications for deploying LLMs in applications where transparency is
a critical requirement.",2025-05-20,"Qianli Wang, Mingyang Wang, Nils Feldhus, Simon Ostermann, Yuan Cao, Hinrich Schütze, Sebastian Möller, Vera Schmitt",http://arxiv.org/pdf/2505.13963v1,cs.CL
Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation,"Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by
integrating external multimodal databases, but introduce unexplored privacy
vulnerabilities. While text-based RAG privacy risks have been studied,
multimodal data presents unique challenges. We provide the first systematic
analysis of MRAG privacy vulnerabilities across vision-language and
speech-language modalities. Using a novel compositional structured prompt
attack in a black-box setting, we demonstrate how attackers can extract private
information by manipulating queries. Our experiments reveal that LMMs can both
directly generate outputs resembling retrieved content and produce descriptions
that indirectly expose sensitive information, highlighting the urgent need for
robust privacy-preserving MRAG techniques.",2025-05-20,"Jiankun Zhang, Shenglai Zeng, Jie Ren, Tianqi Zheng, Hui Liu, Xianfeng Tang, Hui Liu, Yi Chang",http://arxiv.org/pdf/2505.13957v1,cs.CL
FlashThink: An Early Exit Method For Efficient Reasoning,"Large Language Models (LLMs) have shown impressive performance in reasoning
tasks. However, LLMs tend to generate excessively long reasoning content,
leading to significant computational overhead. Our observations indicate that
even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning
content, which is against intuitive expectations. Preliminary experiments show
that at a certain point during the generation process, the model is already
capable of producing the correct solution without completing the full reasoning
content. Therefore, we consider that the reasoning process of the model can be
exited early to achieve the purpose of efficient reasoning. We introduce a
verification model that identifies the exact moment when the model can stop
reasoning and still provide the correct answer. Comprehensive experiments on
four different benchmarks demonstrate that our proposed method, FlashThink,
effectively shortens the reasoning content while preserving the model accuracy.
For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning
content by 77.04% and 77.47%, respectively, without reducing the accuracy.",2025-05-20,"Guochao Jiang, Guofeng Quan, Zepeng Ding, Ziqin Luo, Dixuan Wang, Zheng Hu",http://arxiv.org/pdf/2505.13949v1,cs.CL
Memory-Centric Embodied Question Answer,"Embodied Question Answering (EQA) requires agents to autonomously explore and
understand the environment to answer context-dependent questions. Existing
frameworks typically center around the planner, which guides the stopping
module, memory module, and answering module for reasoning. In this paper, we
propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric
EQA models where the memory module cannot fully interact with other modules,
MemoryEQA flexible feeds memory information into all modules, thereby enhancing
efficiency and accuracy in handling complex tasks, such as those involving
multiple targets across different regions. Specifically, we establish a
multi-modal hierarchical memory mechanism, which is divided into global memory
that stores language-enhanced scene maps, and local memory that retains
historical observations and state information. When performing EQA tasks, the
multi-modal large language model is leveraged to convert memory information
into the required input formats for injection into different modules. To
evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset
based on HM3D, comprising 1,587 question-answer pairs involving multiple
targets across various regions, which requires agents to maintain memory of
exploration-acquired target information. Experimental results on HM-EQA,
MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a
19.8% performance gain on MT-HM3D compared to baseline model further
underscores memory capability's pivotal role in resolving complex tasks.",2025-05-20,"Mingliang Zhai, Zhi Gao, Yuwei Wu, Yunde Jia",http://arxiv.org/pdf/2505.13948v1,cs.CL
Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting,"Memory-based approaches have shown strong performance in Continual Relation
Extraction (CRE). However, storing examples from previous tasks increases
memory usage and raises privacy concerns. Recently, prompt-based methods have
emerged as a promising alternative, as they do not rely on storing past
samples. Despite this progress, current prompt-based techniques face several
core challenges in CRE, particularly in accurately identifying task identities
and mitigating catastrophic forgetting. Existing prompt selection strategies
often suffer from inaccuracies, lack robust mechanisms to prevent forgetting in
shared parameters, and struggle to handle both cross-task and within-task
variations. In this paper, we propose WAVE++, a novel approach inspired by the
connection between prefix-tuning and mixture of experts. Specifically, we
introduce task-specific prompt pools that enhance flexibility and adaptability
across diverse tasks while avoiding boundary-spanning risks; this design more
effectively captures variations within each task and across tasks. To further
refine relation classification, we incorporate label descriptions that provide
richer, more global context, enabling the model to better distinguish among
different relations. We also propose a training-free mechanism to improve task
prediction during inference. Moreover, we integrate a generative model to
consolidate prior knowledge within the shared parameters, thereby removing the
need for explicit data storage. Extensive experiments demonstrate that WAVE++
outperforms state-of-the-art prompt-based and rehearsal-based methods, offering
a more robust solution for continual relation extraction. Our code is publicly
available at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.",2025-05-20,"Bao-Ngoc Dao, Quang Nguyen, Luyen Ngo Dinh, Minh Le, Nam Le, Linh Ngo Van",http://arxiv.org/pdf/2505.13944v1,cs.CL
MLZero: A Multi-Agent System for End-to-end Machine Learning Automation,"Existing AutoML systems have advanced the automation of machine learning
(ML); however, they still require substantial manual configuration and expert
input, particularly when handling multimodal data. We introduce MLZero, a novel
multi-agent framework powered by Large Language Models (LLMs) that enables
end-to-end ML automation across diverse data modalities with minimal human
intervention. A cognitive perception module is first employed, transforming raw
multimodal inputs into perceptual context that effectively guides the
subsequent workflow. To address key limitations of LLMs, such as hallucinated
code generation and outdated API knowledge, we enhance the iterative code
generation process with semantic and episodic memory. MLZero demonstrates
superior performance on MLE-Bench Lite, outperforming all competitors in both
success rate and solution quality, securing six gold medals. Additionally, when
evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more
challenging tasks spanning diverse data modalities, MLZero outperforms the
competing methods by a large margin with a success rate of 0.92 (+263.6\%) and
an average rank of 2.28. Our approach maintains its robust effectiveness even
with a compact 8B LLM, outperforming full-size systems from existing solutions.",2025-05-20,"Haoyang Fang, Boran Han, Nick Erickson, Xiyuan Zhang, Su Zhou, Anirudh Dagar, Jiani Zhang, Ali Caner Turkmen, Cuixiong Hu, Huzefa Rangwala, Ying Nian Wu, Bernie Wang, George Karypis",http://arxiv.org/pdf/2505.13941v1,cs.CL
Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems,"This study aimed to develop and validate two scales of engagement and rapport
to evaluate the user experience quality with multimodal dialogue systems in the
context of foreign language learning. The scales were designed based on
theories of engagement in educational psychology, social psychology, and second
language acquisition.Seventy-four Japanese learners of English completed
roleplay and discussion tasks with trained human tutors and a dialog agent.
After each dialogic task was completed, they responded to the scales of
engagement and rapport. The validity and reliability of the scales were
investigated through two analyses. We first conducted analysis of Cronbach's
alpha coefficient and a series of confirmatory factor analyses to test the
structural validity of the scales and the reliability of our designed items. We
then compared the scores of engagement and rapport between the dialogue with
human tutors and the one with a dialogue agent. The results revealed that our
scales succeeded in capturing the difference in the dialogue experience quality
between the human interlocutors and the dialogue agent from multiple
perspectives.",2025-05-20,"Fuma Kurata, Mao Saeki, Masaki Eguchi, Shungo Suzuki, Hiroaki Takatsu, Yoichi Matsuyama",http://arxiv.org/pdf/2505.17075v1,cs.CL
EEG-to-Text Translation: A Model for Deciphering Human Brain Activity,"With the rapid advancement of large language models like Gemini, GPT, and
others, bridging the gap between the human brain and language processing has
become an important area of focus. To address this challenge, researchers have
developed various models to decode EEG signals into text. However, these models
still face significant performance limitations. To overcome these shortcomings,
we propose a new model, R1 Translator, which aims to improve the performance of
EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM
encoder with a pretrained transformer-based decoder, utilizing EEG features to
produce high-quality text outputs. The model processes EEG embeddings through
the LSTM to capture sequential dependencies, which are then fed into the
transformer decoder for effective text generation. The R1 Translator excels in
ROUGE metrics, outperforming both T5 (previous research) and Brain Translator.
Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%
higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in
ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain
by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower
than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs
better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and
Brain by 3.6% (0.7553). Code is available at
https://github.com/Mmurrad/EEG-To-text.",2025-05-20,"Saydul Akbar Murad, Ashim Dahal, Nick Rahimi",http://arxiv.org/pdf/2505.13936v1,cs.CL
"Word length predicts word order: ""Min-max""-ing drives language evolution","Current theories of language propose an innate (Baker 2001; Chomsky 1981) or
a functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface
structures (i.e. word order) that we observe in languages of the world, while
evolutionary modeling (Dunn et al. 2011) suggests that descent is the primary
factor influencing such patterns. Although there are hypotheses for word order
change from both innate and usage-based perspectives for specific languages and
families, there are key disagreements between the two major proposals for
mechanisms that drive the evolution of language more broadly (Wasow 2002; Levy
2008). This paper proposes a universal underlying mechanism for word order
change based on a large tagged parallel dataset of over 1,500 languages
representing 133 language families and 111 isolates. Results indicate that word
class length is significantly correlated with word order crosslinguistically,
but not in a straightforward manner, partially supporting opposing theories of
processing, while at the same time predicting historical word order change in
two different phylogenetic lines and explaining more variance than descent or
language area in regression models. Such findings suggest an integrated
""Min-Max"" theory of language evolution driven by competing pressures of
processing and information structure, aligning with recent efficiency-oriented
(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et
al. 2025).",2025-05-20,Hiram Ring,http://arxiv.org/pdf/2505.13913v1,cs.CL
Efficient Agent Training for Computer Use,"Scaling up high-quality trajectory data has long been a critical bottleneck
for developing human-like computer use agents. We introduce PC Agent-E, an
efficient agent training framework that significantly reduces reliance on
large-scale human demonstrations. Starting with just 312 human-annotated
computer use trajectories, we further improved data quality by synthesizing
diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched
trajectories, our PC Agent-E model achieved a remarkable 141% relative
improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on
WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC
Agent-E demonstrates strong generalizability to different operating systems on
OSWorld. Our findings suggest that strong computer use capabilities can be
stimulated from a small amount of high-quality trajectory data.",2025-05-20,"Yanheng He, Jiahe Jin, Pengfei Liu",http://arxiv.org/pdf/2505.13909v1,cs.CL
Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology,"Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it
allows for models trained on resource-rich languages to be applied to
low-resource languages more effectively. Recently massively multilingual
pre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot
transfer capabilities[14] [13]. This paper investigates cross-linguistic
transfer through the lens of language families and morphology. Investigating
how language family proximity and morphological similarity affect performance
across NLP tasks. We further discuss our results and how it relates to findings
from recent literature. Overall, we compare multilingual model performance and
review how linguistic distance metrics correlate with transfer outcomes. We
also look into emerging approaches that integrate typological and morphological
information into model pre-training to improve transfer to diverse
languages[18] [19].",2025-05-20,"Ajitesh Bankula, Praney Bankula",http://arxiv.org/pdf/2505.13908v1,cs.CL
Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency,"Speculative decoding accelerates Large Language Model (LLM) inference by
employing a small speculative model (SSM) to generate multiple candidate tokens
and verify them using the LLM in parallel. This technique has been widely
integrated into LLM inference serving systems. However, inference requests
typically exhibit uncertain execution time, which poses a significant challenge
of efficiently scheduling requests in these systems. Existing work estimates
execution time based solely on predicted output length, which could be
inaccurate because execution time depends on both output length and token
acceptance rate of verification by the LLM. In this paper, we propose a
semi-clairvoyant request scheduling algorithm called
Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a
number of inference requests, LAPS-SD can effectively minimize average
inference latency by adaptively scheduling requests according to their features
during decoding. When the token acceptance rate is dynamic and execution time
is difficult to estimate, LAPS-SD maintains multiple priority queues and allows
request execution preemption across different queues. Once the token acceptance
rate becomes stable, LAPS-SD can accurately estimate the execution time and
schedule requests accordingly. Extensive experiments show that LAPS-SD reduces
inference latency by approximately 39\% compared to state-of-the-art scheduling
methods.",2025-05-20,"Ruixiao Li, Fahao Chen, Peng Li",http://arxiv.org/pdf/2505.17074v1,cs.CL
Let's Verify Math Questions Step by Step,"Large Language Models (LLMs) have recently achieved remarkable progress in
mathematical reasoning. To enable such capabilities, many existing works
distill strong reasoning models into long chains of thought or design
algorithms to construct high-quality math QA data for training. However, these
efforts primarily focus on generating correct reasoning paths and answers,
while largely overlooking the validity of the questions themselves. In this
work, we propose Math Question Verification (MathQ-Verify), a novel five-stage
pipeline designed to rigorously filter ill-posed or under-specified math
problems. MathQ-Verify first performs format-level validation to remove
redundant instructions and ensure that each question is syntactically
well-formed. It then formalizes each question, decomposes it into atomic
conditions, and verifies them against mathematical definitions. Next, it
detects logical contradictions among these conditions, followed by a
goal-oriented completeness check to ensure the question provides sufficient
information for solving. To evaluate this task, we use existing benchmarks
along with an additional dataset we construct, containing 2,147 math questions
with diverse error types, each manually double-validated. Experiments show that
MathQ-Verify achieves state-of-the-art performance across multiple benchmarks,
improving the F1 score by up to 25 percentage points over the direct
verification baseline. It further attains approximately 90% precision and 63%
recall through a lightweight model voting scheme. MathQ-Verify offers a
scalable and accurate solution for curating reliable mathematical datasets,
reducing label noise and avoiding unnecessary computation on invalid questions.
Our code and data are available at https://github.com/scuuy/MathQ-Verify.",2025-05-20,"Chengyu Shen, Zhen Hao Wong, Runming He, Hao Liang, Meiyi Qiang, Zimo Meng, Zhengyang Zhao, Bohan Zeng, Zhengzhou Zhu, Bin Cui, Wentao Zhang",http://arxiv.org/pdf/2505.13903v1,cs.CL
InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion,"Recent advances in large language models (LLMs) have intensified efforts to
fuse heterogeneous open-source models into a unified system that inherits their
complementary strengths. Existing logit-based fusion methods maintain inference
efficiency but treat vocabulary dimensions independently, overlooking semantic
dependencies encoded by cross-dimension interactions. These dependencies
reflect how token types interact under a model's internal reasoning and are
essential for aligning models with diverse generation behaviors. To explicitly
model these dependencies, we propose \textbf{InfiGFusion}, the first
structure-aware fusion framework with a novel \textit{Graph-on-Logits
Distillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output
and aggregate their outer products across sequence positions to form a global
co-activation graph, where nodes represent vocabulary channels and edges
quantify their joint activations. To ensure scalability and efficiency, we
design a sorting-based closed-form approximation that reduces the original
$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \log n)$, with provable
approximation guarantees. Experiments across multiple fusion settings show that
GLD consistently improves fusion quality and stability. InfiGFusion outperforms
SOTA models and fusion baselines across 11 benchmarks spanning reasoning,
coding, and mathematics. It shows particular strength in complex reasoning
tasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal
Judgement over SFT, demonstrating superior multi-step and relational inference.",2025-05-20,"Yuanyi Wang, Zhaoyi Yan, Yiming Zhang, Qi Zhou, Yanggan Gu, Fei Wu, Hongxia Yang",http://arxiv.org/pdf/2505.13893v1,cs.CL
Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM,"Recent advances in test-time scaling have enabled Large Language Models
(LLMs) to display sophisticated reasoning abilities via extended
Chain-of-Thought (CoT) generation. Despite their potential, these Reasoning
LLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as
performance degradation under few-shot prompting, that challenge our current
understanding of RLMs. In this work, we introduce a unified graph-based
analytical framework for better modeling the reasoning processes of RLMs. Our
method first clusters long, verbose CoT outputs into semantically coherent
reasoning steps, then constructs directed reasoning graphs to capture
contextual and logical dependencies among these steps. Through comprehensive
analysis across models and prompting regimes, we reveal that structural
properties, such as exploration density, branching, and convergence ratios,
strongly correlate with reasoning accuracy. Our findings demonstrate how
prompting strategies substantially reshape the internal reasoning structure of
RLMs, directly affecting task outcomes. The proposed framework not only enables
quantitative evaluation of reasoning quality beyond conventional metrics but
also provides practical insights for prompt engineering and the cognitive
analysis of LLMs. Code and resources will be released to facilitate future
research in this direction.",2025-05-20,"Zhen Xiong, Yujun Cai, Zhecheng Li, Yiwei Wang",http://arxiv.org/pdf/2505.13890v1,cs.CL
Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation,"The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.",2025-05-20,"Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang",http://arxiv.org/pdf/2505.13887v2,cs.CL
Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning,"Visual-language Chain-of-Thought (CoT) data resources are relatively scarce
compared to text-only counterparts, limiting the improvement of reasoning
capabilities in Vision Language Models (VLMs). However, high-quality
vision-language reasoning data is expensive and labor-intensive to annotate. To
address this issue, we leverage a promising resource: game code, which
naturally contains logical structures and state transition processes.
Therefore, we propose Code2Logic, a novel game-code-driven approach for
multimodal reasoning data synthesis. Our approach leverages Large Language
Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning
processes and results through code execution. Using the Code2Logic approach, we
developed the GameQA dataset to train and evaluate VLMs. GameQA is
cost-effective and scalable to produce, challenging for state-of-the-art
models, and diverse with 30 games and 158 tasks. Surprisingly, despite training
solely on game data, VLMs demonstrated out of domain generalization,
specifically Qwen2.5-VL-7B improving performance by 2.33\% across 7 diverse
vision-language benchmarks. Our code and dataset are available at
https://github.com/tongjingqi/Code2Logic.",2025-05-20,"Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang",http://arxiv.org/pdf/2505.13886v1,cs.CL
InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models,"Model fusion combines multiple Large Language Models (LLMs) with different
strengths into a more powerful, integrated model through lightweight training
methods. Existing works on model fusion focus primarily on supervised
fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for
enhancing LLM performance--largely unexplored. The current few fusion methods
on PA phase, like WRPO, simplify the process by utilizing only response outputs
from source models while discarding their probability information. To address
this limitation, we propose InfiFPO, a preference optimization method for
implicit model fusion. InfiFPO replaces the reference model in Direct
Preference Optimization (DPO) with a fused source model that synthesizes
multi-source probabilities at the sequence level, circumventing complex
vocabulary alignment challenges in previous works and meanwhile maintaining the
probability information. By introducing probability clipping and max-margin
fusion strategies, InfiFPO enables the pivot model to align with human
preferences while effectively distilling knowledge from source models.
Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO
consistently outperforms existing model fusion and preference optimization
methods. When using Phi-4 as the pivot model, InfiFPO improve its average
performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its
capabilities in mathematics, coding, and reasoning tasks.",2025-05-20,"Yanggan Gu, Zhaoyi Yan, Yuanyi Wang, Yiming Zhang, Qi Zhou, Fei Wu, Hongxia Yang",http://arxiv.org/pdf/2505.13878v1,cs.CL
Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning,"Recent reasoning-focused language models achieve high accuracy by generating
lengthy intermediate reasoning paths before producing final answers. While this
approach is effective in solving problems that require logical thinking, long
reasoning paths significantly increase memory usage and throughput of token
generation, limiting the practical deployment of such models. We propose
Reasoning Path Compression (RPC), a training-free method that accelerates
inference by leveraging the semantic sparsity of reasoning paths. RPC
periodically compresses the KV cache by retaining KV cache that receive high
importance score, which are computed using a selector window composed of
recently generated queries. Experiments show that RPC improves generation
throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full
KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our
findings demonstrate that semantic sparsity in reasoning traces can be
effectively exploited for compression, offering a practical path toward
efficient deployment of reasoning LLMs. Our code is available at
https://github.com/jiwonsong-dev/ReasoningPathCompression.",2025-05-20,"Jiwon Song, Dongwon Jo, Yulhwa Kim, Jae-Joon Kim",http://arxiv.org/pdf/2505.13866v1,cs.CL
PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks,"Large language models (LLMs) have achieved remarkable capabilities but remain
vulnerable to adversarial prompts known as jailbreaks, which can bypass safety
alignment and elicit harmful outputs. Despite growing efforts in LLM safety
research, existing evaluations are often fragmented, focused on isolated attack
or defense techniques, and lack systematic, reproducible analysis. In this
work, we introduce PandaGuard, a unified and modular framework that models LLM
jailbreak safety as a multi-agent system comprising attackers, defenders, and
judges. Our framework implements 19 attack methods and 12 defense mechanisms,
along with multiple judgment strategies, all within a flexible plugin
architecture supporting diverse LLM interfaces, multiple interaction modes, and
configuration-driven experimentation that enhances reproducibility and
practical deployment. Built on this framework, we develop PandaBench, a
comprehensive benchmark that evaluates the interactions between these
attack/defense methods across 49 LLMs and various judgment approaches,
requiring over 3 billion tokens to execute. Our extensive evaluation reveals
key insights into model vulnerabilities, defense cost-performance trade-offs,
and judge consistency. We find that no single defense is optimal across all
dimensions and that judge disagreement introduces nontrivial variance in safety
assessments. We release the code, configurations, and evaluation results to
support transparent and reproducible research in LLM safety.",2025-05-20,"Guobin Shen, Dongcheng Zhao, Linghao Feng, Xiang He, Jihang Wang, Sicheng Shen, Haibo Tong, Yiting Dong, Jindong Li, Xiang Zheng, Yi Zeng",http://arxiv.org/pdf/2505.13862v3,cs.CL
Domain Gating Ensemble Networks for AI-Generated Text Detection,"As state-of-the-art language models continue to improve, the need for robust
detection of machine-generated text becomes increasingly critical. However,
current state-of-the-art machine text detectors struggle to adapt to new unseen
domains and generative models. In this paper we present DoGEN (Domain Gating
Ensemble Networks), a technique that allows detectors to adapt to unseen
domains by ensembling a set of domain expert detector models using weights from
a domain classifier. We test DoGEN on a wide variety of domains from leading
benchmarks and find that it achieves state-of-the-art performance on in-domain
detection while outperforming models twice its size on out-of-domain detection.
We release our code and trained models to assist in future research in
domain-adaptive AI detection.",2025-05-20,"Arihant Tripathi, Liam Dugan, Charis Gao, Maggie Huan, Emma Jin, Peter Zhang, David Zhang, Julia Zhao, Chris Callison-Burch",http://arxiv.org/pdf/2505.13855v1,cs.CL
Forensic deepfake audio detection using segmental speech features,"This study explores the potential of using acoustic features of segmental
speech sounds to detect deepfake audio. These features are highly interpretable
because of their close relationship with human articulatory processes and are
expected to be more difficult for deepfake models to replicate. The results
demonstrate that certain segmental features commonly used in forensic voice
comparison are effective in identifying deep-fakes, whereas some global
features provide little value. These findings underscore the need to approach
audio deepfake detection differently for forensic voice comparison and offer a
new perspective on leveraging segmental features for this purpose.",2025-05-20,"Tianle Yang, Chengzhe Sun, Siwei Lyu, Phil Rose",http://arxiv.org/pdf/2505.13847v1,cs.CL
Improve Language Model and Brain Alignment via Associative Memory,"Associative memory engages in the integration of relevant information for
comprehension in the human cognition system. In this work, we seek to improve
alignment between language models and human brain while processing speech
information by integrating associative memory. After verifying the alignment
between language model and brain by mapping language model activations to brain
activity, the original text stimuli expanded with simulated associative memory
are regarded as input to computational language models. We find the alignment
between language model and brain is improved in brain regions closely related
to associative memory processing. We also demonstrate large language models
after specific supervised fine-tuning better align with brain response, by
building the \textit{Association} dataset containing 1000 samples of stories,
with instructions encouraging associative memory as input and associated
content as output.",2025-05-20,"Congchi Yin, Yongpeng Zhang, Xuyun Wen, Piji Li",http://arxiv.org/pdf/2505.13844v1,cs.CL
EfficientLLM: Efficiency in Large Language Models,"Large Language Models (LLMs) have driven significant progress, yet their
growing parameter counts and context windows incur prohibitive compute, energy,
and monetary costs. We introduce EfficientLLM, a novel benchmark and the first
comprehensive empirical study evaluating efficiency techniques for LLMs at
scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our
study systematically explores three key axes: (1) architecture pretraining
(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts
(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and
(3) inference (quantization methods: int4, float16). We define six fine-grained
metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy
Consumption, Compression Rate) to capture hardware saturation,
latency-throughput balance, and carbon cost. Evaluating over 100
model-technique pairs (0.5B-72B parameters), we derive three core insights: (i)
Efficiency involves quantifiable trade-offs: no single method is universally
optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by
40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%
accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal
memory-latency trade-offs for constrained devices, MLA achieves lowest
perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency
only beyond 14B parameters. (iii) Techniques generalize across modalities: we
extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and
Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By
open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM
provides essential guidance for researchers and engineers navigating the
efficiency-performance landscape of next-generation foundation models.",2025-05-20,"Zhengqing Yuan, Weixiang Sun, Yixin Liu, Huichi Zhou, Rong Zhou, Yiyang Li, Zheyuan Zhang, Wei Song, Yue Huang, Haolong Jia, Keerthiram Murugesan, Yu Wang, Lifang He, Jianfeng Gao, Lichao Sun, Yanfang Ye",http://arxiv.org/pdf/2505.13840v1,cs.CL
Mechanistic Interpretability of GPT-like Models on Summarization Tasks,"Mechanistic interpretability research seeks to reveal the inner workings of
large language models, yet most work focuses on classification or generative
tasks rather than summarization. This paper presents an interpretability
framework for analyzing how GPT-like models adapt to summarization tasks. We
conduct differential analysis between pre-trained and fine-tuned models,
quantifying changes in attention patterns and internal activations. By
identifying specific layers and attention heads that undergo significant
transformation, we locate the ""summarization circuit"" within the model
architecture. Our findings reveal that middle layers (particularly 2, 3, and 5)
exhibit the most dramatic changes, with 62% of attention heads showing
decreased entropy, indicating a shift toward focused information selection. We
demonstrate that targeted LoRA adaptation of these identified circuits achieves
significant performance improvement over standard LoRA fine-tuning while
requiring fewer training epochs. This work bridges the gap between black-box
evaluation and mechanistic understanding, providing insights into how neural
networks perform information selection and compression during summarization.",2025-05-20,Anurag Mishra,http://arxiv.org/pdf/2505.17073v1,cs.CL
Structured Agent Distillation for Large Language Model,"Large language models (LLMs) exhibit strong capabilities as decision-making
agents by interleaving reasoning and actions, as seen in ReAct-style
frameworks. Yet, their practical deployment is constrained by high inference
costs and large model sizes. We propose Structured Agent Distillation, a
framework that compresses large LLM-based agents into smaller student models
while preserving both reasoning fidelity and action consistency. Unlike
standard token-level distillation, our method segments trajectories into
{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each
component with the teacher's behavior. This structure-aware supervision enables
compact agents to better replicate the teacher's decision process. Experiments
on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently
outperforms token-level and imitation learning baselines, achieving significant
compression with minimal performance drop. Scaling and ablation results further
highlight the importance of span-level alignment for efficient and deployable
agents.",2025-05-20,"Jun Liu, Zhenglun Kong, Peiyan Dong, Changdi Yang, Tianqi Li, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Pu Zhao, Xue Lin, Dong Huang, Yanzhi Wang",http://arxiv.org/pdf/2505.13820v1,cs.CL
MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models,"Warning: This paper contains examples of harmful language and images. Reader
discretion is advised. Recently, vision-language models have demonstrated
increasing influence in morally sensitive domains such as autonomous driving
and medical analysis, owing to their powerful multimodal reasoning
capabilities. As these models are deployed in high-stakes real-world
applications, it is of paramount importance to ensure that their outputs align
with human moral values and remain within moral boundaries. However, existing
work on moral alignment either focuses solely on textual modalities or relies
heavily on AI-generated images, leading to distributional biases and reduced
realism. To overcome these limitations, we introduce MORALISE, a comprehensive
benchmark for evaluating the moral alignment of vision-language models (VLMs)
using diverse, expert-verified real-world data. We begin by proposing a
comprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory,
spanning the personal, interpersonal, and societal moral domains encountered in
everyday life. Built on this framework, we manually curate 2,481 high-quality
image-text pairs, each annotated with two fine-grained labels: (1) topic
annotation, identifying the violated moral topic(s), and (2) modality
annotation, indicating whether the violation arises from the image or the text.
For evaluation, we encompass two tasks, \textit{moral judgment} and
\textit{moral norm attribution}, to assess models' awareness of moral
violations and their reasoning ability on morally salient content. Extensive
experiments on 19 popular open- and closed-source VLMs show that MORALISE poses
a significant challenge, revealing persistent moral limitations in current
state-of-the-art models. The full benchmark is publicly available at
https://huggingface.co/datasets/Ze1025/MORALISE.",2025-05-20,"Xiao Lin, Zhining Liu, Ze Yang, Gaotang Li, Ruizhong Qiu, Shuke Wang, Hui Liu, Haotian Li, Sumit Keswani, Vishwa Pardeshi, Huijun Zhao, Wei Fan, Hanghang Tong",http://arxiv.org/pdf/2505.14728v1,cs.CL
"Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation","Question Answering (QA) poses a challenging and critical problem,
particularly in today's age of interactive dialogue systems such as ChatGPT,
Perplexity, Microsoft Copilot, etc. where users demand both accuracy and
transparency in the model's outputs. Since smaller language models (SLMs) are
computationally more efficient but often under-perform compared to larger
models, Knowledge Distillation (KD) methods allow for finetuning these smaller
models to improve their final performance. Lately, the intermediate tokens or
the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by
reasoning models such as DeepSeek R1 are used as a training signal for KD.
However, these reasoning traces are often verbose and difficult to interpret or
evaluate. In this work, we aim to address the challenge of evaluating the
faithfulness of these reasoning traces and their correlation with the final
performance. To this end, we employ a KD method leveraging rule-based problem
decomposition. This approach allows us to break down complex queries into
structured sub-problems, generating interpretable traces whose correctness can
be readily evaluated, even at inference time. Specifically, we demonstrate this
approach on Open Book QA, decomposing the problem into a Classification step
and an Information Retrieval step, thereby simplifying trace evaluation. Our
SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft
Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the
striking finding that correct traces do not necessarily imply that the model
outputs the correct final solution. Similarly, we find a low correlation
between correct final solutions and intermediate trace correctness. These
results challenge the implicit assumption behind utilizing reasoning traces for
improving SLMs' final performance via KD.",2025-05-20,"Siddhant Bhambri, Upasana Biswas, Subbarao Kambhampati",http://arxiv.org/pdf/2505.13792v1,cs.CL
Krikri: Advancing Open Large Language Models for Greek,"We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored
for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been
extensively trained on high-quality Greek data to ensure superior adaptation to
linguistic nuances. With 8 billion parameters, it offers advanced capabilities
while maintaining efficient computational performance. Llama-Krikri-8B supports
both Modern Greek and English, and is also equipped to handle polytonic text
and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage
post-training pipeline, utilizing both human and synthetic instruction and
preference data, by applying techniques such as MAGPIE. In addition, for
evaluation, we propose three novel public benchmarks for Greek. Our evaluation
on existing as well as the proposed benchmarks shows notable improvements over
comparable Greek and multilingual LLMs in both natural language understanding
and generation as well as code generation.",2025-05-19,"Dimitris Roussis, Leon Voukoutis, Georgios Paraskevopoulos, Sokratis Sofianopoulos, Prokopis Prokopidis, Vassilis Papavasileiou, Athanasios Katsamanis, Stelios Piperidis, Vassilis Katsouros",http://arxiv.org/pdf/2505.13772v1,cs.CL
Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference,"Reliable causal inference is essential for making decisions in high-stakes
areas like medicine, economics, and public policy. However, it remains unclear
whether large language models (LLMs) can handle rigorous and trustworthy
statistical causal inference. Current benchmarks usually involve simplified
tasks. For example, these tasks might only ask LLMs to identify semantic causal
relationships or draw conclusions directly from raw data. As a result, models
may overlook important statistical pitfalls, such as Simpson's paradox or
selection bias. This oversight limits the applicability of LLMs in the real
world. To address these limitations, we propose CausalPitfalls, a comprehensive
benchmark designed to rigorously evaluate the capability of LLMs in overcoming
common causal inference pitfalls. Our benchmark features structured challenges
across multiple difficulty levels, each paired with grading rubrics. This
approach allows us to quantitatively measure both causal reasoning capabilities
and the reliability of LLMs' responses. We evaluate models using two protocols:
(1) direct prompting, which assesses intrinsic causal reasoning, and (2)
code-assisted prompting, where models generate executable code for explicit
statistical analysis. Additionally, we validate the effectiveness of this judge
by comparing its scoring with assessments from human experts. Our results
reveal significant limitations in current LLMs when performing statistical
causal inference. The CausalPitfalls benchmark provides essential guidance and
quantitative metrics to advance the development of trustworthy causal reasoning
systems.",2025-05-19,"Jin Du, Li Chen, Xun Xian, An Luo, Fangqiao Tian, Ganghua Wang, Charles Doss, Xiaotong Shen, Jie Ding",http://arxiv.org/pdf/2505.13770v1,cs.CL
Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques,"Software Quality Assurance (SQA) is critical for delivering reliable, secure,
and efficient software products. The Software Quality Assurance Process aims to
provide assurance that work products and processes comply with predefined
provisions and plans. Recent advancements in Large Language Models (LLMs)
present new opportunities to enhance existing SQA processes by automating tasks
like requirement analysis, code review, test generation, and compliance checks.
Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010,
ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured
frameworks for ensuring robust quality practices. This paper surveys the
intersection of LLM-based SQA methods and these recognized standards,
highlighting how AI-driven solutions can augment traditional approaches while
maintaining compliance and process maturity. We first review the foundational
software quality standards and the technical fundamentals of LLMs in software
engineering. Next, we explore various LLM-based SQA applications, including
requirement validation, defect detection, test generation, and documentation
maintenance. We then map these applications to key software quality frameworks,
illustrating how LLMs can address specific requirements and metrics within each
standard. Empirical case studies and open-source initiatives demonstrate the
practical viability of these methods. At the same time, discussions on
challenges (e.g., data privacy, model bias, explainability) underscore the need
for deliberate governance and auditing. Finally, we propose future directions
encompassing adaptive learning, privacy-focused deployments, multimodal
analysis, and evolving standards for AI-driven software quality.",2025-05-19,Avinash Patil,http://arxiv.org/pdf/2505.13766v1,cs.CL
Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations,"Large language models (LLMs) can sometimes report the strategies they
actually use to solve tasks, but they can also fail to do so. This suggests
some degree of metacognition -- the capacity to monitor one's own cognitive
processes for subsequent reporting and self-control. Metacognitive abilities
enhance AI capabilities but raise safety concerns, as models might obscure
their internal processes to evade neural-activation-based oversight mechanisms
designed to detect harmful behaviors. Given society's increased reliance on
these models, it is critical that we understand the limits of their
metacognitive abilities, particularly their ability to monitor their internal
activations. To address this, we introduce a neuroscience-inspired
neurofeedback paradigm designed to quantify the ability of LLMs to explicitly
report and control their activation patterns. By presenting models with
sentence-label pairs where labels correspond to sentence-elicited internal
activations along specific directions in the neural representation space, we
demonstrate that LLMs can learn to report and control these activations. The
performance varies with several factors: the number of example pairs provided,
the semantic interpretability of the target neural direction, and the variance
explained by that direction. These results reveal a ""metacognitive space"" with
dimensionality much lower than the model's neural space, suggesting LLMs can
monitor only a subset of their neural mechanisms. Our findings provide
empirical evidence quantifying metacognitive capabilities in LLMs, with
significant implications for AI safety.",2025-05-19,"Li Ji-An, Hua-Dong Xiong, Robert C. Wilson, Marcelo G. Mattar, Marcus K. Benna",http://arxiv.org/pdf/2505.13763v1,cs.CL
Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making,"Simulations, although powerful in accurately replicating real-world systems,
often remain inaccessible to non-technical users due to their complexity.
Conversely, large language models (LLMs) provide intuitive, language-based
interactions but can lack the structured, causal understanding required to
reliably model complex real-world dynamics. We introduce our simulation agent
framework, a novel approach that integrates the strengths of both simulation
models and LLMs. This framework helps empower users by leveraging the
conversational capabilities of LLMs to interact seamlessly with sophisticated
simulation systems, while simultaneously utilizing the simulations to ground
the LLMs in accurate and structured representations of real-world phenomena.
This integrated approach helps provide a robust and generalizable foundation
for empirical validation and offers broad applicability across diverse domains.",2025-05-19,"Jacob Kleiman, Kevin Frank, Joseph Voyles, Sindy Campagna",http://arxiv.org/pdf/2505.13761v2,cs.CL
LLM-Based Compact Reranking with Document Features for Scientific Retrieval,"Scientific retrieval is essential for advancing academic discovery. Within
this process, document reranking plays a critical role by refining first-stage
retrieval results. However, large language model (LLM) listwise reranking faces
unique challenges in the scientific domain. First-stage retrieval is often
suboptimal in the scientific domain, so relevant documents are ranked lower.
Moreover, conventional listwise reranking uses the full text of candidate
documents in the context window, limiting the number of candidates that can be
considered. As a result, many relevant documents are excluded before reranking,
which constrains overall retrieval performance. To address these challenges, we
explore compact document representations based on semantic features such as
categories, sections, and keywords, and propose a training-free, model-agnostic
reranking framework for scientific retrieval called CoRank. The framework
involves three stages: (i) offline extraction of document-level features, (ii)
coarse reranking using these compact representations, and (iii) fine-grained
reranking on full texts of the top candidates from stage (ii). This hybrid
design provides a high-level abstraction of document semantics, expands
candidate coverage, and retains critical details required for precise ranking.
Experiments on LitSearch and CSFCube show that CoRank significantly improves
reranking performance across different LLM backbones, increasing nDCG@10 from
32.0 to 39.7. Overall, these results highlight the value of information
extraction for reranking in scientific retrieval.",2025-05-19,"Runchu Tian, Xueqiang Xu, Bowen Jin, SeongKu Kang, Jiawei Han",http://arxiv.org/pdf/2505.13757v1,cs.CL
Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training,"Efficient LLM pre-training requires well-tuned hyperparameters (HPs),
including learning rate {\eta} and weight decay {\lambda}. We study scaling
laws for HPs: formulas for how to scale HPs as we scale model size N, dataset
size D, and batch size B. Recent work suggests the AdamW timescale,
B/({\eta}{\lambda}D), should remain constant across training settings, and we
verify the implication that optimal {\lambda} scales linearly with B, for a
fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise
power law in the tokens-per-parameter ratio, D/N. This law thus provides a
method to accurately predict {\lambda}opt in advance of large-scale training.
We also study scaling laws for optimal batch size Bopt (the B enabling lowest
loss at a given N,D) and critical batch size Bcrit (the B beyond which further
data parallelism becomes ineffective). In contrast with prior work, we find
both Bopt and Bcrit scale as power laws in D, independent of model size, N.
Finally, we analyze how these findings inform the real-world selection of
Pareto-optimal N and D under dual training time and compute objectives.",2025-05-19,"Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, Joel Hestness",http://arxiv.org/pdf/2505.13738v1,cs.CL
SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs,"Large Language models (LLMs) have demonstrated significant potential in
text-to-SQL reasoning tasks, yet a substantial performance gap persists between
existing open-source models and their closed-source counterparts. In this
paper, we introduce SQLForge, a novel approach for synthesizing reliable and
diverse data to enhance text-to-SQL reasoning in LLMs. We improve data
reliability through SQL syntax constraints and SQL-to-question reverse
translation, ensuring data logic at both structural and semantic levels. We
also propose an SQL template enrichment and iterative data domain exploration
mechanism to boost data diversity. Building on the augmented data, we fine-tune
a variety of open-source models with different architectures and parameter
sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves
the state-of-the-art performance on the widely recognized Spider and BIRD
benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX
accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing
the performance gap with closed-source methods.",2025-05-19,"Yu Guo, Dong Jin, Shenghao Ye, Shuangwu Chen, Jian Yang, Xiaobin Tan",http://arxiv.org/pdf/2505.13725v1,cs.CL
Safety Alignment Can Be Not Superficial With Explicit Safety Signals,"Recent studies on the safety alignment of large language models (LLMs) have
revealed that existing approaches often operate superficially, leaving models
vulnerable to various adversarial attacks. Despite their significance, these
studies generally fail to offer actionable solutions beyond data augmentation
for achieving more robust safety mechanisms. This paper identifies a
fundamental cause of this superficiality: existing alignment approaches often
presume that models can implicitly learn a safety-related reasoning task during
the alignment process, enabling them to refuse harmful requests. However, the
learned safety signals are often diluted by other competing objectives, leading
models to struggle with drawing a firm safety-conscious decision boundary when
confronted with adversarial attacks. Based on this observation, by explicitly
introducing a safety-related binary classification task and integrating its
signals with our attention and decoding strategies, we eliminate this ambiguity
and allow models to respond more responsibly to malicious queries. We emphasize
that, with less than 0.2x overhead cost, our approach enables LLMs to assess
the safety of both the query and the previously generated tokens at each
necessary generating step. Extensive experiments demonstrate that our method
significantly improves the resilience of LLMs against various adversarial
attacks, offering a promising pathway toward more robust generative AI systems.",2025-05-19,"Jianwei Li, Jung-Eun Kim",http://arxiv.org/pdf/2505.17072v1,cs.CL
Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings,"Designing effective reasoning-capable LLMs typically requires training using
Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with
carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on
extensive training data. This creates a major challenge when the amount of
quality training data is scarce. We propose a sample-efficient, two-stage
training strategy to develop reasoning LLMs under limited supervision. In the
first stage, we ""warm up"" the model by distilling Long CoTs from a toy domain,
namely, Knights \& Knaves (K\&K) logic puzzles to acquire general reasoning
skills. In the second stage, we apply RLVR to the warmed-up model using a
limited set of target-domain examples. Our experiments demonstrate that this
two-phase approach offers several benefits: $(i)$ the warmup phase alone
facilitates generalized reasoning, leading to performance improvements across a
range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro; $(ii)$ When both
the base model and the warmed-up model are RLVR trained on the same small
dataset ($\leq100$ examples), the warmed-up model consistently outperforms the
base model; $(iii)$ Warming up before RLVR training allows a model to maintain
cross-domain generalizability even after training on a specific domain; $(iv)$
Introducing warmup in the pipeline improves not only accuracy but also overall
sample efficiency during RLVR training. The results in this paper highlight the
promise of warmup for building robust reasoning LLMs in data-scarce
environments.",2025-05-19,"Safal Shrestha, Minwu Kim, Aadim Nepal, Anubhav Shrestha, Keith Ross",http://arxiv.org/pdf/2505.13718v2,cs.CL
Are Large Language Models Good at Detecting Propaganda?,"Propagandists use rhetorical devices that rely on logical fallacies and
emotional appeals to advance their agendas. Recognizing these techniques is key
to making informed decisions. Recent advances in Natural Language Processing
(NLP) have enabled the development of systems capable of detecting manipulative
content. In this study, we look at several Large Language Models and their
performance in detecting propaganda techniques in news articles. We compare the
performance of these LLMs with transformer-based models. We find that, while
GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude
3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally,
we find that all three LLMs outperform a MultiGranularity Network (MGN)
baseline in detecting instances of one out of six propaganda techniques
(name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in
detecting instances of appeal to fear and flag-waving.",2025-05-19,"Julia Jose, Rachel Greenstadt",http://arxiv.org/pdf/2505.13706v1,cs.CL
Assessing GPT Performance in a Proof-Based University-Level Course Under Blind Grading,"As large language models (LLMs) advance, their role in higher education,
particularly in free-response problem-solving, requires careful examination.
This study assesses the performance of GPT-4o and o1-preview under realistic
educational conditions in an undergraduate algorithms course. Anonymous
GPT-generated solutions to take-home exams were graded by teaching assistants
unaware of their origin. Our analysis examines both coarse-grained performance
(scores) and fine-grained reasoning quality (error patterns). Results show that
GPT-4o consistently struggles, failing to reach the passing threshold, while
o1-preview performs significantly better, surpassing the passing score and even
exceeding the student median in certain exercises. However, both models exhibit
issues with unjustified claims and misleading arguments. These findings
highlight the need for robust assessment strategies and AI-aware grading
policies in education.",2025-05-19,"Ming Ding, Rasmus Kyng, Federico Solda, Weixuan Yuan",http://arxiv.org/pdf/2505.13664v1,cs.CL
Clarifying orthography: Orthographic transparency as compressibility,"Orthographic transparency -- how directly spelling is related to sound --
lacks a unified, script-agnostic metric. Using ideas from algorithmic
information theory, we quantify orthographic transparency in terms of the
mutual compressibility between orthographic and phonological strings. Our
measure provides a principled way to combine two factors that decrease
orthographic transparency, capturing both irregular spellings and rule
complexity in one quantity. We estimate our transparency measure using
prequential code-lengths derived from neural sequence models. Evaluating 22
languages across a broad range of script types (alphabetic, abjad, abugida,
syllabic, logographic) confirms common intuitions about relative transparency
of scripts. Mutual compressibility offers a simple, principled, and general
yardstick for orthographic transparency.",2025-05-19,"Charles J. Torres, Richard Futrell",http://arxiv.org/pdf/2505.13657v1,cs.CL
Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents,"Large language models (LLMs) have recently achieved remarkable results in
complex multi-step tasks, such as mathematical reasoning and agentic software
engineering. However, they often struggle to maintain consistent performance
across multiple solution attempts. One effective approach to narrow the gap
between average-case and best-case performance is guided test-time search,
which explores multiple solution paths to identify the most promising one.
Unfortunately, effective search techniques (e.g. MCTS) are often unsuitable for
non-serializable RL environments, such as Docker containers, where intermediate
environment states cannot be easily saved and restored. We investigate two
complementary search strategies applicable to such environments: 1-step
lookahead and trajectory selection, both guided by a learned action-value
function estimator. On the SWE-bench Verified benchmark, a key testbed for
agentic software engineering, we find these methods to double the average
success rate of a fine-tuned Qwen-72B model, achieving 40.8%, the new
state-of-the-art for open-weights models. Additionally, we show that these
techniques are transferable to more advanced closed models, yielding similar
improvements with GPT-4o.",2025-05-19,"Karina Zainullina, Alexander Golubev, Maria Trofimova, Sergei Polezhaev, Ibragim Badertdinov, Daria Litvintseva, Simon Karasik, Filipp Fisin, Sergei Skvortsov, Maksim Nekrashevich, Anton Shevtsov, Boris Yangel",http://arxiv.org/pdf/2505.13652v1,cs.CL
4Hammer: a board-game reinforcement learning environment for the hour long time frame,"Large Language Models (LLMs) have demonstrated strong performance on tasks
with short time frames, but struggle with tasks requiring longer durations.
While datasets covering extended-duration tasks, such as software engineering
tasks or video games, do exist, there are currently few implementations of
complex board games specifically designed for reinforcement learning and LLM
evaluation. To address this gap, we propose the 4Hammer reinforcement learning
environment, a digital twin simulation of a subset of Warhammer 40,000-a
complex, zero-sum board game. Warhammer 40,000 features intricate rules,
requiring human players to thoroughly read and understand over 50 pages of
detailed natural language rules, grasp the interactions between their game
pieces and those of their opponents, and independently track and communicate
the evolving game state.",2025-05-19,"Massimo Fioravanti, Giovanni Agosta",http://arxiv.org/pdf/2505.13638v1,cs.CL
Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning,"Multilingual alignment of sentence representations has mostly required
bitexts to bridge the gap between languages. We investigate whether visual
information can bridge this gap instead. Image caption datasets are very easy
to create without requiring multilingual expertise, so this offers a more
efficient alternative for low-resource languages. We find that multilingual
image-caption alignment can implicitly align the text representations between
languages, languages unseen by the encoder in pretraining can be incorporated
into this alignment post-hoc, and these aligned representations are usable for
cross-lingual Natural Language Understanding (NLU) and bitext retrieval.",2025-05-19,"Nathaniel Krasner, Nicholas Lanuzo, Antonios Anastasopoulos",http://arxiv.org/pdf/2505.13628v1,cs.CL
CIE: Controlling Language Model Text Generations Using Continuous Signals,"Aligning language models with user intent is becoming increasingly relevant
to enhance user experience. This calls for designing methods that can allow
users to control the properties of the language that LMs generate. For example,
controlling the length of the generation, the complexity of the language that
gets chosen, the sentiment, tone, etc. Most existing work attempts to integrate
users' control by conditioning LM generations on natural language prompts or
discrete control signals, which are often brittle and hard to scale. In this
work, we are interested in \textit{continuous} control signals, ones that exist
along a spectrum that can't easily be captured in a natural language prompt or
via existing techniques in conditional generation. Through a case study in
controlling the precise response-length of generations produced by LMs, we
demonstrate how after fine-tuning, behaviors of language models can be
controlled via continuous signals -- as vectors that are interpolated between a
""low"" and a ""high"" token embedding. Our method more reliably exerts
response-length control than in-context learning methods or fine-tuning methods
that represent the control signal as a discrete signal. Our full open-sourced
code and datasets are available at https://github.com/vsamuel2003/CIE.",2025-05-19,"Vinay Samuel, Harshita Diddee, Yiming Zhang, Daphne Ippolito",http://arxiv.org/pdf/2505.13448v1,cs.CL
"Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards","Large Language Models (LLMs) show great promise in complex reasoning, with
Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement
strategy. However, a prevalent issue is ``superficial self-reflection'', where
models fail to robustly verify their own outputs. We introduce RISE
(Reinforcing Reasoning with Self-Verification), a novel online RL framework
designed to tackle this. RISE explicitly and simultaneously trains an LLM to
improve both its problem-solving and self-verification abilities within a
single, integrated RL process. The core mechanism involves leveraging
verifiable rewards from an outcome verifier to provide on-the-fly feedback for
both solution generation and self-verification tasks. In each iteration, the
model generates solutions, then critiques its own on-policy generated
solutions, with both trajectories contributing to the policy update. Extensive
experiments on diverse mathematical reasoning benchmarks show that RISE
consistently improves model's problem-solving accuracy while concurrently
fostering strong self-verification skills. Our analyses highlight the
advantages of online verification and the benefits of increased verification
compute. Additionally, RISE models exhibit more frequent and accurate
self-verification behaviors during reasoning. These advantages reinforce RISE
as a flexible and effective path towards developing more robust and self-aware
reasoners.",2025-05-19,"Xiaoyuan Liu, Tian Liang, Zhiwei He, Jiahao Xu, Wenxuan Wang, Pinjia He, Zhaopeng Tu, Haitao Mi, Dong Yu",http://arxiv.org/pdf/2505.13445v1,cs.CL
ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models,"Chart understanding presents a unique challenge for large vision-language
models (LVLMs), as it requires the integration of sophisticated textual and
visual reasoning capabilities. However, current LVLMs exhibit a notable
imbalance between these skills, falling short on visual reasoning that is
difficult to perform in text. We conduct a case study using a synthetic dataset
solvable only through visual reasoning and show that model performance degrades
significantly with increasing visual complexity, while human performance
remains robust. We then introduce ChartMuseum, a new Chart Question Answering
(QA) benchmark containing 1,162 expert-annotated questions spanning multiple
reasoning types, curated from real-world charts across 184 sources,
specifically built to evaluate complex visual and textual reasoning. Unlike
prior chart understanding benchmarks -- where frontier models perform similarly
and near saturation -- our benchmark exposes a substantial gap between model
and human performance, while effectively differentiating model capabilities:
although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro
attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct
achieves only 38.5%. Moreover, on questions requiring primarily visual
reasoning, all models experience a 35%-55% performance drop from
text-reasoning-heavy question performance. Lastly, our qualitative error
analysis reveals specific categories of visual reasoning that are challenging
for current LVLMs.",2025-05-19,"Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, Ramya Namuduri, Bodun Hu, Juan Diego Rodriguez, Puyuan Peng, Greg Durrett",http://arxiv.org/pdf/2505.13444v1,cs.CL
Optimizing Anytime Reasoning via Budget Relative Policy Optimization,"Scaling test-time compute is crucial for enhancing the reasoning capabilities
of large language models (LLMs). Existing approaches typically employ
reinforcement learning (RL) to maximize a verifiable reward obtained at the end
of reasoning traces. However, such methods optimize only the final performance
under a large and fixed token budget, which hinders efficiency in both training
and deployment. In this work, we present a novel framework, AnytimeReasoner, to
optimize anytime reasoning performance, which aims to improve token efficiency
and the flexibility of reasoning under varying token budget constraints. To
achieve this, we truncate the complete thinking process to fit within sampled
token budgets from a prior distribution, compelling the model to summarize the
optimal answer for each truncated thinking for verification. This introduces
verifiable dense rewards into the reasoning process, facilitating more
effective credit assignment in RL optimization. We then optimize the thinking
and summary policies in a decoupled manner to maximize the cumulative reward.
Additionally, we introduce a novel variance reduction technique, Budget
Relative Policy Optimization (BRPO), to enhance the robustness and efficiency
of the learning process when reinforcing the thinking policy. Empirical results
in mathematical reasoning tasks demonstrate that our method consistently
outperforms GRPO across all thinking budgets under various prior distributions,
enhancing both training and token efficiency.",2025-05-19,"Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin",http://arxiv.org/pdf/2505.13438v1,cs.CL
SMOTExT: SMOTE meets Large Language Models,"Data scarcity and class imbalance are persistent challenges in training
robust NLP models, especially in specialized domains or low-resource settings.
We propose a novel technique, SMOTExT, that adapts the idea of Synthetic
Minority Over-sampling (SMOTE) to textual data. Our method generates new
synthetic examples by interpolating between BERT-based embeddings of two
existing examples and then decoding the resulting latent point into text with
xRAG architecture. By leveraging xRAG's cross-modal retrieval-generation
framework, we can effectively turn interpolated vectors into coherent text.
While this is preliminary work supported by qualitative outputs only, the
method shows strong potential for knowledge distillation and data augmentation
in few-shot settings. Notably, our approach also shows promise for
privacy-preserving machine learning: in early experiments, training models
solely on generated data achieved comparable performance to models trained on
the original dataset. This suggests a viable path toward safe and effective
learning under data protection constraints.",2025-05-19,"Mateusz Bystroński, Mikołaj Hołysz, Grzegorz Piotrowski, Nitesh V. Chawla, Tomasz Kajdanowicz",http://arxiv.org/pdf/2505.13434v1,cs.CL
Fine-tuning Quantized Neural Networks with Zeroth-order Optimization,"As the size of large language models grows exponentially, GPU memory has
become a bottleneck for adapting these models to downstream tasks. In this
paper, we aim to push the limits of memory-efficient training by minimizing
memory usage on model weights, gradients, and optimizer states, within a
unified framework. Our idea is to eliminate both gradients and optimizer states
using zeroth-order optimization, which approximates gradients by perturbing
weights during forward passes to identify gradient directions. To minimize
memory usage on weights, we employ model quantization, e.g., converting from
bfloat16 to int4. However, directly applying zeroth-order optimization to
quantized weights is infeasible due to the precision gap between discrete
weights and continuous gradients, which would otherwise require de-quantization
and re-quantization. To overcome this challenge, we propose Quantized
Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous
quantization scale for gradient estimation and uses a directional derivative
clipping method to stabilize training. QZO is orthogonal to both scalar-based
and codebook-based post-training quantization methods. Compared to
full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by
more than 18$\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and
Stable Diffusion 3.5 Large within a single 24GB GPU.",2025-05-19,"Sifeng Shang, Jiayi Zhou, Chenyu Lin, Minxian Li, Kaiyang Zhou",http://arxiv.org/pdf/2505.13430v1,cs.CL
Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness,"Cognitive decline often surfaces in language years before diagnosis. It is
frequently non-experts, such as those closest to the patient, who first sense a
change and raise concern. As LLMs become integrated into daily communication
and used over prolonged periods, it may even be an LLM that notices something
is off. But what exactly do they notice--and should be noticing--when making
that judgment? This paper investigates how dementia is perceived through
language by non-experts. We presented transcribed picture descriptions to
non-expert humans and LLMs, asking them to intuitively judge whether each text
was produced by someone healthy or with dementia. We introduce an explainable
method that uses LLMs to extract high-level, expert-guided features
representing these picture descriptions, and use logistic regression to model
human and LLM perceptions and compare with clinical diagnoses. Our analysis
reveals that human perception of dementia is inconsistent and relies on a
narrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a
richer, more nuanced feature set that aligns more closely with clinical
patterns. Still, both groups show a tendency toward false negatives, frequently
overlooking dementia cases. Through our interpretable framework and the
insights it provides, we hope to help non-experts better recognize the
linguistic signs that matter.",2025-05-19,"Lotem Peled-Cohen, Maya Zadok, Nitay Calderon, Hila Gonen, Roi Reichart",http://arxiv.org/pdf/2505.13418v1,cs.CL
AdaptThink: Reasoning Models Can Learn When to Think,"Recently, large reasoning models have achieved impressive performance on
various tasks by employing human-like deep thinking. However, the lengthy
thinking process substantially increases inference overhead, making efficiency
a critical bottleneck. In this work, we first demonstrate that NoThinking,
which prompts the reasoning model to skip thinking and directly generate the
final solution, is a better choice for relatively simple tasks in terms of both
performance and efficiency. Motivated by this, we propose AdaptThink, a novel
RL algorithm to teach reasoning models to choose the optimal thinking mode
adaptively based on problem difficulty. Specifically, AdaptThink features two
core components: (1) a constrained optimization objective that encourages the
model to choose NoThinking while maintaining the overall performance; (2) an
importance sampling strategy that balances Thinking and NoThinking samples
during on-policy training, thereby enabling cold start and allowing the model
to explore and exploit both thinking modes throughout the training process. Our
experiments indicate that AdaptThink significantly reduces the inference costs
while further enhancing performance. Notably, on three math datasets,
AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B
by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive
thinking-mode selection for optimizing the balance between reasoning quality
and efficiency. Our codes and models are available at
https://github.com/THU-KEG/AdaptThink.",2025-05-19,"Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, Juanzi Li",http://arxiv.org/pdf/2505.13417v1,cs.CL
CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process,"Recent Large Reasoning Models significantly improve the reasoning ability of
Large Language Models by learning to reason, exhibiting the promising
performance in solving complex tasks. LRMs solve tasks that require complex
reasoning by explicitly generating reasoning trajectories together with
answers. Nevertheless, judging the quality of such an output answer is not easy
because only considering the correctness of the answer is not enough and the
soundness of the reasoning trajectory part matters as well. Logically, if the
soundness of the reasoning part is poor, even if the answer is correct, the
confidence of the derived answer should be low. Existing methods did consider
jointly assessing the overall output answer by taking into account the
reasoning part, however, their capability is still not satisfactory as the
causal relationship of the reasoning to the concluded answer cannot properly
reflected. In this paper, inspired by classical mechanics, we present a novel
approach towards establishing a CoT-Kinetics energy equation. Specifically, our
CoT-Kinetics energy equation formulates the token state transformation process,
which is regulated by LRM internal transformer layers, as like a particle
kinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy
assigns a scalar score to evaluate specifically the soundness of the reasoning
phase, telling how confident the derived answer could be given the evaluated
reasoning. As such, the LRM's overall output quality can be accurately
measured, rather than a coarse judgment (e.g., correct or incorrect) anymore.",2025-05-19,"Jinhe Bi, Danqi Yan, Yifan Wang, Wenke Huang, Haokun Chen, Guancheng Wan, Mang Ye, Xun Xiao, Hinrich Schuetze, Volker Tresp, Yunpu Ma",http://arxiv.org/pdf/2505.13408v1,cs.CL
Granary: Speech Recognition and Translation Dataset in 25 European Languages,"Multi-task and multilingual approaches benefit large models, yet speech
processing for low-resource languages remains underexplored due to data
scarcity. To address this, we present Granary, a large-scale collection of
speech datasets for recognition and translation across 25 European languages.
This is the first open-source effort at this scale for both transcription and
translation. We enhance data quality using a pseudo-labeling pipeline with
segmentation, two-pass inference, hallucination filtering, and punctuation
restoration. We further generate translation pairs from pseudo-labeled
transcriptions using EuroLLM, followed by a data filtration pipeline. Designed
for efficiency, our pipeline processes vast amount of data within hours. We
assess models trained on processed data by comparing their performance on
previously curated datasets for both high- and low-resource languages. Our
findings show that these models achieve similar performance using approx. 50%
less data. Dataset will be made available at
https://hf.co/datasets/nvidia/Granary",2025-05-19,"Nithin Rao Koluguri, Monica Sekoyan, George Zelenfroynd, Sasha Meister, Shuoyang Ding, Sofia Kostandian, He Huang, Nikolay Karpov, Jagadeesh Balam, Vitaly Lavrukhin, Yifan Peng, Sara Papi, Marco Gaido, Alessio Brutti, Boris Ginsburg",http://arxiv.org/pdf/2505.13404v2,cs.CL
MR. Judge: Multimodal Reasoner as a Judge,"The paradigm of using Large Language Models (LLMs) and Multimodal Large
Language Models (MLLMs) as evaluative judges has emerged as an effective
approach in RLHF and inference-time scaling. In this work, we propose
Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering
general-purpose MLLMs judges with strong reasoning capabilities. Instead of
directly assigning scores for each response, we formulate the judgement process
as a reasoning-inspired multiple-choice problem. Specifically, the judge model
first conducts deliberate reasoning covering different aspects of the responses
and eventually selects the best response from them. This reasoning process not
only improves the interpretibility of the judgement, but also greatly enhances
the performance of MLLM judges. To cope with the lack of questions with scored
responses, we propose the following strategy to achieve automatic annotation:
1) Reverse Response Candidates Synthesis: starting from a supervised
fine-tuning (SFT) dataset, we treat the original response as the best candidate
and prompt the MLLM to generate plausible but flawed negative candidates. 2)
Text-based reasoning extraction: we carefully design a data synthesis pipeline
for distilling the reasoning capability from a text-based reasoning model,
which is adopted to enable the MLLM judges to regain complex reasoning ability
via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge
is effective across a wide range of tasks. Specifically, our MR. Judge-7B
surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet
during inference-time scaling by up to 7.7%.",2025-05-19,"Renjie Pi, Felix Bai, Qibin Chen, Simon Wang, Jiulong Shan, Kieran Liu, Meng Cao",http://arxiv.org/pdf/2505.13403v1,cs.CL
A Minimum Description Length Approach to Regularization in Neural Networks,"State-of-the-art neural networks can be trained to become remarkable
solutions to many problems. But while these architectures can express symbolic,
perfect solutions, trained models often arrive at approximations instead. We
show that the choice of regularization method plays a crucial role: when
trained on formal languages with standard regularization ($L_1$, $L_2$, or
none), expressive architectures not only fail to converge to correct solutions
but are actively pushed away from perfect initializations. In contrast,
applying the Minimum Description Length (MDL) principle to balance model
complexity with data fit provides a theoretically grounded regularization
method. Using MDL, perfect solutions are selected over approximations,
independently of the optimization algorithm. We propose that unlike existing
regularization techniques, MDL introduces the appropriate inductive bias to
effectively counteract overfitting and promote generalization.",2025-05-19,"Matan Abudy, Orr Well, Emmanuel Chemla, Roni Katzir, Nur Lan",http://arxiv.org/pdf/2505.13398v1,cs.CL
IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar,"This article provides an overview of IG Parser, a software that facilitates
qualitative content analysis of formal (e.g., legal) rules or informal (e.g.,
social) norms, and strategies (such as conventions) -- referred to as
institutions -- that govern social systems and operate configurally to describe
institutional systems. To this end, the IG Parser employs a distinctive syntax
that ensures rigorous encoding of natural language, while automating the
transformation into various formats that support the downstream analysis using
diverse analytical techniques. The conceptual core of the IG Parser is an
associated syntax, IG Script, that operationalizes the conceptual foundations
of the Institutional Grammar, and more specifically the Institutional Grammar
2.0, an analytical paradigm for institutional analysis. This article presents
the IG Parser, including its conceptual foundations, the syntax specification
of IG Script, and its architectural principles. This overview is augmented with
selective illustrative examples that highlight its use and the associated
benefits.",2025-05-19,Christopher K. Frantz,http://arxiv.org/pdf/2505.13393v2,cs.CL
R3: Robust Rubric-Agnostic Reward Models,"Reward models are essential for aligning language model outputs with human
preferences, yet existing approaches often lack both controllability and
interpretability. These models are typically optimized for narrow objectives,
limiting their generalizability to broader downstream tasks. Moreover, their
scalar outputs are difficult to interpret without contextual reasoning. To
address these limitations, we introduce R3, a novel reward modeling framework
that is rubric-agnostic, generalizable across evaluation dimensions, and
provides interpretable, reasoned score assignments. R3 enables more transparent
and flexible evaluation of language models, supporting robust alignment with
diverse human values and use cases. Our models, data, and code are available as
open source at https://github.com/rubricreward/r3",2025-05-19,"David Anugraha, Zilu Tang, Lester James V. Miranda, Hanyang Zhao, Mohammad Rifqi Farhansyah, Garry Kuwanto, Derry Wijaya, Genta Indra Winata",http://arxiv.org/pdf/2505.13388v2,cs.CL
CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition,"Sparse mixture of experts (SMoE) offers an appealing solution to scale up the
model complexity beyond the mean of increasing the network's depth or width.
However, we argue that effective SMoE training remains challenging because of
the suboptimal routing process where experts that perform computation do not
directly contribute to the routing process. In this work, we propose
competition, a novel mechanism to route tokens to experts with the highest
neural response. Theoretically, we show that the competition mechanism enjoys a
better sample efficiency than the traditional softmax routing. Furthermore, we
develop CompeteSMoE, a simple yet effective algorithm to train large language
models by deploying a router to learn the competition policy, thus enjoying
strong performances at a low training overhead. Our extensive empirical
evaluations on both the visual instruction tuning and language pre-training
tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE
compared to state-of-the-art SMoE strategies. We have made the implementation
available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an
improved version of the previous study at arXiv:2402.02526",2025-05-19,"Nam V. Nguyen, Huy Nguyen, Quang Pham, Van Nguyen, Savitha Ramasamy, Nhat Ho",http://arxiv.org/pdf/2505.13380v1,cs.CL
Thinkless: LLM Learns When to Think,"Reasoning Language Models, capable of extended chain-of-thought reasoning,
have demonstrated remarkable performance on tasks requiring complex logical
inference. However, applying elaborate reasoning for all queries often results
in substantial computational inefficiencies, particularly when many problems
admit straightforward solutions. This motivates an open question: Can LLMs
learn when to think? To answer this, we propose Thinkless, a learnable
framework that empowers an LLM to adaptively select between short-form and
long-form reasoning, based on both task complexity and the model's ability.
Thinkless is trained under a reinforcement learning paradigm and employs two
control tokens, <short> for concise responses and <think> for detailed
reasoning. At the core of our method is a Decoupled Group Relative Policy
Optimization (DeGRPO) algorithm, which decomposes the learning objective of
hybrid reasoning into two components: (1) a control token loss that governs the
selection of the reasoning mode, and (2) a response loss that improves the
accuracy of the generated answers. This decoupled formulation enables
fine-grained control over the contributions of each objective, stabilizing
training and effectively preventing collapse observed in vanilla GRPO.
Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and
GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -
90%, significantly improving the efficiency of Reasoning Language Models. The
code is available at https://github.com/VainF/Thinkless",2025-05-19,"Gongfan Fang, Xinyin Ma, Xinchao Wang",http://arxiv.org/pdf/2505.13379v1,cs.CL
What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts,"Building LLM-powered software requires developers to communicate their
requirements through natural language, but developer prompts are frequently
underspecified, failing to fully capture many user-important requirements. In
this paper, we present an in-depth analysis of prompt underspecification,
showing that while LLMs can often (41.1%) guess unspecified requirements by
default, such behavior is less robust: Underspecified prompts are 2x more
likely to regress over model or prompt changes, sometimes with accuracy drops
by more than 20%. We then demonstrate that simply adding more requirements to a
prompt does not reliably improve performance, due to LLMs' limited
instruction-following capabilities and competing constraints, and standard
prompt optimizers do not offer much help. To address this, we introduce novel
requirements-aware prompt optimization mechanisms that can improve performance
by 4.8% on average over baselines that naively specify everything in the
prompt. Beyond prompt optimization, we envision that effectively managing
prompt underspecification requires a broader process, including proactive
requirements discovery, evaluation, and monitoring.",2025-05-19,"Chenyang Yang, Yike Shi, Qianou Ma, Michael Xieyang Liu, Christian Kästner, Tongshuang Wu",http://arxiv.org/pdf/2505.13360v1,cs.CL
Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning,"Although modern Large Language Models (LLMs) support extremely large
contexts, their effectiveness in utilizing long context for code reasoning
remains unclear. This paper investigates LLM reasoning ability over code
snippets within large repositories and how it relates to their recall ability.
Specifically, we differentiate between lexical code recall (verbatim retrieval)
and semantic code recall (remembering what the code does). To measure semantic
recall, we propose SemTrace, a code reasoning technique where the impact of
specific statements on output is attributable and unpredictable. We also
present a method to quantify semantic recall sensitivity in existing
benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop
in code reasoning accuracy as a code snippet approaches the middle of the input
context, particularly with techniques requiring high semantic recall like
SemTrace. Moreover, we find that lexical recall varies by granularity, with
models excelling at function retrieval but struggling with line-by-line recall.
Notably, a disconnect exists between lexical and semantic recall, suggesting
different underlying mechanisms. Finally, our findings indicate that current
code reasoning benchmarks may exhibit low semantic recall sensitivity,
potentially underestimating LLM challenges in leveraging in-context
information.",2025-05-19,"Adam Štorek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana",http://arxiv.org/pdf/2505.13353v2,cs.CL
Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks,"Large Language Models (LLMs) are increasingly employed as evaluators
(LLM-as-a-Judge) for assessing the quality of machine-generated text. This
paradigm offers scalability and cost-effectiveness compared to human
annotation. However, the reliability and security of such systems, particularly
their robustness against adversarial manipulations, remain critical concerns.
This paper investigates the vulnerability of LLM-as-a-Judge architectures to
prompt-injection attacks, where malicious inputs are designed to compromise the
judge's decision-making process. We formalize two primary attack strategies:
Comparative Undermining Attack (CUA), which directly targets the final decision
output, and Justification Manipulation Attack (JMA), which aims to alter the
model's generated reasoning. Using the Greedy Coordinate Gradient (GCG)
optimization method, we craft adversarial suffixes appended to one of the
responses being compared. Experiments conducted on the MT-Bench Human Judgments
dataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and
Falcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves
an Attack Success Rate (ASR) exceeding 30\%, while JMA also shows notable
effectiveness. These findings highlight substantial vulnerabilities in current
LLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and
further research into adversarial evaluation and trustworthiness in LLM-based
assessment frameworks.",2025-05-19,"Narek Maloyan, Bislan Ashinov, Dmitry Namiot",http://arxiv.org/pdf/2505.13348v1,cs.CL
J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization,"To keep pace with the increasing pace of large language models (LLM)
development, model output evaluation has transitioned away from time-consuming
human evaluation to automatic evaluation, where LLMs themselves are tasked with
assessing and critiquing other model outputs. LLM-as-judge models are a class
of generative evaluators that excel in evaluating relatively simple domains,
like chat quality, but struggle in reasoning intensive domains where model
responses contain more substantive and challenging content. To remedy existing
judge shortcomings, we explore training judges with reinforcement learning
(RL). We make three key contributions: (1) We propose the Equivalent Initial
State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us
to train our judge to be robust to positional biases that arise in more complex
evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that
evaluates judges in diverse reasoning settings not covered by prior work. (3)
We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that
outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or
exceeding the performance of larger GRPO-trained judges on both JudgeBench and
ReasoningJudgeBench.",2025-05-19,"Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty",http://arxiv.org/pdf/2505.13346v2,cs.CL
Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation,"Current speech-LLMs exhibit limited capability in contextual reasoning
alongside paralinguistic understanding, primarily due to the lack of
Question-Answer (QA) datasets that cover both aspects. We propose a novel
framework for dataset generation from in-the-wild speech data, that integrates
contextual reasoning with paralinguistic information. It consists of a pseudo
paralinguistic label-based data condensation of in-the-wild speech and
LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is
validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct
model on a dataset created by our framework and human-generated CPQA dataset.
The results also reveal the speech-LLM's limitations in handling empathetic
reasoning tasks, highlighting the need for such datasets and more robust
models. The proposed framework is first of its kind and has potential in
training more robust speech-LLMs with paralinguistic reasoning capabilities.",2025-05-19,"Qiongqiong Wang, Hardik B. Sailor, Tianchi Liu, Ai Ti Aw",http://arxiv.org/pdf/2505.13338v1,cs.CL
Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges,"Existing benchmarks that assess Language Models (LMs) as Language Agents
(LAs) for tool use primarily focus on stateless, single-turn interactions or
partial evaluations, such as tool selection in a single turn, overlooking the
inherent stateful nature of interactions in multi-turn applications. To fulfill
this gap, we propose \texttt{DialogTool}, a multi-turn dialogue dataset with
stateful tool interactions considering the whole life cycle of tool use, across
six key tasks in three stages: 1) \textit{tool creation}; 2) \textit{tool
utilization}: tool awareness, tool selection, tool execution; and 3)
\textit{role-consistent response}: response generation and role play.
Furthermore, we build \texttt{VirtualMobile} -- an embodied virtual mobile
evaluation environment to simulate API calls and assess the robustness of the
created APIs\footnote{We will use tools and APIs alternatively, there are no
significant differences between them in this paper.}. Taking advantage of these
artifacts, we conduct comprehensive evaluation on 13 distinct open- and
closed-source LLMs and provide detailed analysis at each stage, revealing that
the existing state-of-the-art LLMs still cannot perform well to use tools over
long horizons.",2025-05-19,"Hongru Wang, Wenyu Huang, Yufei Wang, Yuanhao Xi, Jianqiao Lu, Huan Zhang, Nan Hu, Zeming Liu, Jeff Z. Pan, Kam-Fai Wong",http://arxiv.org/pdf/2505.13328v1,cs.CL
GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection,"Large Language Models (LLMs) have demonstrated strong capabilities in
memorizing vast amounts of knowledge across diverse domains. However, the
ability to selectively forget specific knowledge is critical for ensuring the
safety and compliance of deployed models. Existing unlearning efforts typically
fine-tune the model with resources such as forget data, retain data, and a
calibration model. These additional gradient steps blur the decision boundary
between forget and retain knowledge, making unlearning often at the expense of
overall performance. To avoid the negative impact of fine-tuning, it would be
better to unlearn solely at inference time by safely guarding the model against
generating responses related to the forget target, without destroying the
fluency of text generation. In this work, we propose Generation-time Unlearning
via Adaptive Restriction and Detection (GUARD), a framework that enables
dynamic unlearning during LLM generation. Specifically, we first employ a
prompt classifier to detect unlearning targets and extract the corresponding
forbidden token. We then dynamically penalize and filter candidate tokens
during generation using a combination of token matching and semantic matching,
effectively preventing the model from leaking the forgotten content.
Experimental results on copyright content unlearning tasks over the Harry
Potter dataset and the MUSE benchmark, as well as entity unlearning tasks on
the TOFU dataset, demonstrate that GUARD achieves strong forget quality across
various tasks while causing almost no degradation to the LLM's general
capabilities, striking an excellent trade-off between forgetting and utility.",2025-05-19,"Zhijie Deng, Chris Yuhao Liu, Zirui Pang, Xinlei He, Lei Feng, Qi Xuan, Zhaowei Zhu, Jiaheng Wei",http://arxiv.org/pdf/2505.13312v1,cs.CL
Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space,"Reasoning ability, a core component of human intelligence, continues to pose
a significant challenge for Large Language Models (LLMs) in the pursuit of AGI.
Although model performance has improved under the training scaling law,
significant challenges remain, particularly with respect to training
algorithms, such as catastrophic forgetting, and the limited availability of
novel training data. As an alternative, test-time scaling enhances reasoning
performance by increasing test-time computation without parameter updating.
Unlike prior methods in this paradigm focused on token space, we propose
leveraging latent space for more effective reasoning and better adherence to
the test-time scaling law. We introduce LatentSeek, a novel framework that
enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)
within the model's latent space. Specifically, LatentSeek leverages policy
gradient to iteratively update latent representations, guided by self-generated
reward signals. LatentSeek is evaluated on a range of reasoning benchmarks,
including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.
Results show that LatentSeek consistently outperforms strong baselines, such as
Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our
analysis demonstrates that LatentSeek is highly efficient, typically converging
within a few iterations for problems of average complexity, while also
benefiting from additional iterations, thereby highlighting the potential of
test-time scaling in the latent space. These findings position LatentSeek as a
lightweight, scalable, and effective solution for enhancing the reasoning
capabilities of LLMs.",2025-05-19,"Hengli Li, Chenxi Li, Tong Wu, Xuekai Zhu, Yuxuan Wang, Zhaoxin Yu, Eric Hanchen Jiang, Song-Chun Zhu, Zixia Jia, Ying Nian Wu, Zilong Zheng",http://arxiv.org/pdf/2505.13308v1,cs.CL
RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning,"Chain-of-Thought (CoT) reasoning has proven effective in enhancing large
language models (LLMs) on complex tasks, spurring research into its underlying
mechanisms. However, two primary challenges remain for real-world applications:
(1) the lack of quantitative metrics and actionable guidelines for evaluating
and optimizing measurable boundaries of CoT capability, and (2) the absence of
methods to assess boundaries of unmeasurable CoT capability, such as multimodal
perception. To address these gaps, we introduce the Reasoning Boundary
Framework++ (RBF++). To tackle the first challenge, we define the reasoning
boundary (RB) as the maximum limit of CoT performance. We also propose a
combination law for RBs, enabling quantitative analysis and offering actionable
guidance across various CoT tasks. For the second challenge, particularly in
multimodal scenarios, we introduce a constant assumption, which replaces
unmeasurable RBs with scenario-specific constants. Additionally, we propose the
reasoning boundary division mechanism, which divides unmeasurable RBs into two
sub-boundaries, facilitating the quantification and optimization of both
unmeasurable domain knowledge and multimodal perception capabilities. Extensive
experiments involving 38 models across 13 tasks validate the feasibility of our
framework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,
offer insights into optimization and decay from two complementary perspectives,
and expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope
this work advances the understanding of RBs and optimization strategies in
LLMs. Code and data are available at
https://github.com/LightChen233/reasoning-boundary.",2025-05-19,"Qiguang Chen, Libo Qin, Jinhao Liu, Yue Liao, Jiaqi Wang, Jingxuan Zhou, Wanxiang Che",http://arxiv.org/pdf/2505.13307v1,cs.CL
I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models,"Large language models are increasingly integrated into news recommendation
systems, raising concerns about their role in spreading misinformation. In
humans, visual content is known to boost credibility and shareability of
information, yet its effect on vision-language models (VLMs) remains unclear.
We present the first study examining how images influence VLMs' propensity to
reshare news content, whether this effect varies across model families, and how
persona conditioning and content attributes modulate this behavior. To support
this analysis, we introduce two methodological contributions: a
jailbreaking-inspired prompting strategy that elicits resharing decisions from
VLMs while simulating users with antisocial traits and political alignments;
and a multimodal dataset of fact-checked political news from PolitiFact, paired
with corresponding images and ground-truth veracity labels. Experiments across
model families reveal that image presence increases resharing rates by 4.8% for
true news and 15.0% for false news. Persona conditioning further modulates this
effect: Dark Triad traits amplify resharing of false news, whereas
Republican-aligned profiles exhibit reduced veracity sensitivity. Of all the
tested models, only Claude-3-Haiku demonstrates robustness to visual
misinformation. These findings highlight emerging risks in multimodal model
behavior and motivate the development of tailored evaluation frameworks and
mitigation strategies for personalized AI systems. Code and dataset are
available at: https://github.com/3lis/misinfo_vlm",2025-05-19,"Alice Plebe, Timothy Douglas, Diana Riazi, R. Maria del Rio-Chanona",http://arxiv.org/pdf/2505.13302v1,cs.CL
QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding,"Spoken Language Understanding (SLU) systems must balance performance and
efficiency, particularly in resource-constrained environments. Existing methods
apply distillation and quantization separately, leading to suboptimal
compression as distillation ignores quantization constraints. We propose QUADS,
a unified framework that optimizes both through multi-stage training with a
pre-tuned model, enhancing adaptability to low-bit regimes while maintaining
accuracy. QUADS achieves 71.13\% accuracy on SLURP and 99.20\% on FSC, with
only minor degradations of up to 5.56\% compared to state-of-the-art models.
Additionally, it reduces computational complexity by 60--73$\times$ (GMACs) and
model size by 83--700$\times$, demonstrating strong robustness under extreme
quantization. These results establish QUADS as a highly efficient solution for
real-world, resource-constrained SLU applications.",2025-05-19,"Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam",http://arxiv.org/pdf/2505.14723v1,cs.CL
"Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion","Taxonomies are hierarchical knowledge graphs crucial for recommendation
systems, and web applications. As data grows, expanding taxonomies is
essential, but existing methods face key challenges: (1) discriminative models
struggle with representation limits and generalization, while (2) generative
methods either process all candidates at once, introducing noise and exceeding
context limits, or discard relevant entities by selecting noisy candidates. We
propose LORex ($\textbf{L}$ineage-$\textbf{O}$riented $\textbf{Re}$asoning for
Taxonomy E$\textbf{x}$pansion), a plug-and-play framework that combines
discriminative ranking and generative reasoning for efficient taxonomy
expansion. Unlike prior methods, LORex ranks and chunks candidate terms into
batches, filtering noise and iteratively refining selections by reasoning
candidates' hierarchy to ensure contextual efficiency. Extensive experiments
across four benchmarks and twelve baselines show that LORex improves accuracy
by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.",2025-05-19,"Sahil Mishra, Kumar Arjun, Tanmoy Chakraborty",http://arxiv.org/pdf/2505.13282v3,cs.CL
What's in a prompt? Language models encode literary style in prompt embeddings,"Large language models use high-dimensional latent spaces to encode and
process textual information. Much work has investigated how the conceptual
content of words translates into geometrical relationships between their vector
representations. Fewer studies analyze how the cumulative information of an
entire prompt becomes condensed into individual embeddings under the action of
transformer layers. We use literary pieces to show that information about
intangible, rather than factual, aspects of the prompt are contained in deep
representations. We observe that short excerpts (10 - 100 tokens) from
different novels separate in the latent space independently from what
next-token prediction they converge towards. Ensembles from books from the same
authors are much more entangled than across authors, suggesting that embeddings
encode stylistic features. This geometry of style may have applications for
authorship attribution and literary analysis, but most importantly reveals the
sophistication of information processing and compression accomplished by
language models.",2025-05-19,"Raphaël Sarfati, Haley Moller, Toni J. B. Liu, Nicolas Boullé, Christopher Earls",http://arxiv.org/pdf/2505.17071v1,cs.CL
CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning,"Large language models (LLMs) have demonstrated strong capabilities in
translating natural language questions about relational databases into SQL
queries. In particular, test-time scaling techniques such as Self-Consistency
and Self-Correction can enhance SQL generation accuracy by increasing
computational effort during inference. However, these methods have notable
limitations: Self-Consistency may select suboptimal outputs despite majority
votes, while Self-Correction typically addresses only syntactic errors. To
leverage the strengths of both approaches, we propose CSC-SQL, a novel method
that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two
most frequently occurring outputs from parallel sampling and feeds them into a
merge revision model for correction. Additionally, we employ the Group Relative
Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and
revision models via reinforcement learning, significantly enhancing output
quality. Experimental results confirm the effectiveness and generalizability of
CSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution
accuracy, while the 7B model achieves 69.19%. The code will be open sourced at
https://github.com/CycloneBoy/csc_sql.",2025-05-19,"Lei Sheng, Shuai-Shuai Xu",http://arxiv.org/pdf/2505.13271v1,cs.CL
Representation of perceived prosodic similarity of conversational feedback,"Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of
spoken dialogue and is crucial to ensuring common ground in conversational
systems. The exact meaning of such feedback is conveyed through both lexical
and prosodic form. In this work, we investigate the perceived prosodic
similarity of vocal feedback with the same lexical form, and to what extent
existing speech representations reflect such similarities. A triadic comparison
task with recruited participants is used to measure perceived similarity of
feedback responses taken from two different datasets. We find that spectral and
self-supervised speech representations encode prosody better than extracted
pitch features, especially in the case of feedback from the same speaker. We
also find that it is possible to further condense and align the representations
to human perception through contrastive learning.",2025-05-19,"Livia Qian, Carol Figueroa, Gabriel Skantze",http://arxiv.org/pdf/2505.13268v1,cs.CL
RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection,"Content moderation for large language models (LLMs) remains a significant
challenge, requiring flexible and adaptable solutions that can quickly respond
to emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),
a novel approach that leverages a retrieval-augmented generation (RAG)
architecture to dynamically reject unsafe user queries without model
retraining. By strategically inserting and marking malicious documents into the
vector database, the system can identify and reject harmful requests when these
documents are retrieved. Our preliminary results show that RAR achieves
comparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,
while offering superior flexibility and real-time customization capabilities, a
fundamental feature to timely address critical vulnerabilities. This approach
introduces no architectural changes to existing RAG systems, requiring only the
addition of specially crafted documents and a simple rejection mechanism based
on retrieval results.",2025-05-19,"Tommaso Mario Buonocore, Enea Parimbelli",http://arxiv.org/pdf/2505.13581v1,cs.CL
From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery,"Large Language Models (LLMs) are catalyzing a paradigm shift in scientific
discovery, evolving from task-specific automation tools into increasingly
autonomous agents and fundamentally redefining research processes and human-AI
collaboration. This survey systematically charts this burgeoning field, placing
a central focus on the changing roles and escalating capabilities of LLMs in
science. Through the lens of the scientific method, we introduce a foundational
three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating
autonomy and evolving responsibilities within the research lifecycle. We
further identify pivotal challenges and future research trajectories such as
robotic automation, self-improvement, and ethical governance. Overall, this
survey provides a conceptual architecture and strategic foresight to navigate
and shape the future of AI-driven scientific discovery, fostering both rapid
innovation and responsible advancement. Github Repository:
https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.",2025-05-19,"Tianshi Zheng, Zheye Deng, Hong Ting Tsang, Weiqi Wang, Jiaxin Bai, Zihao Wang, Yangqiu Song",http://arxiv.org/pdf/2505.13259v1,cs.CL
Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability,"Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive domains.
However, although RAG achieved successes across distinct domains, there are
still some unsolved challenges: 1) Effectiveness. Existing research mainly
focuses on developing more powerful RAG retrievers, but how to enhance the
generator's (LLM's) ability to utilize the retrieved information for reasoning
and generation? 2) Transparency. Most RAG methods ignore which retrieved
content actually contributes to the reasoning process, resulting in a lack of
interpretability and visibility. To address this, we propose ARENA
(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator
framework trained via reinforcement learning (RL) with our proposed rewards.
Based on the structured generation and adaptive reward calculation, our
RL-based training enables the model to identify key evidence, perform
structured reasoning, and generate answers with interpretable decision traces.
Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments
with various RAG baselines demonstrate that our model achieves 10-30%
improvements on all multi-hop QA datasets, which is comparable with the SOTA
Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses
show that ARENA has strong flexibility to be adopted on new datasets without
extra training. Our models and codes are publicly released.",2025-05-19,"Jingyi Ren, Yekun Xu, Xiaolong Wang, Weitao Li, Weizhi Ma, Yang Liu",http://arxiv.org/pdf/2505.13258v1,cs.CL
WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?,"Preference alignment has become a standard pipeline in finetuning models to
follow \emph{generic} human preferences. Majority of work seeks to optimize
model to produce responses that would be preferable \emph{on average},
simplifying the diverse and often \emph{contradicting} space of human
preferences. While research has increasingly focused on personalized alignment:
adapting models to individual user preferences, there is a lack of personalized
preference dataset which focus on nuanced individual-level preferences. To
address this, we introduce WikiPersona: the first fine-grained personalization
using well-documented, famous individuals. Our dataset challenges models to
align with these personas through an interpretable process: generating
verifiable textual descriptions of a persona's background and preferences in
addition to alignment. We systematically evaluate different personalization
approaches and find that as few-shot prompting with preferences and fine-tuning
fail to simultaneously ensure effectiveness and efficiency, using
\textit{inferred personal preferences} as prefixes enables effective
personalization, especially in topics where preferences clash while leading to
more equitable generalization across unseen personas.",2025-05-19,"Zilu Tang, Afra Feyza Akyürek, Ekin Akyürek, Derry Wijaya",http://arxiv.org/pdf/2505.13257v1,cs.CL
HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding,"Autoregressive decoding, the standard approach for Large Language Model (LLM)
inference, remains a significant bottleneck due to its sequential nature. While
speculative decoding algorithms mitigate this inefficiency through parallel
verification, they fail to exploit the inherent heterogeneity in linguistic
complexity, a key factor leading to suboptimal resource allocation. We address
this by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding
framework that dynamically optimizes computational resource allocation based on
linguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A
novel cumulative meta-path Top-$K$ entropy metric for efficiently identifying
predictable contexts. (2) A dynamic resource allocation strategy based on
data-driven entropy partitioning, enabling adaptive speculative expansion and
pruning tailored to local context difficulty. Evaluated on five public
benchmarks and four models, HeteroSpec achieves an average speedup of
4.26$\times$. It consistently outperforms state-of-the-art EAGLE-3 across
speedup rates, average acceptance length, and verification cost. Notably,
HeteroSpec requires no draft model retraining, incurs minimal overhead, and is
orthogonal to other acceleration techniques. It demonstrates enhanced
acceleration with stronger draft models, establishing a new paradigm for
context-aware LLM inference acceleration.",2025-05-19,"Siran Liu, Yang Ye, Qianchao Zhu, Zheng Cao, Yongchao He",http://arxiv.org/pdf/2505.13254v1,cs.CL
Natural Language Planning via Coding and Inference Scaling,"Real-life textual planning tasks such as meeting scheduling have posed much
challenge to LLMs especially when the complexity is high. While previous work
primarily studied auto-regressive generation of plans with closed-source
models, we systematically evaluate both closed- and open-source models,
including those that scales output length with complexity during inference, in
generating programs, which are executed to output the plan. We consider not
only standard Python code, but also the code to a constraint satisfaction
problem solver. Despite the algorithmic nature of the task, we show that
programming often but not always outperforms planning. Our detailed error
analysis also indicates a lack of robustness and efficiency in the generated
code that hinders generalization.",2025-05-19,"Rikhil Amonkar, Ronan Le Bras, Li Zhang",http://arxiv.org/pdf/2505.13252v1,cs.CL
Stronger Together: Unleashing the Social Impact of Hate Speech Research,"The advent of the internet has been both a blessing and a curse for once
marginalised communities. When used well, the internet can be used to connect
and establish communities crossing different intersections; however, it can
also be used as a tool to alienate people and communities as well as perpetuate
hate, misinformation, and disinformation especially on social media platforms.
We propose steering hate speech research and researchers away from pre-existing
computational solutions and consider social methods to inform social solutions
to address this social problem. In a similar way linguistics research can
inform language planning policy, linguists should apply what we know about
language and society to mitigate some of the emergent risks and dangers of
anti-social behaviour in digital spaces. We argue linguists and NLP researchers
can play a principle role in unleashing the social impact potential of
linguistics research working alongside communities, advocates, activists, and
policymakers to enable equitable digital inclusion and to close the digital
divide.",2025-05-19,Sidney Wong,http://arxiv.org/pdf/2505.13251v1,cs.CL
JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models,"With the rapid advancement of global digitalization, users from different
countries increasingly rely on social media for information exchange. In this
context, multilingual multi-label emotion detection has emerged as a critical
research area. This study addresses SemEval-2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:
(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.
To tackle multilingual challenges, we leverage pre-trained multilingual models
and focus on two architectures: (1) a fine-tuned BERT-based classification
model and (2) an instruction-tuned generative LLM. Additionally, we propose two
methods for handling multi-label classification: the base method, which maps an
input directly to all its corresponding emotion labels, and the pairwise
method, which models the relationship between the input text and each emotion
category individually. Experimental results demonstrate the strong
generalization ability of our approach in multilingual emotion recognition. In
Track A, our method achieved Top 4 performance across 10 languages, ranking 1st
in Hindi. In Track B, our approach also secured Top 5 performance in 7
languages, highlighting its simplicity and effectiveness\footnote{Our code is
available at https://github.com/yingjie7/mlingual_multilabel_emo_detection.",2025-05-19,"Jieying Xue, Phuong Minh Nguyen, Minh Le Nguyen, Xin Liu",http://arxiv.org/pdf/2505.13244v1,cs.CL
SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information,"Large audio-language models (LALMs) extend the large language models with
multimodal understanding in speech, audio, etc. While their performances on
speech and audio-processing tasks are extensively studied, their reasoning
abilities remain underexplored. Particularly, their multi-hop reasoning, the
ability to recall and integrate multiple facts, lacks systematic evaluation.
Existing benchmarks focus on general speech and audio-processing tasks,
conversational abilities, and fairness but overlook this aspect. To bridge this
gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning
based on speech and audio information. Results show that LALMs struggle to
integrate speech/audio representations for multi-hop reasoning, even when they
extract the relevant information correctly, highlighting a fundamental
challenge in multimodal reasoning. Our findings expose a critical limitation in
LALMs, offering insights and resources for future research.",2025-05-19,"Chih-Kai Yang, Neo Ho, Yen-Ting Piao, Hung-yi Lee",http://arxiv.org/pdf/2505.13237v2,cs.CL
Improving endpoint detection in end-to-end streaming ASR for conversational speech,"ASR endpointing (EP) plays a major role in delivering a good user experience
in products supporting human or artificial agents in human-human/machine
conversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR
modelling technique preferred for streaming. A major limitation of T-ASR is
delayed emission of ASR outputs, which could lead to errors or delays in EP.
Inaccurate EP will cut the user off while speaking, returning incomplete
transcript while delays in EP will increase the perceived latency, degrading
the user experience. We propose methods to improve EP by addressing delayed
emission along with EP mistakes. To address the delayed emission problem, we
introduce an end-of-word token at the end of each word, along with a delay
penalty. The EP delay is addressed by obtaining a reliable frame-level speech
activity detection using an auxiliary network. We apply the proposed methods on
Switchboard conversational speech corpus and evaluate it against a delay
penalty method.",2025-05-19,"Anandh C, Karthik Pandia Durai, Jeena Prakash, Manickavela Arumugam, Kadri Hacioglu, S. Pavankumar Dubagunta, Andreas Stolcke, Shankar Venkatesan, Aravind Ganapathiraju",http://arxiv.org/pdf/2505.17070v1,cs.CL
Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis,"Graphical user interface (GUI) grounding, the ability to map natural language
instructions to specific actions on graphical user interfaces, remains a
critical bottleneck in computer use agent development. Current benchmarks
oversimplify grounding tasks as short referring expressions, failing to capture
the complexity of real-world interactions that require software commonsense,
layout understanding, and fine-grained manipulation capabilities. To address
these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising
564 finely annotated samples across diverse task types including text matching,
element recognition, layout understanding, and precise manipulation.
Additionally, we synthesize and release the largest computer use grounding
dataset Jedi, which contains 4 million examples through multi-perspective
decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its
effectiveness by outperforming existing approaches on ScreenSpot-v2,
ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved
grounding with Jedi directly enhances agentic capabilities of general
foundation models on complex computer tasks, improving from 5% to 27% on
OSWorld. Through detailed ablation studies, we identify key factors
contributing to grounding performance and verify that combining specialized
data for different interface elements enables compositional generalization to
novel interfaces. All benchmark, data, checkpoints, and code are open-sourced
and available at https://osworld-grounding.github.io.",2025-05-19,"Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, Caiming Xiong",http://arxiv.org/pdf/2505.13227v1,cs.CL
SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science,"Seed science is essential for modern agriculture, directly influencing crop
yields and global food security. However, challenges such as interdisciplinary
complexity and high costs with limited returns hinder progress, leading to a
shortage of experts and insufficient technological support. While large
language models (LLMs) have shown promise across various fields, their
application in seed science remains limited due to the scarcity of digital
resources, complex gene-trait relationships, and the lack of standardized
benchmarks. To address this gap, we introduce SeedBench -- the first multi-task
benchmark specifically designed for seed science. Developed in collaboration
with domain experts, SeedBench focuses on seed breeding and simulates key
aspects of modern breeding processes. We conduct a comprehensive evaluation of
26 leading LLMs, encompassing proprietary, open-source, and domain-specific
fine-tuned models. Our findings not only highlight the substantial gaps between
the power of LLMs and the real-world seed science problems, but also make a
foundational step for research on LLMs for seed design.",2025-05-19,"Jie Ying, Zihong Chen, Zhefan Wang, Wanli Jiang, Chenyang Wang, Zhonghang Yuan, Haoyang Su, Huanjun Kong, Fan Yang, Nanqing Dong",http://arxiv.org/pdf/2505.13220v1,cs.CL
Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry,"Classical Chinese poetry is a vital and enduring part of Chinese literature,
conveying profound emotional resonance. Existing studies analyze sentiment
based on textual meanings, overlooking the unique rhythmic and visual features
inherent in poetry,especially since it is often recited and accompanied by
Chinese paintings. In this work, we propose a dialect-enhanced multimodal
framework for classical Chinese poetry sentiment analysis. We extract
sentence-level audio features from the poetry and incorporate audio from
multiple dialects,which may retain regional ancient Chinese phonetic features,
enriching the phonetic representation. Additionally, we generate sentence-level
visual features, and the multimodal features are fused with textual features
enhanced by LLM translation through multimodal contrastive representation
learning. Our framework outperforms state-of-the-art methods on two public
datasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro
F1. We open-source the code to facilitate research in this area and provide
insights for general multimodal Chinese representation.",2025-05-19,"Xiaocong Du, Haoyu Pei, Haipeng Zhang",http://arxiv.org/pdf/2505.13210v1,cs.CL
Efficient Generation of Parameterised Quantum Circuits from Large Texts,"Quantum approaches to natural language processing (NLP) are redefining how
linguistic information is represented and processed. While traditional hybrid
quantum-classical models rely heavily on classical neural networks, recent
advancements propose a novel framework, DisCoCirc, capable of directly encoding
entire documents as parameterised quantum circuits (PQCs), besides enjoying
some additional interpretability and compositionality benefits. Following these
ideas, this paper introduces an efficient methodology for converting
large-scale texts into quantum circuits using tree-like representations of
pregroup diagrams. Exploiting the compositional parallels between language and
quantum mechanics, grounded in symmetric monoidal categories, our approach
enables faithful and efficient encoding of syntactic and discourse
relationships in long and complex texts (up to 6410 words in our experiments)
to quantum circuits. The developed system is provided to the community as part
of the augmented open-source quantum NLP package lambeq Gen II.",2025-05-19,"Colin Krawchuk, Nikhil Khatri, Neil John Ortega, Dimitri Kartsaklis",http://arxiv.org/pdf/2505.13208v1,cs.CL
Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification,"Recent works have revealed the great potential of speculative decoding in
accelerating the autoregressive generation process of large language models.
The success of these methods relies on the alignment between draft candidates
and the sampled outputs of the target model. Existing methods mainly achieve
draft-target alignment with training-based methods, e.g., EAGLE, Medusa,
involving considerable training costs. In this paper, we present a
training-free alignment-augmented speculative decoding algorithm. We propose
alignment sampling, which leverages output distribution obtained in the
prefilling phase to provide more aligned draft candidates. To further benefit
from high-quality but non-aligned draft candidates, we also introduce a simple
yet effective flexible verification strategy. Through an adaptive probability
threshold, our approach can improve generation accuracy while further improving
inference efficiency. Experiments on 8 datasets (including question answering,
summarization and code completion tasks) show that our approach increases the
average generation score by 3.3 points for the LLaMA3 model. Our method
achieves a mean acceptance length up to 2.39 and speed up generation by 2.23.",2025-05-19,"Jikai Wang, Zhenxu Tian, Juntao Li, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Min Zhang",http://arxiv.org/pdf/2505.13204v1,cs.CL
Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space,"We introduce SLED, an alternative approach to speech language modeling by
encoding speech waveforms into sequences of continuous latent representations
and modeling them autoregressively using an energy distance objective. The
energy distance offers an analytical measure of the distributional gap by
contrasting simulated and target samples, enabling efficient training to
capture the underlying continuous autoregressive distribution. By bypassing
reliance on residual vector quantization, SLED avoids discretization errors and
eliminates the need for the complicated hierarchical architectures common in
existing speech language models. It simplifies the overall modeling pipeline
while preserving the richness of speech information and maintaining inference
efficiency. Empirical results demonstrate that SLED achieves strong performance
in both zero-shot and streaming speech synthesis, showing its potential for
broader applications in general-purpose speech language models.",2025-05-19,"Zhengrui Ma, Yang Feng, Chenze Shao, Fandong Meng, Jie Zhou, Min Zhang",http://arxiv.org/pdf/2505.13181v1,cs.CL
ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models,"While integrating external tools into large language models (LLMs) enhances
their ability to access real-time information and domain-specific services,
existing approaches focus narrowly on functional tool selection following user
instructions, overlooking the context-aware personalization in tool selection.
This oversight leads to suboptimal user satisfaction and inefficient tool
utilization, particularly when overlapping toolsets require nuanced selection
based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a
benchmark designed to evaluate LLMs' capabilities in personalized tool
utilization. Specifically, we formalize two key dimensions of personalization,
user profile and environmental factors, and analyze their individual and
synergistic impacts on tool utilization. Through extensive experiments on
ToolSpectrum, we demonstrate that personalized tool utilization significantly
improves user experience across diverse scenarios. However, even
state-of-the-art LLMs exhibit the limited ability to reason jointly about user
profiles and environmental factors, often prioritizing one dimension at the
expense of the other. Our findings underscore the necessity of context-aware
personalization in tool-augmented LLMs and reveal critical limitations for
current models. Our data and code are available at
https://github.com/Chengziha0/ToolSpectrum.",2025-05-19,"Zihao Cheng, Hongru Wang, Zeming Liu, Yuhang Guo, Yuanfang Guo, Yunhong Wang, Haifeng Wang",http://arxiv.org/pdf/2505.13176v2,cs.CL
A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs,"Large Language Models (LLMs) have demonstrated remarkable generalization
capabilities across diverse tasks and languages. In this study, we focus on
natural language understanding in three classical languages -- Sanskrit,
Ancient Greek and Latin -- to investigate the factors affecting cross-lingual
zero-shot generalization. First, we explore named entity recognition and
machine translation into English. While LLMs perform equal to or better than
fine-tuned baselines on out-of-domain data, smaller models often struggle,
especially with niche or abstract entity types. In addition, we concentrate on
Sanskrit by presenting a factoid question-answering (QA) dataset and show that
incorporating context via retrieval-augmented generation approach significantly
boosts performance. In contrast, we observe pronounced performance drops for
smaller LLMs across these QA tasks. These results suggest model scale as an
important factor influencing cross-lingual generalization. Assuming that models
used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical
languages, our findings provide insights into how LLMs may generalize on these
languages and their consequent utility in classical studies.",2025-05-19,"V. S. D. S. Mahesh Akavarapu, Hrishikesh Terdalkar, Pramit Bhattacharyya, Shubhangi Agarwal, Vishakha Deulgaonkar, Pralay Manna, Chaitali Dangarikar, Arnab Bhattacharya",http://arxiv.org/pdf/2505.13173v1,cs.CL
Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks,"Large language models are known to memorize parts of their training data,
posing risk of copyright violations. To systematically examine this risk, we
pretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing
web-scale data with public domain books used to simulate copyrighted content at
controlled frequencies at lengths at least ten times longer than prior work. We
thereby identified the offset effect, a phenomenon characterized by two key
findings: (1) verbatim memorization is most strongly triggered by short
prefixes drawn from the beginning of the context window, with memorization
decreasing counterintuitively as prefix length increases; and (2) a sharp
decline in verbatim recall when prefix begins offset from the initial tokens of
the context window. We attribute this to positional fragility: models rely
disproportionately on the earliest tokens in their context window as retrieval
anchors, making them sensitive to even slight shifts. We further observe that
when the model fails to retrieve memorized content, it often produces
degenerated text. Leveraging these findings, we show that shifting sensitive
data deeper into the context window suppresses both extractable memorization
and degeneration. Our results suggest that positional offset is a critical and
previously overlooked axis for evaluating memorization risks, since prior work
implicitly assumed uniformity by probing only from the beginning of training
sequences.",2025-05-19,"Yixuan Xu, Antoine Bosselut, Imanol Schlag",http://arxiv.org/pdf/2505.13171v1,cs.CL
Role-Playing Evaluation for Large Language Models,"Large Language Models (LLMs) demonstrate a notable capacity for adopting
personas and engaging in role-playing. However, evaluating this ability
presents significant challenges, as human assessments are resource-intensive
and automated evaluations can be biased. To address this, we introduce
Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM
role-playing capabilities across four key dimensions: emotional understanding,
decision-making, moral alignment, and in-character consistency. This article
details the construction of RPEval and presents baseline evaluations. Our code
and dataset are available at https://github.com/yelboudouri/RPEval",2025-05-19,"Yassine El Boudouri, Walter Nuninger, Julian Alvarez, Yvan Peter",http://arxiv.org/pdf/2505.13157v1,cs.CL
Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice,"Natural medicines, particularly Traditional Chinese Medicine (TCM), are
gaining global recognition for their therapeutic potential in addressing human
symptoms and diseases. TCM, with its systematic theories and extensive
practical experience, provides abundant resources for healthcare. However, the
effective application of TCM requires precise syndrome diagnosis, determination
of treatment principles, and prescription formulation, which demand decades of
clinical expertise. Despite advancements in TCM-based decision systems, machine
learning, and deep learning research, limitations in data and single-objective
constraints hinder their practical application. In recent years, large language
models (LLMs) have demonstrated potential in complex tasks, but lack
specialization in TCM and face significant challenges, such as too big model
scale to deploy and issues with hallucination. To address these challenges, we
introduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and
specifically designed for TCM, pre-trained and fine-tuned on diverse TCM
corpora, including classical texts, expert treatises, clinical records, and
knowledge graphs. Tianyi is designed to assimilate interconnected and
systematic TCM knowledge through a progressive learning manner. Additionally,
we establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in
TCM examinations, clinical tasks, domain-specific question-answering, and
real-world trials. The extensive evaluations demonstrate the significant
potential of Tianyi as an AI assistant in TCM clinical practice and research,
bridging the gap between TCM knowledge and practical application.",2025-05-19,"Zhi Liu, Tao Yang, Jing Wang, Yexin Chen, Zhan Gao, Jiaxi Yang, Kui Chen, Bingji Lu, Xiaochen Li, Changyong Luo, Yan Li, Xiaohong Gu, Peng Cao",http://arxiv.org/pdf/2505.13156v1,cs.CL
What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text,"Can deception be detected solely from written text? Cues of deceptive
communication are inherently subtle, even more so in text-only communication.
Yet, prior studies have reported considerable success in automatic deception
detection. We hypothesize that such findings are largely driven by artifacts
introduced during data collection and do not generalize beyond specific
datasets. We revisit this assumption by introducing a belief-based deception
framework, which defines deception as a misalignment between an author's claims
and true beliefs, irrespective of factual accuracy, allowing deception cues to
be studied in isolation. Based on this framework, we construct three corpora,
collectively referred to as DeFaBel, including a German-language corpus of
deceptive and non-deceptive arguments and a multilingual version in German and
English, each collected under varying conditions to account for belief change
and enable cross-linguistic analysis. Using these corpora, we evaluate commonly
reported linguistic cues of deception. Across all three DeFaBel variants, these
cues show negligible, statistically insignificant correlations with deception
labels, contrary to prior work that treats such cues as reliable indicators. We
further benchmark against other English deception datasets following similar
data collection protocols. While some show statistically significant
correlations, effect sizes remain low and, critically, the set of predictive
cues is inconsistent across datasets. We also evaluate deception detection
using feature-based models, pretrained language models, and instruction-tuned
large language models. While some models perform well on established deception
datasets, they consistently perform near chance on DeFaBel. Our findings
challenge the assumption that deception can be reliably inferred from
linguistic cues and call for rethinking how deception is studied and modeled in
NLP.",2025-05-19,"Aswathy Velutharambath, Kai Sassenberg, Roman Klinger",http://arxiv.org/pdf/2505.13147v2,cs.CL
Understanding Cross-Lingual Inconsistency in Large Language Models,"Large language models (LLMs) are demonstrably capable of cross-lingual
transfer, but can produce inconsistent output when prompted with the same
queries written in different languages. To understand how language models are
able to generalize knowledge from one language to the others, we apply the
logit lens to interpret the implicit steps taken by LLMs to solve multilingual
multi-choice reasoning questions. We find LLMs predict inconsistently and are
less accurate because they rely on subspaces of individual languages, rather
than working in a shared semantic space. While larger models are more
multilingual, we show their hidden states are more likely to dissociate from
the shared representation compared to smaller models, but are nevertheless more
capable of retrieving knowledge embedded across different languages. Finally,
we demonstrate that knowledge sharing can be modulated by steering the models'
latent processing towards the shared semantic space. We find reinforcing
utilization of the shared space improves the models' multilingual reasoning
performance, as a result of more knowledge transfer from, and better output
consistency with English.",2025-05-19,"Zheng Wei Lim, Alham Fikri Aji, Trevor Cohn",http://arxiv.org/pdf/2505.13141v1,cs.CL
ModernGBERT: German-only 1B Encoder Model Trained from Scratch,"Despite the prominence of decoder-only language models, encoders remain
crucial for resource-constrained applications. We introduce ModernGBERT (134M,
1B), a fully transparent family of German encoder models trained from scratch,
incorporating architectural innovations from ModernBERT. To evaluate the
practical trade-offs of training encoders from scratch, we also present
LL\""aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German
decoder-only models via LLM2Vec. We benchmark all models on natural language
understanding, text embedding, and long-context reasoning tasks, enabling a
controlled comparison between dedicated encoders and converted decoders. Our
results show that ModernGBERT 1B outperforms prior state-of-the-art German
encoders as well as encoders adapted via LLM2Vec, with regard to performance
and parameter-efficiency. All models, training data, checkpoints and code are
publicly available, advancing the German NLP ecosystem with transparent,
high-performance encoder models.",2025-05-19,"Anton Ehrmanntraut, Julia Wunderle, Jan Pfister, Fotis Jannidis, Andreas Hotho",http://arxiv.org/pdf/2505.13136v1,cs.CL
Zero-Shot Iterative Formalization and Planning in Partially Observable Environments,"Using LLMs not to predict plans but to formalize an environment into the
Planning Domain Definition Language (PDDL) has been shown to improve
performance and control. Existing work focuses on fully observable
environments; we tackle the more realistic and challenging partially observable
environments that lack of complete, reliable information. We propose PDDLego+,
a framework to iteratively formalize, plan, grow, and refine PDDL
representations in a zero-shot manner, without needing access to any existing
trajectories. On two textual simulated environments, we show that PDDLego+
improves goal reaching success and exhibits robustness against problem
complexity. We also show that the domain knowledge captured after a successful
trial can benefit future tasks.",2025-05-19,"Liancheng Gong, Wang Zhu, Jesse Thomason, Li Zhang",http://arxiv.org/pdf/2505.13126v2,cs.CL
Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning,"The popular success of text-based large language models (LLM) has streamlined
the attention of the multimodal community to combine other modalities like
vision and audio along with text to achieve similar multimodal capabilities. In
this quest, large audio language models (LALMs) have to be evaluated on
reasoning related tasks which are different from traditional classification or
generation tasks. Towards this goal, we propose a novel dataset called temporal
reasoning evaluation of audio (TREA).
  We benchmark open-source LALMs and observe that they are consistently behind
human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we
also propose an uncertainty metric, which computes the invariance of the model
to semantically identical perturbations of the input. Our analysis shows that
the accuracy and uncertainty metrics are not necessarily correlated and thus,
points to a need for wholesome evaluation of LALMs for high-stakes
applications.",2025-05-19,"Debarpan Bhattacharya, Apoorva Kulkarni, Sriram Ganapathy",http://arxiv.org/pdf/2505.13115v1,cs.CL
FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference,"Large language models (LLMs) have been widely deployed with rapidly expanding
context windows to support increasingly demanding applications. However, long
contexts pose significant deployment challenges, primarily due to the KV cache
whose size grows proportionally with context length. While KV cache compression
methods are proposed to address this issue, KV dropping methods incur
considerable accuracy loss, and KV retrieval methods suffer from significant
efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization
framework to enhance KV retrieval efficiency while preserving accuracy. On the
algorithm side, FreeKV introduces speculative retrieval to shift the KV
selection and recall processes out of the critical path, combined with
fine-grained correction to ensure accuracy. On the system side, FreeKV employs
hybrid KV layouts across CPU and GPU memory to eliminate fragmented data
transfers, and leverages double-buffered streamed recall to further improve
efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy
across various scenarios and models, delivering up to 13$\times$ speedup
compared to SOTA KV retrieval methods.",2025-05-19,"Guangda Liu, Chengwei Li, Zhenyu Ning, Jing Lin, Yiwu Yao, Danning Ke, Minyi Guo, Jieru Zhao",http://arxiv.org/pdf/2505.13109v1,cs.CL
LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs,"Current Large Language Models (LLMs) can assist developing program code
beside many other things, but can they support working with Knowledge Graphs
(KGs) as well? Which LLM is offering the best capabilities in the field of
Semantic Web and Knowledge Graph Engineering (KGE)? Is this possible to
determine without checking many answers manually? The LLM-KG-Bench framework in
Version 3.0 is designed to answer these questions. It consists of an extensible
set of tasks for automated evaluation of LLM answers and covers different
aspects of working with semantic technologies. In this paper the LLM-KG-Bench
framework is presented in Version 3 along with a dataset of prompts, answers
and evaluations generated with it and several state-of-the-art LLMs.
Significant enhancements have been made to the framework since its initial
release, including an updated task API that offers greater flexibility in
handling evaluation tasks, revised tasks, and extended support for various open
models through the vllm library, among other improvements. A comprehensive
dataset has been generated using more than 30 contemporary open and proprietary
LLMs, enabling the creation of exemplary model cards that demonstrate the
models' capabilities in working with RDF and SPARQL, as well as comparing their
performance on Turtle and JSON-LD RDF serialization tasks.",2025-05-19,"Lars-Peter Meyer, Johannes Frey, Desiree Heim, Felix Brei, Claus Stadler, Kurt Junghanns, Michael Martin",http://arxiv.org/pdf/2505.13098v1,cs.CL
The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation,"Prior research diverges on language diversity in LLM fine-tuning: Some
studies report benefits while others find no advantages. Through controlled
fine-tuning experiments across 132 translation directions, we systematically
resolve these disparities. We find that expanding language diversity during
fine-tuning improves translation quality for both unsupervised and --
surprisingly -- supervised pairs, despite less diverse models being fine-tuned
exclusively on these supervised pairs. However, benefits plateau or decrease
beyond a certain diversity threshold. We show that increased language diversity
creates more language-agnostic representations. These representational
adaptations help explain the improved performance in models fine-tuned with
greater diversity.",2025-05-19,"David Stap, Christof Monz",http://arxiv.org/pdf/2505.13090v1,cs.CL
Systematic Generalization in Language Models Scales with Information Entropy,"Systematic generalization remains challenging for current language models,
which are known to be both sensitive to semantically similar permutations of
the input and to struggle with known concepts presented in novel contexts.
Although benchmarks exist for assessing compositional behavior, it is unclear
how to measure the difficulty of a systematic generalization problem. In this
work, we show how one aspect of systematic generalization can be described by
the entropy of the distribution of component parts in the training data. We
formalize a framework for measuring entropy in a sequence-to-sequence task and
find that the performance of popular model architectures scales with the
entropy. Our work connects systematic generalization to information efficiency,
and our results indicate that success at high entropy can be achieved even
without built-in priors, and that success at low entropy can serve as a target
for assessing progress towards robust systematic generalization.",2025-05-19,"Sondre Wold, Lucas Georges Gabriel Charpentier, Étienne Simon",http://arxiv.org/pdf/2505.13089v1,cs.CL
Advancing Sequential Numerical Prediction in Autoregressive Models,"Autoregressive models have become the de facto choice for sequence generation
tasks, but standard approaches treat digits as independent tokens and apply
cross-entropy loss, overlooking the coherent structure of numerical sequences.
This paper introduces Numerical Token Integrity Loss (NTIL) to address this
gap. NTIL operates at two levels: (1) token-level, where it extends the Earth
Mover's Distance (EMD) to preserve ordinal relationships between numerical
values, and (2) sequence-level, where it penalizes the overall discrepancy
between the predicted and actual sequences. This dual approach improves
numerical prediction and integrates effectively with LLMs/MLLMs. Extensive
experiments show significant performance improvements with NTIL.",2025-05-19,"Xiang Fei, Jinghui Lu, Qi Sun, Hao Feng, Yanjie Wang, Wei Shi, An-Lan Wang, Jingqun Tang, Can Huang",http://arxiv.org/pdf/2505.13077v1,cs.CL
Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset,"The 1st SpeechWellness Challenge conveys the need for speech-based suicide
risk assessment in adolescents. This study investigates a multimodal approach
for this challenge, integrating automatic transcription with WhisperX,
linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.
Additionally, handcrafted acoustic features -- including MFCCs, spectral
contrast, and pitch-related statistics -- were incorporated. We explored three
fusion strategies: early concatenation, modality-specific processing, and
weighted attention with mixup regularization. Results show that weighted
attention provided the best generalization, achieving 69% accuracy on the
development set, though a performance gap between development and test sets
highlights generalization challenges. Our findings, strictly tied to the
MINI-KID framework, emphasize the importance of refining embedding
representations and fusion mechanisms to enhance classification reliability.",2025-05-19,"Ambre Marie, Ilias Maoudj, Guillaume Dardenne, Gwenolé Quellec",http://arxiv.org/pdf/2505.13069v1,cs.CL
SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation,"Adapting to the addressee is crucial for successful explanations, yet poses
significant challenges for dialogsystems. We adopt the approach of treating
explanation generation as a non-stationary decision process, where the optimal
strategy varies according to changing beliefs about the explainee and the
interaction context. In this paper we address the questions of (1) how to track
the interaction context and the relevant listener features in a formally
defined computational partner model, and (2) how to utilize this model in the
dynamically adjusted, rational decision process that determines the currently
best explanation strategy. We propose a Bayesian inference-based approach to
continuously update the partner model based on user feedback, and a
non-stationary Markov Decision Process to adjust decision-making based on the
partner model values. We evaluate an implementation of this framework with five
simulated interlocutors, demonstrating its effectiveness in adapting to
different partners with constant and even changing feedback behavior. The
results show high adaptivity with distinct explanation strategies emerging for
different partners, highlighting the potential of our approach to improve
explainable AI systems and dialogsystems in general.",2025-05-19,"Amelie S. Robrecht, Christoph R. Kowalski, Stefan Kopp",http://arxiv.org/pdf/2505.13053v1,cs.CL
KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025,"The scope of the International Workshop on Spoken Language Translation
(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to
encompass a wider array of tasks, including Speech Question Answering and
Summarization. This shift is partly driven by the growing capabilities of
modern systems, particularly with the success of Large Language Models (LLMs).
In this paper, we present the Karlsruhe Institute of Technology's submissions
for the Offline ST and Instruction Following (IF) tracks, where we leverage
LLMs to enhance performance across all tasks. For the Offline ST track, we
propose a pipeline that employs multiple automatic speech recognition systems,
whose outputs are fused using an LLM with document-level context. This is
followed by a two-step translation process, incorporating additional refinement
step to improve translation quality. For the IF track, we develop an end-to-end
model that integrates a speech encoder with an LLM to perform a wide range of
instruction-following tasks. We complement it with a final document-level
refinement stage to further enhance output quality by using contextual
information.",2025-05-19,"Sai Koneru, Maike Züfle, Thai-Binh Nguyen, Seymanur Akti, Jan Niehues, Alexander Waibel",http://arxiv.org/pdf/2505.13036v1,cs.CL
"topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation","Topic models are statistical tools that allow their users to gain qualitative
and quantitative insights into the contents of textual corpora without the need
for close reading. They can be applied in a wide range of settings from
discourse analysis, through pretraining data curation, to text filtering. Topic
models are typically parameter-rich, complex models, and interpreting these
parameters can be challenging for their users. It is typical practice for users
to interpret topics based on the top 10 highest ranking terms on a given topic.
This list-of-words approach, however, gives users a limited and biased picture
of the content of topics. Thoughtful user interface design and visualizations
can help users gain a more complete and accurate understanding of topic models'
output. While some visualization utilities do exist for topic models, these are
typically limited to a certain type of topic model. We introduce topicwizard, a
framework for model-agnostic topic model interpretation, that provides
intuitive and interactive tools that help users examine the complex semantic
relations between documents, words and topics learned by topic models.",2025-05-19,"Márton Kardos, Kenneth C. Enevoldsen, Kristoffer Laigaard Nielbo",http://arxiv.org/pdf/2505.13034v1,cs.CL
"MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix","We introduce MMAR, a new benchmark designed to evaluate the deep reasoning
capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary
tasks. MMAR comprises 1,000 meticulously curated audio-question-answer
triplets, collected from real-world internet videos and refined through
iterative error corrections and quality checks to ensure high quality. Unlike
existing benchmarks that are limited to specific domains of sound, music, or
speech, MMAR extends them to a broad spectrum of real-world audio scenarios,
including mixed-modality combinations of sound, music, and speech. Each
question in MMAR is hierarchically categorized across four reasoning layers:
Signal, Perception, Semantic, and Cultural, with additional sub-categories
within each layer to reflect task diversity and complexity. To further foster
research in this area, we annotate every question with a Chain-of-Thought (CoT)
rationale to promote future advancements in audio reasoning. Each item in the
benchmark demands multi-step deep reasoning beyond surface-level understanding.
Moreover, a part of the questions requires graduate-level perceptual and
domain-specific knowledge, elevating the benchmark's difficulty and depth. We
evaluate MMAR using a broad set of models, including Large Audio-Language
Models (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models
(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with
audio caption inputs. The performance of these models on MMAR highlights the
benchmark's challenging nature, and our analysis further reveals critical
limitations of understanding and reasoning capabilities among current models.
We hope MMAR will serve as a catalyst for future advances in this important but
little-explored area.",2025-05-19,"Ziyang Ma, Yinghao Ma, Yanqiao Zhu, Chen Yang, Yi-Wen Chao, Ruiyang Xu, Wenxi Chen, Yuanzhe Chen, Zhuo Chen, Jian Cong, Kai Li, Keliang Li, Siyou Li, Xinfeng Li, Xiquan Li, Zheng Lian, Yuzhe Liang, Minghao Liu, Zhikang Niu, Tianrui Wang, Yuping Wang, Yuxuan Wang, Yihao Wu, Guanrou Yang, Jianwei Yu, Ruibin Yuan, Zhisheng Zheng, Ziya Zhou, Haina Zhu, Wei Xue, Emmanouil Benetos, Kai Yu, Eng-Siong Chng, Xie Chen",http://arxiv.org/pdf/2505.13032v1,cs.CL
Evaluating the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset,"Large Language Models (LLMs) are increasingly integrated into critical
systems in industries like healthcare and finance. Users can often submit
queries to LLM-enabled chatbots, some of which can enrich responses with
information retrieved from internal databases storing sensitive data. This
gives rise to a range of attacks in which a user submits a malicious query and
the LLM-system outputs a response that creates harm to the owner, such as
leaking internal data or creating legal liability by harming a third-party.
While security tools are being developed to counter these threats, there is
little formal evaluation of their effectiveness and usability. This study
addresses this gap by conducting a thorough comparative analysis of LLM
security tools. We identified 13 solutions (9 closed-source, 4 open-source),
but only 7 were evaluated due to a lack of participation by proprietary model
owners.To evaluate, we built a benchmark dataset of malicious prompts, and
evaluate these tools performance against a baseline LLM model
(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many
false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard
emerged as the best overall tools showcasing the tradeoff between usability and
performance. The study concluded with recommendations for greater transparency
among closed source providers, improved context-aware detections, enhanced
open-source engagement, increased user awareness, and the adoption of more
representative performance metrics.",2025-05-19,"Sayon Palit, Daniel Woods",http://arxiv.org/pdf/2505.13028v2,cs.CL
To Bias or Not to Bias: Detecting bias in News with bias-detector,"Media bias detection is a critical task in ensuring fair and balanced
information dissemination, yet it remains challenging due to the subjectivity
of bias and the scarcity of high-quality annotated data. In this work, we
perform sentence-level bias classification by fine-tuning a RoBERTa-based model
on the expert-annotated BABE dataset. Using McNemar's test and the 5x2
cross-validation paired t-test, we show statistically significant improvements
in performance when comparing our model to a domain-adaptively pre-trained
DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model
avoids common pitfalls like oversensitivity to politically charged terms and
instead attends more meaningfully to contextually relevant tokens. For a
comprehensive examination of media bias, we present a pipeline that combines
our model with an already-existing bias-type classifier. Our method exhibits
good generalization and interpretability, despite being constrained by
sentence-level analysis and dataset size because of a lack of larger and more
advanced bias corpora. We talk about context-aware modeling, bias
neutralization, and advanced bias type classification as potential future
directions. Our findings contribute to building more robust, explainable, and
socially responsible NLP systems for media bias detection.",2025-05-19,"Himel Ghosh, Ahmed Mosharafa, Georg Groh",http://arxiv.org/pdf/2505.13010v1,cs.CL
Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning,"In health-related topics, user toxicity in online discussions frequently
becomes a source of social conflict or promotion of dangerous, unscientific
behaviour; common approaches for battling it include different forms of
detection, flagging and/or removal of existing toxic comments, which is often
counterproductive for platforms and users alike. In this work, we propose the
alternative of combatting user toxicity predictively, anticipating where a user
could interact toxically in health-related online discussions. Applying a
Collaborative Filtering-based Machine Learning methodology, we predict the
toxicity in COVID-related conversations between any user and subcommunity of
Reddit, surpassing 80% predictive performance in relevant metrics, and allowing
us to prevent the pairing of conflicting users and subcommunities.",2025-05-19,"Jorge Paz-Ruza, Amparo Alonso-Betanzos, Bertha Guijarro-Berdiñas, Carlos Eiras-Franco",http://arxiv.org/pdf/2505.17068v1,cs.CL
Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain,"Airports from the top 20 in terms of annual passengers are highly dynamic
environments with thousands of flights daily, and they aim to increase the
degree of automation. To contribute to this, we implemented a Conversational AI
system that enables staff in an airport to communicate with flight information
systems. This system not only answers standard airport queries but also
resolves airport terminology, jargon, abbreviations, and dynamic questions
involving reasoning. In this paper, we built three different
Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL
RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that
traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally
produced hallucinations, which is risky to airport safety. In contrast, SQL RAG
and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with
significantly fewer hallucinations. Moreover, Graph RAG was especially
effective for questions that involved reasoning. Based on our observations, we
thus recommend SQL RAG and Graph RAG are better for airport environments, due
to fewer hallucinations and the ability to handle dynamic questions.",2025-05-19,"Yuyang Li, Philip J. M. Kerbusch, Raimon H. R. Pruim, Tobias Käfer",http://arxiv.org/pdf/2505.13006v1,cs.CL
EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code,"Existing code generation benchmarks primarily evaluate functional
correctness, with limited focus on code efficiency and often restricted to a
single language like Python. To address this gap, we introduce EffiBench-X, the
first multi-language benchmark designed to measure the efficiency of
LLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,
and Golang. It comprises competitive programming tasks with human-expert
solutions as efficiency baselines. Evaluating state-of-the-art LLMs on
EffiBench-X reveals that while models generate functionally correct code, they
consistently underperform human experts in efficiency. Even the most efficient
LLM-generated solutions (Qwen3-32B) achieve only around \textbf{62\%} of human
efficiency on average, with significant language-specific variations. LLMs show
better efficiency in Python, Ruby, and JavaScript than in Java, C++, and
Golang. For instance, DeepSeek-R1's Python code is significantly more efficient
than its Java code. These results highlight the critical need for research into
LLM optimization techniques to improve code efficiency across diverse
languages. The dataset and evaluation infrastructure are submitted and
available at https://github.com/EffiBench/EffiBench-X.git and
https://huggingface.co/datasets/EffiBench/effibench-x.",2025-05-19,"Yuhao Qing, Boyu Zhu, Mingzhe Du, Zhijiang Guo, Terry Yue Zhuo, Qianru Zhang, Jie M. Zhang, Heming Cui, Siu-Ming Yiu, Dong Huang, See-Kiong Ng, Luu Anh Tuan",http://arxiv.org/pdf/2505.13004v1,cs.CL
ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning,"In recent years, the emergence of large reasoning models (LRMs), such as
OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex
problems, e.g., mathematics and coding. Some pioneering studies attempt to
bring the success of LRMs in neural machine translation (MT). They try to build
LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite
some progress that has been made, these attempts generally focus on several
high-resource languages, e.g., English and Chinese, leaving the performance on
other languages unclear. Besides, the reward modeling methods in previous work
do not fully unleash the potential of reinforcement learning in MT. In this
work, we first design a new reward modeling method that compares the
translation results of the policy MT model with a strong LRM (i.e.,
DeepSeek-R1-671B), and quantifies the comparisons to provide rewards.
Experimental results demonstrate the superiority of the reward modeling method.
Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new
state-of-the-art performance in literary translation, and outperforms strong
LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to
the multilingual settings with 11 languages. With a carefully designed
lightweight reward modeling in RL, we can simply transfer the strong MT ability
from a single direction into multiple (i.e., 90) translation directions and
achieve impressive multilingual MT performance.",2025-05-19,"Jiaan Wang, Fandong Meng, Jie Zhou",http://arxiv.org/pdf/2505.12996v1,cs.CL
Fractured Chain-of-Thought Reasoning,"Inference-time scaling techniques have significantly bolstered the reasoning
capabilities of large language models (LLMs) by harnessing additional
computational effort at inference without retraining. Similarly,
Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy
by generating rich intermediate reasoning trajectories, but these approaches
incur substantial token costs that impede their deployment in latency-sensitive
settings. In this work, we first show that truncated CoT, which stops reasoning
before completion and directly generates the final answer, often matches full
CoT sampling while using dramatically fewer tokens. Building on this insight,
we introduce Fractured Sampling, a unified inference-time strategy that
interpolates between full CoT and solution-only sampling along three orthogonal
axes: (1) the number of reasoning trajectories, (2) the number of final
solutions per trajectory, and (3) the depth at which reasoning traces are
truncated. Through extensive experiments on five diverse reasoning benchmarks
and several model scales, we demonstrate that Fractured Sampling consistently
achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling
gains in Pass@k versus token budget. Our analysis reveals how to allocate
computation across these dimensions to maximize performance, paving the way for
more efficient and scalable LLM reasoning. Code is available at
https://github.com/BaohaoLiao/frac-cot.",2025-05-19,"Baohao Liao, Hanze Dong, Yuhui Xu, Doyen Sahoo, Christof Monz, Junnan Li, Caiming Xiong",http://arxiv.org/pdf/2505.12992v2,cs.CL
An Empirical Study of Many-to-Many Summarization with Large Language Models,"Many-to-many summarization (M2MS) aims to process documents in any language
and generate the corresponding summaries also in any language. Recently, large
language models (LLMs) have shown strong multi-lingual abilities, giving them
the potential to perform M2MS in real applications. This work presents a
systematic empirical study on LLMs' M2MS ability. Specifically, we first
reorganize M2MS data based on eight previous domain-specific datasets. The
reorganized data contains 47.8K samples spanning five domains and six
languages, which could be used to train and evaluate LLMs. Then, we benchmark
18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned
traditional models (e.g., mBART) are also conducted for comparisons. Our
experiments reveal that, zero-shot LLMs achieve competitive results with
fine-tuned traditional models. After instruct-tuning, open-source LLMs can
significantly improve their M2MS ability, and outperform zero-shot LLMs
(including GPT-4) in terms of automatic evaluations. In addition, we
demonstrate that this task-specific improvement does not sacrifice the LLMs'
general task-solving abilities. However, as revealed by our human evaluation,
LLMs still face the factuality issue, and the instruction tuning might
intensify the issue. Thus, how to control factual errors becomes the key when
building LLM summarizers in real applications, and is worth noting in future
research.",2025-05-19,"Jiaan Wang, Fandong Meng, Zengkui Sun, Yunlong Liang, Yuxuan Cao, Jiarong Xu, Haoxiang Shi, Jie Zhou",http://arxiv.org/pdf/2505.12983v1,cs.CL
"Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models","Homograph disambiguation remains a significant challenge in
grapheme-to-phoneme (G2P) conversion, especially for low-resource languages.
This challenge is twofold: (1) creating balanced and comprehensive homograph
datasets is labor-intensive and costly, and (2) specific disambiguation
strategies introduce additional latency, making them unsuitable for real-time
applications such as screen readers and other accessibility tools. In this
paper, we address both issues. First, we propose a semi-automated pipeline for
constructing homograph-focused datasets, introduce the HomoRich dataset
generated through this pipeline, and demonstrate its effectiveness by applying
it to enhance a state-of-the-art deep learning-based G2P system for Persian.
Second, we advocate for a paradigm shift - utilizing rich offline datasets to
inform the development of fast, rule-based methods suitable for
latency-sensitive accessibility applications like screen readers. To this end,
we improve one of the most well-known rule-based G2P systems, eSpeak, into a
fast homograph-aware version, HomoFast eSpeak. Our results show an approximate
30% improvement in homograph disambiguation accuracy for the deep
learning-based and eSpeak systems.",2025-05-19,"Mahta Fetrat Qharabagh, Zahra Dehghanian, Hamid R. Rabiee",http://arxiv.org/pdf/2505.12973v1,cs.CL
A Structured Literature Review on Traditional Approaches in Current Natural Language Processing,"The continued rise of neural networks and large language models in the more
recent past has altered the natural language processing landscape, enabling new
approaches towards typical language tasks and achieving mainstream success.
Despite the huge success of large language models, many disadvantages still
remain and through this work we assess the state of the art in five application
scenarios with a particular focus on the future perspectives and sensible
application scenarios of traditional and older approaches and techniques.
  In this paper we survey recent publications in the application scenarios
classification, information and relation extraction, text simplification as
well as text summarization. After defining our terminology, i.e., which
features are characteristic for traditional techniques in our interpretation
for the five scenarios, we survey if such traditional approaches are still
being used, and if so, in what way they are used. It turns out that all five
application scenarios still exhibit traditional models in one way or another,
as part of a processing pipeline, as a comparison/baseline to the core model of
the respective paper, or as the main model(s) of the paper. For the complete
statistics, see https://zenodo.org/records/13683801",2025-05-19,"Robin Jegan, Andreas Henrich",http://arxiv.org/pdf/2505.12970v1,cs.CL
Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down,"OpenAI's Whisper has achieved significant success in Automatic Speech
Recognition. However, it has consistently been found to exhibit hallucination
issues, particularly in non-speech segments, which limits its broader
application in complex industrial settings.
  In this paper, we introduce a novel method to reduce Whisper's hallucination
on non-speech segments without using any pre- or post-possessing techniques.
Specifically, we benchmark the contribution of each self-attentional head in
the Whisper-large-v3 decoder to the hallucination problem by performing a
head-wise mask. Our findings reveal that only 3 of the 20 heads account for
over 75% of the hallucinations on the UrbanSound dataset. We then fine-tune
these three crazy heads using a collection of non-speech data. The results show
that our best fine-tuned model, namely Calm-Whisper, achieves over 80%
reduction in non-speech hallucination with only less than 0.1% WER degradation
on LibriSpeech test-clean and test-other.",2025-05-19,"Yingzhi Wang, Anas Alhmoud, Saad Alsahly, Muhammad Alqurishi, Mirco Ravanelli",http://arxiv.org/pdf/2505.12969v1,cs.CL
MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition,"Recognizing biomedical concepts in the text is vital for ontology refinement,
knowledge graph construction, and concept relationship discovery. However,
traditional concept recognition methods, relying on explicit mention
identification, often fail to capture complex concepts not explicitly stated in
the text. To overcome this limitation, we introduce MA-COIR, a framework that
reformulates concept recognition as an indexing-recognition task. By assigning
semantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in
ontology entries and enhances recognition efficiency. Using a pretrained
BART-based model fine-tuned on small datasets, our approach reduces
computational requirements to facilitate adoption by domain experts.
Furthermore, we incorporate large language models (LLMs)-generated queries and
synthetic data to improve recognition in low-resource settings. Experimental
results on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of
MA-COIR in recognizing both explicit and implicit concepts without the need for
mention-level annotations during inference, advancing ontology-driven concept
recognition in biomedical domain applications. Our code and constructed data
are available at https://github.com/sl-633/macoir-master.",2025-05-19,"Shanshan Liu, Noriki Nishida, Rumana Ferdous Munne, Narumi Tokunaga, Yuki Yamagata, Kouji Kozaki, Yuji Matsumoto",http://arxiv.org/pdf/2505.12964v1,cs.CL
GuRE:Generative Query REwriter for Legal Passage Retrieval,"Legal Passage Retrieval (LPR) systems are crucial as they help practitioners
save time when drafting legal arguments. However, it remains an underexplored
avenue. One primary reason is the significant vocabulary mismatch between the
query and the target passage. To address this, we propose a simple yet
effective method, the Generative query REwriter (GuRE). We leverage the
generative capabilities of Large Language Models (LLMs) by training the LLM for
query rewriting. ""Rewritten queries"" help retrievers to retrieve target
passages by mitigating vocabulary mismatch. Experimental results show that GuRE
significantly improves performance in a retriever-agnostic manner,
outperforming all baseline methods. Further analysis reveals that different
training objectives lead to distinct retrieval behaviors, making GuRE more
suitable than direct retriever fine-tuning for real-world applications. Codes
are avaiable at github.com/daehuikim/GuRE.",2025-05-19,"Daehee Kim, Deokhyung Kang, Jonghwi Kim, Sangwon Ryu, Gary Geunbae Lee",http://arxiv.org/pdf/2505.12950v1,cs.CL
Neural Morphological Tagging for Nguni Languages,"Morphological parsing is the task of decomposing words into morphemes, the
smallest units of meaning in a language, and labelling their grammatical roles.
It is a particularly challenging task for agglutinative languages, such as the
Nguni languages of South Africa, which construct words by concatenating
multiple morphemes. A morphological parsing system can be framed as a pipeline
with two separate components, a segmenter followed by a tagger. This paper
investigates the use of neural methods to build morphological taggers for the
four Nguni languages. We compare two classes of approaches: training neural
sequence labellers (LSTMs and neural CRFs) from scratch and finetuning
pretrained language models. We compare performance across these two categories,
as well as to a traditional rule-based morphological parser. Neural taggers
comfortably outperform the rule-based baseline and models trained from scratch
tend to outperform pretrained models. We also compare parsing results across
different upstream segmenters and with varying linguistic input features. Our
findings confirm the viability of employing neural taggers based on
pre-existing morphological segmenters for the Nguni languages.",2025-05-19,"Cael Marquard, Simbarashe Mawere, Francois Meyer",http://arxiv.org/pdf/2505.12949v1,cs.CL
A3 : an Analytical Low-Rank Approximation Framework for Attention,"Large language models have demonstrated remarkable performance; however,
their massive parameter counts make deployment highly expensive. Low-rank
approximation offers a promising compression solution, yet existing approaches
have two main limitations: (1) They focus on minimizing the output error of
individual linear layers, without considering the architectural characteristics
of Transformers, and (2) they decompose a large weight matrix into two small
low-rank matrices. Consequently, these methods often fall short compared to
other compression techniques like pruning and quantization, and introduce
runtime overhead such as the extra GEMM kernel launches for decomposed small
matrices. To address these limitations, we propose $\tt A^\tt 3$, a
post-training low-rank approximation framework. $\tt A^\tt 3$ splits a
Transformer layer into three functional components, namely $\tt QK$, $\tt OV$,
and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical
solution that reduces the hidden dimension size inside each component while
minimizing the component's functional loss ($\it i.e.$, error in attention
scores, attention outputs, and MLP outputs). This approach directly reduces
model sizes, KV cache sizes, and FLOPs without introducing any runtime
overheads. In addition, it provides a new narrative in advancing the
optimization problem from singular linear layer loss optimization toward
improved end-to-end performance. Through extensive experiments, we show that
$\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example,
under the same reduction budget in computation and memory, our low-rank
approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,
outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the
versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and
mixed-rank assignments for enhanced performance.",2025-05-19,"Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao",http://arxiv.org/pdf/2505.12942v1,cs.CL
Leveraging LLM Inconsistency to Boost Pass@k Performance,"Large language models (LLMs) achieve impressive abilities in numerous
domains, but exhibit inconsistent performance in response to minor input
changes. Rather than view this as a drawback, in this paper we introduce a
novel method for leveraging models' inconsistency to boost Pass@k performance.
Specifically, we present a ""Variator"" agent that generates k variants of a
given task and submits one candidate solution for each one. Our variant
generation approach is applicable to a wide range of domains as it is task
agnostic and compatible with free-form inputs. We demonstrate the efficacy of
our agent theoretically using a probabilistic model of the inconsistency
effect, and show empirically that it outperforms the baseline on the APPS
dataset. Furthermore, we establish that inconsistency persists even in frontier
reasoning models across coding and cybersecurity domains, suggesting our method
is likely to remain relevant for future model generations.",2025-05-19,"Uri Dalal, Meirav Segal, Zvika Ben-Haim, Dan Lahav, Omer Nevo",http://arxiv.org/pdf/2505.12938v2,cs.CL
Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs,"Reinforcement learning (RL) has become a cornerstone for enhancing the
reasoning capabilities of large language models (LLMs), with recent innovations
such as Group Relative Policy Optimization (GRPO) demonstrating exceptional
effectiveness. In this study, we identify a critical yet underexplored issue in
RL training: low-probability tokens disproportionately influence model updates
due to their large gradient magnitudes. This dominance hinders the effective
learning of high-probability tokens, whose gradients are essential for LLMs'
performance but are substantially suppressed. To mitigate this interference, we
propose two novel methods: Advantage Reweighting and Low-Probability Token
Isolation (Lopti), both of which effectively attenuate gradients from
low-probability tokens while emphasizing parameter updates driven by
high-probability tokens. Our approaches promote balanced updates across tokens
with varying probabilities, thereby enhancing the efficiency of RL training.
Experimental results demonstrate that they substantially improve the
performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K
Logic Puzzle reasoning tasks. Our implementation is available at
https://github.com/zhyang2226/AR-Lopti.",2025-05-19,"Zhihe Yang, Xufang Luo, Zilong Wang, Dongqi Han, Zhiyuan He, Dongsheng Li, Yunjian Xu",http://arxiv.org/pdf/2505.12929v1,cs.CL
PyFCG: Fluid Construction Grammar in Python,"We present PyFCG, an open source software library that ports Fluid
Construction Grammar (FCG) to the Python programming language. PyFCG enables
its users to seamlessly integrate FCG functionality into Python programs, and
to use FCG in combination with other libraries within Python's rich ecosystem.
Apart from a general description of the library, this paper provides three
walkthrough tutorials that demonstrate example usage of PyFCG in typical use
cases of FCG: (i) formalising and testing construction grammar analyses, (ii)
learning usage-based construction grammars from corpora, and (iii) implementing
agent-based experiments on emergent communication.",2025-05-19,"Paul Van Eecke, Katrien Beuls",http://arxiv.org/pdf/2505.12920v1,cs.CL
AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models,"Geospatial code generation is emerging as a key direction in the integration
of artificial intelligence and geoscientific analysis. However, there remains a
lack of standardized tools for automatic evaluation in this domain. To address
this gap, we propose AutoGEEval, the first multimodal, unit-level automated
evaluation framework for geospatial code generation tasks on the Google Earth
Engine (GEE) platform powered by large language models (LLMs). Built upon the
GEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)
comprising 1325 test cases that span 26 GEE data types. The framework
integrates both question generation and answer verification components to
enable an end-to-end automated evaluation pipeline-from function invocation to
execution validation. AutoGEEval supports multidimensional quantitative
analysis of model outputs in terms of accuracy, resource consumption, execution
efficiency, and error types. We evaluate 18 state-of-the-art LLMs-including
general-purpose, reasoning-augmented, code-centric, and geoscience-specialized
models-revealing their performance characteristics and potential optimization
pathways in GEE code generation. This work provides a unified protocol and
foundational resource for the development and assessment of geospatial code
generation models, advancing the frontier of automated natural language to
domain-specific code translation.",2025-05-19,"Shuyang Hou, Zhangxiao Shen, Huayi Wu, Jianyuan Liang, Haoyue Jiao, Yaxian Qing, Xiaopu Zhang, Xu Li, Zhipeng Gui, Xuefeng Guan, Longgang Xiang",http://arxiv.org/pdf/2505.12900v1,cs.CL
On the Thinking-Language Modeling Gap in Large Language Models,"System 2 reasoning is one of the defining characteristics of intelligence,
which requires slow and logical thinking. Human conducts System 2 reasoning via
the language of thoughts that organizes the reasoning process as a causal
sequence of mental language, or thoughts. Recently, it has been observed that
System 2 reasoning can be elicited from Large Language Models (LLMs)
pre-trained on large-scale natural languages. However, in this work, we show
that there is a significant gap between the modeling of languages and thoughts.
As language is primarily a tool for humans to share knowledge and thinking,
modeling human language can easily absorb language biases into LLMs deviated
from the chain of thoughts in minds. Furthermore, we show that the biases will
mislead the eliciting of ""thoughts"" in LLMs to focus only on a biased part of
the premise. To this end, we propose a new prompt technique termed
Language-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of
directly eliciting the chain of thoughts from partial information, LoT
instructs LLMs to adjust the order and token used for the expressions of all
the relevant information. We show that the simple strategy significantly
reduces the language modeling biases in LLMs and improves the performance of
LLMs across a variety of reasoning tasks.",2025-05-19,"Chenxi Liu, Yongqiang Chen, Tongliang Liu, James Cheng, Bo Han, Kun Zhang",http://arxiv.org/pdf/2505.12896v1,cs.CL
TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios,"Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend
the real world. However, existing works neglect the real-world challenges for
temporal reasoning: (1) intensive temporal information, (2) fast-changing event
dynamics, and (3) complex temporal dependencies in social interactions. To
bridge this gap, we propose a multi-level benchmark TIME, designed for temporal
reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3
levels with 11 fine-grained sub-tasks. This benchmark encompasses 3
sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,
and TIME-Dial. We conduct extensive experiments on reasoning models and
non-reasoning models. And we conducted an in-depth analysis of temporal
reasoning performance across diverse real-world scenarios and tasks, and
summarized the impact of test-time scaling on temporal reasoning capabilities.
Additionally, we release TIME-Lite, a human-annotated subset to foster future
research and standardized evaluation in temporal reasoning. The code is
available at https://github.com/sylvain-wei/TIME , and the dataset is available
at https://huggingface.co/datasets/SylvainWei/TIME .",2025-05-19,"Shaohang Wei, Wei Li, Feifan Song, Wen Luo, Tianyi Zhuang, Haochen Tan, Zhijiang Guo, Houfeng Wang",http://arxiv.org/pdf/2505.12891v1,cs.CL
GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation,"Medication recommendations have become an important task in the healthcare
domain, especially in measuring the accuracy and safety of medical dialogue
systems (MDS). Different from the recommendation task based on electronic
health records (EHRs), dialogue-based medication recommendations require
research on the interaction details between patients and doctors, which is
crucial but may not exist in EHRs. Recent advancements in large language models
(LLM) have extended the medical dialogue domain. These LLMs can interpret
patients' intent and provide medical suggestions including medication
recommendations, but some challenges are still worth attention. During a
multi-turn dialogue, LLMs may ignore the fine-grained medical information or
connections across the dialogue turns, which is vital for providing accurate
suggestions. Besides, LLMs may generate non-factual responses when there is a
lack of domain-specific knowledge, which is more risky in the medical domain.
To address these challenges, we propose a \textbf{G}raph-\textbf{A}ssisted
\textbf{P}rompts (\textbf{GAP}) framework for dialogue-based medication
recommendation. It extracts medical concepts and corresponding states from
dialogue to construct an explicitly patient-centric graph, which can describe
the neglected but important information. Further, combined with external
medical knowledge graphs, GAP can generate abundant queries and prompts, thus
retrieving information from multiple sources to reduce the non-factual
responses. We evaluate GAP on a dialogue-based medication recommendation
dataset and further explore its potential in a more difficult scenario,
dynamically diagnostic interviewing. Extensive experiments demonstrate its
competitive performance when compared with strong baselines.",2025-05-19,"Jialun Zhong, Yanzeng Li, Sen Hu, Yang Zhang, Teng Xu, Lei Zou",http://arxiv.org/pdf/2505.12888v1,cs.CL
CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models,"Code-switching (CS) poses a significant challenge for Large Language Models
(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce
CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue
to English summarization. CS-Sum is the first benchmark for CS dialogue
summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and
Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language
pair. Evaluating ten LLMs, including open and closed-source models, we analyze
performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA
on synthetic data) approaches. Our findings show that though the scores on
automated metrics are high, LLMs make subtle mistakes that alter the complete
meaning of the dialogue. To this end, we introduce 3 most common type of errors
that LLMs make when handling CS input. Error rates vary across CS pairs and
LLMs, with some LLMs showing more frequent errors on certain language pairs,
underscoring the need for specialized training on code-switched data.",2025-05-19,"Sathya Krishnan Suresh, Tanmay Surana, Lim Zhi Hao, Eng Siong Chng",http://arxiv.org/pdf/2505.13559v1,cs.CL
Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective,"Large Reasoning Models (LRMs) have shown impressive capabilities in
multi-step reasoning tasks. However, alongside these successes, a more
deceptive form of model error has emerged--Reasoning Hallucination--where
logically coherent but factually incorrect reasoning traces lead to persuasive
yet faulty conclusions. Unlike traditional hallucinations, these errors are
embedded within structured reasoning, making them more difficult to detect and
potentially more harmful. In this work, we investigate reasoning hallucinations
from a mechanistic perspective. We propose the Reasoning Score, which
quantifies the depth of reasoning by measuring the divergence between logits
obtained from projecting late layers of LRMs to the vocabulary space,
effectively distinguishing shallow pattern-matching from genuine deep
reasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA
dataset and identify two key reasoning hallucination patterns: early-stage
fluctuation in reasoning depth and incorrect backtracking to flawed prior
steps. These insights motivate our Reasoning Hallucination Detection (RHD)
framework, which achieves state-of-the-art performance across multiple domains.
To mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced
reinforcement learning algorithm that incorporates step-level deep reasoning
rewards via potential-based shaping. Our theoretical analysis establishes
stronger generalization guarantees, and experiments demonstrate improved
reasoning quality and reduced hallucination rates.",2025-05-19,"Zhongxiang Sun, Qipeng Wang, Haoyu Wang, Xiao Zhang, Jun Xu",http://arxiv.org/pdf/2505.12886v1,cs.CL
Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?,"Low rank adaptation (LoRA) has emerged as a prominent technique for
fine-tuning large language models (LLMs) thanks to its superb efficiency gains
over previous methods. While extensive studies have examined the performance
and structural properties of LoRA, its behavior upon training-time attacks
remain underexplored, posing significant security risks. In this paper, we
theoretically investigate the security implications of LoRA's low-rank
structure during fine-tuning, in the context of its robustness against data
poisoning and backdoor attacks. We propose an analytical framework that models
LoRA's training dynamics, employs the neural tangent kernel to simplify the
analysis of the training process, and applies information theory to establish
connections between LoRA's low rank structure and its vulnerability against
training-time attacks. Our analysis indicates that LoRA exhibits better
robustness to backdoor attacks than full fine-tuning, while becomes more
vulnerable to untargeted data poisoning due to its over-simplified information
geometry. Extensive experimental evaluations have corroborated our theoretical
findings.",2025-05-19,"Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Ronghua Li",http://arxiv.org/pdf/2505.12871v1,cs.CL
LEXam: Benchmarking Legal Reasoning on 340 Law Exams,"Long-form legal reasoning remains a key challenge for large language models
(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a
novel benchmark derived from 340 law exams spanning 116 law school courses
across a range of subjects and degree levels. The dataset comprises 4,886 law
exam questions in English and German, including 2,841 long-form, open-ended
questions and 2,045 multiple-choice questions. Besides reference answers, the
open questions are also accompanied by explicit guidance outlining the expected
legal reasoning approach such as issue spotting, rule recall, or rule
application. Our evaluation on both open-ended and multiple-choice questions
present significant challenges for current LLMs; in particular, they notably
struggle with open questions that require structured, multi-step legal
reasoning. Moreover, our results underscore the effectiveness of the dataset in
differentiating between models with varying capabilities. Adopting an
LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate
how model-generated reasoning steps can be evaluated consistently and
accurately. Our evaluation setup provides a scalable method to assess legal
reasoning quality beyond simple accuracy metrics. Project page:
https://lexam-benchmark.github.io/",2025-05-19,"Yu Fan, Jingwei Ni, Jakob Merane, Etienne Salimbeni, Yang Tian, Yoan Hermstrüwer, Yinya Huang, Mubashara Akhtar, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, Joel Niklaus",http://arxiv.org/pdf/2505.12864v1,cs.CL
Re-identification of De-identified Documents with Autoregressive Infilling,"Documents revealing sensitive information about individuals must typically be
de-identified. This de-identification is often done by masking all mentions of
personally identifiable information (PII), thereby making it more difficult to
uncover the identity of the person(s) in question. To investigate the
robustness of de-identification methods, we present a novel, RAG-inspired
approach that attempts the reverse process of re-identification based on a
database of documents representing background knowledge. Given a text in which
personal identifiers have been masked, the re-identification proceeds in two
steps. A retriever first selects from the background knowledge passages deemed
relevant for the re-identification. Those passages are then provided to an
infilling model which seeks to infer the original content of each text span.
This process is repeated until all masked spans are replaced. We evaluate the
re-identification on three datasets (Wikipedia biographies, court rulings and
clinical notes). Results show that (1) as many as 80% of de-identified text
spans can be successfully recovered and (2) the re-identification accuracy
increases along with the level of background knowledge.",2025-05-19,"Lucas Georges Gabriel Charpentier, Pierre Lison",http://arxiv.org/pdf/2505.12859v1,cs.CL
GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents,"Graphical user interface (GUI) agents have recently emerged as an intriguing
paradigm for human-computer interaction, capable of automatically executing
user instructions to operate intelligent terminal devices. However, when
encountering out-of-distribution (OOD) instructions that violate environmental
constraints or exceed the current capabilities of agents, GUI agents may suffer
task breakdowns or even pose security threats. Therefore, effective OOD
detection for GUI agents is essential. Traditional OOD detection methods
perform suboptimally in this domain due to the complex embedding space and
evolving GUI environments. In this work, we observe that the in-distribution
input semantic space of GUI agents exhibits a clustering pattern with respect
to the distance from the centroid. Based on the finding, we propose GEM, a
novel method based on fitting a Gaussian mixture model over input embedding
distances extracted from the GUI Agent that reflect its capability boundary.
Evaluated on eight datasets spanning smartphones, computers, and web browsers,
our method achieves an average accuracy improvement of 23.70\% over the
best-performing baseline. Analysis verifies the generalization ability of our
method through experiments on nine different backbones. The codes are available
at https://github.com/Wuzheng02/GEM-OODforGUIagents.",2025-05-19,"Zheng Wu, Pengzhou Cheng, Zongru Wu, Lingzhong Dong, Zhuosheng Zhang",http://arxiv.org/pdf/2505.12842v2,cs.CL
The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting,"Legal contracts possess an inherent, semantically vital structure (e.g.,
sections, clauses) that is crucial for human comprehension but whose impact on
LLM processing remains under-explored. This paper investigates the effects of
explicit input text structure and prompt engineering on the performance of
GPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the
CUAD. We compare model exact-match accuracy across various input formats:
well-structured plain-text (human-generated from CUAD), plain-text cleaned of
line breaks, extracted plain-text from Azure OCR, plain-text extracted by
GPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o
Vision. To give an indication of the impact of possible prompt engineering, we
assess the impact of shifting task instructions to the system prompt and
explicitly informing the model about the structured nature of the input. Our
findings reveal that GPT-4o demonstrates considerable robustness to variations
in input structure, but lacks in overall performance. Conversely, GPT-4.1's
performance is markedly sensitive; poorly structured inputs yield suboptimal
results (but identical with GPT-4o), while well-structured formats (original
CUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by
~20 percentage points. Optimizing the system prompt to include task details and
an advisory about structured input further elevates GPT-4.1's accuracy by an
additional ~10-13 percentage points, with Markdown ultimately achieving the
highest performance under these conditions (79 percentage points overall
exact-match accuracy). This research empirically demonstrates that while newer
models exhibit greater resilience, careful input structuring and strategic
prompt design remain critical for optimizing the performance of LLMs, and can
significantly affect outcomes in high-stakes legal applications.",2025-05-19,"Christian Braun, Alexander Lilienbeck, Daniel Mentjukov",http://arxiv.org/pdf/2505.12837v1,cs.CL
FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models,"Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital
for applications such as disaster response, logistics delivery, and urban
inspection. However, existing methods often struggle with insufficient
multimodal fusion, weak generalization, and poor interpretability. To address
these challenges, we propose FlightGPT, a novel UAV VLN framework built upon
Vision-Language Models (VLMs) with powerful multimodal perception capabilities.
We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)
using high-quality demonstrations to improve initialization and structured
reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by
a composite reward that considers goal accuracy, reasoning quality, and format
compliance, to enhance generalization and adaptability. Furthermore, FlightGPT
introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve
decision interpretability. Extensive experiments on the city-scale dataset
CityNav demonstrate that FlightGPT achieves state-of-the-art performance across
all scenarios, with a 9.22\% higher success rate than the strongest baseline in
unseen environments. Our implementation is publicly available.",2025-05-19,"Hengxing Cai, Jinhan Dong, Jingjun Tan, Jingcheng Deng, Sihang Li, Zhifeng Gao, Haidong Wang, Zicheng Su, Agachai Sumalee, Renxin Zhong",http://arxiv.org/pdf/2505.12835v1,cs.CL
Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering,"Extracting sentence embeddings from large language models (LLMs) is a
practical direction, as it requires neither additional data nor fine-tuning.
Previous studies usually focus on prompt engineering to guide LLMs to encode
the core semantic information of the sentence into the embedding of the last
token. However, the last token in these methods still encodes an excess of
non-essential information, such as stop words, limiting its encoding capacity.
To this end, we propose a Contrastive Prompting (CP) method that introduces an
extra auxiliary prompt to elicit better sentence embedding. By contrasting with
the auxiliary prompt, CP can steer existing prompts to encode the core
semantics of the sentence, rather than non-essential information. CP is a
plug-and-play inference-time intervention method that can be combined with
various prompt-based methods. Extensive experiments on Semantic Textual
Similarity (STS) tasks and downstream classification tasks demonstrate that our
method can improve the performance of existing prompt-based methods across
different LLMs. Our code will be released at https://github.com/zifengcheng/CP.",2025-05-19,"Zifeng Cheng, Zhonghui Wang, Yuchen Fu, Zhiwei Jiang, Yafeng Yin, Cong Wang, Qing Gu",http://arxiv.org/pdf/2505.12831v1,cs.CL
SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models,"Large Language Models (LLMs) are emerging as dominant forces for textual
style transfer. However, for arbitrary style transfer, LLMs face two key
challenges: (1) considerable reliance on manually-constructed prompts and (2)
rigid stylistic biases inherent in LLMs. In this paper, we propose a novel
Synthesize-then-Decode (SynDec) approach, which automatically synthesizes
high-quality prompts and amplifies their roles during decoding process.
Specifically, our approach synthesizes prompts by selecting representative
few-shot samples, conducting a four-dimensional style analysis, and reranking
the candidates. At LLM decoding stage, the TST effect is amplified by
maximizing the contrast in output probabilities between scenarios with and
without the synthesized prompt, as well as between prompts and negative
samples. We conduct extensive experiments and the results show that SynDec
outperforms existing state-of-the-art LLM-based methods on five out of six
benchmarks (e.g., achieving up to a 9\% increase in accuracy for
modern-to-Elizabethan English transfer). Detailed ablation studies further
validate the effectiveness of SynDec.",2025-05-19,"Han Sun, Zhen Sun, Zongmin Zhang, Linzhao Jia, Wei Shao, Min Zhang",http://arxiv.org/pdf/2505.12821v1,cs.CL
PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs,"Existing LLM-based role-playing methods often rely on superficial textual
descriptions or simplistic metrics, inadequately modeling both intrinsic and
extrinsic character dimensions. Additionally, they typically simulate character
memory with implicit model knowledge or basic retrieval augment generation
without explicit memory alignment, compromising memory consistency. The two
issues weaken reliability of role-playing LLMs in several applications, such as
trustworthy social simulation. To address these limitations, we propose PsyMem,
a novel framework integrating fine-grained psychological attributes and
explicit memory control for role-playing. PsyMem supplements textual
descriptions with 26 psychological indicators to detailed model character.
Additionally, PsyMem implements memory alignment training, explicitly trains
the model to align character's response with memory, thereby enabling dynamic
memory-controlled responding during inference. By training Qwen2.5-7B-Instruct
on our specially designed dataset (including 5,414 characters and 38,962
dialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,
outperforms baseline models in role-playing, achieving the best performance in
human-likeness and character fidelity.",2025-05-19,"Xilong Cheng, Yunxiao Qin, Yuting Tan, Zhengnan Li, Ye Wang, Hongjiang Xiao, Yuan Zhang",http://arxiv.org/pdf/2505.12814v1,cs.CL
Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models,"The recent explosion of large language models (LLMs), each with its own
general or specialized strengths, makes scalable, reliable benchmarking more
urgent than ever. Standard practices nowadays face fundamental trade-offs:
closed-ended question-based benchmarks (eg MMLU) struggle with saturation as
newer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely
on costly and slow human judges. Recently, automated methods (eg
LLM-as-a-judge) shed light on the scalability, but risk bias by relying on one
or a few ""authority"" models. To tackle these issues, we propose Decentralized
Arena (dearena), a fully automated framework leveraging collective intelligence
from all LLMs to evaluate each other. It mitigates single-model judge bias by
democratic, pairwise evaluation, and remains efficient at scale through two key
components: (1) a coarse-to-fine ranking algorithm for fast incremental
insertion of new models with sub-quadratic complexity, and (2) an automatic
question selection strategy for the construction of new evaluation dimensions.
Across extensive experiments across 66 LLMs, dearena attains up to 97%
correlation with human judgements, while significantly reducing the cost. Our
code and data will be publicly released on
https://github.com/maitrix-org/de-arena.",2025-05-19,"Yanbin Yin, Kun Zhou, Zhen Wang, Xiangdong Zhang, Yifei Shao, Shibo Hao, Yi Gu, Jieyuan Liu, Somanshu Singla, Tianyang Liu, Eric P. Xing, Zhengzhong Liu, Haojian Jin, Zhiting Hu",http://arxiv.org/pdf/2505.12808v1,cs.CL
EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs,"The rapid evolution of large language models (LLMs) has revolutionized
various fields, including the identification and discovery of human values
within text data. While traditional NLP models, such as BERT, have been
employed for this task, their ability to represent textual data is
significantly outperformed by emerging LLMs like GPTs. However, the performance
of online LLMs often degrades when handling long contexts required for value
identification, which also incurs substantial computational costs. To address
these challenges, we propose EAVIT, an efficient and accurate framework for
human value identification that combines the strengths of both locally
fine-tunable and online black-box LLMs. Our framework employs a value detector
- a small, local language model - to generate initial value estimations. These
estimations are then used to construct concise input prompts for online LLMs,
enabling accurate final value identification. To train the value detector, we
introduce explanation-based training and data generation techniques
specifically tailored for value identification, alongside sampling strategies
to optimize the brevity of LLM input prompts. Our approach effectively reduces
the number of input tokens by up to 1/6 compared to directly querying online
LLMs, while consistently outperforming traditional NLP methods and other
LLM-based strategies.",2025-05-19,"Wenhao Zhu, Yuhang Xie, Guojie Song, Xin Zhang",http://arxiv.org/pdf/2505.12792v1,cs.CL
"A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone","Training high-performing Small Language Models (SLMs) remains costly, even
with knowledge distillation and pruning from larger teacher models. Existing
work often faces three key challenges: (1) information loss from hard pruning,
(2) inefficient alignment of representations, and (3) underutilization of
informative activations, particularly from Feed-Forward Networks (FFNs). To
address these challenges, we introduce Low-Rank Clone (LRC), an efficient
pre-training method that constructs SLMs aspiring to behavioral equivalence
with strong teacher models. LRC trains a set of low-rank projection matrices
that jointly enable soft pruning by compressing teacher weights, and activation
clone by aligning student activations, including FFN signals, with those of the
teacher. This unified design maximizes knowledge transfer while removing the
need for explicit alignment modules. Extensive experiments with open-source
teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC
matches or surpasses state-of-the-art models trained on trillions of
tokens--while using only 20B tokens, achieving over 1,000x training efficiency.
Our codes and model checkpoints are available at
https://github.com/CURRENTF/LowRankClone and
https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.",2025-05-19,"Jitai Hao, Qiang Huang, Hao Liu, Xinyan Xiao, Zhaochun Ren, Jun Yu",http://arxiv.org/pdf/2505.12781v1,cs.CL
Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation,"Large language model (LLM) shows promising performances in a variety of
downstream tasks, such as machine translation (MT). However, using LLMs for
translation suffers from high computational costs and significant latency.
Based on our evaluation, in most cases, translations using LLMs are comparable
to that generated by neural machine translation (NMT) systems. Only in
particular scenarios, LLM and NMT models show respective advantages. As a
result, integrating NMT and LLM for translation and using LLM only when
necessary seems to be a sound solution. A scheduling policy that optimizes
translation result while ensuring fast speed and as little LLM usage as
possible is thereby required. We compare several scheduling policies and
propose a novel and straightforward decider that leverages source sentence
features. We conduct extensive experiments on multilingual test sets and the
result shows that we can achieve optimal translation performance with minimal
LLM usage, demonstrating effectiveness of our decider.",2025-05-19,"Zhanglin Wu, Daimeng Wei, Xiaoyu Chen, Hengchao Shang, Jiaxin Guo, Zongyao Li, Yuanchang Luo, Jinlong Yang, Zhiqiang Rao, Hao Yang",http://arxiv.org/pdf/2505.13554v1,cs.CL
ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL,"In Text-to-SQL, execution feedback is essential for guiding large language
models (LLMs) to reason accurately and generate reliable SQL queries. However,
existing methods treat execution feedback solely as a post-hoc signal for
correction or selection, failing to integrate it into the generation process.
This limitation hinders their ability to address reasoning errors as they
occur, ultimately reducing query accuracy and robustness. To address this
issue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement
Learning), a framework for Text-to-SQL that enables models to interact with the
database during decoding and dynamically adjust their reasoning based on
execution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm
that interleaves intermediate SQL execution into reasoning paths, facilitating
context-sensitive revisions. It achieves this through structured prompts with
markup tags and a stepwise rollout strategy that integrates execution feedback
into each stage of generation. To supervise policy learning, we develop a
composite reward function that includes an exploration reward, explicitly
encouraging effective database interaction. Additionally, ReEx-SQL adopts a
tree-based decoding strategy to support exploratory reasoning, enabling dynamic
expansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on
Spider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning
baseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving
85.2% on Spider-Realistic with leading performance. In addition, its
tree-structured decoding improves efficiency and performance over linear
decoding, reducing inference time by 51.9% on the BIRD development set.",2025-05-19,"Yaxun Dai, Wenxuan Xie, Xialie Zhuang, Tianyu Yang, Yiying Yang, Haiqin Yang, Yuhang Zhao, Pingfu Chao, Wenhao Jiang",http://arxiv.org/pdf/2505.12768v2,cs.CL
Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization,"Reward models (RMs) play a crucial role in reinforcement learning from human
feedback (RLHF), aligning model behavior with human preferences. However,
existing benchmarks for reward models show a weak correlation with the
performance of optimized policies, suggesting that they fail to accurately
assess the true capabilities of RMs. To bridge this gap, we explore several
evaluation designs through the lens of reward overoptimization\textemdash a
phenomenon that captures both how well the reward model aligns with human
preferences and the dynamics of the learning signal it provides to the policy.
The results highlight three key findings on how to construct a reliable
benchmark: (i) it is important to minimize differences between chosen and
rejected responses beyond correctness, (ii) evaluating reward models requires
multiple comparisons across a wide range of chosen and rejected responses, and
(iii) given that reward models encounter responses with diverse
representations, responses should be sourced from a variety of models. However,
we also observe that a extremely high correlation with degree of
overoptimization leads to comparatively lower correlation with certain
downstream performance. Thus, when designing a benchmark, it is desirable to
use the degree of overoptimization as a useful tool, rather than the end goal.",2025-05-19,"Sunghwan Kim, Dongjin Kang, Taeyoon Kwon, Hyungjoo Chae, Dongha Lee, Jinyoung Yeo",http://arxiv.org/pdf/2505.12763v1,cs.CL
"What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma","Mental-health stigma remains a pervasive social problem that hampers
treatment-seeking and recovery. Existing resources for training neural models
to finely classify such stigma are limited, relying primarily on social-media
or synthetic data without theoretical underpinnings. To remedy this gap, we
present an expert-annotated, theory-informed corpus of human-chatbot
interviews, comprising 4,141 snippets from 684 participants with documented
socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural
models and empirically unpack the challenges of stigma detection. This dataset
can facilitate research on computationally detecting, neutralizing, and
counteracting mental-health stigma.",2025-05-19,"Han Meng, Yancan Chen, Yunan Li, Yitian Yang, Jungup Lee, Renwen Zhang, Yi-Chieh Lee",http://arxiv.org/pdf/2505.12727v1,cs.CL
On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding,"Large language models (LLMs) achieve remarkable performance in code
generation tasks. However, a significant performance disparity persists between
popular programming languages (e.g., Python, C++) and others. To address this
capability gap, we leverage the code translation task to train LLMs, thereby
facilitating the transfer of coding proficiency across diverse programming
languages. Moreover, we introduce OORL for training, a novel reinforcement
learning (RL) framework that integrates on-policy and off-policy strategies.
Within OORL, on-policy RL is applied during code translation, guided by a
rule-based reward signal derived from unit tests. Complementing this
coarse-grained rule-based reward, we propose Group Equivalent Preference
Optimization (GEPO), a novel preference optimization method. Specifically, GEPO
trains the LLM using intermediate representations (IRs) groups. LLMs can be
guided to discern IRs equivalent to the source code from inequivalent ones,
while also utilizing signals about the mutual equivalence between IRs within
the group. This process allows LLMs to capture nuanced aspects of code
functionality. By employing OORL for training with code translation tasks, LLMs
improve their recognition of code functionality and their understanding of the
relationships between code implemented in different languages. Extensive
experiments demonstrate that our OORL for LLMs training with code translation
tasks achieves significant performance improvements on code benchmarks across
multiple programming languages.",2025-05-19,"Haoyuan Wu, Rui Ming, Jilong Gao, Hangyu Zhao, Xueyi Chen, Yikai Yang, Haisheng Zheng, Zhuolun He, Bei Yu",http://arxiv.org/pdf/2505.12723v1,cs.CL
Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework,"Recent advances in Generative Artificial Intelligence (GenAI) have
transformed educational content creation, particularly in developing tutor
training materials. However, biases embedded in AI-generated content--such as
gender, racial, or national stereotypes--raise significant ethical and
educational concerns. Despite the growing use of GenAI, systematic methods for
detecting and evaluating such biases in educational materials remain limited.
This study proposes an automated bias assessment approach that integrates the
Contextualized Embedding Association Test with a prompt-engineered word
extraction method within a Retrieval-Augmented Generation framework. We applied
this method to AI-generated texts used in tutor training lessons. Results show
a high alignment between the automated and manually curated word sets, with a
Pearson correlation coefficient of r = 0.993, indicating reliable and
consistent bias assessment. Our method reduces human subjectivity and enhances
fairness, scalability, and reproducibility in auditing GenAI-produced
educational content.",2025-05-19,"Jingyang Peng, Wenyuan Shen, Jiarui Rao, Jionghao Lin",http://arxiv.org/pdf/2505.12718v1,cs.CL
ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving,"Large language models (LLMs) demonstrate significant reasoning capabilities,
particularly through long chain-of-thought (CoT) processes, which can be
elicited by reinforcement learning (RL). However, prolonged CoT reasoning
presents limitations, primarily verbose outputs due to excessive introspection.
The reasoning process in these LLMs often appears to follow a trial-and-error
methodology rather than a systematic, logical deduction. In contrast,
tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling
reasoning as an exploration within a tree structure. This reasoning structure
facilitates the parallel generation and evaluation of multiple reasoning
branches, allowing for the active identification, assessment, and pruning of
unproductive paths. This process can potentially lead to improved performance
and reduced token costs. Building upon the long CoT capability of LLMs, we
introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a
rule-based reward. ToTRL is designed to guide LLMs in developing the parallel
ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs
as players in a puzzle game during the ToTRL training process. Solving puzzle
games inherently necessitates exploring interdependent choices and managing
multiple constraints, which requires the construction and exploration of a
thought tree, providing challenging tasks for cultivating the ToT reasoning
capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,
trained with our ToTRL, achieves significant improvement in performance and
reasoning efficiency on complex reasoning tasks.",2025-05-19,"Haoyuan Wu, Xueyi Chen, Rui Ming, Jilong Gao, Shoubo Hu, Zhuolun He, Bei Yu",http://arxiv.org/pdf/2505.12717v1,cs.CL
Shadow-FT: Tuning Instruct via Base,"Large language models (LLMs) consistently benefit from further fine-tuning on
various tasks. However, we observe that directly tuning the INSTRUCT (i.e.,
instruction tuned) models often leads to marginal improvements and even
performance degeneration. Notably, paired BASE models, the foundation for these
INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on
average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to
tune the INSTRUCT models by leveraging the corresponding BASE models. The key
insight is to fine-tune the BASE model, and then directly graft the learned
weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no
additional parameters, is easy to implement, and significantly improves
performance. We conduct extensive experiments on tuning mainstream LLMs, such
as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering
coding, reasoning, and mathematical tasks. Experimental results demonstrate
that Shadow-FT consistently outperforms conventional full-parameter and
parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT
can be applied to multimodal large language models (MLLMs) and combined with
direct preference optimization (DPO). Codes and weights are available at
\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.",2025-05-19,"Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, Yujiu Yang",http://arxiv.org/pdf/2505.12716v1,cs.CL
Bullying the Machine: How Personas Increase LLM Vulnerability,"Large Language Models (LLMs) are increasingly deployed in interactions where
they are prompted to adopt personas. This paper investigates whether such
persona conditioning affects model safety under bullying, an adversarial
manipulation that applies psychological pressures in order to force the victim
to comply to the attacker. We introduce a simulation framework in which an
attacker LLM engages a victim LLM using psychologically grounded bullying
tactics, while the victim adopts personas aligned with the Big Five personality
traits. Experiments using multiple open-source LLMs and a wide range of
adversarial goals reveal that certain persona configurations -- such as
weakened agreeableness or conscientiousness -- significantly increase victim's
susceptibility to unsafe outputs. Bullying tactics involving emotional or
sarcastic manipulation, such as gaslighting and ridicule, are particularly
effective. These findings suggest that persona-driven interaction introduces a
novel vector for safety risks in LLMs and highlight the need for persona-aware
safety evaluation and alignment strategies.",2025-05-19,"Ziwei Xu, Udit Sanghi, Mohan Kankanhalli",http://arxiv.org/pdf/2505.12692v1,cs.CL
Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities,"LLM-based formal proof assistants (e.g., in Lean) hold great promise for
automating mathematical discovery. But beyond syntactic correctness, do these
systems truly understand mathematical structure as humans do? We investigate
this question through the lens of mathematical inequalities -- a fundamental
tool across many domains. While modern provers can solve basic inequalities, we
probe their ability to handle human-intuitive compositionality. We introduce
Ineq-Comp, a benchmark built from elementary inequalities through systematic
transformations, including variable duplication, algebraic rewriting, and
multi-step composition. Although these problems remain easy for humans, we find
that most provers -- including Goedel, STP, and Kimina-7B -- struggle
significantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly
because it is trained to decompose the problems into sub-problems -- but still
suffers a 20\% performance drop (pass@32). Strikingly, performance remains poor
for all models even when formal proofs of the constituent parts are provided in
context, revealing that the source of weakness is indeed in compositional
reasoning. Our results expose a persisting gap between the generalization
behavior of current AI provers and human mathematical intuition.",2025-05-19,"Haoyu Zhao, Yihan Geng, Shange Tang, Yong Lin, Bohan Lyu, Hongzhou Lin, Chi Jin, Sanjeev Arora",http://arxiv.org/pdf/2505.12680v1,cs.CL
Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems,"Automatic prompt generation plays a crucial role in enabling general-purpose
multi-agent systems to perform diverse tasks autonomously. Existing methods
typically evaluate prompts based on their immediate task performance,
overlooking the intrinsic qualities that determine their reliability. This
outcome-centric view not only limits interpretability but also fails to account
for the inherent stochasticity of large language models (LLMs). In this work,
we bring attention to prompt stability-the consistency of model responses
across repeated executions-as a key factor for building robust and effective
prompt generation systems. To quantify this, we propose semantic stability as a
criterion for assessing the response consistency of prompts, and fine-tune a
LLaMA-based evaluator to measure it automatically across tasks. These
components have enabled us to develop the first stability-aware general-purpose
prompt generation system that leverages stability feedback to iteratively
enhance both prompt quality and system-level performance. Furthermore, we
establish a logical chain between prompt stability and task success by
analyzing the structural dependencies within our system, proving stability as a
necessary condition for effective system-level execution. Empirical results
across general and domain-specific tasks demonstrate that our stability-aware
framework improves both accuracy and output consistency. By shifting the focus
from one-off results to persistent reliability, our work offers a new
perspective on prompt design and contributes practical tools for building more
trustworthy general-purpose systems.",2025-05-19,"Ke Chen, Yufei Zhou, Xitong Zhang, Haohan Wang",http://arxiv.org/pdf/2505.13546v1,cs.CL
"Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering","Recent advances in large language models (LLMs) have led to impressive
progress in natural language generation, yet their tendency to produce
hallucinated or unsubstantiated content remains a critical concern. To improve
factual reliability, Retrieval-Augmented Generation (RAG) integrates external
knowledge during inference. However, existing RAG systems face two major
limitations: (1) unreliable adaptive control due to limited external knowledge
supervision, and (2) hallucinations caused by inaccurate or irrelevant
references. To address these issues, we propose Know3-RAG, a knowledge-aware
RAG framework that leverages structured knowledge from knowledge graphs (KGs)
to guide three core stages of the RAG process, including retrieval, generation,
and filtering. Specifically, we introduce a knowledge-aware adaptive retrieval
module that employs KG embedding to assess the confidence of the generated
answer and determine retrieval necessity, a knowledge-enhanced reference
generation strategy that enriches queries with KG-derived entities to improve
generated reference relevance, and a knowledge-driven reference filtering
mechanism that ensures semantic alignment and factual accuracy of references.
Experiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG
consistently outperforms strong baselines, significantly reducing
hallucinations and enhancing answer reliability.",2025-05-19,"Xukai Liu, Ye Liu, Shiwen Wu, Yanghai Zhang, Yihao Yuan, Kai Zhang, Qi Liu",http://arxiv.org/pdf/2505.12662v1,cs.CL
"Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals","This paper addresses the gap in predicting turn-taking and backchannel
actions in human-machine conversations using multi-modal signals (linguistic,
acoustic, and visual). To overcome the limitation of existing datasets, we
propose an automatic data collection pipeline that allows us to collect and
annotate over 210 hours of human conversation videos. From this, we construct a
Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over
1.5M words and corresponding turn-taking and backchannel annotations from
approximately 20M frames. Additionally, we present an end-to-end framework that
predicts the probability of turn-taking and backchannel actions from
multi-modal signals. The proposed model emphasizes the interrelation between
modalities and supports any combination of text, audio, and video inputs,
making it adaptable to a variety of realistic scenarios. Our experiments show
that our approach achieves state-of-the-art performance on turn-taking and
backchannel prediction tasks, achieving a 10% increase in F1-score on
turn-taking and a 33% increase on backchannel prediction. Our dataset and code
are publicly available online to ease of subsequent research.",2025-05-19,"Yuxin Lin, Yinglin Zheng, Ming Zeng, Wangzheng Shi",http://arxiv.org/pdf/2505.12654v2,cs.CL
Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning,"Detecting Mild Cognitive Impairment from picture descriptions is critical yet
challenging, especially in multilingual and multiple picture settings. Prior
work has primarily focused on English speakers describing a single picture
(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by
introducing multilingual speakers and multiple pictures, which presents new
challenges in analyzing picture-dependent content. To address these challenges,
we propose a framework with three components: (1) enhancing discriminative
representation learning via supervised contrastive learning, (2) involving
image modality rather than relying solely on speech and text modalities, and
(3) applying a Product of Experts (PoE) strategy to mitigate spurious
correlations and overfitting. Our framework improves MCI detection performance,
achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to
75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the
text unimodal baseline. Notably, the contrastive learning component yields
greater gains for the text modality compared to speech. These results highlight
our framework's effectiveness in multilingual and multi-picture MCI detection.",2025-05-19,"Kristin Qi, Jiali Cheng, Youxiang Zhu, Hadi Amiri, Xiaohui Liang",http://arxiv.org/pdf/2505.17067v2,cs.CL
Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing,"Knowledge editing, which aims to update the knowledge encoded in language
models, can be deceptive. Despite the fact that many existing knowledge editing
algorithms achieve near-perfect performance on conventional metrics, the models
edited by them are still prone to generating original knowledge. This paper
introduces the concept of ""superficial editing"" to describe this phenomenon.
Our comprehensive evaluation reveals that this issue presents a significant
challenge to existing algorithms. Through systematic investigation, we identify
and validate two key factors contributing to this issue: (1) the residual
stream at the last subject position in earlier layers and (2) specific
attention modules in later layers. Notably, certain attention heads in later
layers, along with specific left singular vectors in their output matrices,
encapsulate the original knowledge and exhibit a causal relationship with
superficial editing. Furthermore, we extend our analysis to the task of
superficial unlearning, where we observe consistent patterns in the behavior of
specific attention heads and their corresponding left singular vectors, thereby
demonstrating the robustness and broader applicability of our methodology and
conclusions. Our code is available here.",2025-05-19,"Jiakuan Xie, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao",http://arxiv.org/pdf/2505.12636v1,cs.CL
Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents,"Recent advancements in Large Language Models (LLMs) and Vision-Language
Models (VLMs) have sparked significant interest in developing GUI visual
agents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from
YouTube), a large-scale dataset of 313K annotated frames from 20K instructional
videos capturing diverse real-world mobile OS navigation across multiple
platforms. Models that include MONDAY in their pre-training phases demonstrate
robust cross-platform generalization capabilities, consistently outperforming
models trained on existing single OS datasets while achieving an average
performance gain of 18.11%p on an unseen mobile OS platform. To enable
continuous dataset expansion as mobile platforms evolve, we present an
automated framework that leverages publicly available video content to create
comprehensive task datasets without manual annotation. Our framework comprises
robust OCR-based scene detection (95.04% F1score), near-perfect UI element
detection (99.87% hit ratio), and novel multi-step action identification to
extract reliable action sequences across diverse interface configurations. We
contribute both the MONDAY dataset and our automated collection framework to
facilitate future research in mobile OS navigation.",2025-05-19,"Yunseok Jang, Yeda Song, Sungryull Sohn, Lajanugen Logeswaran, Tiange Luo, Dong-Ki Kim, Kyunghoon Bae, Honglak Lee",http://arxiv.org/pdf/2505.12632v1,cs.CL
Enhancing Latent Computation in Transformers with Latent Tokens,"Augmenting large language models (LLMs) with auxiliary tokens has emerged as
a promising strategy for enhancing model performance. In this work, we
introduce a lightweight method termed latent tokens; these are dummy tokens
that may be non-interpretable in natural language but steer the autoregressive
decoding process of a Transformer-based LLM via the attention mechanism. The
proposed latent tokens can be seamlessly integrated with a pre-trained
Transformer, trained in a parameter-efficient manner, and applied flexibly at
inference time, while adding minimal complexity overhead to the existing
infrastructure of standard Transformers. We propose several hypotheses about
the underlying mechanisms of latent tokens and design synthetic tasks
accordingly to verify them. Numerical results confirm that the proposed method
noticeably outperforms the baselines, particularly in the out-of-distribution
generalization scenarios, highlighting its potential in improving the
adaptability of LLMs.",2025-05-19,"Yuchang Sun, Yanxi Chen, Yaliang Li, Bolin Ding",http://arxiv.org/pdf/2505.12629v1,cs.CL
R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model,"DeepSeek recently released R1, a high-performing large language model (LLM)
optimized for reasoning tasks. Despite its efficient training pipeline, R1
achieves competitive performance, even surpassing leading reasoning models like
OpenAI's o1 on several benchmarks. However, emerging reports suggest that R1
refuses to answer certain prompts related to politically sensitive topics in
China. While existing LLMs often implement safeguards to avoid generating
harmful or offensive outputs, R1 represents a notable shift - exhibiting
censorship-like behavior on politically charged queries. In this paper, we
investigate this phenomenon by first introducing a large-scale set of heavily
curated prompts that get censored by R1, covering a range of politically
sensitive topics, but are not censored by other models. We then conduct a
comprehensive analysis of R1's censorship patterns, examining their
consistency, triggers, and variations across topics, prompt phrasing, and
context. Beyond English-language queries, we explore censorship behavior in
other languages. We also investigate the transferability of censorship to
models distilled from the R1 language model. Finally, we propose techniques for
bypassing or removing this censorship. Our findings reveal possible additional
censorship integration likely shaped by design choices during training or
alignment, raising concerns about transparency, bias, and governance in
language model deployment.",2025-05-19,"Ali Naseh, Harsh Chaudhari, Jaechul Roh, Mingshi Wu, Alina Oprea, Amir Houmansadr",http://arxiv.org/pdf/2505.12625v1,cs.CL
Think Before You Attribute: Improving the Performance of LLMs Attribution Systems,"Large Language Models (LLMs) are increasingly applied in various science
domains, yet their broader adoption remains constrained by a critical
challenge: the lack of trustworthy, verifiable outputs. Current LLMs often
generate answers without reliable source attribution, or worse, with incorrect
attributions, posing a barrier to their use in scientific and high-stakes
settings, where traceability and accountability are non-negotiable. To be
reliable, attribution systems need high accuracy and retrieve data with short
lengths, i.e., attribute to a sentence within a document rather than a whole
document. We propose a sentence-level pre-attribution step for
Retrieve-Augmented Generation (RAG) systems that classify sentences into three
categories: not attributable, attributable to a single quote, and attributable
to multiple quotes. By separating sentences before attribution, a proper
attribution method can be selected for the type of sentence, or the attribution
can be skipped altogether. Our results indicate that classifiers are
well-suited for this task. In this work, we propose a pre-attribution step to
reduce the computational complexity of attribution, provide a clean version of
the HAGRID dataset, and provide an end-to-end attribution system that works out
of the box.",2025-05-19,"João Eduardo Batista, Emil Vatai, Mohamed Wahib",http://arxiv.org/pdf/2505.12621v1,cs.CL
Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval,"This paper presents the Duluth approach to the SemEval-2025 Task 7 on
Multilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a
TF-IDF-based retrieval system with experimentation on vector dimensions and
tokenization strategies. Our best-performing configuration used word-level
tokenization with a vocabulary size of 15,000 features, achieving an average
success@10 score of 0.78 on the development set and 0.69 on the test set across
ten languages. Our system showed stronger performance on higher-resource
languages but still lagged significantly behind the top-ranked system, which
achieved 0.96 average success@10. Our findings suggest that though advanced
neural architectures are increasingly dominant in multilingual retrieval tasks,
properly optimized traditional methods like TF-IDF remain competitive
baselines, especially in limited compute resource scenarios.",2025-05-19,"Shujauddin Syed, Ted Pedersen",http://arxiv.org/pdf/2505.12616v1,cs.CL
AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection,"Anomaly detection (AD) is essential in areas such as fraud detection, network
monitoring, and scientific research. However, the diversity of data modalities
and the increasing number of specialized AD libraries pose challenges for
non-expert users who lack in-depth library-specific knowledge and advanced
programming skills. To tackle this, we present AD-AGENT, an LLM-driven
multi-agent framework that turns natural-language instructions into fully
executable AD pipelines. AD-AGENT coordinates specialized agents for intent
parsing, data preparation, library and model selection, documentation mining,
and iterative code generation and debugging. Using a shared short-term
workspace and a long-term cache, the agents integrate popular AD libraries like
PyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that
AD-AGENT produces reliable scripts and recommends competitive models across
libraries. The system is open-sourced to support further research and practical
applications in AD.",2025-05-19,"Tiankai Yang, Junjun Liu, Wingchun Siu, Jiahang Wang, Zhuangzhuang Qian, Chanjuan Song, Cheng Cheng, Xiyang Hu, Yue Zhao",http://arxiv.org/pdf/2505.12594v1,cs.CL
PromptPrism: A Linguistically-Inspired Taxonomy for Prompts,"Prompts are the interface for eliciting the capabilities of large language
models (LLMs). Understanding their structure and components is critical for
analyzing LLM behavior and optimizing performance. However, the field lacks a
comprehensive framework for systematic prompt analysis and understanding. We
introduce PromptPrism, a linguistically-inspired taxonomy that enables prompt
analysis across three hierarchical levels: functional structure, semantic
component, and syntactic pattern. We show the practical utility of PromptPrism
by applying it to three applications: (1) a taxonomy-guided prompt refinement
approach that automatically improves prompt quality and enhances model
performance across a range of tasks; (2) a multi-dimensional dataset profiling
method that extracts and aggregates structural, semantic, and syntactic
characteristics from prompt datasets, enabling comprehensive analysis of prompt
distributions and patterns; (3) a controlled experimental framework for prompt
sensitivity analysis by quantifying the impact of semantic reordering and
delimiter modifications on LLM performance. Our experimental results validate
the effectiveness of our taxonomy across these applications, demonstrating that
PromptPrism provides a foundation for refining, profiling, and analyzing
prompts.",2025-05-19,"Sullam Jeoung, Yueyan Chen, Yi Zhang, Shuai Wang, Haibo Ding, Lin Lee Cheong",http://arxiv.org/pdf/2505.12592v1,cs.CL
CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling,"Code-mixed languages, characterized by frequent within-sentence language
transitions, present structural challenges that standard language models fail
to address. In this work, we propose CMLFormer, an enhanced multi-layer
dual-decoder Transformer with a shared encoder and synchronized decoder
cross-attention, designed to model the linguistic and semantic dynamics of
code-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with
switching point and translation annotations with multiple new objectives
specifically aimed at capturing switching behavior, cross-lingual structure,
and code-mixing complexity. Our experiments show that CMLFormer improves F1
score, precision, and accuracy over other approaches on the HASOC-2021
benchmark under select pre-training setups. Attention analyses further show
that it can identify and attend to switching points, validating its sensitivity
to code-mixed structure. These results demonstrate the effectiveness of
CMLFormer's architecture and multi-task pre-training strategy for modeling
code-mixed languages.",2025-05-19,"Aditeya Baral, Allen George Ajith, Roshan Nayak, Mrityunjay Abhijeet Bhanja",http://arxiv.org/pdf/2505.12587v1,cs.CL
Improving Multilingual Language Models by Aligning Representations through Steering,"In this paper, we investigate how large language models (LLMS) process
non-English tokens within their layer representations, an open question despite
significant advancements in the field. Using representation steering,
specifically by adding a learned vector to a single model layer's activations,
we demonstrate that steering a single model layer can notably enhance
performance. Our analysis shows that this approach achieves results comparable
to translation baselines and surpasses state of the art prompt optimization
methods. Additionally, we highlight how advanced techniques like supervised
fine tuning (\textsc{sft}) and reinforcement learning from human feedback
(\textsc{rlhf}) improve multilingual capabilities by altering representation
spaces. We further illustrate how these methods align with our approach to
reshaping LLMS layer representations.",2025-05-19,"Omar Mahmoud, Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana",http://arxiv.org/pdf/2505.12584v1,cs.CL
Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio,"Writing novels with Large Language Models (LLMs) raises a critical question:
how much human-authored outline is necessary to generate high-quality
million-word novels? While frameworks such as DOME, Plan&Write, and Long Writer
have improved stylistic coherence and logical consistency, they primarily
target shorter novels (10k--100k words), leaving ultra-long generation largely
unexplored. Drawing on insights from recent text compression methods like
LLMZip and LLM2Vec, we conduct an information-theoretic analysis that
quantifies distortion occurring when LLMs compress and reconstruct ultra-long
novels under varying compression-expansion ratios. We introduce a hierarchical
two-stage generation pipeline (outline -> detailed outline -> manuscript) and
find an optimal outline length that balances information preservation with
human effort. Through extensive experimentation with Chinese novels, we
establish that a two-stage hierarchical outline approach significantly reduces
semantic distortion compared to single-stage methods. Our findings provide
empirically-grounded guidance for authors and researchers collaborating with
LLMs to create million-word novels.",2025-05-18,"Hanwen Shen, Ting Ying",http://arxiv.org/pdf/2505.12572v1,cs.CL
Enriching Patent Claim Generation with European Patent Dataset,"Drafting patent claims is time-intensive, costly, and requires professional
skill. Therefore, researchers have investigated large language models (LLMs) to
assist inventors in writing claims. However, existing work has largely relied
on datasets from the United States Patent and Trademark Office (USPTO). To
enlarge research scope regarding various jurisdictions, drafting conventions,
and legal standards, we introduce EPD, a European patent dataset. EPD presents
rich textual data and structured metadata to support multiple patent-related
tasks, including claim generation. This dataset enriches the field in three
critical aspects: (1) Jurisdictional diversity: Patents from different offices
vary in legal and drafting conventions. EPD fills a critical gap by providing a
benchmark for European patents to enable more comprehensive evaluation. (2)
Quality improvement: EPD offers high-quality granted patents with finalized and
legally approved texts, whereas others consist of patent applications that are
unexamined or provisional. Experiments show that LLMs fine-tuned on EPD
significantly outperform those trained on previous datasets and even GPT-4o in
claim quality and cross-domain generalization. (3) Real-world simulation: We
propose a difficult subset of EPD to better reflect real-world challenges of
claim generation. Results reveal that all tested LLMs perform substantially
worse on these challenging samples, which highlights the need for future
research.",2025-05-18,"Lekang Jiang, Chengzu Li, Stephan Goetz",http://arxiv.org/pdf/2505.12568v1,cs.CL
mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model,"Despite their ability to understand chemical knowledge and accurately
generate sequential representations, large language models (LLMs) remain
limited in their capacity to propose novel molecules with drug-like properties.
In addition, the molecules that LLMs propose can often be challenging to make
in the lab. To more effectively enable the discovery of functional small
molecules, LLMs need to learn a molecular language. However, LLMs are currently
limited by encoding molecules from atoms. In this paper, we argue that just
like tokenizing texts into (sub-)word tokens instead of characters, molecules
should be decomposed and reassembled at the level of functional building
blocks, i.e., parts of molecules that bring unique functions and serve as
effective building blocks for real-world automated laboratory synthesis. This
motivates us to propose mCLM, a modular Chemical-Language Model tokenizing
molecules into building blocks and learning a bilingual language model of both
natural language descriptions of functions and molecule building blocks. By
reasoning on such functional building blocks, mCLM guarantees to generate
efficiently synthesizable molecules thanks to recent progress in block-based
chemistry, while also improving the functions of molecules in a principled
manner. In experiments on 430 FDA-approved drugs, we find mCLM capable of
significantly improving 5 out of 6 chemical functions critical to determining
drug potentials. More importantly, mCLM can reason on multiple functions and
improve the FDA-rejected drugs (``fallen angels'') over multiple iterations to
greatly improve their shortcomings.",2025-05-18,"Carl Edwards, Chi Han, Gawon Lee, Thao Nguyen, Bowen Jin, Chetan Kumar Prasad, Sara Szymkuć, Bartosz A. Grzybowski, Ying Diao, Jiawei Han, Ge Liu, Hao Peng, Martin D. Burke, Heng Ji",http://arxiv.org/pdf/2505.12565v1,cs.CL
The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations,"Existing datasets available for crosslinguistic investigations have tended to
focus on large amounts of data for a small group of languages or a small amount
of data for a large number of languages. This means that claims based on these
datasets are limited in what they reveal about universal properties of the
human language faculty. While this has begun to change through the efforts of
projects seeking to develop tagged corpora for a large number of languages,
such efforts are still constrained by limits on resources. The current paper
reports on a large automatically tagged parallel dataset which has been
developed to partially address this issue. The taggedPBC contains more than
1,800 sentences of pos-tagged parallel text data from over 1,500 languages,
representing 133 language families and 111 isolates, dwarfing previously
available resources. The accuracy of tags in this dataset is shown to correlate
well with both existing SOTA taggers for high-resource languages (SpaCy,
Trankit) as well as hand-tagged corpora (Universal Dependencies Treebanks).
Additionally, a novel measure derived from this dataset, the N1 ratio,
correlates with expert determinations of word order in three typological
databases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier
trained on this feature can accurately identify basic word order for languages
not in those databases. While much work is still needed to expand and develop
this dataset, the taggedPBC is an important step to enable corpus-based
crosslinguistic investigations, and is made available for research and
collaboration via GitHub.",2025-05-18,Hiram Ring,http://arxiv.org/pdf/2505.12560v1,cs.CL
Extracting memorized pieces of (copyrighted) books from open-weight language models,"Plaintiffs and defendants in copyright lawsuits over generative AI often make
sweeping, opposing claims about the extent to which large language models
(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial
ML and copyright law, we show that these polarized positions dramatically
oversimplify the relationship between memorization and copyright. To do so, we
leverage a recent probabilistic extraction technique to extract pieces of the
Books3 dataset from 13 open-weight LLMs. Through numerous experiments, we show
that it's possible to extract substantial parts of at least some books from
different LLMs. This is evidence that the LLMs have memorized the extracted
text; this memorized content is copied inside the model parameters. But the
results are complicated: the extent of memorization varies both by model and by
book. With our specific experiments, we find that the largest LLMs don't
memorize most books -- either in whole or in part. However, we also find that
Llama 3.1 70B memorizes some books, like Harry Potter and 1984, almost
entirely. We discuss why our results have significant implications for
copyright cases, though not ones that unambiguously favor either side.",2025-05-18,"A. Feder Cooper, Aaron Gokaslan, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley, Daniel E. Ho, Percy Liang",http://arxiv.org/pdf/2505.12546v1,cs.CL
Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models,"Predicting crash events is crucial for understanding crash distributions and
their contributing factors, thereby enabling the design of proactive traffic
safety policy interventions. However, existing methods struggle to interpret
the complex interplay among various sources of traffic crash data, including
numeric characteristics, textual reports, crash imagery, environmental
conditions, and driver behavior records. As a result, they often fail to
capture the rich semantic information and intricate interrelationships embedded
in these diverse data sources, limiting their ability to identify critical
crash risk factors. In this research, we propose TrafficSafe, a framework that
adapts LLMs to reframe crash prediction and feature attribution as text-based
reasoning. A multi-modal crash dataset including 58,903 real-world reports
together with belonged infrastructure, environmental, driver, and vehicle
information is collected and textualized into TrafficSafe Event Dataset. By
customizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves
a 42% average improvement in F1-score over baselines. To interpret these
predictions and uncover contributing factors, we introduce TrafficSafe
Attribution, a sentence-level feature attribution framework enabling
conditional risk analysis. Findings show that alcohol-impaired driving is the
leading factor in severe crashes, with aggressive and impairment-related
behaviors having nearly twice the contribution for severe crashes compared to
other driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal
features during model training, guiding strategic crash data collection for
iterative performance improvements. The proposed TrafficSafe offers a
transformative leap in traffic safety research, providing a blueprint for
translating advanced AI technologies into responsible, actionable, and
life-saving outcomes.",2025-05-18,"Yang Zhao, Pu Wang, Yibo Zhao, Hongru Du, Hao Frank Yang",http://arxiv.org/pdf/2505.12545v2,cs.CL
Disambiguation in Conversational Question Answering in the Era of LLM: A Survey,"Ambiguity remains a fundamental challenge in Natural Language Processing
(NLP) due to the inherent complexity and flexibility of human language. With
the advent of Large Language Models (LLMs), addressing ambiguity has become
even more critical due to their expanded capabilities and applications. In the
context of Conversational Question Answering (CQA), this paper explores the
definition, forms, and implications of ambiguity for language driven systems,
particularly in the context of LLMs. We define key terms and concepts,
categorize various disambiguation approaches enabled by LLMs, and provide a
comparative analysis of their advantages and disadvantages. We also explore
publicly available datasets for benchmarking ambiguity detection and resolution
techniques and highlight their relevance for ongoing research. Finally, we
identify open problems and future research directions, proposing areas for
further investigation. By offering a comprehensive review of current research
on ambiguities and disambiguation with LLMs, we aim to contribute to the
development of more robust and reliable language systems.",2025-05-18,"Md Mehrab Tanjim, Yeonjun In, Xiang Chen, Victor S. Bursztyn, Ryan A. Rossi, Sungchul Kim, Guang-Jie Ren, Vaishnavi Muppala, Shun Jiang, Yongsung Kim, Chanyoung Park",http://arxiv.org/pdf/2505.12543v1,cs.CL
Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE,"Analysing the generalisation capabilities of relation extraction (RE) models
is crucial for assessing whether they learn robust relational patterns or rely
on spurious correlations. Our cross-dataset experiments find that RE models
struggle with unseen data, even within similar domains. Notably, higher
intra-dataset performance does not indicate better transferability, instead
often signaling overfitting to dataset-specific artefacts. Our results also
show that data quality, rather than lexical similarity, is key to robust
transfer, and the choice of optimal adaptation strategy depends on the quality
of data available: while fine-tuning yields the best cross-dataset performance
with high-quality data, few-shot in-context learning (ICL) is more effective
with noisier data. However, even in these cases, zero-shot baselines
occasionally outperform all cross-dataset results. Structural issues in RE
benchmarks, such as single-relation per sample constraints and non-standardised
negative class definitions, further hinder model transferability.",2025-05-18,"Varvara Arzt, Allan Hanbury, Michael Wiegand, Gábor Recski, Terra Blevins",http://arxiv.org/pdf/2505.12533v1,cs.CL
ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents,"Large language models (LLMs) increasingly power mental-health chatbots, yet
the field still lacks a scalable, theory-grounded way to decide which model is
most effective to deploy. We present ESC-Judge, the first end-to-end evaluation
framework that (i) grounds head-to-head comparisons of emotional-support LLMs
in Clara Hill's established Exploration-Insight-Action counseling model,
providing a structured and interpretable view of performance, and (ii) fully
automates the evaluation pipeline at scale. ESC-Judge operates in three stages:
first, it synthesizes realistic help-seeker roles by sampling empirically
salient attributes such as stressors, personality, and life history; second, it
has two candidate support agents conduct separate sessions with the same role,
isolating model-specific strategies; and third, it asks a specialized judge LLM
to express pairwise preferences across rubric-anchored skills that span the
Exploration, Insight, and Action spectrum. In our study, ESC-Judge matched
PhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and
86 percent of Action decisions, demonstrating human-level reliability at a
fraction of the cost. All code, prompts, synthetic roles, transcripts, and
judgment scripts are released to promote transparent progress in emotionally
supportive AI.",2025-05-18,"Navid Madani, Rohini Srihari",http://arxiv.org/pdf/2505.12531v1,cs.CL
DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design,"Inverse Protein Folding (IPF) is a critical subtask in the field of protein
design, aiming to engineer amino acid sequences capable of folding correctly
into a specified three-dimensional (3D) conformation. Although substantial
progress has been achieved in recent years, existing methods generally rely on
either backbone coordinates or molecular surface features alone, which
restricts their ability to fully capture the complex chemical and geometric
constraints necessary for precise sequence prediction. To address this
limitation, we present DS-ProGen, a dual-structure deep language model for
functional protein design, which integrates both backbone geometry and
surface-level representations. By incorporating backbone coordinates as well as
surface chemical and geometric descriptors into a next-amino-acid prediction
paradigm, DS-ProGen is able to generate functionally relevant and structurally
stable sequences while satisfying both global and local conformational
constraints. On the PRIDE dataset, DS-ProGen attains the current
state-of-the-art recovery rate of 61.47%, demonstrating the synergistic
advantage of multi-modal structural encoding in protein design. Furthermore,
DS-ProGen excels in predicting interactions with a variety of biological
partners, including ligands, ions, and RNA, confirming its robust functional
retention capabilities.",2025-05-18,"Yanting Li, Jiyue Jiang, Zikang Wang, Ziqian Lin, Dongchen He, Yuheng Shan, Yanruisheng Shao, Jiayi Li, Xiangyu Shi, Jiuming Wang, Yanyu Chen, Yimin Fan, Han Li, Yu Li",http://arxiv.org/pdf/2505.12511v1,cs.CL
LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection,"The impressive ability of large language models to generate natural text
across various tasks has led to critical challenges in authorship
authentication. Although numerous detection methods have been developed to
differentiate between machine-generated texts (MGT) and human-generated texts
(HGT), the explainability of these methods remains a significant gap.
Traditional explainability techniques often fall short in capturing the complex
word relationships that distinguish HGT from MGT. To address this limitation,
we present LM$^2$otifs, a novel explainable framework for MGT detection.
Inspired by probabilistic graphical models, we provide a theoretical rationale
for the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks
to achieve both accurate detection and interpretability. The LM$^2$otifs
pipeline operates in three key stages: first, it transforms text into graphs
based on word co-occurrence to represent lexical dependencies; second, graph
neural networks are used for prediction; and third, a post-hoc explainability
method extracts interpretable motifs, offering multi-level explanations from
individual words to sentence structures. Extensive experiments on multiple
benchmark datasets demonstrate the comparable performance of LM$^2$otifs. The
empirical evaluation of the extracted explainable motifs confirms their
effectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis
reveals distinct and visible linguistic fingerprints characteristic of MGT.",2025-05-18,"Xu Zheng, Zhuomin Chen, Esteban Schafir, Sipeng Chen, Hojat Allah Salehi, Haifeng Chen, Farhad Shirani, Wei Cheng, Dongsheng Luo",http://arxiv.org/pdf/2505.12507v1,cs.CL
KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation,"The increasing context length of modern language models has created a need
for evaluating their ability to retrieve and process information across
extensive documents. While existing benchmarks test long-context capabilities,
they often lack a structured way to systematically vary question complexity. We
introduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a
framework that (1) extracts QA pairs at multiple complexity levels (2) by
leveraging structured representations of financial agreements (3) along three
key dimensions -- multi-hop retrieval, set operations, and answer plurality --
enabling fine-grained assessment of model performance across controlled
difficulty levels. Using this framework, we construct a dataset of 20,139 QA
pairs (the largest number among the long-context benchmarks) and open-source a
part of it. We evaluate 13 proprietary and open-source LLMs and observe that
even the best-performing models are struggling with set-based comparisons and
multi-hop logical inference. Our analysis reveals systematic failure modes tied
to semantic misinterpretation and inability to handle implicit relations.",2025-05-18,"Nikita Tatarinov, Vidhyakshaya Kannan, Haricharana Srinivasa, Arnav Raj, Harpreet Singh Anand, Varun Singh, Aditya Luthra, Ravij Lade, Agam Shah, Sudheer Chava",http://arxiv.org/pdf/2505.12495v1,cs.CL
Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering,"Recently, large language models (LLMs) have demonstrated impressive
performance in Knowledge Graph Question Answering (KGQA) tasks, which aim to
find answers based on knowledge graphs (KGs) for natural language questions.
Existing LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented
Generation (GraphRAG) paradigm, which first retrieves reasoning paths from the
large KGs, and then generates the answers based on them. However, these methods
emphasize the exploration of new optimal reasoning paths in KGs while ignoring
the exploitation of historical reasoning paths, which may lead to sub-optimal
reasoning paths. Additionally, the complex semantics contained in questions may
lead to the retrieval of inaccurate reasoning paths. To address these issues,
this paper proposes a novel and training-free framework for KGQA tasks called
Reward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original
question into a series of simpler and well-defined sub-questions to handle the
complex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided
by a reward model is introduced to iteratively retrieve weighted reasoning
paths as contextual knowledge. Finally, it stacks the weighted reasoning paths
according to their weights to generate the final answers. Extensive experiments
on four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves
8.7\% and 7.0\% performance improvement over the state-of-the-art method on the
GrailQA and the WebQSP respectively.",2025-05-18,"Xiao Long, Liansheng Zhuang, Chen Shen, Shaotian Yan, Yifei Li, Shafei Wang",http://arxiv.org/pdf/2505.12476v1,cs.CL
What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization,"In this work, we investigate the performance of LLMs on a new task that
requires combining discussion with background knowledge for summarization. This
aims to address the limitation of outside observer confusion in existing
dialogue summarization systems due to their reliance solely on discussion
information. To achieve this, we model the task output as background and
opinion summaries and define two standardized summarization patterns. To
support assessment, we introduce the first benchmark comprising high-quality
samples consistently annotated by human experts and propose a novel
hierarchical evaluation framework with fine-grained, interpretable metrics. We
evaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our
findings reveal: (1) LLMs struggle with background summary retrieval,
generation, and opinion summary integration. (2) Even top LLMs achieve less
than 69% average performance across both patterns. (3) Current LLMs lack
adequate self-evaluation and self-correction capabilities for this task.",2025-05-18,"Weixiao Zhou, Junnan Zhu, Gengyao Li, Xianfu Cheng, Xinnian Liang, Feifei Zhai, Zhoujun Li",http://arxiv.org/pdf/2505.12474v1,cs.CL
Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases,"Recent advances in artificial intelligence, particularly large language
models LLMs, have shown promising capabilities in transforming rare disease
research. This survey paper explores the integration of LLMs in the analysis of
rare diseases, highlighting significant strides and pivotal studies that
leverage textual data to uncover insights and patterns critical for diagnosis,
treatment, and patient care. While current research predominantly employs
textual data, the potential for multimodal data integration combining genetic,
imaging, and electronic health records stands as a promising frontier. We
review foundational papers that demonstrate the application of LLMs in
identifying and extracting relevant medical information, simulating intelligent
conversational agents for patient interaction, and enabling the formulation of
accurate and timely diagnoses. Furthermore, this paper discusses the challenges
and ethical considerations inherent in deploying LLMs, including data privacy,
model transparency, and the need for robust, inclusive data sets. As part of
this exploration, we present a section on experimentation that utilizes
multiple LLMs alongside structured questionnaires, specifically designed for
diagnostic purposes in the context of different diseases. We conclude with
future perspectives on the evolution of LLMs towards truly multimodal
platforms, which would integrate diverse data types to provide a more
comprehensive understanding of rare diseases, ultimately fostering better
outcomes in clinical settings.",2025-05-18,"Valentina Carbonari, Pierangelo Veltri, Pietro Hiram Guzzi",http://arxiv.org/pdf/2505.17065v1,cs.CL
UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection,"Scaling RL for LLMs is computationally expensive, largely due to
multi-sampling for policy optimization and evaluation, making efficient data
selection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,
we hypothesize LLMs learn best from data within their potential comprehension
zone. Addressing the limitation of conventional, computationally intensive
multi-sampling methods for data assessment, we introduce UFO-RL. This novel
framework uses a computationally efficient single-pass uncertainty estimation
to identify informative data instances, achieving up to 185x faster data
evaluation. UFO-RL leverages this metric to select data within the estimated
ZPD for training. Experiments show that training with just 10% of data selected
by UFO-RL yields performance comparable to or surpassing full-data training,
reducing overall training time by up to 16x while enhancing stability and
generalization. UFO-RL offers a practical and highly efficient strategy for
scaling RL fine-tuning of LLMs by focusing learning on valuable data.",2025-05-18,"Yang Zhao, Kai Xiong, Xiao Ding, Li Du, YangouOuyang, Zhouhao Sun, Jiannan Guan, Wenbin Zhang, Bin Liu, Dong Hu, Bing Qin, Ting Liu",http://arxiv.org/pdf/2505.12457v1,cs.CL
Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations,"Distantly supervised named entity recognition (DS-NER) has emerged as a cheap
and convenient alternative to traditional human annotation methods, enabling
the automatic generation of training data by aligning text with external
resources. Despite the many efforts in noise measurement methods, few works
focus on the latent noise distribution between different distant annotation
methods. In this work, we explore the effectiveness and robustness of DS-NER by
two aspects: (1) distant annotation techniques, which encompasses both
traditional rule-based methods and the innovative large language model
supervision approach, and (2) noise assessment, for which we introduce a novel
framework. This framework addresses the challenges by distinctly categorizing
them into the unlabeled-entity problem (UEP) and the noisy-entity problem
(NEP), subsequently providing specialized solutions for each. Our proposed
method achieves significant improvements on eight real-world distant
supervision datasets originating from three different data sources and
involving four distinct annotation techniques, confirming its superiority over
current state-of-the-art methods.",2025-05-18,"Yuyang Ding, Dan Qiao, Juntao Li, Jiajie Xu, Pingfu Chao, Xiaofang Zhou, Min Zhang",http://arxiv.org/pdf/2505.12454v1,cs.CL
Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment,"Large language models (LLMs) increasingly demonstrate signs of conceptual
understanding, yet much of their internal knowledge remains latent, loosely
structured, and difficult to access or evaluate. We propose self-questioning as
a lightweight and scalable strategy to improve LLMs' understanding,
particularly in domains where success depends on fine-grained semantic
distinctions. To evaluate this approach, we introduce a challenging new
benchmark of 1.3 million post-2015 computer science patent pairs, characterized
by dense technical jargon and strategically complex writing. The benchmark
centers on a pairwise differentiation task: can a model distinguish between
closely related but substantively different inventions? We show that prompting
LLMs to generate and answer their own questions - targeting the background
knowledge required for the task - significantly improves performance. These
self-generated questions and answers activate otherwise underutilized internal
knowledge. Allowing LLMs to retrieve answers from external scientific texts
further enhances performance, suggesting that model knowledge is compressed and
lacks the full richness of the training data. We also find that
chain-of-thought prompting and self-questioning converge, though
self-questioning remains more effective for improving understanding of
technical concepts. Notably, we uncover an asymmetry in prompting: smaller
models often generate more fundamental, more open-ended, better-aligned
questions for mid-sized models than large models with better understanding do,
revealing a new strategy for cross-model collaboration. Altogether, our
findings establish self-questioning as both a practical mechanism for
automatically improving LLM comprehension, especially in domains with sparse
and underrepresented knowledge, and a diagnostic probe of how internal and
external knowledge are organized.",2025-05-18,"Siyang Wu, Honglin Bao, Nadav Kunievsky, James A. Evans",http://arxiv.org/pdf/2505.12452v1,cs.CL
IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems,"The rapid advancement of Large Language Models (LLMs) has led to the
emergence of Multi-Agent Systems (MAS) to perform complex tasks through
collaboration. However, the intricate nature of MAS, including their
architecture and agent interactions, raises significant concerns regarding
intellectual property (IP) protection. In this paper, we introduce MASLEAK, a
novel attack framework designed to extract sensitive information from MAS
applications. MASLEAK targets a practical, black-box setting, where the
adversary has no prior knowledge of the MAS architecture or agent
configurations. The adversary can only interact with the MAS through its public
API, submitting attack query $q$ and observing outputs from the final agent.
Inspired by how computer worms propagate and infect vulnerable network hosts,
MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain
responses from each MAS agent that reveal a full set of proprietary components,
including the number of agents, system topology, system prompts, task
instructions, and tool usages. We construct the first synthetic dataset of MAS
applications with 810 applications and also evaluate MASLEAK against real-world
MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in
extracting MAS IP, with an average attack success rate of 87% for system
prompts and task instructions, and 92% for system architecture in most cases.
We conclude by discussing the implications of our findings and the potential
defenses.",2025-05-18,"Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, Shing-Chi Cheung",http://arxiv.org/pdf/2505.12442v2,cs.CL
Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games,"Interactive Fiction games (IF games) are where players interact through
natural language commands. While recent advances in Artificial Intelligence
agents have reignited interest in IF games as a domain for studying
decision-making, existing approaches prioritize task-specific performance
metrics over human-like comprehension of narrative context and gameplay logic.
This work presents a cognitively inspired framework that guides Large Language
Models (LLMs) to learn and play IF games systematically. Our proposed
**L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three
key components: (1) structured map building to capture spatial and narrative
relationships, (2) action learning to identify context-appropriate commands,
and (3) feedback-driven experience analysis to refine decision-making over
time. By aligning LLMs-based agents' behavior with narrative intent and
commonsense constraints, LPLH moves beyond purely exploratory strategies to
deliver more interpretable, human-like performance. Crucially, this approach
draws on cognitive science principles to more closely simulate how human
players read, interpret, and respond within narrative worlds. As a result, LPLH
reframes the IF games challenge as a learning problem for LLMs-based agents,
offering a new path toward robust, context-aware gameplay in complex text-based
environments.",2025-05-18,"Jinming Zhang, Yunfei Long",http://arxiv.org/pdf/2505.12439v1,cs.CL
PSC: Extending Context Window of Large Language Models via Phase Shift Calibration,"Rotary Position Embedding (RoPE) is an efficient position encoding approach
and is widely utilized in numerous large language models (LLMs). Recently, a
lot of methods have been put forward to further expand the context window based
on RoPE. The core concept of those methods is to predefine or search for a set
of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a
challenge for existing methods to predefine an optimal factor due to the
exponential search space. In view of this, we introduce PSC (Phase Shift
Calibration), a small module for calibrating the frequencies predefined by
existing methods. With the employment of PSC, we demonstrate that many existing
methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted
extensive experiments across multiple models and tasks. The results demonstrate
that (1) when PSC is enabled, the comparative reductions in perplexity increase
as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our
approach is broadly applicable and exhibits robustness across a variety of
models and tasks. The code can be found at https://github.com/WNQzhu/PSC.",2025-05-18,"Wenqiao Zhu, Chao Xu, Lulu Wang, Jun Wu",http://arxiv.org/pdf/2505.12423v1,cs.CL
Table-R1: Region-based Reinforcement Learning for Table Understanding,"Tables present unique challenges for language models due to their structured
row-column interactions, necessitating specialized approaches for effective
comprehension. While large language models (LLMs) have demonstrated potential
in table reasoning through prompting and techniques like chain-of-thought (CoT)
and program-of-thought (PoT), optimizing their performance for table question
answering remains underexplored. In this paper, we introduce region-based
Table-R1, a novel reinforcement learning approach that enhances LLM table
understanding by integrating region evidence into reasoning steps. Our method
employs Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in
identifying relevant table regions before generating answers, incorporating
textual, symbolic, and program-based reasoning. Additionally, Table-Aware Group
Relative Policy Optimization (TARPO) introduces a mixed reward system to
dynamically balance region accuracy and answer correctness, with decaying
region rewards and consistency penalties to align reasoning steps. Experiments
show that Table-R1 achieves an average performance improvement of 14.36 points
across multiple base models on three benchmark datasets, even outperforming
baseline models with ten times the parameters, while TARPO reduces response
token consumption by 67.5% compared to GRPO, significantly advancing LLM
capabilities in efficient tabular reasoning.",2025-05-18,"Zhenhe Wu, Jian Yang, Jiaheng Liu, Xianjie Wu, Changzai Pan, Jie Zhang, Yu Zhao, Shuangyong Song, Yongxiang Li, Zhoujun Li",http://arxiv.org/pdf/2505.12415v1,cs.CL
The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT,"Generative AI paraphrased text can be used for copyright infringement and the
AI paraphrased content can deprive substantial revenue from original content
creators. Despite this recent surge of malicious use of generative AI, there
are few academic publications that research this threat. In this article, we
demonstrate the ability of pattern-based similarity detection for AI
paraphrased news recognition. We propose an algorithmic scheme, which is not
limited to detect whether an article is an AI paraphrase, but, more
importantly, to identify that the source of infringement is the ChatGPT. The
proposed method is tested with a benchmark dataset specifically created for
this task that incorporates real articles from BBC, incorporating a total of
2,224 articles across five different news categories, as well as 2,224
paraphrased articles created with ChatGPT. Results show that our pattern
similarity-based method, that makes no use of deep learning, can detect ChatGPT
assisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for
precision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1
score.",2025-05-18,"Konstantinos Xylogiannopoulos, Petros Xanthopoulos, Panagiotis Karampelas, Georgios Bakamitsos",http://arxiv.org/pdf/2505.12405v1,cs.CL
InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data,"Finding interesting phenomena is the core of scientific discovery, but it is
a manual, ill-defined concept. We present an integrative pipeline for
automating the discovery of interesting simple hypotheses (feature-target
relations with effect direction and a potential underlying mechanism) in
structured biomedical data. The pipeline combines machine learning, knowledge
graphs, literature search and Large Language Models. We formalize
""interestingness"" as a combination of novelty, utility and plausibility. On 8
major diseases from the UK Biobank, our pipeline consistently recovers risk
factors years before their appearance in the literature. 40--53% of our top
candidates were validated as interesting, compared to 0--7% for a SHAP-based
baseline. Overall, 28% of 109 candidates were interesting to medical experts.
The pipeline addresses the challenge of operationalizing ""interestingness""
scalably and for any target. We release data and code:
https://github.com/LinialLab/InterFeat",2025-05-18,"Dan Ofer, Michal Linial, Dafna Shahaf",http://arxiv.org/pdf/2505.13534v1,cs.CL
KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection,"Fake news detection remains a challenging problem due to the complex
interplay between textual misinformation, manipulated images, and external
knowledge reasoning. While existing approaches have achieved notable results in
verifying veracity and cross-modal consistency, two key challenges persist: (1)
Existing methods often consider only the global image context while neglecting
local object-level details, and (2) they fail to incorporate external knowledge
and entity relationships for deeper semantic understanding. To address these
challenges, we propose a novel multi-modal fake news detection framework that
integrates visual, textual, and knowledge-based representations. Our approach
leverages bottom-up attention to capture fine-grained object details, CLIP for
global image semantics, and RoBERTa for context-aware text encoding. We further
enhance knowledge utilization by retrieving and adaptively selecting relevant
entities from a knowledge graph. The fused multi-modal features are processed
through a Transformer-based classifier to predict news veracity. Experimental
results demonstrate that our model outperforms recent approaches, showcasing
the effectiveness of neighbor selection mechanism and multi-modal fusion for
fake news detection. Our proposal introduces a new paradigm: knowledge-grounded
multimodal reasoning. By integrating explicit entity-level selection and
NLI-guided filtering, we shift fake news detection from feature fusion to
semantically grounded verification. For reproducibility and further research,
the source code is publicly at
\href{https://github.com/latuanvinh1998/KGAlign}{github.com/latuanvinh1998/KGAlign}.",2025-05-18,"Tuan-Vinh La, Minh-Hieu Nguyen, Minh-Son Dao",http://arxiv.org/pdf/2505.14714v1,cs.CL
Traversal Verification for Speculative Tree Decoding,"Speculative decoding is a promising approach for accelerating large language
models. The primary idea is to use a lightweight draft model to speculate the
output of the target model for multiple subsequent timesteps, and then verify
them in parallel to determine whether the drafted tokens should be accepted or
rejected. To enhance acceptance rates, existing frameworks typically construct
token trees containing multiple candidates in each timestep. However, their
reliance on token-level verification mechanisms introduces two critical
limitations: First, the probability distribution of a sequence differs from
that of individual tokens, leading to suboptimal acceptance length. Second,
current verification schemes begin from the root node and proceed layer by
layer in a top-down manner. Once a parent node is rejected, all its child nodes
should be discarded, resulting in inefficient utilization of speculative
candidates. This paper introduces Traversal Verification, a novel speculative
decoding algorithm that fundamentally rethinks the verification paradigm
through leaf-to-root traversal. Our approach considers the acceptance of the
entire token sequence from the current node to the root, and preserves
potentially valid subsequences that would be prematurely discarded by existing
methods. We theoretically prove that the probability distribution obtained
through Traversal Verification is identical to that of the target model,
guaranteeing lossless inference while achieving substantial acceleration gains.
Experimental results across different large language models and multiple tasks
show that our method consistently improves acceptance length and throughput
over existing methods",2025-05-18,"Yepeng Weng, Qiao Hu, Xujie Chen, Li Liu, Dianwen Mei, Huishi Qiu, Jiang Tian, Zhongchao Shi",http://arxiv.org/pdf/2505.12398v1,cs.CL
SLOT: Sample-specific Language Model Optimization at Test-time,"We propose SLOT (Sample-specific Language Model Optimization at Test-time), a
novel and parameter-efficient test-time inference approach that enhances a
language model's ability to more accurately respond to individual prompts.
Existing Large Language Models (LLMs) often struggle with complex instructions,
leading to poor performances on those not well represented among general
samples. To address this, SLOT conducts few optimization steps at test-time to
update a light-weight sample-specific parameter vector. It is added to the
final hidden layer before the output head, and enables efficient adaptation by
caching the last layer features during per-sample optimization. By minimizing
the cross-entropy loss on the input prompt only, SLOT helps the model better
aligned with and follow each given instruction. In experiments, we demonstrate
that our method outperforms the compared models across multiple benchmarks and
LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on
GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT
achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is
available at https://github.com/maple-research-lab/SLOT.",2025-05-18,"Yang Hu, Xingyu Zhang, Xueji Fang, Zhiyang Chen, Xiao Wang, Huatian Zhang, Guojun Qi",http://arxiv.org/pdf/2505.12392v2,cs.CL
From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling,"Current research on bias in language models (LMs) predominantly focuses on
data quality, with significantly less attention paid to model architecture and
temporal influences of data. Even more critically, few studies systematically
investigate the origins of bias. We propose a methodology grounded in
comparative behavioral theory to interpret the complex interaction between
training data and model architecture in bias propagation during language
modeling. Building on recent work that relates transformers to n-gram LMs, we
evaluate how data, model design choices, and temporal dynamics affect bias
propagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to
context window size in bias propagation, while transformers demonstrate
architectural robustness; (2) the temporal provenance of training data
significantly affects bias; and (3) different model architectures respond
differentially to controlled bias injection, with certain biases (e.g. sexual
orientation) being disproportionately amplified. As language models become
ubiquitous, our findings highlight the need for a holistic approach -- tracing
bias to its origins across both data and model dimensions, not just symptoms,
to mitigate harm.",2025-05-18,"Mohsinul Kabir, Tasfia Tahsin, Sophia Ananiadou",http://arxiv.org/pdf/2505.12381v1,cs.CL
MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks,"The rapid advancement of Large Language Models (LLMs) has stimulated interest
in multi-agent collaboration for addressing complex medical tasks. However, the
practical advantages of multi-agent collaboration approaches remain
insufficiently understood. Existing evaluations often lack generalizability,
failing to cover diverse tasks reflective of real-world clinical practice, and
frequently omit rigorous comparisons against both single-LLM-based and
established conventional methods. To address this critical gap, we introduce
MedAgentBoard, a comprehensive benchmark for the systematic evaluation of
multi-agent collaboration, single-LLM, and conventional approaches.
MedAgentBoard encompasses four diverse medical task categories: (1) medical
(visual) question answering, (2) lay summary generation, (3) structured
Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow
automation, across text, medical images, and structured EHR data. Our extensive
experiments reveal a nuanced landscape: while multi-agent collaboration
demonstrates benefits in specific scenarios, such as enhancing task
completeness in clinical workflow automation, it does not consistently
outperform advanced single LLMs (e.g., in textual medical QA) or, critically,
specialized conventional methods that generally maintain better performance in
tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital
resource and actionable insights, emphasizing the necessity of a task-specific,
evidence-based approach to selecting and developing AI solutions in medicine.
It underscores that the inherent complexity and overhead of multi-agent
collaboration must be carefully weighed against tangible performance gains. All
code, datasets, detailed prompts, and experimental results are open-sourced at
https://medagentboard.netlify.app/.",2025-05-18,"Yinghao Zhu, Ziyi He, Haoran Hu, Xiaochen Zheng, Xichen Zhang, Zixiang Wang, Junyi Gao, Liantao Ma, Lequan Yu",http://arxiv.org/pdf/2505.12371v1,cs.CL
CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement,"Prompt injection remains a major security risk for large language models.
However, the efficacy of existing guardrail models in context-aware settings
remains underexplored, as they often rely on static attack benchmarks.
Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel
context-aware benchmark assessing both attack detection and over-defense
tendencies with minimal in-domain examples. Our experiments reveal that current
prompt injection guardrail models suffer from high false negatives in
adversarial cases and excessive false positives in benign scenarios,
highlighting critical limitations.",2025-05-18,"Gauri Kholkar, Ratinder Ahuja",http://arxiv.org/pdf/2505.12368v1,cs.CL
Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts,"While Multimodal Large Language Models (MLLMs) excel at general
vision-language tasks, visuospatial cognition - reasoning about spatial
layouts, relations, and dynamics - remains a significant challenge. Existing
models often lack the necessary architectural components and specialized
training data for fine-grained spatial understanding. We introduce ViCA2
(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial
reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP
for semantics and Hiera for spatial structure, coupled with a token ratio
control mechanism for efficiency. We also developed ViCA-322K, a new
large-scale dataset with over 322,000 spatially grounded question-answer pairs
for targeted instruction tuning. On the challenging VSI-Bench benchmark, our
ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly
surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and
leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the
effectiveness of our approach in achieving strong visuospatial intelligence
with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset
to facilitate further research.",2025-05-18,"Qi Feng, Hidetoshi Shimodaira",http://arxiv.org/pdf/2505.12363v1,cs.CL
Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds,"Despite their performance, large language models (LLMs) can inadvertently
perpetuate biases found in the data they are trained on. By analyzing LLM
responses to bias-eliciting headlines, we find that these models often mirror
human biases. To address this, we explore crowd-based strategies for mitigating
bias through response aggregation. We first demonstrate that simply averaging
responses from multiple LLMs, intended to leverage the ""wisdom of the crowd"",
can exacerbate existing biases due to the limited diversity within LLM crowds.
In contrast, we show that locally weighted aggregation methods more effectively
leverage the wisdom of the LLM crowd, achieving both bias mitigation and
improved accuracy. Finally, recognizing the complementary strengths of LLMs
(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing
both significantly enhance performance and further reduce biases across ethnic
and gender-related contexts.",2025-05-18,"Axel Abels, Tom Lenaerts",http://arxiv.org/pdf/2505.12349v1,cs.CL
UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models,"Model editing aims to enhance the accuracy and reliability of large language
models (LLMs) by efficiently adjusting their internal parameters. Currently,
most LLM editing datasets are confined to narrow knowledge domains and cover a
limited range of editing evaluation. They often overlook the broad scope of
editing demands and the diversity of ripple effects resulting from edits. In
this context, we introduce UniEdit, a unified benchmark for LLM editing
grounded in open-domain knowledge. First, we construct editing samples by
selecting entities from 25 common domains across five major categories,
utilizing the extensive triple knowledge available in open-domain knowledge
graphs to ensure comprehensive coverage of the knowledge domains. To address
the issues of generality and locality in editing, we design an Neighborhood
Multi-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given
knowledge piece to entail comprehensive ripple effects to evaluate. Finally, we
employ proprietary LLMs to convert the sampled knowledge subgraphs into natural
language text, guaranteeing grammatical accuracy and syntactical diversity.
Extensive statistical analysis confirms the scale, comprehensiveness, and
diversity of our UniEdit benchmark. We conduct comprehensive experiments across
multiple LLMs and editors, analyzing their performance to highlight strengths
and weaknesses in editing across open knowledge domains and various evaluation
criteria, thereby offering valuable insights for future research endeavors.",2025-05-18,"Qizhou Chen, Dakan Wang, Taolin Zhang, Zaoming Yan, Chengsong You, Chengyu Wang, Xiaofeng He",http://arxiv.org/pdf/2505.12345v2,cs.CL
LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning,"We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which
evaluates large language models on producing fine-grained, controllable, and
interpretable reasoning processes. Systems must extract all problem conditions,
decompose a chain of thought into statement-evidence pairs, and verify the
logical validity of each pair. Leveraging only the off-the-shelf
Meta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that
first enumerates all conditions and then guides the model to label, cite, and
adjudicate every reasoning step. A lightweight post-processor based on regular
expressions normalises spans and enforces the official JSON schema. Without
fine-tuning, external retrieval, or ensembling, our method ranks 5th overall,
achieving macro F1 scores on par with substantially more complex and
resource-consuming pipelines. We conclude by analysing the strengths and
limitations of our approach and outlining directions for future research in
structural reasoning with LLMs. Our code is available at
https://github.com/asdfo123/LLMSR-asdfo123.",2025-05-18,"Xinye Li, Mingqi Wan, Dianbo Sui",http://arxiv.org/pdf/2505.12328v1,cs.CL
AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference,"Assessing Large Language Models (LLMs)' underlying value differences enables
comprehensive comparison of their misalignment, cultural adaptability, and
biases. Nevertheless, current value measurement datasets face the
informativeness challenge: with often outdated, contaminated, or generic test
questions, they can only capture the shared value orientations among different
LLMs, leading to saturated and thus uninformative results. To address this
problem, we introduce AdAEM, a novel, self-extensible assessment framework for
revealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM
can automatically and adaptively generate and extend its test questions. This
is achieved by probing the internal value boundaries of a diverse set of LLMs
developed across cultures and time periods in an in-context optimization
manner. The optimization process theoretically maximizes an
information-theoretic objective to extract the latest or culturally
controversial topics, providing more distinguishable and informative insights
about models' value differences. In this way, AdAEM is able to co-evolve with
the development of LLMs, consistently tracking their value dynamics. Using
AdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct
an extensive analysis to manifest our method's validity and effectiveness, and
benchmark the values of 16 LLMs, laying the groundwork for better value
research.",2025-05-18,"Shitong Duan, Xiaoyuan Yi, Peng Zhang, Dongkuan Xu, Jing Yao, Tun Lu, Ning Gu, Xing Xie",http://arxiv.org/pdf/2505.13531v1,cs.CL
ExpertSteer: Intervening in LLMs through Expert Knowledge,"Large Language Models (LLMs) exhibit remarkable capabilities across various
tasks, yet guiding them to follow desired behaviours during inference remains a
significant challenge. Activation steering offers a promising method to control
the generation process of LLMs by modifying their internal activations.
However, existing methods commonly intervene in the model's behaviour using
steering vectors generated by the model itself, which constrains their
effectiveness to that specific model and excludes the possibility of leveraging
powerful external expert models for steering. To address these limitations, we
propose ExpertSteer, a novel approach that leverages arbitrary specialized
expert models to generate steering vectors, enabling intervention in any LLMs.
ExpertSteer transfers the knowledge from an expert model to a target LLM
through a cohesive four-step process: first aligning representation dimensions
with auto-encoders to enable cross-model transfer, then identifying
intervention layer pairs based on mutual information analysis, next generating
steering vectors from the expert model using Recursive Feature Machines, and
finally applying these vectors on the identified layers during inference to
selectively guide the target LLM without updating model parameters. We conduct
comprehensive experiments using three LLMs on 15 popular benchmarks across four
distinct domains. Experiments demonstrate that ExpertSteer significantly
outperforms established baselines across diverse tasks at minimal cost.",2025-05-18,"Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch",http://arxiv.org/pdf/2505.12313v1,cs.CL
Visuospatial Cognitive Assistant,"Video-based spatial cognition is vital for robotics and embodied AI but
challenges current Vision-Language Models (VLMs). This paper makes two key
contributions. First, we introduce ViCA (Visuospatial Cognitive
Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor
videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D
metadata-grounded queries and video-based complex reasoning. Second, we develop
ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all
eight VSI-Bench tasks, outperforming existing models, including larger ones
(e.g., +26.1 on Absolute Distance). For interpretability, we present
ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune
ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial
reasoning. Our work highlights the importance of targeted data and suggests
paths for improved temporal-spatial modeling. We release all resources to
foster research in robust visuospatial intelligence.",2025-05-18,"Qi Feng, Hidetoshi Shimodaira",http://arxiv.org/pdf/2505.12312v1,cs.CL
LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?,"Recent advances in Large Multimodal Models (LMMs) have significantly improved
their reasoning and Optical Character Recognition (OCR) capabilities. However,
their performance on complex logical reasoning tasks involving text-rich images
remains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark
comprising 1,100 multiple-choice questions designed to evaluate LMMs' logical
reasoning abilities on text-rich images, while minimizing reliance on
domain-specific knowledge (e.g., mathematics). We construct LogicOCR by
curating a text corpus from the Chinese National Civil Servant Examination and
develop a scalable, automated pipeline to convert it into multimodal samples.
First, we design prompt templates to steer GPT-Image-1 to generate images with
diverse backgrounds, interleaved text-illustration layouts, and varied fonts,
ensuring contextual relevance and visual realism. Then, the generated images
are manually verified, with low-quality examples discarded. We evaluate a range
of representative open-source and proprietary LMMs under both Chain-of-Thought
(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key
insights, such as the impact of test-time scaling, input modality differences,
and sensitivity to visual-text orientation. Notably, LMMs still lag in
multimodal reasoning compared to text-only inputs, indicating that they have
not fully bridged visual reading with reasoning. We hope LogicOCR will serve as
a valuable resource for advancing multimodal reasoning research. The dataset is
available at https://github.com/MiliLab/LogicOCR.",2025-05-18,"Maoyuan Ye, Jing Zhang, Juhua Liu, Bo Du, Dacheng Tao",http://arxiv.org/pdf/2505.12307v1,cs.CL
Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection,"Despite significant advances in large language models (LLMs), their knowledge
memorization capabilities remain underexplored, due to the lack of standardized
and high-quality test ground. In this paper, we introduce a novel, real-world
and large-scale knowledge injection benchmark that evolves continuously over
time without requiring human intervention. Specifically, we propose WikiDYK,
which leverages recently-added and human-written facts from Wikipedia's ""Did
You Know..."" entries. These entries are carefully selected by expert Wikipedia
editors based on criteria such as verifiability and clarity. Each entry is
converted into multiple question-answer pairs spanning diverse task formats
from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290
facts and 77,180 questions, which is also seamlessly extensible with future
updates from Wikipedia editors. Extensive experiments using continued
pre-training reveal a surprising insight: despite their prevalence in modern
LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge
memorization capabilities compared to Bidirectional Language Models (BiLMs),
exhibiting a 23% lower accuracy in terms of reliability. To compensate for the
smaller scales of current BiLMs, we introduce a modular collaborative framework
utilizing ensembles of BiLMs as external knowledge repositories to integrate
with LLMs. Experiment shows that our framework further improves the reliability
accuracy by up to 29.1%.",2025-05-18,"Yuwei Zhang, Wenhao Yu, Shangbin Feng, Yifan Zhu, Letian Peng, Jayanth Srinivasa, Gaowen Liu, Jingbo Shang",http://arxiv.org/pdf/2505.12306v1,cs.CL
Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge,"LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,
offering significant efficiency and flexibility compared to human judgments.
However, previous methods primarily rely on single-point evaluations,
overlooking the inherent diversity and uncertainty in human evaluations. This
approach leads to information loss and decreases the reliability of
evaluations. To address this limitation, we propose a novel training framework
that explicitly aligns the LLM-generated judgment distribution with empirical
human distributions. Specifically, we propose a distributional alignment
objective based on KL divergence, combined with an auxiliary cross-entropy
regularization to stabilize the training process. Furthermore, considering that
empirical distributions may derive from limited human annotations, we
incorporate adversarial training to enhance model robustness against
distribution perturbations. Extensive experiments across various LLM backbones
and evaluation tasks demonstrate that our framework significantly outperforms
existing closed-source LLMs and conventional single-point alignment methods,
with improved alignment quality, evaluation accuracy, and robustness.",2025-05-18,"Luyu Chen, Zeyu Zhang, Haoran Tan, Quanyu Dai, Hao Yang, Zhenhua Dong, Xu Chen",http://arxiv.org/pdf/2505.12301v1,cs.CL
HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models,"Fine-tuning large language models (LLMs) on a mixture of diverse datasets
poses challenges due to data imbalance and heterogeneity. Existing methods
often address these issues across datasets (globally) but overlook the
imbalance and heterogeneity within individual datasets (locally), which limits
their effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a
novel method that enables LLMs to autonomously adjust data allocation during
fine-tuning both across datasets (globally) and within each individual dataset
(locally). HBO employs a bilevel optimization strategy with two types of
actors: a Global Actor, which balances data sampling across different subsets
of the training mixture, and several Local Actors, which optimizes data usage
within each subset based on difficulty levels. These actors are guided by
reward functions derived from the LLM's training state, which measure learning
progress and relative performance improvement. We evaluate HBO on three LLM
backbones across nine diverse tasks in multilingual and multitask setups.
Results show that HBO consistently outperforms existing baselines, achieving
significant accuracy gains. Our in-depth analysis further demonstrates that
both the global actor and local actors of HBO effectively adjust data usage
during fine-tuning. HBO provides a comprehensive solution to the challenges of
data imbalance and heterogeneity in LLM fine-tuning, enabling more effective
training across diverse datasets.",2025-05-18,"Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch",http://arxiv.org/pdf/2505.12300v1,cs.CL
Enhance Mobile Agents Thinking Process Via Iterative Preference Learning,"The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to
improve the reasoning performance of VLM-based mobile agents in GUI tasks.
However, the scarcity of diverse CoaT trajectories limits the expressiveness
and generalization ability of such agents. While self-training is commonly
employed to address data scarcity, existing approaches either overlook the
correctness of intermediate reasoning steps or depend on expensive
process-level annotations to construct process reward models (PRM). To address
the above problems, we propose an Iterative Preference Learning (IPL) that
constructs a CoaT-tree through interative sampling, scores leaf nodes using
rule-based reward, and backpropagates feedback to derive Thinking-level Direct
Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up
supervised fine-tuning, we further introduce a three-stage instruction
evolution, which leverages GPT-4o to generate diverse Q\&A pairs based on real
mobile UI screenshots, enhancing both generality and layout understanding.
Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our
agent MobileIPL outperforms strong baselines, including continual pretraining
models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance
across three standard Mobile GUI-Agents benchmarks and shows strong
generalization to out-of-domain scenarios.",2025-05-18,"Kun Huang, Weikai Xu, Yuxuan Liu, Quandong Wang, Pengzhi Gao, Wei Liu, Jian Luan, Bin Wang, Bo An",http://arxiv.org/pdf/2505.12299v1,cs.CL
The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models,"Large language models (LLMs) have seen widespread applications across various
domains, yet remain vulnerable to adversarial prompt injections. While most
existing research on jailbreak attacks and hallucination phenomena has focused
primarily on open-source models, we investigate the frontier of closed-source
LLMs under multilingual attack scenarios. We present a first-of-its-kind
integrated adversarial framework that leverages diverse attack techniques to
systematically evaluate frontier proprietary solutions, including GPT-4o,
DeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories
of security contents in both English and Chinese, generating 38,400 responses
across 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as
the quantitative metric to assess performance from three dimensions: prompt
design, model architecture, and language environment. Our findings suggest that
Qwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.
Notably, prompts in Chinese consistently yield higher ASRs than their English
counterparts, and our novel Two-Sides attack technique proves to be the most
effective across all models. This work highlights a dire need for
language-aware alignment and robust cross-lingual defenses in LLMs, and we hope
it will inspire researchers, developers, and policymakers toward more robust
and inclusive AI systems.",2025-05-18,"Linghan Huang, Haolin Jin, Zhaoge Bi, Pengyue Yang, Peizhou Zhao, Taozhao Chen, Xiongfei Wu, Lei Ma, Huaming Chen",http://arxiv.org/pdf/2505.12287v1,cs.CL
Efficient RL Training for Reasoning Models via Length-Aware Optimization,"Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated
remarkable performance on reasoning tasks but often incur a long reasoning path
with significant memory and time costs. Existing methods primarily aim to
shorten reasoning paths by introducing additional training data and stages. In
this paper, we propose three critical reward designs integrated directly into
the reinforcement learning process of large reasoning models, which reduce the
response length without extra training stages. Experiments on four settings
show that our method significantly decreases response length while maintaining
or even improving performance. Specifically, in a logic reasoning setting, we
achieve a 40% reduction in response length averaged by steps alongside a 14%
gain in performance. For math problems, we reduce response length averaged by
steps by 33% while preserving performance.",2025-05-18,"Danlong Yuan, Tian Xie, Shaohan Huang, Zhuocheng Gong, Huishuai Zhang, Chong Luo, Furu Wei, Dongyan Zhao",http://arxiv.org/pdf/2505.12284v1,cs.CL
BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs,"Recent advances in Large Reasoning Models (LRMs) have shown impressive
capabilities in mathematical and logical reasoning. However, current LRMs
rarely admit ignorance or respond with ""I don't know"". Instead, they often
produce incorrect answers while showing undue confidence, raising concerns
about their factual reliability. In this work, we identify two pathological
reasoning patterns characterized by overthinking that contribute to the
overconfident and incorrect answers: last-minute guessing and second-thought
spiraling. To address these issues, we propose BARREL-a novel framework that
promotes concise and boundary-aware factual reasoning. Our experiments show
that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B
from 39.33% to 61.48%, while still achieving accuracy comparable to models
finetuned on reasoning data generated by R1. These results demonstrate that our
pilot study is inspiring to build more reliable and factual System 2 LRMs.",2025-05-18,"Junxiao Yang, Jinzhe Tu, Haoran Liu, Xiaoce Wang, Chujie Zheng, Zhexin Zhang, Shiyao Cui, Caishun Chen, Tiantian He, Hongning Wang, Yew-Soon Ong, Minlie Huang",http://arxiv.org/pdf/2505.13529v1,cs.CL
LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark,"Evaluating machine translation (MT) for low-resource languages poses a
persistent challenge, primarily due to the limited availability of high quality
reference translations. This issue is further exacerbated in languages with
multiple dialects, where linguistic diversity and data scarcity hinder robust
evaluation. Large Language Models (LLMs) present a promising solution through
reference-free evaluation techniques; however, their effectiveness diminishes
in the absence of dialect-specific context and tailored guidance. In this work,
we propose a comprehensive framework that enhances LLM-based MT evaluation
using a dialect guided approach. We extend the ONUBAD dataset by incorporating
Sylheti-English sentence pairs, corresponding machine translations, and Direct
Assessment (DA) scores annotated by native speakers. To address the vocabulary
gap, we augment the tokenizer vocabulary with dialect-specific terms. We
further introduce a regression head to enable scalar score prediction and
design a dialect-guided (DG) prompting strategy. Our evaluation across multiple
LLMs shows that the proposed pipeline consistently outperforms existing
methods, achieving the highest gain of +0.1083 in Spearman correlation, along
with improvements across other evaluation settings. The dataset and the code
are available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage.",2025-05-18,"Md. Atiqur Rahman, Sabrina Islam, Mushfiqul Haque Omi",http://arxiv.org/pdf/2505.12273v1,cs.CL
Vague Knowledge: Evidence from Analyst Reports,"People in the real world often possess vague knowledge of future payoffs, for
which quantification is not feasible or desirable. We argue that language, with
differing ability to convey vague information, plays an important but
less-known role in representing subjective expectations. Empirically, we find
that in their reports, analysts include useful information in linguistic
expressions but not numerical forecasts. Specifically, the textual tone of
analyst reports has predictive power for forecast errors and subsequent
revisions in numerical forecasts, and this relation becomes stronger when
analyst's language is vaguer, when uncertainty is higher, and when analysts are
busier. Overall, our theory and evidence suggest that some useful information
is vaguely known and only communicated through language.",2025-05-18,"Kerry Xiao, Amy Zang",http://arxiv.org/pdf/2505.12269v3,cs.CL
$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks,"Understanding which neural components drive specific capabilities in
mid-sized language models ($\leq$10B parameters) remains a key challenge. We
introduce the $(\bm{K}, \epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),
a methodology to identify minimal sets of attention heads crucial for
classification tasks as well as Search-K-MSHC, an efficient algorithm for
discovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,
we analyze three syntactic task families: grammar acceptability, arithmetic
verification, and arithmetic word problems. Our findings reveal distinct
task-specific head circuits, with grammar tasks predominantly utilizing early
layers, word problems showing pronounced activity in both shallow and deep
regions, and arithmetic verification demonstrating a more distributed pattern
across the network. We discover non-linear circuit overlap patterns, where
different task pairs share computational components at varying levels of
importance. While grammar and arithmetic share many ""weak"" heads, arithmetic
and word problems share more consistently critical ""strong"" heads. Importantly,
we find that each task maintains dedicated ""super-heads"" with minimal
cross-task overlap, suggesting that syntactic and numerical competencies emerge
from specialized yet partially reusable head circuits.",2025-05-18,Pratim Chowdhary,http://arxiv.org/pdf/2505.12268v1,cs.CL
Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation,"Hallucination, the generation of factually incorrect information, remains a
significant challenge for large language models (LLMs), especially in
open-domain long-form generation. Existing approaches for detecting
hallucination in long-form tasks either focus on limited domains or rely
heavily on external fact-checking tools, which may not always be available.
  In this work, we systematically investigate reference-free hallucination
detection in open-domain long-form responses. Our findings reveal that internal
states (e.g., model's output probability and entropy) alone are insufficient
for reliably (i.e., better than random guessing) distinguishing between factual
and hallucinated content. To enhance detection, we explore various existing
approaches, including prompting-based methods, probing, and fine-tuning, with
fine-tuning proving the most effective. To further improve the accuracy, we
introduce a new paradigm, named RATE-FT, that augments fine-tuning with an
auxiliary task for the model to jointly learn with the main task of
hallucination detection. With extensive experiments and analysis using a
variety of model families & datasets, we demonstrate the effectiveness and
generalizability of our method, e.g., +3% over general fine-tuning methods on
LongFact.",2025-05-18,"Chengwei Qin, Wenxuan Zhou, Karthik Abinav Sankararaman, Nanshu Wang, Tengyu Xu, Alexander Radovic, Eryk Helenowski, Arya Talebzadeh, Aditya Tayade, Sinong Wang, Shafiq Joty, Han Fang, Hao Ma",http://arxiv.org/pdf/2505.12265v1,cs.CL
LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference,"Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode
queries and documents into low-dimensional dense or high-dimensional sparse
vectors. It retrieves documents relevant to search queries based on vector
similarities. Documents are pre-encoded offline, while queries arrive in
real-time, necessitating an efficient online query encoder. Although LLMs
significantly enhance retrieval capabilities, serving deeply parameterized LLMs
slows down query inference throughput and increases demands for online
deployment resources. In this paper, we propose LightRetriever, a novel
LLM-based hybrid retriever with extremely lightweight query encoders. Our
method retains a full-sized LLM for document encoding, but reduces the workload
of query encoding to no more than an embedding lookup. Compared to serving a
full-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for
query inference with GPU acceleration, and even a 20x speedup without GPU.
Experiments on large-scale retrieval benchmarks demonstrate that our method
generalizes well across diverse retrieval tasks, retaining an average of 95%
full-sized performance.",2025-05-18,"Guangyuan Ma, Yongliang Ma, Xuanrui Gou, Zhenpeng Su, Ming Zhou, Songlin Hu",http://arxiv.org/pdf/2505.12260v1,cs.CL
Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches,"Recent progress in large language models (LLMs) has outpaced the development
of effective evaluation methods. Traditional benchmarks rely on task-specific
metrics and static datasets, which often suffer from fairness issues, limited
scalability, and contamination risks. In this paper, we introduce Teach2Eval,
an indirect evaluation framework inspired by the Feynman Technique. Instead of
directly testing LLMs on predefined tasks, our method evaluates a model's
multiple abilities to teach weaker student models to perform tasks effectively.
By converting open-ended tasks into standardized multiple-choice questions
(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,
automated, and multi-dimensional assessment. Our approach not only avoids data
leakage and memorization but also captures a broad range of cognitive abilities
that are orthogonal to current benchmarks. Experimental results across 26
leading LLMs show strong alignment with existing human and model-based dynamic
rankings, while offering additional interpretability for training guidance.",2025-05-18,"Yuhang Zhou, Xutian Chen, Yixin Cao, Yuchen Ni, Yu He, Siyu Tian, Xiang Liu, Jian Zhang, Chuanjun Ji, Guangnan Ye, Xipeng Qiu",http://arxiv.org/pdf/2505.12259v1,cs.CL
Not All Documents Are What You Need for Extracting Instruction Tuning Data,"Instruction tuning improves the performance of large language models (LLMs),
but it heavily relies on high-quality training data. Recently, LLMs have been
used to synthesize instruction data using seed question-answer (QA) pairs.
However, these synthesized instructions often lack diversity and tend to be
similar to the input seeds, limiting their applicability in real-world
scenarios. To address this, we propose extracting instruction tuning data from
web corpora that contain rich and diverse knowledge. A naive solution is to
retrieve domain-specific documents and extract all QA pairs from them, but this
faces two key challenges: (1) extracting all QA pairs using LLMs is
prohibitively expensive, and (2) many extracted QA pairs may be irrelevant to
the downstream tasks, potentially degrading model performance. To tackle these
issues, we introduce EQUAL, an effective and scalable data extraction framework
that iteratively alternates between document selection and high-quality QA pair
extraction to enhance instruction tuning. EQUAL first clusters the document
corpus based on embeddings derived from contrastive learning, then uses a
multi-armed bandit strategy to efficiently identify clusters that are likely to
contain valuable QA pairs. This iterative approach significantly reduces
computational cost while boosting model performance. Experiments on
AutoMathText and StackOverflow across four downstream tasks show that EQUAL
reduces computational costs by 5-10x and improves accuracy by 2.5 percent on
LLaMA-3.1-8B and Mistral-7B",2025-05-18,"Chi Zhang, Huaping Zhong, Hongtao Li, Chengliang Chai, Jiawei Hong, Yuhao Deng, Jiacheng Wang, Tian Tan, Yizhou Yan, Jiantao Qiu, Ye Yuan, Guoren Wang, Conghui He, Lei Cao",http://arxiv.org/pdf/2505.12250v1,cs.CL
Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce,"Autoregressive neural language models (LMs) generate a probability
distribution over tokens at each time step given a prompt. In this work, we
attempt to systematically understand the probability distributions that LMs can
produce, showing that some distributions are significantly harder to elicit
than others. Specifically, for any target next-token distribution over the
vocabulary, we attempt to find a prompt that induces the LM to output a
distribution as close as possible to the target, using either soft or hard
gradient-based prompt tuning. We find that (1) in general, distributions with
very low or very high entropy are easier to approximate than those with
moderate entropy; (2) among distributions with the same entropy, those
containing ''outlier tokens'' are easier to approximate; (3) target
distributions generated by LMs -- even LMs with different tokenizers -- are
easier to approximate than randomly chosen targets. These results offer
insights into the expressiveness of LMs and the challenges of using them as
probability distribution proposers.",2025-05-18,"Haojin Wang, Zining Zhu, Freda Shi",http://arxiv.org/pdf/2505.12244v1,cs.CL
Synthetic Data RL: Task Definition Is All You Need,"Reinforcement learning (RL) is a powerful way to adapt foundation models to
specialized tasks, but its reliance on large-scale human-labeled data limits
broad adoption. We introduce Synthetic Data RL, a simple and general framework
that reinforcement fine-tunes models using only synthetic data generated from a
task definition. Our method first generates question and answer pairs from the
task definition and retrieved documents, then adapts the difficulty of the
question based on model solvability, and selects questions using the average
pass rate of the model across samples for RL training. On Qwen-2.5-7B, our
method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9
pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on
GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA
(finance). It surpasses supervised fine-tuning under the same data budget and
nearly matches RL with full human data across datasets (e.g., +17.2 pp on
GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only
by 0.4 pp, showing a limited added value. By reducing human data annotation,
Synthetic Data RL enables scalable and efficient RL-based model adaptation.
Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",2025-05-18,"Yiduo Guo, Zhen Guo, Chuanwei Huang, Zi-Ang Wang, Zekai Zhang, Haofei Yu, Huishuai Zhang, Yikang Shen",http://arxiv.org/pdf/2505.17063v1,cs.CL
PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs,"The memorization of sensitive and personally identifiable information (PII)
by large language models (LLMs) poses growing privacy risks as models scale and
are increasingly deployed in real-world applications. Existing efforts to study
sensitive and PII data memorization and develop mitigation strategies are
hampered by the absence of comprehensive, realistic, and ethically sourced
datasets reflecting the diversity of sensitive information found on the web. We
introduce PANORAMA - Profile-based Assemblage for Naturalistic Online
Representation and Attribute Memorization Analysis, a large-scale synthetic
corpus of 384,789 samples derived from 9,674 synthetic profiles designed to
closely emulate the distribution, variety, and context of PII and sensitive
data as it naturally occurs in online environments. Our data generation
pipeline begins with the construction of internally consistent, multi-attribute
human profiles using constrained selection to reflect real-world demographics
such as education, health attributes, financial status, etc. Using a
combination of zero-shot prompting and OpenAI o3-mini, we generate diverse
content types - including wiki-style articles, social media posts, forum
discussions, online reviews, comments, and marketplace listings - each
embedding realistic, contextually appropriate PII and other sensitive
information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B
model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and
measure PII memorization rates - revealing not only consistent increases with
repetition but also variation across content types, highlighting PANORAMA's
ability to model how memorization risks differ by context. Our dataset and code
are publicly available, providing a much-needed resource for privacy risk
assessment, model auditing, and the development of privacy-preserving LLMs.",2025-05-18,"Sriram Selvam, Anneswa Ghosh",http://arxiv.org/pdf/2505.12238v1,cs.CL
Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training,"Few-Shot Relation Extraction (FSRE) remains a challenging task due to the
scarcity of annotated data and the limited generalization capabilities of
existing models. Although large language models (LLMs) have demonstrated
potential in FSRE through in-context learning (ICL), their general-purpose
training objectives often result in suboptimal performance for task-specific
relation extraction. To overcome these challenges, we propose TKRE (Two-Stage
Knowledge-Guided Pre-training for Relation Extraction), a novel framework that
synergistically integrates LLMs with traditional relation extraction models,
bridging generative and discriminative learning paradigms. TKRE introduces two
key innovations: (1) leveraging LLMs to generate explanation-driven knowledge
and schema-constrained synthetic data, addressing the issue of data scarcity;
and (2) a two-stage pre-training strategy combining Masked Span Language
Modeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational
reasoning and generalization. Together, these components enable TKRE to
effectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets
demonstrate the efficacy of TKRE, achieving new state-of-the-art performance in
FSRE and underscoring its potential for broader application in low-resource
scenarios. \footnote{The code and data are released on
https://github.com/UESTC-GQJ/TKRE.",2025-05-18,"Quanjiang Guo, Jinchuan Zhang, Sijie Wang, Ling Tian, Zhao Kang, Bin Yan, Weidong Xiao",http://arxiv.org/pdf/2505.12236v1,cs.CL
Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression,"Despite substantial advancements in aligning large language models (LLMs)
with human values, current safety mechanisms remain susceptible to jailbreak
attacks. We hypothesize that this vulnerability stems from distributional
discrepancies between alignment-oriented prompts and malicious prompts. To
investigate this, we introduce LogiBreak, a novel and universal black-box
jailbreak method that leverages logical expression translation to circumvent
LLM safety systems. By converting harmful natural language prompts into formal
logical expressions, LogiBreak exploits the distributional gap between
alignment data and logic-based inputs, preserving the underlying semantic
intent and readability while evading safety constraints. We evaluate LogiBreak
on a multilingual jailbreak dataset spanning three languages, demonstrating its
effectiveness across various evaluation settings and linguistic contexts.",2025-05-18,"Jingyu Peng, Maolin Wang, Nan Wang, Xiangyu Zhao, Jiatong Li, Kai Zhang, Qi Liu",http://arxiv.org/pdf/2505.13527v1,cs.CL
Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling,"High-quality reward models are crucial for unlocking the reasoning potential
of large language models (LLMs), with best-of-N voting demonstrating
significant performance gains. However, current reward models, which typically
operate on the textual output of LLMs, are computationally expensive and
parameter-heavy, limiting their real-world applications. We introduce the
Efficient Linear Hidden State Reward (ELHSR) model - a novel, highly
parameter-efficient approach that leverages the rich information embedded in
LLM hidden states to address these issues. ELHSR systematically outperform
baselines with less than 0.005% of the parameters of baselines, requiring only
a few samples for training. ELHSR also achieves orders-of-magnitude efficiency
improvement with significantly less time and fewer FLOPs per sample than
baseline reward models. Moreover, ELHSR exhibits robust performance even when
trained only on logits, extending its applicability to some closed-source LLMs.
In addition, ELHSR can also be combined with traditional reward models to
achieve additional performance gains.",2025-05-18,"Jizhou Guo, Zhaomin Wu, Philip S. Yu",http://arxiv.org/pdf/2505.12225v1,cs.CL
Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers,"Large Language Models (LLMs), such as ChatGPT, have prompted academic
concerns about their impact on academic writing. Existing studies have
primarily examined LLM usage in academic writing through quantitative
approaches, such as word frequency statistics and probability-based analyses.
However, few have systematically examined the potential impact of LLMs on the
linguistic characteristics of academic writing. To address this gap, we
conducted a large-scale analysis across 823,798 abstracts published in last
decade from arXiv dataset. Through the linguistic analysis of features such as
the frequency of LLM-preferred words, lexical complexity, syntactic complexity,
cohesion, readability and sentiment, the results indicate a significant
increase in the proportion of LLM-preferred words in abstracts, revealing the
widespread influence of LLMs on academic writing. Additionally, we observed an
increase in lexical complexity and sentiment in the abstracts, but a decrease
in syntactic complexity, suggesting that LLMs introduce more new vocabulary and
simplify sentence structure. However, the significant decrease in cohesion and
readability indicates that abstracts have fewer connecting words and are
becoming more difficult to read. Moreover, our analysis reveals that scholars
with weaker English proficiency were more likely to use the LLMs for academic
writing, and focused on improving the overall logic and fluency of the
abstracts. Finally, at discipline level, we found that scholars in Computer
Science showed more pronounced changes in writing style, while the changes in
Mathematics were minimal.",2025-05-18,"Tong Bao, Yi Zhao, Jin Mao, Chengzhi Zhang",http://arxiv.org/pdf/2505.12218v1,cs.CL
One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models,"Existing pruning methods for large language models (LLMs) focus on achieving
high compression rates while maintaining model performance. Although these
methods have demonstrated satisfactory performance in handling a single user's
compression request, their processing time increases linearly with the number
of requests, making them inefficient for real-world scenarios with multiple
simultaneous requests. To address this limitation, we propose a Univeral Model
for Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that
learns to map arbitrary requests to their optimal pruning strategy. The
challenge in training StratNet lies in the high computational cost of
evaluating pruning strategies and the non-differentiable nature of the pruning
process, which hinders gradient backpropagation for StratNet updates. To
overcome these challenges, we leverage a Gaussian process to approximate the
evaluation process. Since the gradient of the Gaussian process is computable,
we can use it to approximate the gradient of the non-differentiable pruning
process, thereby enabling StratNet updates. Experimental results show that
UniCuCo is 28 times faster than baselines in processing 64 requests, while
maintaining comparable accuracy to baselines.",2025-05-18,"Rongguang Ye, Ming Tang",http://arxiv.org/pdf/2505.12216v2,cs.CL
GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment,"Large language models (LLMs) have achieved impressive performance in a
variety of natural language processing (NLP) tasks. However, when applied to
long-context scenarios, they face two challenges, i.e., low computational
efficiency and much redundant information. This paper introduces GMSA, a
context compression framework based on the encoder-decoder architecture, which
addresses these challenges by reducing input sequence length and redundant
information. Structurally, GMSA has two key components: Group Merging and Layer
Semantic Alignment (LSA). Group merging is used to effectively and efficiently
extract summary vectors from the original context. Layer semantic alignment, on
the other hand, aligns the high-level summary vectors with the low-level
primary input semantics, thus bridging the semantic gap between different
layers. In the training process, GMSA first learns soft tokens that contain
complete semantics through autoencoder training. To furtherly adapt GMSA to
downstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract
knowledge from the soft tokens for downstream tasks. We train GMSA by randomly
sampling the compression rate for each sample in the dataset. Under this
condition, GMSA not only significantly outperforms the traditional compression
paradigm in context restoration but also achieves stable and significantly
faster convergence with only a few encoder layers. In downstream
question-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in
end-to-end inference while outperforming both the original input prompts and
various state-of-the-art (SOTA) methods by a large margin.",2025-05-18,"Jiwei Tang, Zhicheng Zhang, Shunlong Wu, Jingheng Ye, Lichen Bai, Zitai Wang, Tingwei Lu, Jiaqi Chen, Lin Hai, Hai-Tao Zheng, Hong-Gee Kim",http://arxiv.org/pdf/2505.12215v1,cs.CL
Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning,"Fine-tuning large language models (LLMs) on task-specific data is essential
for their effective deployment. As dataset sizes grow, efficiently selecting
optimal subsets for training becomes crucial to balancing performance and
computational costs. Traditional data selection methods often require
fine-tuning a scoring model on the target dataset, which is time-consuming and
resource-intensive, or rely on heuristics that fail to fully leverage the
model's predictive capabilities. To address these challenges, we propose Data
Whisperer, an efficient, training-free, attention-based method that leverages
few-shot in-context learning with the model to be fine-tuned. Comprehensive
evaluations were conducted on both raw and synthetic datasets across diverse
tasks and models. Notably, Data Whisperer achieves superior performance
compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just
10% of the data, and outperforms existing methods with a 3.1-point improvement
and a 7.4$\times$ speedup.",2025-05-18,"Shaobo Wang, Xiangqi Jin, Ziming Wang, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, Linfeng Zhang",http://arxiv.org/pdf/2505.12212v2,cs.CL
How Reliable is Multilingual LLM-as-a-Judge?,"LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced
large language models assess generation results in alignment with human
instructions. While these models serve as a promising alternative to human
annotators, their reliability in multilingual evaluation remains uncertain. To
bridge this gap, we conduct a comprehensive analysis of multilingual
LLM-as-a-Judge. Specifically, we evaluate five models from different model
families across five diverse tasks involving 25 languages. Our findings reveal
that LLMs struggle to achieve consistent judgment results across languages,
with an average Fleiss' Kappa of approximately 0.3, and some models performing
even worse. To investigate the cause of inconsistency, we analyze various
influencing factors. We observe that consistency varies significantly across
languages, with particularly poor performance in low-resource languages.
Additionally, we find that neither training on multilingual data nor increasing
model scale directly improves judgment consistency. These findings suggest that
LLMs are not yet reliable for evaluating multilingual predictions. We finally
propose an ensemble strategy which improves the consistency of the multilingual
judge in real-world applications.",2025-05-18,"Xiyan Fu, Wei Liu",http://arxiv.org/pdf/2505.12201v1,cs.CL
Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled,"The impressive linguistic abilities of large language models (LLMs) have
recommended them as models of human sentence processing, with some conjecturing
a positive 'quality-power' relationship (Wilcox et al., 2023), in which
language models' (LMs') fit to psychometric data continues to improve as their
ability to predict words in context increases. This is important because it
suggests that elements of LLM architecture, such as veridical attention to
context and a unique objective of predicting upcoming words, reflect the
architecture of the human sentence processing faculty, and that any
inadequacies in predicting human reading time and brain imaging data may be
attributed to insufficient model complexity, which recedes as larger models
become available. Recent studies (Oh and Schuler, 2023) have shown this scaling
inverts after a point, as LMs become excessively large and accurate, when word
prediction probability (as information-theoretic surprisal) is used as a
predictor. Other studies propose the use of entire vectors from differently
sized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting
doubt on the value of surprisal as a predictor, but do not control for the
larger number of predictors in vectors from larger LMs. This study evaluates
LLM scaling using entire LLM vectors, while controlling for the larger number
of predictors in vectors from larger LLMs. Results show that inverse scaling
obtains, suggesting that inadequacies in predicting human reading time and
brain imaging data may be due to substantial misalignment between LLMs and
human sentence processing, which worsens as larger models are used.",2025-05-18,"Yi-Chien Lin, Hongao Zhu, William Schuler",http://arxiv.org/pdf/2505.12196v1,cs.CL
Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering,"Large language models (LLMs) frequently demonstrate reasoning limitations,
often conflating content plausibility (i.e., material inference) with logical
validity (i.e., formal inference). This can result in biased inferences, where
plausible arguments are incorrectly deemed logically valid or vice versa.
Mitigating this limitation is critical, as it undermines the trustworthiness
and generalizability of LLMs in applications that demand rigorous logical
consistency. This paper investigates the problem of mitigating content biases
on formal reasoning through activation steering. Specifically, we curate a
controlled syllogistic reasoning dataset to disentangle formal validity from
content plausibility. After localising the layers responsible for formal and
material inference, we investigate contrastive activation steering methods for
test-time interventions. An extensive empirical analysis on different LLMs
reveals that contrastive steering consistently supports linear control over
content biases. However, we observe that a static approach is insufficient for
improving all the tested models. We then leverage the possibility to control
content effects by dynamically determining the value of the steering parameters
via fine-grained conditional methods. We found that conditional steering is
effective on unresponsive models, achieving up to 15% absolute improvement in
formal reasoning accuracy with a newly introduced kNN-based method (K-CAST).
Finally, additional experiments reveal that steering for content effects is
robust to prompt variations, incurs minimal side effects on language modeling
capabilities, and can partially generalize to out-of-distribution reasoning
tasks. Practically, this paper demonstrates that activation-level interventions
can offer a scalable strategy for enhancing the robustness of LLMs,
contributing towards more systematic and unbiased formal reasoning.",2025-05-18,"Marco Valentino, Geonhee Kim, Dhairya Dalal, Zhixue Zhao, André Freitas",http://arxiv.org/pdf/2505.12189v1,cs.CL
EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective,"Assessing the programming capabilities of Large Language Models (LLMs) is
crucial for their effective use in software engineering. Current evaluations,
however, predominantly measure the accuracy of generated code on static
benchmarks, neglecting the critical aspect of model robustness during
programming tasks. While adversarial attacks offer insights on model
robustness, their effectiveness is limited and evaluation could be constrained.
Current adversarial attack methods for robustness evaluation yield inconsistent
results, struggling to provide a unified evaluation across different LLMs. We
introduce EVALOOP, a novel assessment framework that evaluate the robustness
from a self-consistency perspective, i.e., leveraging the natural duality
inherent in popular software engineering tasks, e.g., code generation and code
summarization. EVALOOP initiates a self-contained feedback loop: an LLM
generates output (e.g., code) from an input (e.g., natural language
specification), and then use the generated output as the input to produce a new
output (e.g., summarizes that code into a new specification). EVALOOP repeats
the process to assess the effectiveness of EVALOOP in each loop. This cyclical
strategy intrinsically evaluates robustness without rely on any external attack
setups, providing a unified metric to evaluate LLMs' robustness in programming.
We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found
that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1
performance within ten loops. Intriguingly, robustness does not always align
with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,
despite superior initial code generation compared to DeepSeek-V2, demonstrated
lower robustness over repeated evaluation loop.",2025-05-18,"Sen Fang, Weiyuan Ding, Bowen Xu",http://arxiv.org/pdf/2505.12185v1,cs.CL
Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases,"The widespread integration of Large Language Models (LLMs) across various
sectors has highlighted the need for empirical research to understand their
biases, thought patterns, and societal implications to ensure ethical and
effective use. In this study, we propose a novel framework for evaluating LLMs,
focusing on uncovering their ideological biases through a quantitative analysis
of 436 binary-choice questions, many of which have no definitive answer. By
applying our framework to ChatGPT and Gemini, findings revealed that while LLMs
generally maintain consistent opinions on many topics, their ideologies differ
across models and languages. Notably, ChatGPT exhibits a tendency to change
their opinion to match the questioner's opinion. Both models also exhibited
problematic biases, unethical or unfair claims, which might have negative
societal impacts. These results underscore the importance of addressing both
ideological and ethical considerations when evaluating LLMs. The proposed
framework offers a flexible, quantitative method for assessing LLM behavior,
providing valuable insights for the development of more socially aligned AI
systems.",2025-05-18,"Manari Hirose, Masato Uchida",http://arxiv.org/pdf/2505.12183v1,cs.CL
Truth Neurons,"Despite their remarkable success and deployment across diverse workflows,
language models sometimes produce untruthful responses. Our limited
understanding of how truthfulness is mechanistically encoded within these
models jeopardizes their reliability and safety. In this paper, we propose a
method for identifying representations of truthfulness at the neuron level. We
show that language models contain truth neurons, which encode truthfulness in a
subject-agnostic manner. Experiments conducted across models of varying scales
validate the existence of truth neurons, confirming that the encoding of
truthfulness at the neuron level is a property shared by many language models.
The distribution patterns of truth neurons over layers align with prior
findings on the geometry of truthfulness. Selectively suppressing the
activations of truth neurons found through the TruthfulQA dataset degrades
performance both on TruthfulQA and on other benchmarks, showing that the
truthfulness mechanisms are not tied to a specific dataset. Our results offer
novel insights into the mechanisms underlying truthfulness in language models
and highlight potential directions toward improving their trustworthiness and
reliability.",2025-05-18,"Haohang Li, Yupeng Cao, Yangyang Yu, Jordan W. Suchow, Zining Zhu",http://arxiv.org/pdf/2505.12182v1,cs.CL
Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse,"Social media platforms like X (formerly Twitter) play a crucial role in
shaping public discourse and societal norms. This study examines the term
Sessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise
of anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and
the TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)
tailored for Turkish, achieving 92.62% accuracy in categorizing emotions such
as happiness, fear, anger, sadness, disgust, and surprise. By applying this
model to large-scale X data, the study uncovers emotional nuances in Turkish
discourse, contributing to computational social science by advancing sentiment
analysis in underrepresented languages and enhancing our understanding of
global digital discourse and the unique linguistic challenges of Turkish. The
findings underscore the transformative potential of localized NLP tools, with
our ERM model offering practical applications for real-time sentiment analysis
in Turkish-language contexts. By addressing critical areas, including
marketing, public relations, and crisis management, these models facilitate
improved decision-making through timely and accurate sentiment tracking. This
highlights the significance of advancing research that accounts for regional
and linguistic nuances.",2025-05-17,"Darmawan Wicaksono, Hasri Akbar Awal Rozaq, Nevfel Boz",http://arxiv.org/pdf/2505.12160v1,cs.CL
The AI Gap: How Socioeconomic Status Affects Language Technology Interactions,"Socioeconomic status (SES) fundamentally influences how people interact with
each other and more recently, with digital technologies like Large Language
Models (LLMs). While previous research has highlighted the interaction between
SES and language technology, it was limited by reliance on proxy metrics and
synthetic data. We survey 1,000 individuals from diverse socioeconomic
backgrounds about their use of language technologies and generative AI, and
collect 6,482 prompts from their previous interactions with LLMs. We find
systematic differences across SES groups in language technology usage (i.e.,
frequency, performed tasks), interaction styles, and topics. Higher SES entails
a higher level of abstraction, convey requests more concisely, and topics like
'inclusivity' and 'travel'. Lower SES correlates with higher
anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more
concrete language. Our findings suggest that while generative language
technologies are becoming more accessible to everyone, socioeconomic linguistic
differences still stratify their use to exacerbate the digital divide. These
differences underscore the importance of considering SES in developing language
technologies to accommodate varying linguistic needs rooted in socioeconomic
factors and limit the AI Gap across SES groups.",2025-05-17,"Elisa Bassignana, Amanda Cercas Curry, Dirk Hovy",http://arxiv.org/pdf/2505.12158v2,cs.CL
LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs,"Assessing the capacity of Large Language Models (LLMs) to plan and reason
within the constraints of interactive environments is crucial for developing
capable AI agents. We introduce $\textbf{LLM-BabyBench}$, a new benchmark suite
designed specifically for this purpose. Built upon a textual adaptation of the
procedurally generated BabyAI grid world, this suite evaluates LLMs on three
fundamental aspects of grounded intelligence: (1) predicting the consequences
of actions on the environment state ($\textbf{Predict}$ task), (2) generating
sequences of low-level actions to achieve specified objectives ($\textbf{Plan}$
task), and (3) decomposing high-level instructions into coherent subgoal
sequences ($\textbf{Decompose}$ task). We detail the methodology for generating
the three corresponding datasets ($\texttt{LLM-BabyBench-Predict}$,
$\texttt{-Plan}$, $\texttt{-Decompose}$) by extracting structured information
from an expert agent operating within the text-based environment. Furthermore,
we provide a standardized evaluation harness and metrics, including environment
interaction for validating generated plans, to facilitate reproducible
assessment of diverse LLMs. Initial baseline results highlight the challenges
posed by these grounded reasoning tasks. The benchmark suite, datasets, data
generation code, and evaluation code are made publicly available
($\href{https://github.com/choukrani/llm-babybench}{\text{GitHub}}$,
$\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\text{HuggingFace}}$).",2025-05-17,"Omar Choukrani, Idriss Malek, Daniil Orel, Zhuohan Xie, Zangir Iklassov, Martin Takáč, Salem Lahlou",http://arxiv.org/pdf/2505.12135v1,cs.CL
A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings,"Content moderation research has recently made significant advances, but still
fails to serve the majority of the world's languages due to the lack of
resources, leaving millions of vulnerable users to online hostility. This work
presents a large-scale human-annotated multi-task benchmark dataset for abusive
language detection in Tigrinya social media with joint annotations for three
tasks: abusiveness, sentiment, and topic classification. The dataset comprises
13,717 YouTube comments annotated by nine native speakers, collected from 7,373
videos with a total of over 1.2 billion views across 51 channels. We developed
an iterative term clustering approach for effective data selection. Recognizing
that around 64% of Tigrinya social media content uses Romanized
transliterations rather than native Ge'ez script, our dataset accommodates both
writing systems to reflect actual language use. We establish strong baselines
across the tasks in the benchmark, while leaving significant challenges for
future contributions. Our experiments reveal that small, specialized multi-task
models outperform the current frontier models in the low-resource setting,
achieving up to 86% accuracy (+7 points) in abusiveness detection. We make the
resources publicly available to promote research on online safety.",2025-05-17,"Fitsum Gaim, Hoyun Song, Huije Lee, Changgeon Ko, Eui Jun Hwang, Jong C. Park",http://arxiv.org/pdf/2505.12116v1,cs.CL
Improving Fairness in LLMs Through Testing-Time Adversaries,"Large Language Models (LLMs) push the bound-aries in natural language
processing and generative AI, driving progress across various aspects of modern
society. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,
predictions) poses a significant and open challenge, hindering their
application in tasks involving ethical sensitivity and responsible
decision-making. In this work, we propose a straightforward, user-friendly and
practical method to mitigate such biases, enhancing the reliability and
trustworthiness of LLMs. Our method creates multiple variations of a given
sentence by modifying specific attributes and evaluates the corresponding
prediction behavior compared to the original, unaltered, prediction/sentence.
The idea behind this process is that critical ethical predictions often exhibit
notable inconsistencies, indicating the presence of bias. Unlike previous
approaches, our method relies solely on forward passes (i.e., testing-time
adversaries), eliminating the need for training, fine-tuning, or prior
knowledge of the training data distribution. Through extensive experiments on
the popular Llama family, we demonstrate the effectiveness of our method in
improving various fairness metrics, focusing on the reduction of disparities in
how the model treats individuals from different racial groups. Specifically,
using standard metrics, we improve the fairness in Llama3 in up to 27
percentage points. Overall, our approach significantly enhances fairness,
equity, and reliability in LLM-generated results without parameter tuning or
training data modifications, confirming its effectiveness in practical
scenarios. We believe our work establishes an important step toward enabling
the use of LLMs in tasks that require ethical considerations and responsible
decision-making.",2025-05-17,"Isabela Pereira Gregio, Ian Pons, Anna Helena Reali Costa, Artur Jordão",http://arxiv.org/pdf/2505.12100v1,cs.CL
Personalized Author Obfuscation with Large Language Models,"In this paper, we investigate the efficacy of large language models (LLMs) in
obfuscating authorship by paraphrasing and altering writing styles. Rather than
adopting a holistic approach that evaluates performance across the entire
dataset, we focus on user-wise performance to analyze how obfuscation
effectiveness varies across individual authors. While LLMs are generally
effective, we observe a bimodal distribution of efficacy, with performance
varying significantly across users. To address this, we propose a personalized
prompting method that outperforms standard prompting techniques and partially
mitigates the bimodality issue.",2025-05-17,"Mohammad Shokri, Sarah Ita Levitan, Rivka Levitan",http://arxiv.org/pdf/2505.12090v1,cs.CL
Model Merging in Pre-training of Large Language Models,"Model merging has emerged as a promising technique for enhancing large
language models, though its application in large-scale pre-training remains
relatively unexplored. In this paper, we present a comprehensive investigation
of model merging techniques during the pre-training process. Through extensive
experiments with both dense and Mixture-of-Experts (MoE) architectures ranging
from millions to over 100 billion parameters, we demonstrate that merging
checkpoints trained with constant learning rates not only achieves significant
performance improvements but also enables accurate prediction of annealing
behavior. These improvements lead to both more efficient model development and
significantly lower training costs. Our detailed ablation studies on merging
strategies and hyperparameters provide new insights into the underlying
mechanisms while uncovering novel applications. Through comprehensive
experimental analysis, we offer the open-source community practical
pre-training guidelines for effective model merging.",2025-05-17,"Yunshui Li, Yiyuan Ma, Shen Yan, Chaoyi Zhang, Jing Liu, Jianqiao Lu, Ziwen Xu, Mengzhao Chen, Minrui Wang, Shiyi Zhan, Jin Ma, Xunhao Lai, Deyi Liu, Yao Luo, Xingyan Bin, Hongbin Ren, Mingji Han, Wenhao Hao, Bairen Yi, LingJun Liu, Bole Ma, Xiaoying Jia, Xun Zhou, Siyuan Qiao, Liang Xiang, Yonghui Wu",http://arxiv.org/pdf/2505.12082v3,cs.CL
Do different prompting methods yield a common task representation in language models?,"Demonstrations and instructions are two primary approaches for prompting
language models to perform in-context learning (ICL) tasks. Do identical tasks
elicited in different ways result in similar representations of the task? An
improved understanding of task representation mechanisms would offer
interpretability insights and may aid in steering models. We study this through
\textit{function vectors} (FVs), recently proposed as a mechanism to extract
few-shot ICL task representations. We generalize FVs to alternative task
presentations, focusing on short textual instruction prompts, and successfully
extract instruction function vectors that promote zero-shot task accuracy. We
find evidence that demonstration- and instruction-based function vectors
leverage different model components, and offer several controls to dissociate
their contributions to task performance. Our results suggest that different
task promptings forms do not induce a common task representation through FVs
but elicit different, partly overlapping mechanisms. Our findings offer
principled support to the practice of combining instructions and task
demonstrations, imply challenges in universally monitoring task inference
across presentation forms, and encourage further examinations of LLM task
inference mechanisms.",2025-05-17,"Guy Davidson, Todd M. Gureckis, Brenden M. Lake, Adina Williams",http://arxiv.org/pdf/2505.12075v2,cs.CL
Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach,"In this study, we approach morphological productivity from two perspectives:
a cognitive-computational perspective, and a diachronic perspective zooming in
on an actual speaker, Thomas Mann. For developing the first perspective, we
make use of a cognitive computational model of the mental lexicon, the
discriminative lexicon model. For computational mappings between form and
meaning to be productive, in the sense that novel, previously unencountered
words, can be understood and produced, there must be systematicities between
the form space and the semantic space. If the relation between form and meaning
would be truly arbitrary, a model could memorize form and meaning pairings, but
there is no way in which the model would be able to generalize to novel test
data. For Finnish nominal inflection, Malay derivation, and English
compounding, we explore, using the Discriminative Lexicon Model as a
computational tool, to trace differences in the degree to which inflectional
and word formation patterns are productive. We show that the DLM tends to
associate affix-like sublexical units with the centroids of the embeddings of
the words with a given affix. For developing the second perspective, we study
how the intake and output of one prolific writer, Thomas Mann, changes over
time. We show by means of an examination of what Thomas Mann is likely to have
read, and what he wrote, that the rate at which Mann produces novel derived
words is extremely low. There are far more novel words in his input than in his
output. We show that Thomas Mann is less likely to produce a novel derived word
with a given suffix the greater the average distance is of the embeddings of
all derived words to the corresponding centroid, and discuss the challenges of
using speaker-specific embeddings for low-frequency and novel words.",2025-05-17,"Harald Baayen, Kristian Berg, Maziyah Mohamed",http://arxiv.org/pdf/2505.12071v1,cs.CL
Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents,"Large Language Model (LLM)-based search agents have shown remarkable
capabilities in solving complex tasks by dynamically decomposing problems and
addressing them through interleaved reasoning and retrieval. However, this
interleaved paradigm introduces substantial efficiency bottlenecks. First, we
observe that both highly accurate and overly approximate retrieval methods
degrade system efficiency: exact search incurs significant retrieval overhead,
while coarse retrieval requires additional reasoning steps during generation.
Second, we identify inefficiencies in system design, including improper
scheduling and frequent retrieval stalls, which lead to cascading latency --
where even minor delays in retrieval amplify end-to-end inference time. To
address these challenges, we introduce SearchAgent-X, a high-efficiency
inference framework for LLM-based search agents. SearchAgent-X leverages
high-recall approximate retrieval and incorporates two key techniques:
priority-aware scheduling and non-stall retrieval. Extensive experiments
demonstrate that SearchAgent-X consistently outperforms state-of-the-art
systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving
up to 3.4$\times$ higher throughput and 5$\times$ lower latency, without
compromising generation quality. SearchAgent-X is available at
https://github.com/tiannuo-yang/SearchAgent-X.",2025-05-17,"Tiannuo Yang, Zebin Yao, Bowen Jin, Lixiao Cui, Yusen Li, Gang Wang, Xiaoguang Liu",http://arxiv.org/pdf/2505.12065v1,cs.CL
Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement,"Large Language Models (LLMs) have shown impressive capabilities across
various tasks but remain vulnerable to meticulously crafted jailbreak attacks.
In this paper, we identify a critical safety gap: while LLMs are adept at
detecting jailbreak prompts, they often produce unsafe responses when directly
processing these inputs. Inspired by this insight, we propose SAGE (Self-Aware
Guard Enhancement), a training-free defense strategy designed to align LLMs'
strong safety discrimination performance with their relatively weaker safety
generation ability. SAGE consists of two core components: a Discriminative
Analysis Module and a Discriminative Response Module, enhancing resilience
against sophisticated jailbreak attempts through flexible safety discrimination
instructions. Extensive experiments demonstrate SAGE's effectiveness and
robustness across various open-source and closed-source LLMs of different sizes
and architectures, achieving an average 99% defense success rate against
numerous complex and covert jailbreak methods while maintaining helpfulness on
general benchmarks. We further conduct mechanistic interpretability analysis
through hidden states and attention distributions, revealing the underlying
mechanisms of this detection-generation discrepancy. Our work thus contributes
to developing future LLMs with coherent safety awareness and generation
behavior. Our code and datasets are publicly available at
https://github.com/NJUNLP/SAGE.",2025-05-17,"Peng Ding, Jun Kuang, Zongyu Wang, Xuezhi Cao, Xunliang Cai, Jiajun Chen, Shujian Huang",http://arxiv.org/pdf/2505.12060v1,cs.CL
"Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation","Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual
smoke-test suite designed to give large-language-model (LLM) pipelines a
unit-test style safety net dataset that runs in seconds with minimal cost. Born
out of the tight feedback-loop demands building the Comet Opik
prompt-optimization SDK, where waiting on heavyweight benchmarks breaks
developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with
a tiny synthetic-data generator pypi package built on provider-agnostic
LiteLLM. The generator lets practitioners mint their own tiny packs in any
language, domain, or difficulty, while ten ready-made packs already cover
Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,
Spanish, and Turkish. Every dataset ships with Croissant metadata and
plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so
teams can drop deterministic micro-benchmarks directly into pull-request gates,
prompt-engineering loops, and production dashboards without touching GPU
budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet
reliably flags prompt-template errors, tokenizer drift, and fine-tuning
side-effects long before full-scale suites like MMLU or BIG-Bench would finish
configuring. The entire framework is released to accelerate continuous,
resource-efficient quality assurance across the generative-AI ecosystem.",2025-05-17,Vincent Koc,http://arxiv.org/pdf/2505.12058v1,cs.CL
GenderBench: Evaluation Suite for Gender Biases in LLMs,"We present GenderBench -- a comprehensive evaluation suite designed to
measure gender biases in LLMs. GenderBench includes 14 probes that quantify 19
gender-related harmful behaviors exhibited by LLMs. We release GenderBench as
an open-source and extensible library to improve the reproducibility and
robustness of benchmarking across the field. We also publish our evaluation of
12 LLMs. Our measurements reveal consistent patterns in their behavior. We show
that LLMs struggle with stereotypical reasoning, equitable gender
representation in generated texts, and occasionally also with discriminatory
behavior in high-stakes scenarios, such as hiring.",2025-05-17,Matúš Pikuliak,http://arxiv.org/pdf/2505.12054v1,cs.CL
ABoN: Adaptive Best-of-N Alignment,"Recent advances in test-time alignment methods, such as Best-of-N sampling,
offer a simple and effective way to steer language models (LMs) toward
preferred behaviors using reward models (RM). However, these approaches can be
computationally expensive, especially when applied uniformly across prompts
without accounting for differences in alignment difficulty. In this work, we
propose a prompt-adaptive strategy for Best-of-N alignment that allocates
inference-time compute more efficiently. Motivated by latency concerns, we
develop a two-stage algorithm: an initial exploratory phase estimates the
reward distribution for each prompt using a small exploration budget, and a
second stage adaptively allocates the remaining budget using these estimates.
Our method is simple, practical, and compatible with any LM/RM combination.
Empirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different
batches of prompts show that our adaptive strategy consistently outperforms the
uniform allocation with the same inference budget. Moreover, our experiments
show that our adaptive strategy remains competitive against uniform allocations
with 20% larger inference budgets and even improves in performance as the batch
size grows.",2025-05-17,"Vinod Raman, Hilal Asi, Satyen Kale",http://arxiv.org/pdf/2505.12050v1,cs.CL
MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities,"Although large language models (LLMs) perform well in general tasks,
domain-specific applications suffer from hallucinations and accuracy
limitations. Continual Pre-Training (CPT) approaches encounter two key issues:
(1) domain-biased data degrades general language skills, and (2) improper
corpus-mixture ratios limit effective adaptation. To address these, we propose
a novel framework, Mixture of Losses (MoL), which decouples optimization
objectives for domain-specific and general corpora. Specifically, cross-entropy
(CE) loss is applied to domain-corpus to ensure knowledge acquisition, while
Kullback-Leibler (KL) divergence aligns general-corpus training with the base
model's foundational capabilities. This dual-loss architecture preserves
universal skills while enhancing domain expertise, avoiding catastrophic
forgetting. Empirically, we validate that a 1:1 domain-to-general corpus ratio
optimally balances training and overfitting without the need for extensive
tuning or resource-intensive experiments. Furthermore, our experiments
demonstrate significant performance gains compared to traditional CPT
approaches, which often suffer from degradation in general language
capabilities; our model achieves 27.9% higher accuracy on the Math-500
benchmark in the non-think reasoning mode, and an impressive 83.3% improvement
on the challenging AIME25 subset in the think mode, underscoring the
effectiveness of our approach.",2025-05-17,"Jingxue Chen, Qingkun Tang, Qianchun Lu, Siyuan Fang",http://arxiv.org/pdf/2505.12043v2,cs.CL
AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research,"The Science of Science (SoS) explores the mechanisms underlying scientific
discovery, and offers valuable insights for enhancing scientific efficiency and
fostering innovation. Traditional approaches often rely on simplistic
assumptions and basic statistical tools, such as linear regression and
rule-based simulations, which struggle to capture the complexity and scale of
modern research ecosystems. The advent of artificial intelligence (AI) presents
a transformative opportunity for the next generation of SoS, enabling the
automation of large-scale pattern discovery and uncovering insights previously
unattainable. This paper offers a forward-looking perspective on the
integration of Science of Science with AI for automated research pattern
discovery and highlights key open challenges that could greatly benefit from
AI. We outline the advantages of AI over traditional methods, discuss potential
limitations, and propose pathways to overcome them. Additionally, we present a
preliminary multi-agent system as an illustrative example to simulate research
societies, showcasing AI's ability to replicate real-world research patterns
and accelerate progress in Science of Science research.",2025-05-17,"Renqi Chen, Haoyang Su, Shixiang Tang, Zhenfei Yin, Qi Wu, Hui Li, Ye Sun, Nanqing Dong, Wanli Ouyang, Philip Torr",http://arxiv.org/pdf/2505.12039v1,cs.CL
"Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method","Argument mining has garnered increasing attention over the years, with the
recent advancement of Large Language Models (LLMs) further propelling this
trend. However, current argument relations remain relatively simplistic and
foundational, struggling to capture the full scope of argument information,
particularly when it comes to representing complex argument structures in
real-world scenarios. To address this limitation, we propose 14 fine-grained
relation types from both vertical and horizontal dimensions, thereby capturing
the intricate interplay between argument components for a thorough
understanding of argument structure. On this basis, we conducted extensive
experiments on three tasks: argument component detection, relation prediction,
and automated essay grading. Additionally, we explored the impact of writing
quality on argument component detection and relation prediction, as well as the
connections between discourse relations and argumentative features. The
findings highlight the importance of fine-grained argumentative annotations for
argumentative writing quality assessment and encourage multi-dimensional
argument analysis.",2025-05-17,"Yupei Ren, Xinyi Zhou, Ning Zhang, Shangqing Zhao, Man Lan, Xiaopeng Bai",http://arxiv.org/pdf/2505.12028v1,cs.CL
Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation,"Considering the inherent limitations of parametric knowledge in large
language models (LLMs), retrieval-augmented generation (RAG) is widely employed
to expand their knowledge scope. Since RAG has shown promise in
knowledge-intensive tasks like open-domain question answering, its broader
application to complex tasks and intelligent assistants has further advanced
its utility. Despite this progress, the underlying knowledge utilization
mechanisms of LLM-based RAG remain underexplored. In this paper, we present a
systematic investigation of the intrinsic mechanisms by which LLMs integrate
internal (parametric) and external (retrieved) knowledge in RAG scenarios.
Specially, we employ knowledge stream analysis at the macroscopic level, and
investigate the function of individual modules at the microscopic level.
Drawing on knowledge streaming analyses, we decompose the knowledge utilization
process into four distinct stages within LLM layers: knowledge refinement,
knowledge elicitation, knowledge expression, and knowledge contestation. We
further demonstrate that the relevance of passages guides the streaming of
knowledge through these stages. At the module level, we introduce a new method,
knowledge activation probability entropy (KAPE) for neuron identification
associated with either internal or external knowledge. By selectively
deactivating these neurons, we achieve targeted shifts in the LLM's reliance on
one knowledge source over the other. Moreover, we discern complementary roles
for multi-head attention and multi-layer perceptron layers during knowledge
formation. These insights offer a foundation for improving interpretability and
reliability in retrieval-augmented LLMs, paving the way for more robust and
transparent generative solutions in knowledge-intensive domains.",2025-05-17,"Yuhao Wang, Ruiyang Ren, Yucheng Wang, Wayne Xin Zhao, Jing Liu, Hua Wu, Haifeng Wang",http://arxiv.org/pdf/2505.11995v1,cs.CL
Introduction to Analytical Software Engineering Design Paradigm,"As modern software systems expand in scale and complexity, the challenges
associated with their modeling and formulation grow increasingly intricate.
Traditional approaches often fall short in effectively addressing these
complexities, particularly in tasks such as design pattern detection for
maintenance and assessment, as well as code refactoring for optimization and
long-term sustainability. This growing inadequacy underscores the need for a
paradigm shift in how such challenges are approached and resolved. This paper
presents Analytical Software Engineering (ASE), a novel design paradigm aimed
at balancing abstraction, tool accessibility, compatibility, and scalability.
ASE enables effective modeling and resolution of complex software engineering
problems. The paradigm is evaluated through two frameworks
Behavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),
both developed in accordance with ASE principles. BSS offers a compact,
language-agnostic representation of codebases to facilitate precise design
pattern detection. ODR unifies artifact and solution representations to
optimize code refactoring via heuristic algorithms while eliminating iterative
computational overhead. By providing a structured approach to software design
challenges, ASE lays the groundwork for future research in encoding and
analyzing complex software metrics.",2025-05-17,"Tarik Houichime, Younes El Amrani",http://arxiv.org/pdf/2505.11979v1,cs.CL
An Annotated Corpus of Arabic Tweets for Hate Speech Analysis,"Identifying hate speech content in the Arabic language is challenging due to
the rich quality of dialectal variations. This study introduces a multilabel
hate speech dataset in the Arabic language. We have collected 10000 Arabic
tweets and annotated each tweet, whether it contains offensive content or not.
If a text contains offensive content, we further classify it into different
hate speech targets such as religion, gender, politics, ethnicity, origin, and
others. A text can contain either single or multiple targets. Multiple
annotators are involved in the data annotation task. We calculated the
inter-annotator agreement, which was reported to be 0.86 for offensive content
and 0.71 for multiple hate speech targets. Finally, we evaluated the data
annotation task by employing a different transformers-based model in which
AraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of
0.786.",2025-05-17,"Wajdi Zaghouani, Md. Rafiul Biswas",http://arxiv.org/pdf/2505.11969v2,cs.CL
CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation,"We present the system developed by the Central China Normal University (CCNU)
team for the Mu-SHROOM shared task, which focuses on identifying hallucinations
in question-answering systems across 14 different languages. Our approach
leverages multiple Large Language Models (LLMs) with distinct areas of
expertise, employing them in parallel to annotate hallucinations, effectively
simulating a crowdsourcing annotation process. Furthermore, each LLM-based
annotator integrates both internal and external knowledge related to the input
during the annotation process. Using the open-source LLM DeepSeek-V3, our
system achieves the top ranking (\#1) for Hindi data and secures a Top-5
position in seven other languages. In this paper, we also discuss unsuccessful
approaches explored during our development process and share key insights
gained from participating in this shared task.",2025-05-17,"Xu Liu, Guanyi Chen",http://arxiv.org/pdf/2505.11965v1,cs.CL
EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English and Arabic,"This research introduces a bilingual dataset comprising 23,456 entries for
Arabic and 10,036 entries for English, annotated for emotions and hope speech,
addressing the scarcity of multi-emotion (Emotion and hope) datasets. The
dataset provides comprehensive annotations capturing emotion intensity,
complexity, and causes, alongside detailed classifications and subcategories
for hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,
revealing 0.75-0.85 agreement among annotators both for Arabic and English
language. The evaluation metrics (micro-F1-Score=0.67) obtained from the
baseline model (i.e., using a machine learning model) validate that the data
annotations are worthy. This dataset offers a valuable resource for advancing
natural language processing in underrepresented languages, fostering better
cross-linguistic analysis of emotions and hope speech.",2025-05-17,"Wajdi Zaghouani, Md. Rafiul Biswas",http://arxiv.org/pdf/2505.11959v2,cs.CL
Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning,"Counterspeech has proven to be a powerful tool to combat hate speech online.
Previous studies have focused on generating counterspeech conditioned only on
specific intents (single attributed). However, a holistic approach considering
multiple attributes simultaneously can yield more nuanced and effective
responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with
Preference Optimization, a novel two-stage framework that utilizes the
effectiveness of attribute-specific prefix embedding spaces hierarchically
optimized during the counterspeech generation process in the first phase.
Thereafter, we incorporate both reference and reward-free preference
optimization to generate more constructive counterspeech. Furthermore, we
extend IntentCONANv2 by annotating all 13,973 counterspeech instances with
emotion labels by five annotators. HiPPrO leverages hierarchical prefix
optimization to integrate these dual attributes effectively. An extensive
evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent
conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,
respectively, compared to several baseline models. Human evaluations further
substantiate the superiority of our approach, highlighting the enhanced
relevance and appropriateness of the generated counterspeech. This work
underscores the potential of multi-attribute conditioning in advancing the
efficacy of counterspeech generation systems.",2025-05-17,"Aswini Kumar Padhi, Anil Bandhakavi, Tanmoy Chakraborty",http://arxiv.org/pdf/2505.11958v2,cs.CL
ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing,"Although multimodal large language models (MLLMs) show promise in generating
chart rendering code, chart editing presents a greater challenge. This
difficulty stems from its nature as a labor-intensive task for humans that also
demands MLLMs to integrate chart understanding, complex reasoning, and precise
intent interpretation. While many MLLMs claim such editing capabilities,
current assessments typically rely on limited case studies rather than robust
evaluation methodologies, highlighting the urgent need for a comprehensive
evaluation framework. In this work, we propose ChartEdit, a new high-quality
benchmark designed for chart editing tasks. This benchmark comprises $1,405$
diverse editing instructions applied to $233$ real-world charts, with each
instruction-chart instance having been manually annotated and validated for
accuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream
MLLMs across two types of experiments, assessing them at both the code and
chart levels. The results suggest that large-scale models can generate code to
produce images that partially match the reference images. However, their
ability to generate accurate edits according to the instructions remains
limited. The state-of-the-art (SOTA) model achieves a score of only $59.96$,
highlighting significant challenges in precise modification. In contrast,
small-scale models, including chart-domain models, struggle both with following
editing instructions and generating overall chart images, underscoring the need
for further development in this area. Code is available at
https://github.com/xxlllz/ChartEdit.",2025-05-17,"Xuanle Zhao, Xuexin Liu, Haoyue Yang, Xianzhen Luo, Fanhu Zeng, Jianling Li, Qi Shi, Chi Chen",http://arxiv.org/pdf/2505.11935v1,cs.CL
Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models,"Large Vision-Language Models (LVLMs) have exhibited impressive capabilities
across various visual tasks, yet they remain hindered by the persistent
challenge of hallucinations. To address this critical issue, we propose Mixture
of Decoding (MoD), a novel approach for hallucination mitigation that
dynamically adapts decoding strategies by evaluating the correctness of the
model's attention on image tokens. Specifically, MoD measures the consistency
between outputs generated from the original image tokens and those derived from
the model's attended image tokens, to distinguish the correctness
aforementioned. If the outputs are consistent, indicating correct attention,
MoD employs a complementary strategy to amplify critical information.
Conversely, if the outputs are inconsistent, suggesting erroneous attention,
MoD utilizes a contrastive strategy to suppress misleading information.
Extensive experiments demonstrate that MoD significantly outperforms existing
decoding methods across multiple mainstream benchmarks, effectively mitigating
hallucinations in LVLMs. The code is available at
https://github.com/xlchen0205/MoD.",2025-05-17,"Xinlong Chen, Yuanxing Zhang, Qiang Liu, Junfei Wu, Fuzheng Zhang, Tieniu Tan",http://arxiv.org/pdf/2505.17061v1,cs.CL
Neuro-Symbolic Query Compiler,"Precise recognition of search intent in Retrieval-Augmented Generation (RAG)
systems remains a challenging goal, especially under resource constraints and
for complex queries with nested structures and dependencies. This paper
presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar
rules and compiler design, to bridge this gap. It theoretically designs a
minimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize
complex queries. Unlike previous methods, this grammar maintains completeness
while minimizing redundancy. Based on this, QCompiler includes a Query
Expression Translator, a Lexical Syntax Parser, and a Recursive Descent
Processor to compile queries into Abstract Syntax Trees (ASTs) for execution.
The atomicity of the sub-queries in the leaf nodes ensures more precise
document retrieval and response generation, significantly improving the RAG
system's ability to address complex queries.",2025-05-17,"Yuyao Zhang, Zhicheng Dou, Xiaoxi Li, Jiajie Jin, Yongkang Wu, Zhonghua Li, Qi Ye, Ji-Rong Wen",http://arxiv.org/pdf/2505.11932v1,cs.CL
An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts,"We provide an explanation for the performance gains of intrinsic
self-correction, a process where a language model iteratively refines its
outputs without external feedback. More precisely, we investigate how prompting
induces interpretable changes in hidden states and thus affects the output
distributions. We hypothesize that each prompt-induced shift lies in a linear
span of some linear representation vectors, naturally separating tokens based
on individual concept alignment. Building around this idea, we give a
mathematical formulation of self-correction and derive a concentration result
for output tokens based on alignment magnitudes. Our experiments on text
detoxification with zephyr-7b-sft reveal a substantial gap in the inner
products of the prompt-induced shifts and the unembeddings of the top-100 most
toxic tokens vs. those of the unembeddings of the bottom-100 least toxic
tokens, under toxic instructions. This suggests that self-correction prompts
enhance a language model's capability of latent concept recognition. Our
analysis offers insights into the underlying mechanism of self-correction by
characterizing how prompting works explainably. For reproducibility, our code
is available.",2025-05-17,"Yu-Ting Lee, Hui-Ying Shih, Fu-Chieh Chang, Pei-Yuan Wu",http://arxiv.org/pdf/2505.11924v1,cs.CL
Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning,"Large language models (LLMs) exhibit remarkable capabilities in handling
natural language tasks; however, they may struggle to consistently follow
complex instructions including those involve multiple constraints.
Post-training LLMs using supervised fine-tuning (SFT) is a standard approach to
improve their ability to follow instructions. In addressing complex instruction
following, existing efforts primarily focus on data-driven methods that
synthesize complex instruction-output pairs for SFT. However, insufficient
attention allocated to crucial sub-contexts may reduce the effectiveness of
SFT. In this work, we propose transforming sequentially structured input
instruction into multiple parallel instructions containing subcontexts. To
support processing this multi-input, we propose MISO (Multi-Input
Single-Output), an extension to currently dominant decoder-only
transformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that
jointly considers the overall instruction-output alignment and the influence of
individual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning
to complex instructionfollowing datasets and evaluate it with standard LLM
inference. Empirical results demonstrate the superiority of MISO as a
fine-tuning method for LLMs, both in terms of effectiveness in complex
instruction-following scenarios and its potential for training efficiency.",2025-05-17,"Yuheng Lu, ZiMeng Bai, Caixia Yuan, Huixing Jiang, Xiaojie Wang",http://arxiv.org/pdf/2505.11922v1,cs.CL
ELITE: Embedding-Less retrieval with Iterative Text Exploration,"Large Language Models (LLMs) have achieved impressive progress in natural
language processing, but their limited ability to retain long-term context
constrains performance on document-level or multi-turn tasks.
Retrieval-Augmented Generation (RAG) mitigates this by retrieving relevant
information from an external corpus. However, existing RAG systems often rely
on embedding-based retrieval trained on corpus-level semantic similarity, which
can lead to retrieving content that is semantically similar in form but
misaligned with the question's true intent. Furthermore, recent RAG variants
construct graph- or hierarchy-based structures to improve retrieval accuracy,
resulting in significant computation and storage overhead. In this paper, we
propose an embedding-free retrieval framework. Our method leverages the logical
inferencing ability of LLMs in retrieval using iterative search space
refinement guided by our novel importance measure and extend our retrieval
results with logically related information without explicit graph construction.
Experiments on long-context QA benchmarks, including NovelQA and Marathon, show
that our approach outperforms strong baselines while reducing storage and
runtime by over an order of magnitude.",2025-05-17,"Zhangyu Wang, Siyuan Gao, Rong Zhou, Hao Wang, Li Ning",http://arxiv.org/pdf/2505.11908v1,cs.CL
Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data,"Question answering over mixed sources, like text and tables, has been
advanced by verbalizing all contents and encoding it with a language model. A
prominent case of such heterogeneous data is personal information: user devices
log vast amounts of data every day, such as calendar entries, workout
statistics, shopping records, streaming history, and more. Information needs
range from simple look-ups to queries of analytical nature. The challenge is to
provide humans with convenient access with small footprint, so that all
personal data stays on the user devices. We present ReQAP, a novel method that
creates an executable operator tree for a given question, via recursive
decomposition. Operators are designed to enable seamless integration of
structured and unstructured sources, and the execution of the operator tree
yields a traceable answer. We further release the PerQA benchmark, with
persona-based data and questions, covering a diverse spectrum of realistic user
needs.",2025-05-17,"Philipp Christmann, Gerhard Weikum",http://arxiv.org/pdf/2505.11900v1,cs.CL
SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation,"In order to enable fluid and natural human-machine speech interaction,
existing full-duplex conversational systems often adopt modular architectures
with auxiliary components such as voice activity detectors, interrupters,
conversation state predictors, or multiple LLMs. These systems, however, suffer
from error accumulation across modules and struggle with key challenges such as
context-dependent barge-in and echo cancellation. Recent approaches, most
notably Moshi, simplify the pipeline by injecting audio codecs into the token
space of a single LLM. However, such methods still incur significant
performance degradation when operating on the speech rather than text modality.
In this paper, we introduce SALMONN-omni, the first single, standalone
full-duplex speech LLM that operates without audio codecs in its token space.
It features a novel dynamic thinking mechanism within the LLM backbone,
enabling the model to learn when to transition between speaking and listening
states. Experiments on widely used benchmarks for spoken question answering and
open-domain dialogue show that SALMONN-omni achieves at least 30\% relative
performance improvement over existing open-source full-duplex models and
performs highly competitively to half-duplex and turn-based systems, despite
using substantially less training data. Moreover, SALMONN-omni demonstrates
strong performance in complex conversational scenarios, including turn-taking,
backchanneling, echo cancellation and context-dependent barge-in, with further
improvements achieved through reinforcement learning. Some demo conversations
between user and SALMONN-omni are provided in the following repository
https://github.com/bytedance/SALMONN.",2025-05-17,"Wenyi Yu, Siyin Wang, Xiaoyu Yang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang",http://arxiv.org/pdf/2505.17060v1,cs.CL
RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving,"Multi-step planning has been widely employed to enhance the performance of
large language models (LLMs) on downstream natural language processing (NLP)
tasks, which decomposes the original task into multiple subtasks and guide LLMs
to solve them sequentially without additional training. When addressing task
instances, existing methods either preset the order of steps or attempt
multiple paths at each step. However, these methods overlook instances'
linguistic features and rely on the intrinsic planning capabilities of LLMs to
evaluate intermediate feedback and then select subtasks, resulting in
suboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this
paper we propose a Reinforcement Learning enhanced Adaptive Planning framework
(RLAP). In our framework, we model an NLP task as a Markov decision process
(MDP) and employ an LLM directly into the environment. In particular, a
lightweight Actor model is trained to estimate Q-values for natural language
sequences consisting of states and actions through reinforcement learning.
Therefore, during sequential planning, the linguistic features of each sequence
in the MDP can be taken into account, and the Actor model interacts with the
LLM to determine the optimal order of subtasks for each task instance. We apply
RLAP on three different types of NLP tasks and conduct extensive experiments on
multiple datasets to verify RLAP's effectiveness and robustness.",2025-05-17,"Zepeng Ding, Dixuan Wang, Ziqin Luo, Guochao Jiang, Deqing Yang, Jiaqing Liang",http://arxiv.org/pdf/2505.11893v1,cs.CL
Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents,"VLM-based mobile agents are increasingly popular due to their capabilities to
interact with smartphone GUIs and XML-structured texts and to complete daily
tasks. However, existing online benchmarks struggle with obtaining stable
reward signals due to dynamic environmental changes. Offline benchmarks
evaluate the agents through single-path trajectories, which stands in contrast
to the inherently multi-solution characteristics of GUI tasks. Additionally,
both types of benchmarks fail to assess whether mobile agents can handle noise
or engage in proactive interactions due to a lack of noisy apps or overly full
instructions during the evaluation process. To address these limitations, we
use a slot-based instruction generation method to construct a more realistic
and comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a
common task split, with offline multi-path evaluation to assess the agent's
ability to obtain step rewards during task execution. It contains a noisy split
based on pop-ups and ads apps, and a contaminated split named AITZ-Noise to
formulate a real noisy environment. Furthermore, an ambiguous instruction split
with preset Q\&A interactions is released to evaluate the agent's proactive
interaction capabilities. We conduct evaluations on these splits using the
single-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,
as well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are
available at https://huggingface.co/datasets/xwk123/MobileBench-v2.",2025-05-17,"Weikai Xu, Zhizheng Jiang, Yuxuan Liu, Pengzhi Gao, Wei Liu, Jian Luan, Yuanchun Li, Yunxin Liu, Bin Wang, Bo An",http://arxiv.org/pdf/2505.11891v2,cs.CL
AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation,"With the proliferation of large language models (LLMs) in the medical domain,
there is increasing demand for improved evaluation techniques to assess their
capabilities. However, traditional metrics like F1 and ROUGE, which rely on
token overlaps to measure quality, significantly overlook the importance of
medical terminology. While human evaluation tends to be more reliable, it can
be very costly and may as well suffer from inaccuracies due to limits in human
expertise and motivation. Although there are some evaluation methods based on
LLMs, their usability in the medical field is limited due to their proprietary
nature or lack of expertise. To tackle these challenges, we present
AutoMedEval, an open-sourced automatic evaluation model with 13B parameters
specifically engineered to measure the question-answering proficiency of
medical LLMs. The overarching objective of AutoMedEval is to assess the quality
of responses produced by diverse models, aspiring to significantly reduce the
dependence on human evaluation. Specifically, we propose a hierarchical
training method involving curriculum instruction tuning and an iterative
knowledge introspection mechanism, enabling AutoMedEval to acquire professional
medical assessment capabilities with limited instructional data. Human
evaluations indicate that AutoMedEval surpasses other baselines in terms of
correlation with human judgments.",2025-05-17,"Xiechi Zhang, Zetian Ouyang, Linlin Wang, Gerard de Melo, Zhu Cao, Xiaoling Wang, Ya Zhang, Yanfeng Wang, Liang He",http://arxiv.org/pdf/2505.11887v1,cs.CL
Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large,"Understanding medical texts presents significant challenges due to complex
terminology and context-specific language. This paper introduces Medalyze, an
AI-powered application designed to enhance the comprehension of medical texts
using three specialized FLAN-T5-Large models. These models are fine-tuned for
(1) summarizing medical reports, (2) extracting health issues from
patient-doctor conversations, and (3) identifying the key question in a
passage. Medalyze is deployed across a web and mobile platform with real-time
inference, leveraging scalable API and YugabyteDB. Experimental evaluations
demonstrate the system's superior summarization performance over GPT-4 in
domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and
SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and
lightweight solution for improving information accessibility in healthcare.",2025-05-17,"Van-Tinh Nguyen, Hoang-Duong Pham, Thanh-Hai To, Cong-Tuan Hung Do, Thi-Thu-Trang Dong, Vu-Trung Duong Le, Van-Phuc Hoang",http://arxiv.org/pdf/2505.17059v1,cs.CL
NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization,"Model editing techniques are essential for efficiently updating knowledge in
large language models (LLMs). However, the effectiveness of existing approaches
degrades in massive editing scenarios, particularly when evaluated with
practical metrics or in context-rich settings. We attribute these failures to
embedding collisions among knowledge items, which undermine editing reliability
at scale. To address this, we propose NAMET (Noise-aware Model Editing in
Transformers), a simple yet effective method that introduces noise during
memory extraction via a one-line modification to MEMIT. Extensive experiments
across six LLMs and three datasets demonstrate that NAMET consistently
outperforms existing methods when editing thousands of facts.",2025-05-17,"Yanbo Dai, Zhenlan Ji, Zongjie Li, Shuai Wang",http://arxiv.org/pdf/2505.11876v1,cs.CL
J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge,"The current focus of AI research is shifting from emphasizing model training
towards enhancing evaluation quality, a transition that is crucial for driving
further advancements in AI systems. Traditional evaluation methods typically
rely on reward models assigning scalar preference scores to outputs. Although
effective, such approaches lack interpretability, leaving users often uncertain
about why a reward model rates a particular response as high or low. The advent
of LLM-as-a-Judge provides a more scalable and interpretable method of
supervision, offering insights into the decision-making process. Moreover, with
the emergence of large reasoning models, which consume more tokens for deeper
thinking and answer refinement, scaling test-time computation in the
LLM-as-a-Judge paradigm presents an avenue for further boosting performance and
providing more interpretability through reasoning traces. In this paper, we
introduce $\textbf{J1-7B}$, which is first supervised fine-tuned on
reflection-enhanced datasets collected via rejection-sampling and subsequently
trained using Reinforcement Learning (RL) with verifiable rewards. At inference
time, we apply Simple Test-Time Scaling (STTS) strategies for additional
performance improvement. Experimental results demonstrate that $\textbf{J1-7B}$
surpasses the previous state-of-the-art LLM-as-a-Judge by $ \textbf{4.8}$\% and
exhibits a $ \textbf{5.1}$\% stronger scaling trend under STTS. Additionally,
we present three key findings: (1) Existing LLM-as-a-Judge does not inherently
exhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced
datasets continues to demonstrate similarly weak scaling behavior. (3)
Significant scaling trend emerges primarily during the RL phase, suggesting
that effective STTS capability is acquired predominantly through RL training.",2025-05-17,"Chi-Min Chan, Chunpu Xu, Jiaming Ji, Zhen Ye, Pengcheng Wen, Chunyang Jiang, Yaodong Yang, Wei Xue, Sirui Han, Yike Guo",http://arxiv.org/pdf/2505.11875v1,cs.CL
DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation,"Domain-specific QA systems require not just generative fluency but high
factual accuracy grounded in structured expert knowledge. While recent
Retrieval-Augmented Generation (RAG) frameworks improve context recall, they
struggle with integrating heterogeneous data and maintaining reasoning
consistency. To address these challenges, we propose DO-RAG, a scalable and
customizable hybrid QA framework that integrates multi-level knowledge graph
construction with semantic vector retrieval. Our system employs a novel agentic
chain-of-thought architecture to extract structured relationships from
unstructured, multimodal documents, constructing dynamic knowledge graphs that
enhance retrieval precision. At query time, DO-RAG fuses graph and vector
retrieval results to generate context-aware responses, followed by
hallucination mitigation via grounded refinement. Experimental evaluations in
the database and electrical domains show near-perfect recall and over 94%
answer relevancy, with DO-RAG outperforming baseline frameworks by up to
33.38%. By combining traceability, adaptability, and performance efficiency,
DO-RAG offers a reliable foundation for multi-domain, high-precision QA at
scale.",2025-05-17,"David Osei Opoku, Ming Sheng, Yong Zhang",http://arxiv.org/pdf/2505.17058v1,cs.CL
Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity,"Human preference plays a crucial role in the refinement of large language
models (LLMs). However, collecting human preference feedback is costly and most
existing datasets neglect the correlation between personalization and
preferences. To address this issue, we introduce Fair-PP, a synthetic dataset
of personalized preferences targeting social equity, derived from real-world
social survey data, which includes 28 social groups, 98 equity topics, and 5
personal preference dimensions. Leveraging GPT-4o-mini, we engage in
role-playing based on seven representative persona portrayals guided by
existing social survey data, yielding a total of 238,623 preference records.
Through Fair-PP, we also contribute (i) An automated framework for generating
preference data, along with a more fine-grained dataset of personalized
preferences; (ii) analysis of the positioning of the existing mainstream LLMs
across five major global regions within the personalized preference space; and
(iii) a sample reweighting method for personalized preference alignment,
enabling alignment with a target persona while maximizing the divergence from
other personas. Empirical experiments show our method outperforms the
baselines.",2025-05-17,"Qi Zhou, Jie Zhang, Dongxia Wang, Qiang Liu, Tianlin Li, Jin Song Dong, Wenhai Wang, Qing Guo",http://arxiv.org/pdf/2505.11861v1,cs.CL
When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research,"Recent advances in large language models (LLMs) have fueled the vision of
automated scientific discovery, often called AI Co-Scientists. To date, prior
work casts these systems as generative co-authors responsible for crafting
hypotheses, synthesizing code, or drafting manuscripts. In this work, we
explore a complementary application: using LLMs as verifiers to automate the
\textbf{academic verification of scientific manuscripts}. To that end, we
introduce SPOT, a dataset of 83 published papers paired with 91 errors
significant enough to prompt errata or retraction, cross-validated with actual
authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find
that none surpasses 21.1\% recall or 6.1\% precision (o3 achieves the best
scores, with all others near zero). Furthermore, confidence estimates are
uniformly low, and across eight independent runs, models rarely rediscover the
same errors, undermining their reliability. Finally, qualitative analysis with
domain experts reveals that even the strongest models make mistakes resembling
student-level misconceptions derived from misunderstandings. These findings
highlight the substantial gap between current LLM capabilities and the
requirements for dependable AI-assisted academic verification.",2025-05-17,"Guijin Son, Jiwoo Hong, Honglu Fan, Heejeong Nam, Hyunwoo Ko, Seungwon Lim, Jinyeop Song, Jinha Choi, Gonçalo Paulo, Youngjae Yu, Stella Biderman",http://arxiv.org/pdf/2505.11855v1,cs.CL
Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective,"AI is transforming education by enabling powerful tools that enhance learning
experiences. Among recent advancements, large language models (LLMs) hold
particular promise for revolutionizing how learners interact with educational
content. In this work, we investigate the potential of LLMs to support
standardized test preparation by focusing on English Standardized Tests (ESTs).
Specifically, we assess their ability to generate accurate and contextually
appropriate solutions across a diverse set of EST question types. We introduce
ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of
LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,
encompassing 29 question types and over 10,576 questions across multiple
modalities, including text, images, audio, tables, and mathematical symbols.
Using ESTBOOK, we systematically evaluate both the accuracy and inference
efficiency of LLMs. Additionally, we propose a breakdown analysis framework
that decomposes complex EST questions into task-specific solution steps. This
framework allows us to isolate and assess LLM performance at each stage of the
reasoning process. Evaluation findings offer insights into the capability of
LLMs in educational contexts and point toward targeted strategies for improving
their reliability as intelligent tutoring systems.",2025-05-17,"Luoxi Tang, Tharunya Sundar, Shuai Yang, Ankita Patra, Manohar Chippada, Giqi Zhao, Yi Li, Riteng Zhang, Tunan Zhao, Ting Yang, Yuqiao Meng, Weicheng Ma, Zhaohan Xi",http://arxiv.org/pdf/2505.17056v1,cs.CL
Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs,"The increasing deployment of Large Vision-Language Models (LVLMs) raises
safety concerns under potential malicious inputs. However, existing multimodal
safety evaluations primarily focus on model vulnerabilities exposed by static
image inputs, ignoring the temporal dynamics of video that may induce distinct
safety risks. To bridge this gap, we introduce Video-SafetyBench, the first
comprehensive benchmark designed to evaluate the safety of LVLMs under
video-text attacks. It comprises 2,264 video-text pairs spanning 48
fine-grained unsafe categories, each pairing a synthesized video with either a
harmful query, which contains explicit malice, or a benign query, which appears
harmless but triggers harmful behavior when interpreted alongside the video. To
generate semantically accurate videos for safety evaluation, we design a
controllable pipeline that decomposes video semantics into subject images (what
is shown) and motion text (how it moves), which jointly guide the synthesis of
query-relevant videos. To effectively evaluate uncertain or borderline harmful
outputs, we propose RJScore, a novel LLM-based metric that incorporates the
confidence of judge models and human-aligned decision threshold calibration.
Extensive experiments show that benign-query video composition achieves average
attack success rates of 67.2%, revealing consistent vulnerabilities to
video-induced attacks. We believe Video-SafetyBench will catalyze future
research into video-based safety evaluation and defense strategies.",2025-05-17,"Xuannan Liu, Zekun Li, Zheqi He, Peipei Li, Shuhan Xia, Xing Cui, Huaibo Huang, Xi Yang, Ran He",http://arxiv.org/pdf/2505.11842v1,cs.CL
Multilingual Collaborative Defense for Large Language Models,"The robustness and security of large language models (LLMs) has become a
prominent research area. One notable vulnerability is the ability to bypass LLM
safeguards by translating harmful queries into rare or underrepresented
languages, a simple yet effective method of ""jailbreaking"" these models.
Despite the growing concern, there has been limited research addressing the
safeguarding of LLMs in multilingual scenarios, highlighting an urgent need to
enhance multilingual safety. In this work, we investigate the correlation
between various attack features across different languages and propose
Multilingual Collaborative Defense (MCD), a novel learning method that
optimizes a continuous, soft safety prompt automatically to facilitate
multilingual safeguarding of LLMs. The MCD approach offers three advantages:
First, it effectively improves safeguarding performance across multiple
languages. Second, MCD maintains strong generalization capabilities while
minimizing false refusal rates. Third, MCD mitigates the language safety
misalignment caused by imbalances in LLM training corpora. To evaluate the
effectiveness of MCD, we manually construct multilingual versions of commonly
used jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess
various safeguarding methods. Additionally, we introduce these datasets in
underrepresented (zero-shot) languages to verify the language transferability
of MCD. The results demonstrate that MCD outperforms existing approaches in
safeguarding against multilingual jailbreak attempts while also exhibiting
strong language transfer capabilities. Our code is available at
https://github.com/HLiang-Lee/MCD.",2025-05-17,"Hongliang Li, Jinan Xu, Gengping Cui, Changhao Guan, Fengran Mo, Kaiyu Huang",http://arxiv.org/pdf/2505.11835v1,cs.CL
Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks,"Detecting deviant language such as sexism, or nuanced language such as
metaphors or sarcasm, is crucial for enhancing the safety, clarity, and
interpretation of online social discourse. While existing classifiers deliver
strong results on these tasks, they often come with significant computational
cost and high data demands. In this work, we propose \textbf{Cla}ss
\textbf{D}istillation (ClaD), a novel training paradigm that targets the core
challenge: distilling a small, well-defined target class from a highly diverse
and heterogeneous background. ClaD integrates two key innovations: (i) a loss
function informed by the structural properties of class distributions, based on
Mahalanobis distance, and (ii) an interpretable decision algorithm optimized
for class separation. Across three benchmark detection tasks -- sexism,
metaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with
smaller language models and orders of magnitude fewer parameters, achieves
performance comparable to several large language models (LLMs). These results
demonstrate ClaD as an efficient tool for pragmatic language understanding
tasks that require gleaning a small target class from a larger heterogeneous
background.",2025-05-17,"Chenlu Wang, Weimin Lyu, Ritwik Banerjee",http://arxiv.org/pdf/2505.11829v1,cs.CL
Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning,"Compressing long chain-of-thought (CoT) from large language models (LLMs) is
an emerging strategy to improve the reasoning efficiency of LLMs. Despite its
promising benefits, existing studies equally compress all thoughts within a
long CoT, hindering more concise and effective reasoning. To this end, we first
investigate the importance of different thoughts by examining their
effectiveness and efficiency in contributing to reasoning through automatic
long CoT chunking and Monte Carlo rollouts. Building upon the insights, we
propose a theoretically bounded metric to jointly measure the effectiveness and
efficiency of different thoughts. We then propose Long$\otimes$Short, an
efficient reasoning framework that enables two LLMs to collaboratively solve
the problem: a long-thought LLM for more effectively generating important
thoughts, while a short-thought LLM for efficiently generating remaining
thoughts. Specifically, we begin by synthesizing a small amount of cold-start
data to fine-tune LLMs for long-thought and short-thought reasoning styles,
respectively. Furthermore, we propose a synergizing-oriented multi-turn
reinforcement learning, focusing on the model self-evolution and collaboration
between long-thought and short-thought LLMs. Experimental results show that our
method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance
compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while
reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and
GPQA Diamond benchmarks. Our data and code are available at
https://github.com/usail-hkust/LongShort.",2025-05-17,"Yansong Ning, Wei Li, Jun Fang, Naiqiang Tan, Hao Liu",http://arxiv.org/pdf/2505.11827v2,cs.CL
LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades,"As Large Language Models (LLMs) are frequently updated, LoRA weights trained
on earlier versions quickly become obsolete. The conventional practice of
retraining LoRA weights from scratch on the latest model is costly,
time-consuming, and environmentally detrimental, particularly as the diversity
of LLMs and downstream tasks expands. This motivates a critical question: ""How
can we efficiently leverage existing LoRA weights to adapt to newer model
versions?"" To address this, we propose LoRASuite, a modular approach tailored
specifically to various types of LLM updates. First, we compute a transfer
matrix utilizing known parameters from both old and new LLMs. Next, we allocate
corresponding layers and attention heads based on centered kernel alignment and
cosine similarity metrics, respectively. A subsequent small-scale, skillful
fine-tuning step ensures numerical stability. Experimental evaluations
demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA
methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even
exceeds the performance of full-scale LoRA retraining, with average
improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally,
LoRASuite significantly reduces memory consumption by 5.5 GB and computational
time by 78.23%.",2025-05-17,"Yanan Li, Fanxu Meng, Muhan Zhang, Shiai Zhu, Shangguang Wang, Mengwei Xu",http://arxiv.org/pdf/2505.13515v1,cs.CL
Chain-of-Model Learning for Language Model,"In this paper, we propose a novel learning paradigm, termed Chain-of-Model
(CoM), which incorporates the causal relationship into the hidden states of
each layer as a chain style, thereby introducing great scaling efficiency in
model training and inference flexibility in deployment. We introduce the
concept of Chain-of-Representation (CoR), which formulates the hidden states at
each layer as a combination of multiple sub-representations (i.e., chains) at
the hidden dimension level. In each layer, each chain from the output
representations can only view all of its preceding chains in the input
representations. Consequently, the model built upon CoM framework can
progressively scale up the model size by increasing the chains based on the
previous models (i.e., chains), and offer multiple sub-models at varying sizes
for elastic inference by using different chain numbers. Based on this
principle, we devise Chain-of-Language-Model (CoLM), which incorporates the
idea of CoM into each layer of Transformer architecture. Based on CoLM, we
further introduce CoLM-Air by introducing a KV sharing mechanism, that computes
all keys and values within the first chain and then shares across all chains.
This design demonstrates additional extensibility, such as enabling seamless LM
switching, prefilling acceleration and so on. Experimental results demonstrate
our CoLM family can achieve comparable performance to the standard Transformer,
while simultaneously enabling greater flexiblity, such as progressive scaling
to improve training efficiency and offer multiple varying model sizes for
elastic inference, paving a a new way toward building language models. Our code
will be released in the future at: https://github.com/microsoft/CoLM.",2025-05-17,"Kaitao Song, Xiaohua Wang, Xu Tan, Huiqiang Jiang, Chengruidong Zhang, Yongliang Shen, Cen LU, Zihao Li, Zifan Song, Caihua Shan, Yansen Wang, Kan Ren, Xiaoqing Zheng, Tao Qin, Yuqing Yang, Dongsheng Li, Lili Qiu",http://arxiv.org/pdf/2505.11820v2,cs.CL
VenusX: Unlocking Fine-Grained Functional Understanding of Proteins,"Deep learning models have driven significant progress in predicting protein
function and interactions at the protein level. While these advancements have
been invaluable for many biological applications such as enzyme engineering and
function annotation, a more detailed perspective is essential for understanding
protein functional mechanisms and evaluating the biological knowledge captured
by models. To address this demand, we introduce VenusX, the first large-scale
benchmark for fine-grained functional annotation and function-based protein
pairing at the residue, fragment, and domain levels. VenusX comprises three
major task categories across six types of annotations, including residue-level
binary classification, fragment-level multi-class classification, and pairwise
functional similarity scoring for identifying critical active sites, binding
sites, conserved sites, motifs, domains, and epitopes. The benchmark features
over 878,000 samples curated from major open-source databases such as InterPro,
BioLiP, and SAbDab. By providing mixed-family and cross-family splits at three
sequence identity thresholds, our benchmark enables a comprehensive assessment
of model performance on both in-distribution and out-of-distribution scenarios.
For baseline evaluation, we assess a diverse set of popular and open-source
models, including pre-trained protein language models, sequence-structure
hybrids, structure-based methods, and alignment-based techniques. Their
performance is reported across all benchmark datasets and evaluation settings
using multiple metrics, offering a thorough comparison and a strong foundation
for future research. Code and data are publicly available at
https://github.com/ai4protein/VenusX.",2025-05-17,"Yang Tan, Wenrui Gou, Bozitao Zhong, Liang Hong, Huiqun Yu, Bingxin Zhou",http://arxiv.org/pdf/2505.11812v1,cs.CL
BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering,"Multi-hop question answering (QA) involves finding multiple relevant passages
and performing step-by-step reasoning to answer complex questions. Previous
works on multi-hop QA employ specific methods from different modeling
perspectives based on large language models (LLMs), regardless of the question
types. In this paper, we first conduct an in-depth analysis of public multi-hop
QA benchmarks, dividing the questions into four types and evaluating five types
of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step,
Iterative-step, Sub-step, and Adaptive-step. We find that different types of
multi-hop questions have varying degrees of sensitivity to different types of
methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to
address multi-hop QA by specifically focusing on the correspondence between
question types and methods, where each type of method is regarded as an
''operator'' by prompting LLMs differently. The first level of BELLE includes
multiple agents that debate to obtain an executive plan of combined
''operators'' to address the multi-hop QA task comprehensively. During the
debate, in addition to the basic roles of affirmative debater, negative
debater, and judge, at the second level, we further leverage fast and slow
debaters to monitor whether changes in viewpoints are reasonable. Extensive
experiments demonstrate that BELLE significantly outperforms strong baselines
in various datasets. Additionally, the model consumption of BELLE is higher
cost-effectiveness than that of single models in more complex multi-hop QA
scenarios.",2025-05-17,"Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, Xiaofeng He",http://arxiv.org/pdf/2505.11811v1,cs.CL
Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model,"General-purpose large language models demonstrate notable capabilities in
language comprehension and generation, achieving results that are comparable
to, or even surpass, human performance in many natural language processing
tasks. Nevertheless, when general models are applied to some specific domains,
e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and
fine-tuning open-source foundational models similarly struggles to adequately
incorporate domain-specific knowledge. To address this challenge, this study
developed a large language model, AI Taiyan, specifically designed for
understanding and generating Classical Chinese. Experiments show that with a
reasonable model design, data processing, foundational training, and
fine-tuning, satisfactory results can be achieved with only 1.8 billion
parameters. In key tasks related to language processing of Classical Chinese
such as punctuation, identification of allusions, explanation of word meanings,
and translation between ancient and modern Chinese, this model exhibits a clear
advantage over both general-purpose large models and domain-specific
traditional models, achieving levels close to or surpassing human baselines.
This research provides a reference for the efficient construction of
specialized domain-specific large language models. Furthermore, the paper
discusses the application of this model in fields such as the collation of
ancient texts, dictionary editing, and language research, combined with case
studies.",2025-05-17,"Shen Li, Renfen Hu, Lijun Wang",http://arxiv.org/pdf/2505.11810v2,cs.CL
Retrospex: Language Agent Meets Offline Reinforcement Learning Critic,"Large Language Models (LLMs) possess extensive knowledge and commonsense
reasoning capabilities, making them valuable for creating powerful agents.
However, existing LLM agent frameworks have not fully utilized past experiences
for improvement. This work introduces a new LLM-based agent framework called
Retrospex, which addresses this challenge by analyzing past experiences in
depth. Unlike previous approaches, Retrospex does not directly integrate
experiences into the LLM's context. Instead, it combines the LLM's action
likelihood with action values estimated by a Reinforcement Learning (RL)
Critic, which is trained on past experiences through an offline
''retrospection'' process. Additionally, Retrospex employs a dynamic action
rescoring mechanism that increases the importance of experience-based values
for tasks that require more interaction with the environment. We evaluate
Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its
advantages over strong, contemporary baselines.",2025-05-17,"Yufei Xiang, Yiqun Shen, Yeqin Zhang, Cam-Tu Nguyen",http://arxiv.org/pdf/2505.11807v1,cs.CL
Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models,"Repetition curse is a phenomenon where Large Language Models (LLMs) generate
repetitive sequences of tokens or cyclic sequences. While the repetition curse
has been widely observed, its underlying mechanisms remain poorly understood.
In this work, we investigate the role of induction heads--a specific type of
attention head known for their ability to perform in-context learning--in
driving this repetitive behavior. Specifically, we focus on the ""toxicity"" of
induction heads, which we define as their tendency to dominate the model's
output logits during repetition, effectively excluding other attention heads
from contributing to the generation process. Our findings have important
implications for the design and training of LLMs. By identifying induction
heads as a key driver of the repetition curse, we provide a mechanistic
explanation for this phenomenon and suggest potential avenues for mitigation.
We also propose a technique with attention head regularization that could be
employed to reduce the dominance of induction heads during generation, thereby
promoting more diverse and coherent outputs.",2025-05-17,"Shuxun Wang, Qingyu Yin, Chak Tou Leong, Qiang Zhang, Linyi Yang",http://arxiv.org/pdf/2505.13514v1,cs.CL
Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors,"Interpretability research now offers a variety of techniques for identifying
abstract internal mechanisms in neural networks. Can such techniques be used to
predict how models will behave on out-of-distribution examples? In this work,
we provide a positive answer to this question. Through a diverse set of
language modeling tasks--including symbol manipulation, knowledge retrieval,
and instruction following--we show that the most robust features for
correctness prediction are those that play a distinctive causal role in the
model's behavior. Specifically, we propose two methods that leverage causal
mechanisms to predict the correctness of model outputs: counterfactual
simulation (checking whether key causal variables are realized) and value
probing (using the values of those variables to make predictions). Both achieve
high AUC-ROC in distribution and outperform methods that rely on
causal-agnostic features in out-of-distribution settings, where predicting
model behaviors is more crucial. Our work thus highlights a novel and
significant application for internal causal analysis of language models.",2025-05-17,"Jing Huang, Junyi Tao, Thomas Icard, Diyi Yang, Christopher Potts",http://arxiv.org/pdf/2505.11770v1,cs.CL
Towards Universal Semantics With Large Language Models,"The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a
universal set of semantic primes: simple, primitive word-meanings that have
been shown to exist in most, if not all, languages of the world. According to
this framework, any word, regardless of complexity, can be paraphrased using
these primes, revealing a clear and universally translatable meaning. These
paraphrases, known as explications, can offer valuable applications for many
natural language processing (NLP) tasks, but producing them has traditionally
been a slow, manual process. In this work, we present the first study of using
large language models (LLMs) to generate NSM explications. We introduce
automatic evaluation methods, a tailored dataset for training and evaluation,
and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in
producing accurate, cross-translatable explications, marking a significant step
toward universal semantic representation with LLMs and opening up new
possibilities for applications in semantic analysis, translation, and beyond.",2025-05-17,"Raymond Baartmans, Matthew Raffel, Rahul Vikram, Aiden Deringer, Lizhong Chen",http://arxiv.org/pdf/2505.11764v1,cs.CL
Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders,"It is assumed that sparse autoencoders (SAEs) decompose polysemantic
activations into interpretable linear directions, as long as the activations
are composed of sparse linear combinations of underlying features. However, we
find that if an SAE is more narrow than the number of underlying ""true
features"" on which it is trained, and there is correlation between features,
the SAE will merge components of correlated features together, thus destroying
monosemanticity. In LLM SAEs, these two conditions are almost certainly true.
This phenomenon, which we call feature hedging, is caused by SAE reconstruction
loss, and is more severe the narrower the SAE. In this work, we introduce the
problem of feature hedging and study it both theoretically in toy models and
empirically in SAEs trained on LLMs. We suspect that feature hedging may be one
of the core reasons that SAEs consistently underperform supervised baselines.
Finally, we use our understanding of feature hedging to propose an improved
variant of matryoshka SAEs. Our work shows there remain fundamental issues with
SAEs, but we are hopeful that that highlighting feature hedging will catalyze
future advances that allow SAEs to achieve their full potential of interpreting
LLMs at scale.",2025-05-16,"David Chanin, Tomáš Dulka, Adrià Garriga-Alonso",http://arxiv.org/pdf/2505.11756v1,cs.CL
Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation,"Multi-hop Question Answering (MHQA) adds layers of complexity to question
answering, making it more challenging. When Language Models (LMs) are prompted
with multiple search results, they are tasked not only with retrieving relevant
information but also employing multi-hop reasoning across the information
sources. Although LMs perform well on traditional question-answering tasks, the
causal mask can hinder their capacity to reason across complex contexts. In
this paper, we explore how LMs respond to multi-hop questions by permuting
search results (retrieved documents) under various configurations. Our study
reveals interesting findings as follows: 1) Encoder-decoder models, such as the
ones in the Flan-T5 family, generally outperform causal decoder-only LMs in
MHQA tasks, despite being significantly smaller in size; 2) altering the order
of gold documents reveals distinct trends in both Flan T5 models and fine-tuned
decoder-only models, with optimal performance observed when the document order
aligns with the reasoning chain order; 3) enhancing causal decoder-only models
with bi-directional attention by modifying the causal mask can effectively
boost their end performance. In addition to the above, we conduct a thorough
investigation of the distribution of LM attention weights in the context of
MHQA. Our experiments reveal that attention weights tend to peak at higher
values when the resulting answer is correct. We leverage this finding to
heuristically improve LMs' performance on this task. Our code is publicly
available at https://github.com/hwy9855/MultiHopQA-Reasoning.",2025-05-16,"Wenyu Huang, Pavlos Vougiouklis, Mirella Lapata, Jeff Z. Pan",http://arxiv.org/pdf/2505.11754v1,cs.CL
Token Masking Improves Transformer-Based Text Classification,"While transformer-based models achieve strong performance on text
classification, we explore whether masking input tokens can further enhance
their effectiveness. We propose token masking regularization, a simple yet
theoretically motivated method that randomly replaces input tokens with a
special [MASK] token at probability p. This introduces stochastic perturbations
during training, leading to implicit gradient averaging that encourages the
model to capture deeper inter-token dependencies. Experiments on language
identification and sentiment analysis -- across diverse models (mBERT,
Qwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard
regularization techniques. We identify task-specific optimal masking rates,
with p = 0.1 as a strong general default. We attribute the gains to two key
effects: (1) input perturbation reduces overfitting, and (2) gradient-level
smoothing acts as implicit ensembling.",2025-05-16,"Xianglong Xu, John Bowen, Rojin Taheri",http://arxiv.org/pdf/2505.11746v1,cs.CL
ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training,"Recently, training-free methods for improving large language models (LLMs)
have attracted growing interest, with token-level attention tuning emerging as
a promising and interpretable direction. However, existing methods typically
rely on auxiliary mechanisms to identify important or irrelevant task-specific
tokens, introducing potential bias and limiting applicability. In this paper,
we uncover a surprising and elegant alternative: the semantically empty initial
token is a powerful and underexplored control point for optimizing model
behavior. Through theoretical analysis, we show that tuning the initial token's
attention sharpens or flattens the attention distribution over subsequent
tokens, and its role as an attention sink amplifies this effect. Empirically,
we find that: (1) tuning its attention improves LLM performance more
effectively than tuning other task-specific tokens; (2) the effect follows a
consistent trend across layers, with earlier layers having greater impact, but
varies across attention heads, with different heads showing distinct
preferences in how they attend to this token. Based on these findings, we
propose ZeroTuning, a training-free approach that improves LLM performance by
applying head-specific attention adjustments to this special token. Despite
tuning only one token, ZeroTuning achieves higher performance on text
classification, multiple-choice, and multi-turn conversation tasks across
models such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves
Llama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its
multi-turn score from 7.804 to 7.966. The method is also robust to limited
resources, few-shot settings, long contexts, quantization, decoding strategies,
and prompt variations. Our work sheds light on a previously overlooked control
point in LLMs, offering new insights into both inference-time tuning and model
interpretability.",2025-05-16,"Feijiang Han, Xiaodong Yu, Jianheng Tang, Lyle Ungar",http://arxiv.org/pdf/2505.11739v1,cs.CL
Token-Level Uncertainty Estimation for Large Language Model Reasoning,"While Large Language Models (LLMs) have demonstrated impressive capabilities,
their output quality remains inconsistent across various application scenarios,
making it difficult to identify trustworthy responses, especially in complex
tasks requiring multi-step reasoning. In this paper, we propose a token-level
uncertainty estimation framework to enable LLMs to self-assess and self-improve
their generation quality in mathematical reasoning. Specifically, we introduce
low-rank random weight perturbation to LLM decoding, generating predictive
distributions that we use to estimate token-level uncertainties. We then
aggregate these uncertainties to reflect semantic uncertainty of the generated
sequences. Experiments on mathematical reasoning datasets of varying difficulty
demonstrate that our token-level uncertainty metrics strongly correlate with
answer correctness and model robustness. Additionally, we explore using
uncertainty to directly enhance the model's reasoning performance through
multiple generations and the particle filtering algorithm. Our approach
consistently outperforms existing uncertainty estimation methods, establishing
effective uncertainty estimation as a valuable tool for both evaluating and
improving reasoning generation in LLMs.",2025-05-16,"Tunyu Zhang, Haizhou Shi, Yibin Wang, Hengyi Wang, Xiaoxiao He, Zhuowei Li, Haoxian Chen, Ligong Han, Kai Xu, Huan Zhang, Dimitris Metaxas, Hao Wang",http://arxiv.org/pdf/2505.11737v1,cs.CL
"Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale","This study explores Large Language Models (LLMs) as autonomous agents for
real-world tasks, including freelance software development. This work presents
a new benchmark that evaluates LLMs on freelance programming and data analysis
tasks derived from economic data. We construct the benchmark using synthetic
tasks created from a Kaggle Freelancer dataset of job postings, with all job
prices standardized to USD (median fixed-project price around $250, and an
average of $306). Each task is accompanied by structured input-output test
cases and an estimated price tag, enabling automated correctness checking and a
monetary performance valuation. This approach is inspired by OpenAI's recent
SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our
framework simplifies evaluation using programmatically testable tasks and
predicted price values, making it highly scalable and repeatable. On this
benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen
2.5, and Mistral. We report each model's accuracy (task success rate and
test-case pass rate) and the total ""freelance earnings"" it achieves (sum of
prices of solved tasks). Our results show that Claude 3.5 Haiku performs best,
earning approximately $1.52 million USD, followed closely by GPT-4o-mini at
$1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the
distribution of errors per task and observe that the strongest models solve the
most tasks and rarely fail completely on any project. We discuss the
implications of these results for the feasibility of AI as a freelance
developer, the advantages and limitations of our automated benchmark approach,
and the gap between performance on structured tasks versus the true complexity
of real-world freelance jobs.",2025-05-16,"David Noever, Forrest McKee",http://arxiv.org/pdf/2505.13511v1,cs.CL
MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports,"Doctors and patients alike increasingly use Large Language Models (LLMs) to
diagnose clinical cases. However, unlike domains such as math or coding, where
correctness can be objectively defined by the final answer, medical diagnosis
requires both the outcome and the reasoning process to be accurate. Currently,
widely used medical benchmarks like MedQA and MMLU assess only accuracy in the
final answer, overlooking the quality and faithfulness of the clinical
reasoning process. To address this limitation, we introduce MedCaseReasoning,
the first open-access dataset for evaluating LLMs on their ability to align
with clinician-authored diagnostic reasoning. The dataset includes 14,489
diagnostic question-and-answer cases, each paired with detailed reasoning
statements derived from open-access medical case reports. We evaluate
state-of-the-art reasoning LLMs on MedCaseReasoning and find significant
shortcomings in their diagnoses and reasoning: for instance, the top-performing
open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy
and mentions only 64% of the clinician reasoning statements (recall). However,
we demonstrate that fine-tuning LLMs on the reasoning traces derived from
MedCaseReasoning significantly improves diagnostic accuracy and clinical
reasoning recall by an average relative gain of 29% and 41%, respectively. The
open-source dataset, code, and models are available at
https://github.com/kevinwu23/Stanford-MedCaseReasoning.",2025-05-16,"Kevin Wu, Eric Wu, Rahul Thapa, Kevin Wei, Angela Zhang, Arvind Suresh, Jacqueline J. Tao, Min Woo Sun, Alejandro Lozano, James Zou",http://arxiv.org/pdf/2505.11733v2,cs.CL
Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models,"Recent advances in uncertainty estimation for Large Language Models (LLMs)
during downstream adaptation have addressed key challenges of reliability and
simplicity. However, existing Bayesian methods typically require multiple
sampling iterations during inference, creating significant efficiency issues
that limit practical deployment. In this paper, we investigate the possibility
of eliminating the need for test-time sampling for LLM uncertainty estimation.
Specifically, when given an off-the-shelf Bayesian LLM, we distill its aligned
confidence into a non-Bayesian student LLM by minimizing the divergence between
their predictive distributions. Unlike typical calibration methods, our
distillation is carried out solely on the training dataset without the need of
an additional validation dataset. This simple yet effective approach achieves
N-times more efficient uncertainty estimation during testing, where N is the
number of samples traditionally required by Bayesian LLMs. Our extensive
experiments demonstrate that uncertainty estimation capabilities on training
data can successfully generalize to unseen test data through our distillation
technique, consistently producing results comparable to (or even better than)
state-of-the-art Bayesian LLMs.",2025-05-16,"Harshil Vejendla, Haizhou Shi, Yibin Wang, Tunyu Zhang, Huan Zhang, Hao Wang",http://arxiv.org/pdf/2505.11731v2,cs.CL
Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures,"Multimodal reference resolution, including phrase grounding, aims to
understand the semantic relations between mentions and real-world objects.
Phrase grounding between images and their captions is a well-established task.
In contrast, for real-world applications, it is essential to integrate textual
and multimodal reference resolution to unravel the reference relations within
dialogue, especially in handling ambiguities caused by pronouns and ellipses.
This paper presents a framework that unifies textual and multimodal reference
resolution by mapping mention embeddings to object embeddings and selecting
mentions or objects based on their similarity. Our experiments show that
learning textual reference resolution, such as coreference resolution and
predicate-argument structure analysis, positively affects performance in
multimodal reference resolution. In particular, our model with coreference
resolution performs better in pronoun phrase grounding than representative
models for this task, MDETR and GLIP. Our qualitative analysis demonstrates
that incorporating textual reference relations strengthens the confidence
scores between mentions, including pronouns and predicates, and objects, which
can reduce the ambiguities that arise in visually grounded dialogues.",2025-05-16,"Shun Inadumi, Nobuhiro Ueda, Koichiro Yoshino",http://arxiv.org/pdf/2505.11726v1,cs.CL
EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents,"Multi-modal large language model (MLLM)-based web agents interact with
webpage environments by generating actions based on screenshots of the
webpages. Environmental prompt injection attacks manipulate the environment to
induce the web agent to perform a specific, attacker-chosen action--referred to
as the target action. However, existing attacks suffer from limited
effectiveness or stealthiness, or are impractical in real-world settings. In
this work, we propose EnvInjection, a new attack that addresses these
limitations. Our attack adds a perturbation to the raw pixel values of the
rendered webpage, which can be implemented by modifying the webpage's source
code. After these perturbed pixels are mapped into a screenshot, the
perturbation induces the web agent to perform the target action. We formulate
the task of finding the perturbation as an optimization problem. A key
challenge in solving this problem is that the mapping between raw pixel values
and screenshot is non-differentiable, making it difficult to backpropagate
gradients to the perturbation. To overcome this, we train a neural network to
approximate the mapping and apply projected gradient descent to solve the
reformulated optimization problem. Extensive evaluation on multiple webpage
datasets shows that EnvInjection is highly effective and significantly
outperforms existing baselines.",2025-05-16,"Xilong Wang, John Bloch, Zedian Shao, Yuepeng Hu, Shuyan Zhou, Neil Zhenqiang Gong",http://arxiv.org/pdf/2505.11717v1,cs.CL
Hierarchical Bracketing Encodings for Dependency Parsing as Tagging,"We present a family of encodings for sequence labeling dependency parsing,
based on the concept of hierarchical bracketing. We prove that the existing
4-bit projective encoding belongs to this family, but it is suboptimal in the
number of labels used to encode a tree. We derive an optimal hierarchical
bracketing, which minimizes the number of symbols used and encodes projective
trees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also
extend optimal hierarchical bracketing to support arbitrary non-projectivity in
a more compact way than previous encodings. Our new encodings yield competitive
accuracy on a diverse set of treebanks.",2025-05-16,"Ana Ezquerro, David Vilares, Anssi Yli-Jyrä, Carlos Gómez-Rodríguez",http://arxiv.org/pdf/2505.11693v1,cs.CL
Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions,"Automatic Speech Recognition (ASR) technologies have transformed
human-computer interaction; however, low-resource languages in Africa remain
significantly underrepresented in both research and practical applications.
This study investigates the major challenges hindering the development of ASR
systems for these languages, which include data scarcity, linguistic
complexity, limited computational resources, acoustic variability, and ethical
concerns surrounding bias and privacy. The primary goal is to critically
analyze these barriers and identify practical, inclusive strategies to advance
ASR technologies within the African context. Recent advances and case studies
emphasize promising strategies such as community-driven data collection,
self-supervised and multilingual learning, lightweight model architectures, and
techniques that prioritize privacy. Evidence from pilot projects involving
various African languages showcases the feasibility and impact of customized
solutions, which encompass morpheme-based modeling and domain-specific ASR
applications in sectors like healthcare and education. The findings highlight
the importance of interdisciplinary collaboration and sustained investment to
tackle the distinct linguistic and infrastructural challenges faced by the
continent. This study offers a progressive roadmap for creating ethical,
efficient, and inclusive ASR systems that not only safeguard linguistic
diversity but also improve digital accessibility and promote socioeconomic
participation for speakers of African languages.",2025-05-16,"Sukairaj Hafiz Imam, Babangida Sani, Dawit Ketema Gete, Bedru Yimam Ahamed, Ibrahim Said Ahmad, Idris Abdulmumin, Seid Muhie Yimam, Muhammad Yahuza Bello, Shamsuddeen Hassan Muhammad",http://arxiv.org/pdf/2505.11690v1,cs.CL
Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation,"Entity disambiguation (ED) is the task of linking mentions in text to
corresponding entries in a knowledge base. Dual Encoders address this by
embedding mentions and label candidates in a shared embedding space and
applying a similarity metric to predict the correct label. In this work, we
focus on evaluating key design decisions for Dual Encoder-based ED, such as its
loss function, similarity metric, label verbalization format, and negative
sampling strategy. We present the resulting model VerbalizED, a document-level
Dual Encoder model that includes contextual label verbalizations and efficient
hard negative sampling. Additionally, we explore an iterative prediction
variant that aims to improve the disambiguation of challenging data points.
Comprehensive experiments on AIDA-Yago validate the effectiveness of our
approach, offering insights into impactful design choices that result in a new
State-of-the-Art system on the ZELDA benchmark.",2025-05-16,"Susanna Rücker, Alan Akbik",http://arxiv.org/pdf/2505.11683v1,cs.CL
Ambiguity Resolution in Text-to-Structured Data Mapping,"Ambiguity in natural language is a significant obstacle for achieving
accurate text to structured data mapping through large language models (LLMs),
which affects the performance of tasks such as mapping text to agentic tool
calling and text-to-SQL queries. Existing methods of ambiguity handling either
exploit ReACT framework to produce the correct mapping through trial and error,
or supervised fine tuning to guide models to produce a biased mapping to
improve certain tasks. In this paper, we adopt a different approach that
characterizes the representation difference of ambiguous text in the latent
space and leverage the difference to identify ambiguity before mapping them to
structured data. To detect ambiguity of a sentence, we focused on the
relationship between ambiguous questions and their interpretations and what
cause the LLM ignore multiple interpretations. Different to the distance
calculated by dense embedding vectors, we utilize the observation that
ambiguity is caused by concept missing in latent space of LLM to design a new
distance measurement, computed through the path kernel by the integral of
gradient values for each concepts from sparse-autoencoder (SAE) under each
state. We identify patterns to distinguish ambiguous questions with this
measurement. Based on our observation, We propose a new framework to improve
the performance of LLMs on ambiguous agentic tool calling through missing
concepts prediction.",2025-05-16,"Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-Young Paik, Liming Zhu",http://arxiv.org/pdf/2505.11679v1,cs.CL
Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks,"Large language models (LLMs) have demonstrated impressive performance across
a wide range of Natural Language Processing (NLP) tasks. However, ensuring
their effectiveness across multiple languages presents unique challenges.
Multilingual prompt engineering has emerged as a key approach to enhance LLMs'
capabilities in diverse linguistic settings without requiring extensive
parameter re-training or fine-tuning. With growing interest in multilingual
prompt engineering over the past two to three years, researchers have explored
various strategies to improve LLMs' performance across languages and NLP tasks.
By crafting structured natural language prompts, researchers have successfully
extracted knowledge from LLMs across different languages, making these
techniques an accessible pathway for a broader audience, including those
without deep expertise in machine learning, to harness the capabilities of
LLMs. In this paper, we survey and categorize different multilingual prompting
techniques based on the NLP tasks they address across a diverse set of datasets
that collectively span around 250 languages. We further highlight the LLMs
employed, present a taxonomy of approaches and discuss potential
state-of-the-art (SoTA) methods for specific multilingual datasets.
Additionally, we derive a range of insights across language families and
resource levels (high-resource vs. low-resource), including analyses such as
the distribution of NLP tasks by language resource type and the frequency of
prompting methods across different language families. Our survey reviews 36
research papers covering 39 prompting techniques applied to 30 multilingual NLP
tasks, with the majority of these studies published in the last two years.",2025-05-16,"Shubham Vatsal, Harsh Dubey, Aditi Singh",http://arxiv.org/pdf/2505.11665v1,cs.CL
Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset,"The study aims to enhance mathematics education accessibility for
hard-of-hearing students by developing an accurate Palestinian sign language
PSL recognition system using advanced artificial intelligence techniques. Due
to the scarcity of digital resources for PSL, a custom dataset comprising 41
mathematical gesture classes was created, and recorded by PSL experts to ensure
linguistic accuracy and domain specificity. To leverage
state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was
fine-tuned for gesture classification. The model achieved an accuracy of
97.59%, demonstrating its effectiveness in recognizing mathematical signs with
high precision and reliability. This study highlights the role of deep learning
in developing intelligent educational tools that bridge the learning gap for
hard-of-hearing students by providing AI-driven interactive solutions to
enhance mathematical comprehension. This work represents a significant step
toward innovative and inclusive frosting digital integration in specialized
learning environments. The dataset is hosted on Hugging Face at
https://huggingface.co/datasets/fidaakh/STEM_data.",2025-05-16,"Fidaa khandaqji, Huthaifa I. Ashqar, Abdelrahem Atawnih",http://arxiv.org/pdf/2505.17055v1,cs.CL
Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2,"We demonstrate that a developmentally ordered curriculum markedly improves
reasoning transparency and sample-efficiency in small language models (SLMs).
Concretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage
syllabus that ascends from lexical matching to multi-step symbolic inference
and then evaluate it without any task-specific fine-tuning. Cognivolve reaches
target accuracy in half the optimization steps of a single-phase baseline,
activates an order-of-magnitude more gradient-salient reasoning heads, and
shifts those heads toward deeper layers, yielding higher-entropy attention that
balances local and long-range context. The same curriculum applied out of order
or with optimizer resets fails to reproduce these gains, confirming that
progression--not extra compute--drives the effect. We also identify open
challenges: final-answer success still lags a conventional run by about 30%,
and our saliency probe under-detects verbal-knowledge heads in the hardest
stage, suggesting directions for mixed-stage fine-tuning and probe expansion.",2025-05-16,Xiang Fu,http://arxiv.org/pdf/2505.11643v1,cs.CL
Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation,"Supervised fine-tuning (SFT) using expert demonstrations often suffer from
the imitation problem, where the model learns to reproduce the correct
responses without understanding the underlying rationale. To address this
limitation, we propose Critique-Guided Distillation (CGD), a novel multi-stage
framework that integrates teacher model generated explanatory critiques and
refined responses into the SFT process. A student model is then trained to map
the triplet of prompt, teacher critique, and its own initial response to the
corresponding refined teacher response, thereby learning both what to imitate
and why. Using entropy-based analysis, we show that CGD reduces refinement
uncertainty and can be interpreted as a Bayesian posterior update. We perform
extensive empirical evaluation of CGD, on variety of benchmark tasks, and
demonstrate significant gains on both math (AMC23 +17.5%) and language
understanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format
drift issues observed in previous critique fine-tuning (CFT) techniques.",2025-05-16,"Berkcan Kapusuzoglu, Supriyo Chakraborty, Chia-Hsuan Lee, Sambit Sahu",http://arxiv.org/pdf/2505.11628v2,cs.CL
THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering,"We propose THELMA (Task Based Holistic Evaluation of Large Language Model
Applications), a reference free framework for RAG (Retrieval Augmented
generation) based question answering (QA) applications. THELMA consist of six
interdependent metrics specifically designed for holistic, fine grained
evaluation of RAG QA applications. THELMA framework helps developers and
application owners evaluate, monitor and improve end to end RAG QA pipelines
without requiring labelled sources or reference responses.We also present our
findings on the interplay of the proposed THELMA metrics, which can be
interpreted to identify the specific RAG component needing improvement in QA
applications.",2025-05-16,"Udita Patel, Rutu Mulkar, Jay Roberts, Cibi Chakravarthy Senthilkumar, Sujay Gandhi, Xiaofei Zheng, Naumaan Nayyar, Rafael Castrillo",http://arxiv.org/pdf/2505.11626v1,cs.CL
Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations,"Changing the behavior of large language models (LLMs) can be as
straightforward as editing the Transformer's residual streams using
appropriately constructed ""steering vectors."" These modifications to internal
neural activations, a form of representation engineering, offer an effective
and targeted means of influencing model behavior without retraining or
fine-tuning the model. But how can such steering vectors be systematically
identified? We propose a principled approach for uncovering steering vectors by
aligning latent representations elicited through behavioral methods
(specifically, Markov chain Monte Carlo with LLMs) with their neural
counterparts. To evaluate this approach, we focus on extracting latent risk
preferences from LLMs and steering their risk-related outputs using the aligned
representations as steering vectors. We show that the resulting steering
vectors successfully and reliably modulate LLM outputs in line with the
targeted behavior.",2025-05-16,"Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths",http://arxiv.org/pdf/2505.11615v1,cs.CL
Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions,"A central goal of cognitive modeling is to develop models that not only
predict human behavior but also provide insight into the underlying cognitive
mechanisms. While neural network models trained on large-scale behavioral data
often achieve strong predictive performance, they typically fall short in
offering interpretable explanations of the cognitive processes they capture. In
this work, we explore the potential of pretrained large language models (LLMs)
to serve as dual-purpose cognitive models--capable of both accurate prediction
and interpretable explanation in natural language. Specifically, we employ
reinforcement learning with outcome-based rewards to guide LLMs toward
generating explicit reasoning traces for explaining human risky choices. Our
findings demonstrate that this approach produces high-quality explanations
alongside strong quantitative predictions of human decisions.",2025-05-16,"Jian-Qiao Zhu, Hanbo Xie, Dilip Arumugam, Robert C. Wilson, Thomas L. Griffiths",http://arxiv.org/pdf/2505.11614v1,cs.CL
MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models,"Clinical guidelines, typically structured as decision trees, are central to
evidence-based medical practice and critical for ensuring safe and accurate
diagnostic decision-making. However, it remains unclear whether Large Language
Models (LLMs) can reliably follow such structured protocols. In this work, we
introduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to
make guideline-consistent clinical decisions. MedGUIDE is constructed from 55
curated NCCN decision trees across 17 cancer types and uses clinical scenarios
generated by LLMs to create a large pool of multiple-choice diagnostic
questions. We apply a two-stage quality selection process, combining
expert-labeled reward models and LLM-as-a-judge ensembles across ten clinical
and linguistic criteria, to select 7,747 high-quality samples. We evaluate 25
LLMs spanning general-purpose, open-source, and medically specialized models,
and find that even domain-specific LLMs often underperform on tasks requiring
structured guideline adherence. We also test whether performance can be
improved via in-context guideline inclusion or continued pretraining. Our
findings underscore the importance of MedGUIDE in assessing whether LLMs can
operate safely within the procedural frameworks expected in real-world clinical
settings.",2025-05-16,"Xiaomin Li, Mingye Gao, Yuexing Hao, Taoran Li, Guangya Wan, Zihan Wang, Yijun Wang",http://arxiv.org/pdf/2505.11613v1,cs.CL
Probing the Vulnerability of Large Language Models to Polysemantic Interventions,"Polysemanticity -- where individual neurons encode multiple unrelated
features -- is a well-known characteristic of large neural networks and remains
a central challenge in the interpretability of language models. At the same
time, its implications for model safety are also poorly understood. Leveraging
recent advances in sparse autoencoders, we investigate the polysemantic
structure of two small models (Pythia-70M and GPT-2-Small) and evaluate their
vulnerability to targeted, covert interventions at the prompt, feature, token,
and neuron levels. Our analysis reveals a consistent polysemantic topology
shared across both models. Strikingly, we demonstrate that this structure can
be exploited to mount effective interventions on two larger, black-box
instruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These
findings suggest not only the generalizability of the interventions but also
point to a stable and transferable polysemantic structure that could
potentially persist across architectures and training regimes.",2025-05-16,"Bofan Gong, Shiyang Lai, Dawn Song",http://arxiv.org/pdf/2505.11611v1,cs.CL
Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing,"Editing presentation slides remains one of the most common and time-consuming
tasks faced by millions of users daily, despite significant advances in
automated slide generation. Existing approaches have successfully demonstrated
slide editing via graphic user interface (GUI)-based agents, offering intuitive
visual control. However, such methods often suffer from high computational cost
and latency. In this paper, we propose Talk-to-Your-Slides, an LLM-powered
agent designed to edit slides %in active PowerPoint sessions by leveraging
structured information about slide objects rather than relying on image
modality. The key insight of our work is designing the editing process with
distinct high-level and low-level layers to facilitate interaction between user
commands and slide objects. By providing direct access to application objects
rather than screen pixels, our system enables 34.02% faster processing, 34.76%
better instruction fidelity, and 87.42% cheaper operation than baselines. To
evaluate slide editing capabilities, we introduce TSBench, a human-annotated
dataset comprising 379 diverse editing instructions paired with corresponding
slide variations in four categories. Our code, benchmark and demos are
available at https://anonymous.4open.science/r/Talk-to-Your-Slides-0F4C.",2025-05-16,"Kyudan Jung, Hojun Cho, Jooyeol Yun, Soyoung Yang, Jaehyeok Jang, Jaegul Choo",http://arxiv.org/pdf/2505.11604v3,cs.CL
Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO,"Reinforcement learning (RL) has demonstrated significant success in enhancing
reasoning capabilities in large language models (LLMs). One of the most widely
used RL methods is Group Relative Policy Optimization
(GRPO)~\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and
success in training DeepSeek-R1~\cite{Guo-2025-Deepseek}. However, GRPO stalls
when all sampled responses in a group are incorrect -- referred to as an
\emph{all-negative-sample} group -- as it fails to update the policy, hindering
learning progress. The contributions of this paper are two-fold. First, we
propose a simple yet effective framework that introduces response diversity
within all-negative-sample groups in GRPO using AI feedback. We also provide a
theoretical analysis, via a stylized model, showing how this diversification
improves learning dynamics. Second, we empirically validate our approach,
showing the improved performance across various model sizes (7B, 14B, 32B) in
both offline and online learning settings with 10 benchmarks, including base
and distilled variants. Our findings highlight that learning from
all-negative-sample groups is not only feasible but beneficial, advancing
recent insights from \citet{Xiong-2025-Minimalist}.",2025-05-16,"Peter Chen, Xiaopeng Li, Ziniu Li, Xi Chen, Tianyi Lin",http://arxiv.org/pdf/2505.11595v1,cs.CL
Modeling cognitive processes of natural reading with transformer-based Language Models,"Recent advances in Natural Language Processing (NLP) have led to the
development of highly sophisticated language models for text generation. In
parallel, neuroscience has increasingly employed these models to explore
cognitive processes involved in language comprehension. Previous research has
shown that models such as N-grams and LSTM networks can partially account for
predictability effects in explaining eye movement behaviors, specifically Gaze
Duration, during reading. In this study, we extend these findings by evaluating
transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate
this relationship. Our results indicate that these architectures outperform
earlier models in explaining the variance in Gaze Durations recorded from
Rioplantense Spanish readers. However, similar to previous studies, these
models still fail to account for the entirety of the variance captured by human
predictability. These findings suggest that, despite their advancements,
state-of-the-art language models continue to predict language in ways that
differ from human readers.",2025-05-16,"Bruno Bianchi, Fermín Travi, Juan E. Kamienkowski",http://arxiv.org/pdf/2505.11485v1,cs.CL
SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning,"Test-Time Scaling (TTS) refers to approaches that improve reasoning
performance by allocating extra computation during inference, without altering
the model's parameters. While existing TTS methods operate in a discrete token
space by generating more intermediate steps, recent studies in Coconut and
SoftCoT have demonstrated that thinking in the continuous latent space can
further enhance the reasoning performance. Such latent thoughts encode
informative thinking without the information loss associated with
autoregressive token generation, sparking increased interest in
continuous-space reasoning. Unlike discrete decoding, where repeated sampling
enables exploring diverse reasoning paths, latent representations in continuous
space are fixed for a given input, which limits diverse exploration, as all
decoded paths originate from the same latent thought. To overcome this
limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling
paradigm by enabling diverse exploration of thinking paths. Specifically, we
perturb latent thoughts via multiple specialized initial tokens and apply
contrastive learning to promote diversity among soft thought representations.
Experiments across five reasoning benchmarks and two distinct LLM architectures
demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms
SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility
with conventional scaling techniques such as self-consistency. Source code is
available at https://github.com/xuyige/SoftCoT.",2025-05-16,"Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao",http://arxiv.org/pdf/2505.11484v1,cs.CL
Improving Assembly Code Performance with Large Language Models via Reinforcement Learning,"Large language models (LLMs) have demonstrated strong performance across a
wide range of programming tasks, yet their potential for code optimization
remains underexplored. This work investigates whether LLMs can optimize the
performance of assembly code, where fine-grained control over execution enables
improvements that are difficult to express in high-level languages. We present
a reinforcement learning framework that trains LLMs using Proximal Policy
Optimization (PPO), guided by a reward function that considers both functional
correctness, validated through test cases, and execution performance relative
to the industry-standard compiler gcc -O3. To support this study, we introduce
a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,
achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3
baseline, outperforming all 20 other models evaluated, including
Claude-3.7-sonnet. These results indicate that reinforcement learning can
unlock the potential of LLMs to serve as effective optimizers for assembly code
performance.",2025-05-16,"Anjiang Wei, Tarun Suresh, Huanmi Tan, Yinglun Xu, Gagandeep Singh, Ke Wang, Alex Aiken",http://arxiv.org/pdf/2505.11480v1,cs.CL
HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages,"Preference datasets are essential for training general-domain,
instruction-following language models with Reinforcement Learning from Human
Feedback (RLHF). Each subsequent data release raises expectations for future
data collection, meaning there is a constant need to advance the quality and
diversity of openly available preference data. To address this need, we
introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),
high-quality, human-annotated preference dataset comprising of over 40,000
samples. These samples span diverse real-world applications of large language
models (LLMs), including tasks relating to STEM, coding and multilingual
scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that
achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This
represents a substantial improvement (~10% absolute) over the previously
best-reported results from existing RMs. We demonstrate HelpSteer3-Preference
can also be applied to train Generative RMs and how policy models can be
aligned with RLHF using our RMs. Dataset (CC-BY-4.0):
https://huggingface.co/datasets/nvidia/HelpSteer3#preference",2025-05-16,"Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin, Ellie Evans, Yi Dong, Oleksii Kuchaiev",http://arxiv.org/pdf/2505.11475v1,cs.CL
"No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies","We introduce two reference-free metrics for quality evaluation of taxonomies.
The first metric evaluates robustness by calculating the correlation between
semantic and taxonomic similarity, covering a type of error not handled by
existing metrics. The second uses Natural Language Inference to assess logical
adequacy. Both metrics are tested on five taxonomies and are shown to correlate
well with F1 against gold-standard taxonomies.",2025-05-16,"Pascal Wullschleger, Majid Zarharan, Donnacha Daly, Marc Pouly, Jennifer Foster",http://arxiv.org/pdf/2505.11470v1,cs.CL
Disentangling Reasoning and Knowledge in Medical Large Language Models,"Medical reasoning in large language models (LLMs) aims to emulate clinicians'
diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and
PubMedQA often mix reasoning with factual recall. We address this by separating
11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using
a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human
performance. Our analysis shows that only 32.8 percent of questions require
complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)
and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent
gaps between knowledge and reasoning performance. For example, m1 scores 60.5
on knowledge but only 47.1 on reasoning. In adversarial tests where models are
misled with incorrect initial reasoning, biomedical models degrade sharply,
while larger or RL-trained general models show more robustness. To address
this, we train BioMed-R1 using fine-tuning and reinforcement learning on
reasoning-heavy examples. It achieves the strongest performance among similarly
sized models. Further gains may come from incorporating clinical case reports
and training with adversarial and backtracking scenarios.",2025-05-16,"Rahul Thapa, Qingyang Wu, Kevin Wu, Harrison Zhang, Angela Zhang, Eric Wu, Haotian Ye, Suhana Bedi, Nevin Aresh, Joseph Boen, Shriya Reddy, Ben Athiwaratkun, Shuaiwen Leon Song, James Zou",http://arxiv.org/pdf/2505.11462v1,cs.CL
Is Compression Really Linear with Code Intelligence?,"Understanding the relationship between data compression and the capabilities
of Large Language Models (LLMs) is crucial, especially in specialized domains
like code intelligence. Prior work posited a linear relationship between
compression and general intelligence. However, it overlooked the multifaceted
nature of code that encompasses diverse programming languages and tasks, and
struggled with fair evaluation of modern Code LLMs. We address this by
evaluating a diverse array of open-source Code LLMs on comprehensive
multi-language, multi-task code benchmarks. To address the challenge of
efficient and fair evaluation of pre-trained LLMs' code intelligence, we
introduce \textit{Format Annealing}, a lightweight, transparent training
methodology designed to assess the intrinsic capabilities of these pre-trained
models equitably. Compression efficacy, measured as bits-per-character (BPC),
is determined using a novel, large-scale, and previously unseen code validation
set derived from GitHub. Our empirical results reveal a fundamental logarithmic
relationship between measured code intelligence and BPC. This finding refines
prior hypotheses of linearity, which we suggest are likely observations of the
logarithmic curve's tail under specific, limited conditions. Our work provides
a more nuanced understanding of compression's role in developing code
intelligence and contributes a robust evaluation framework in the code domain.",2025-05-16,"Xianzhen Luo, Shijie Xuyang, Tianhao Cheng, Zheng Chu, Houyi Li, ziqi wang, Siming Huang, Qingfu Zhu, Qiufeng Wang, Xiangyu Zhang, Shuigeng Zhou, Wanxiang Che",http://arxiv.org/pdf/2505.11441v2,cs.CL
GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art,"Video Comment Art enhances user engagement by providing creative content that
conveys humor, satire, or emotional resonance, requiring a nuanced and
comprehensive grasp of cultural and contextual subtleties. Although Multimodal
Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated
strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they
still struggle to generate creative expressions such as resonant jokes and
insightful satire. Moreover, existing benchmarks are constrained by their
limited modalities and insufficient categories, hindering the exploration of
comprehensive creativity in video-based Comment Art creation. To address these
limitations, we introduce GODBench, a novel benchmark that integrates video and
text modalities to systematically evaluate MLLMs' abilities to compose Comment
Art. Furthermore, inspired by the propagation patterns of waves in physics, we
propose Ripple of Thought (RoT), a multi-step reasoning framework designed to
enhance the creativity of MLLMs. Extensive experiments reveal that existing
MLLMs and CoT methods still face significant challenges in understanding and
generating creative video comments. In contrast, RoT provides an effective
approach to improve creative composing, highlighting its potential to drive
meaningful advancements in MLLM-based creativity. GODBench is publicly
available at https://github.com/stan-lei/GODBench-ACL2025.",2025-05-16,"Yiming Lei, Chenkai Zhang, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang",http://arxiv.org/pdf/2505.11436v2,cs.CL
When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs,"Reasoning-enhanced large language models (RLLMs), whether explicitly trained
for reasoning or prompted via chain-of-thought (CoT), have achieved
state-of-the-art performance on many complex reasoning tasks. However, we
uncover a surprising and previously overlooked phenomenon: explicit CoT
reasoning can significantly degrade instruction-following accuracy. Evaluating
15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)
and ComplexBench (with complex, compositional constraints), we consistently
observe performance drops when CoT prompting is applied. Through large-scale
case studies and an attention-based analysis, we identify common patterns where
reasoning either helps (e.g., with formatting or lexical precision) or hurts
(e.g., by neglecting simple constraints or introducing unnecessary content). We
propose a metric, constraint attention, to quantify model focus during
generation and show that CoT reasoning often diverts attention away from
instruction-relevant tokens. To mitigate these effects, we introduce and
evaluate four strategies: in-context learning, self-reflection, self-selective
reasoning, and classifier-selective reasoning. Our results demonstrate that
selective reasoning strategies, particularly classifier-selective reasoning,
can substantially recover lost performance. To our knowledge, this is the first
work to systematically expose reasoning-induced failures in
instruction-following and offer practical mitigation strategies.",2025-05-16,"Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal",http://arxiv.org/pdf/2505.11423v2,cs.CL
Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model,"This work explores the journey towards achieving Bahnaric-Vietnamese
translation for the sake of culturally bridging the two ethnic groups in
Vietnam. However, translating from Bahnaric to Vietnamese also encounters some
difficulties. The most prominent challenge is the lack of available original
Bahnaric resources source language, including vocabulary, grammar, dialogue
patterns and bilingual corpus, which hinders the data collection process for
training. To address this, we leverage a transfer learning approach using
sequence-to-sequence pre-training language model. First of all, we leverage a
pre-trained Vietnamese language model to capture the characteristics of this
language. Especially, to further serve the purpose of machine translation, we
aim for a sequence-to-sequence model, not encoder-only like BERT or
decoder-only like GPT. Taking advantage of significant similarity between the
two languages, we continue training the model with the currently limited
bilingual resources of Vietnamese-Bahnaric text to perform the transfer
learning from language model to machine translation. Thus, this approach can
help to handle the problem of imbalanced resources between two languages, while
also optimizing the training and computational processes. Additionally, we also
enhanced the datasets using data augmentation to generate additional resources
and defined some heuristic methods to help the translation more precise. Our
approach has been validated to be highly effective for the Bahnaric-Vietnamese
translation model, contributing to the expansion and preservation of languages,
and facilitating better mutual understanding between the two ethnic people.",2025-05-16,"Phan Tran Minh Dat, Vo Hoang Nhat Khang, Quan Thanh Tho",http://arxiv.org/pdf/2505.11421v1,cs.CL
CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs,"Large language models (LLMs) are increasingly deployed in medical contexts,
raising critical concerns about safety, alignment, and susceptibility to
adversarial manipulation. While prior benchmarks assess model refusal
capabilities for harmful prompts, they often lack clinical specificity, graded
harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES
(Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for
evaluating LLM safety in healthcare. CARES includes over 18,000 prompts
spanning eight medical safety principles, four harm levels, and four prompting
styles: direct, indirect, obfuscated, and role-play, to simulate both malicious
and benign use cases. We propose a three-way response evaluation protocol
(Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess
model behavior. Our analysis reveals that many state-of-the-art LLMs remain
vulnerable to jailbreaks that subtly rephrase harmful prompts, while also
over-refusing safe but atypically phrased queries. Finally, we propose a
mitigation strategy using a lightweight classifier to detect jailbreak attempts
and steer models toward safer behavior via reminder-based conditioning. CARES
provides a rigorous framework for testing and improving medical LLM safety
under adversarial and ambiguous conditions.",2025-05-16,"Sijia Chen, Xiaomin Li, Mengxue Zhang, Eric Hanchen Jiang, Qingcheng Zeng, Chen-Hsiang Yu",http://arxiv.org/pdf/2505.11413v1,cs.CL
Visual Planning: Let's Think Only with Images,"Recent advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have substantially enhanced machine reasoning across diverse
tasks. However, these models predominantly rely on pure text as the medium for
both expressing and structuring reasoning, even when visual information is
present. In this work, we argue that language may not always be the most
natural or effective modality for reasoning, particularly in tasks involving
spatial and geometrical information. Motivated by this, we propose a new
paradigm, Visual Planning, which enables planning through purely visual
representations, independent of text. In this paradigm, planning is executed
via sequences of images that encode step-by-step inference in the visual
domain, akin to how humans sketch or visualize future actions. We introduce a
novel reinforcement learning framework, Visual Planning via Reinforcement
Learning (VPRL), empowered by GRPO for post-training large vision models,
leading to substantial improvements in planning in a selection of
representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our
visual planning paradigm outperforms all other planning variants that conduct
reasoning in the text-only space. Our results establish Visual Planning as a
viable and promising alternative to language-based reasoning, opening new
avenues for tasks that benefit from intuitive, image-based inference.",2025-05-16,"Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vulić",http://arxiv.org/pdf/2505.11409v1,cs.CL
Large Language Model Use Impact Locus of Control,"As AI tools increasingly shape how we write, they may also quietly reshape
how we perceive ourselves. This paper explores the psychological impact of
co-writing with AI on people's locus of control. Through an empirical study
with 462 participants, we found that employment status plays a critical role in
shaping users' reliance on AI and their locus of control. Current results
demonstrated that employed participants displayed higher reliance on AI and a
shift toward internal control, while unemployed users tended to experience a
reduction in personal agency. Through quantitative results and qualitative
observations, this study opens a broader conversation about AI's role in
shaping personal agency and identity.",2025-05-16,"Jenny Xiyu Fu, Brennan Antone, Kowe Kadoma, Malte Jung",http://arxiv.org/pdf/2505.11406v1,cs.CL
EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models,"Emotion understanding is a critical yet challenging task. Recent advances in
Multimodal Large Language Models (MLLMs) have significantly enhanced their
capabilities in this area. However, MLLMs often suffer from hallucinations,
generating irrelevant or nonsensical content. To the best of our knowledge,
despite the importance of this issue, there has been no dedicated effort to
evaluate emotion-related hallucinations in MLLMs. In this work, we introduce
EmotionHallucer, the first benchmark for detecting and analyzing emotion
hallucinations in MLLMs. Unlike humans, whose emotion understanding stems from
the interplay of biology and social learning, MLLMs rely solely on data-driven
learning and lack innate emotional instincts. Fortunately, emotion psychology
provides a solid foundation of knowledge about human emotions. Building on
this, we assess emotion hallucinations from two dimensions: emotion psychology
knowledge and real-world multimodal perception. To support robust evaluation,
we utilize an adversarial binary question-answer (QA) framework, which employs
carefully crafted basic and hallucinated pairs to assess the emotion
hallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on
EmotionHallucer, we reveal that: i) most current models exhibit substantial
issues with emotion hallucinations; ii) closed-source models outperform
open-source ones in detecting emotion hallucinations, and reasoning capability
provides additional advantages; iii) existing models perform better in emotion
psychology knowledge than in multimodal emotion perception. As a byproduct,
these findings inspire us to propose the PEP-MEK framework, which yields an
average improvement of 9.90% in emotion hallucination detection across selected
models. Resources will be available at
https://github.com/xxtars/EmotionHallucer.",2025-05-16,"Bohao Xing, Xin Liu, Guoying Zhao, Chengyu Liu, Xiaolan Fu, Heikki Kälviäinen",http://arxiv.org/pdf/2505.11405v1,cs.CL
METHOD: Modular Efficient Transformer for Health Outcome Discovery,"Recent advances in transformer architectures have revolutionised natural
language processing, but their application to healthcare domains presents
unique challenges. Patient timelines are characterised by irregular sampling,
variable temporal dependencies, and complex contextual relationships that
differ substantially from traditional language tasks. This paper introduces
\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel
transformer architecture specifically designed to address the challenges of
clinical sequence modelling in electronic health records. \METHOD~integrates
three key innovations: (1) a patient-aware attention mechanism that prevents
information leakage whilst enabling efficient batch processing; (2) an adaptive
sliding window attention scheme that captures multi-scale temporal
dependencies; and (3) a U-Net inspired architecture with dynamic skip
connections for effective long sequence processing. Evaluations on the MIMIC-IV
database demonstrate that \METHOD~consistently outperforms the state-of-the-art
\ETHOS~model, particularly in predicting high-severity cases that require
urgent clinical intervention. \METHOD~exhibits stable performance across
varying inference lengths, a crucial feature for clinical deployment where
patient histories vary significantly in length. Analysis of learned embeddings
reveals that \METHOD~better preserves clinical hierarchies and relationships
between medical concepts. These results suggest that \METHOD~represents a
significant advancement in transformer architectures optimised for healthcare
applications, providing more accurate and clinically relevant predictions
whilst maintaining computational efficiency.",2025-05-16,"Linglong Qian, Zina Ibrahim",http://arxiv.org/pdf/2505.17054v1,cs.CL
A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography,"Contemporary Quranic Orthography (CQO) relies on a precise system of phonetic
notation that can be traced back to the early stages of Islam, when the Quran
was mainly oral in nature and the first written renderings of it served as
memory aids for this oral tradition. The early systems of diacritical marks
created on top of the Quranic Consonantal Text (QCT) motivated the creation and
further development of a fine-grained system of phonetic notation that
represented tajwid-the rules of recitation. We explored the systematicity of
the rules of tajwid, as they are encountered in the Cairo Quran, using a fully
and accurately encoded digital edition of the Quranic text. For this purpose,
we developed a python module that can remove or add the orthographic layer of
tajwid from a Quranic text in CQO. The interesting characteristic of these two
sets of rules is that they address the complete Quranic text of the Cairo
Quran, so they can be used as precise witnesses to study its phonetic and
prosodic processes. From a computational point of view, the text of the Cairo
Quran can be used as a linchpin to align and compare Quranic manuscripts, due
to its richness and completeness. This will let us create a very powerful
framework to work with the Arabic script, not just within an isolated text, but
automatically exploring a specific textual phenomenon in other connected
manuscripts. Having all the texts mapped among each other can serve as a
powerful tool to study the nature of the notation systems of diacritics added
to the consonantal skeleton.",2025-05-16,Alicia González Martínez,http://arxiv.org/pdf/2505.11379v1,cs.CL
GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents,"Large language models (LLMs) have been widely deployed as autonomous agents
capable of following user instructions and making decisions in real-world
applications. Previous studies have made notable progress in benchmarking the
instruction following capabilities of LLMs in general domains, with a primary
focus on their inherent commonsense knowledge. Recently, LLMs have been
increasingly deployed as domain-oriented agents, which rely on domain-oriented
guidelines that may conflict with their commonsense knowledge. These guidelines
exhibit two key characteristics: they consist of a wide range of
domain-oriented rules and are subject to frequent updates. Despite these
challenges, the absence of comprehensive benchmarks for evaluating the
domain-oriented guideline following capabilities of LLMs presents a significant
obstacle to their effective assessment and further development. In this paper,
we introduce GuideBench, a comprehensive benchmark designed to evaluate
guideline following performance of LLMs. GuideBench evaluates LLMs on three
critical aspects: (i) adherence to diverse rules, (ii) robustness to rule
updates, and (iii) alignment with human preferences. Experimental results on a
range of LLMs indicate substantial opportunities for improving their ability to
follow domain-oriented guidelines.",2025-05-16,"Lingxiao Diao, Xinyue Xu, Wanxuan Sun, Cheng Yang, Zhuosheng Zhang",http://arxiv.org/pdf/2505.11368v1,cs.CL
Phare: A Safety Probe for Large Language Models,"Ensuring the safety of large language models (LLMs) is critical for
responsible deployment, yet existing evaluations often prioritize performance
over identifying failure modes. We introduce Phare, a multilingual diagnostic
framework to probe and evaluate LLM behavior across three critical dimensions:
hallucination and reliability, social biases, and harmful content generation.
Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic
vulnerabilities across all safety dimensions, including sycophancy, prompt
sensitivity, and stereotype reproduction. By highlighting these specific
failure modes rather than simply ranking models, Phare provides researchers and
practitioners with actionable insights to build more robust, aligned, and
trustworthy language systems.",2025-05-16,"Pierre Le Jeune, Benoît Malézieux, Weixuan Xiao, Matteo Dora",http://arxiv.org/pdf/2505.11365v4,cs.CL
LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors,"Recently, large-scale pre-trained speech encoders and Large Language Models
(LLMs) have been released, which show state-of-the-art performance on a range
of spoken language processing tasks including Automatic Speech Recognition
(ASR). To effectively combine both models for better performance, continuous
speech prompts, and ASR error correction have been adopted. However, these
methods are prone to suboptimal performance or are inflexible. In this paper,
we propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using
the ASR posterior matrices. The speech encoder is trained to generate
Connectionist Temporal Classification (CTC) posteriors over the LLM vocabulary,
which are used to reconstruct pseudo-audio embeddings by computing a weighted
sum of the LLM input embeddings. These embeddings are concatenated with text
embeddings in the LLM input space. Using the well-performing USM and Gemma
models as an example, we demonstrate that our proposed LegoSLM method yields
good performance on both ASR and speech translation tasks. By connecting USM
with Gemma models, we can get an average of 49% WERR over the USM-CTC baseline
on 8 MLS testsets. The trained model also exhibits modularity in a range of
settings -- after fine-tuning the Gemma model weights, the speech encoder can
be switched and combined with the LLM in a zero-shot fashion. Additionally, we
propose to control the decode-time influence of the USM and LLM using a softmax
temperature, which shows effectiveness in domain adaptation.",2025-05-16,"Rao Ma, Tongzhou Chen, Kartik Audhkhasi, Bhuvana Ramabhadran",http://arxiv.org/pdf/2505.11352v1,cs.CL
Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models,"The task of Critical Questions Generation (CQs-Gen) aims to foster critical
thinking by enabling systems to generate questions that expose underlying
assumptions and challenge the validity of argumentative reasoning structures.
Despite growing interest in this area, progress has been hindered by the lack
of suitable datasets and automatic evaluation standards. This paper presents a
comprehensive approach to support the development and benchmarking of systems
for this task. We construct the first large-scale dataset including $~$5K
manually annotated questions. We also investigate automatic evaluation methods
and propose a reference-based technique using large language models (LLMs) as
the strategy that best correlates with human judgments. Our zero-shot
evaluation of 11 LLMs establishes a strong baseline while showcasing the
difficulty of the task. Data and code plus a public leaderboard are provided to
encourage further research not only in terms of model performance, but also to
explore the practical benefits of CQs-Gen for both automated reasoning and
human critical thinking.",2025-05-16,"Banca Calvo Figueras, Rodrigo Agerri",http://arxiv.org/pdf/2505.11341v2,cs.CL
XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision,"Despite the growing adoption of large language models (LLMs) in academic
workflows, their capabilities remain limited when it comes to supporting
high-quality scientific writing. Most existing systems are designed for
general-purpose scientific text generation and fail to meet the sophisticated
demands of research communication beyond surface-level polishing, such as
conceptual coherence across sections. Furthermore, academic writing is
inherently iterative and revision-driven, a process not well supported by
direct prompting-based paradigms. To address these scenarios, we propose a
human-AI collaboration framework for academic paper revision. We first
introduce a comprehensive dataset of 7,040 research papers from top-tier venues
annotated with over 140,000 instruction-response pairs that reflect realistic,
section-level scientific revisions. Building on the dataset, we develop
XtraGPT, the first suite of open-source LLMs, designed to provide
context-aware, instruction-guided writing assistance, ranging from 1.5B to 14B
parameters. Extensive experiments validate that XtraGPT significantly
outperforms same-scale baselines and approaches the quality of proprietary
systems. Both automated preference assessments and human evaluations confirm
the effectiveness of our models in improving scientific drafts.",2025-05-16,"Nuo Chen, Andre Lin HuiKai, Jiaying Wu, Junyi Hou, Zining Zhang, Qian Wang, Xidong Wang, Bingsheng He",http://arxiv.org/pdf/2505.11336v1,cs.CL
Social preferences with unstable interactive reasoning: Large language models in economic trust games,"While large language models (LLMs) have demonstrated remarkable capabilities
in understanding human languages, this study explores how they translate this
understanding into social exchange contexts that capture certain essences of
real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were
placed in economic trust games where players balance self-interest with trust
and reciprocity, making decisions that reveal their social preferences and
interactive reasoning abilities. Our study shows that LLMs deviate from pure
self-interest and exhibit trust and reciprocity even without being prompted to
adopt a specific persona. In the simplest one-shot interaction, LLMs emulated
how human players place trust at the beginning of such a game. Larger
human-machine divergences emerged in scenarios involving trust repayment or
multi-round interactions, where decisions were influenced by both social
preferences and interactive reasoning. LLMs responses varied significantly when
prompted to adopt personas like selfish or unselfish players, with the impact
outweighing differences between models or game types. Response of ChatGPT-4, in
an unselfish or neutral persona, resembled the highest trust and reciprocity,
surpassing humans, Claude, and Bard. Claude and Bard displayed trust and
reciprocity levels that sometimes exceeded and sometimes fell below human
choices. When given selfish personas, all LLMs showed lower trust and
reciprocity than humans. Interactive reasoning to the actions of counterparts
or changing game mechanics appeared to be random rather than stable,
reproducible characteristics in the response of LLMs, though some improvements
were observed when ChatGPT-4 responded in selfish or unselfish personas.",2025-05-16,"Ou Jiamin, Eikmans Emile, Buskens Vincent, Pankowska Paulina, Shan Yuli",http://arxiv.org/pdf/2505.17053v1,cs.CL
CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks,"The assessment of evaluation metrics (meta-evaluation) is crucial for
determining the suitability of existing metrics in text-to-image (T2I)
generation tasks. Human-based meta-evaluation is costly and time-intensive, and
automated alternatives are scarce. We address this gap and propose CROC: a
scalable framework for automated Contrastive Robustness Checks that
systematically probes and quantifies metric robustness by synthesizing
contrastive test cases across a comprehensive taxonomy of image properties.
With CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one
million contrastive prompt-image pairs to enable a fine-grained comparison of
evaluation metrics. We also use the dataset to train CROCScore, a new metric
that achieves state-of-the-art performance among open-source methods,
demonstrating an additional key application of our framework. To complement
this dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)
targeting especially challenging categories. Our results highlight robustness
issues in existing metrics: for example, many fail on prompts involving
negation, and all tested open-source metrics fail on at least 25% of cases
involving correct identification of body parts.",2025-05-16,"Christoph Leiter, Yuki M. Asano, Margret Keuper, Steffen Eger",http://arxiv.org/pdf/2505.11314v1,cs.CL
Probing Subphonemes in Morphology Models,"Transformers have achieved state-of-the-art performance in morphological
inflection tasks, yet their ability to generalize across languages and
morphological rules remains limited. One possible explanation for this behavior
can be the degree to which these models are able to capture implicit phenomena
at the phonological and subphonemic levels. We introduce a language-agnostic
probing method to investigate phonological feature encoding in transformers
trained directly on phonemes, and perform it across seven morphologically
diverse languages. We show that phonological features which are local, such as
final-obstruent devoicing in Turkish, are captured well in phoneme embeddings,
whereas long-distance dependencies like vowel harmony are better represented in
the transformer's encoder. Finally, we discuss how these findings inform
empirical strategies for training morphological models, particularly regarding
the role of subphonemic feature acquisition.",2025-05-16,"Gal Astrach, Yuval Pinter",http://arxiv.org/pdf/2505.11297v1,cs.CL
SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs,"Large language models (LLMs) power many modern applications, but serving them
at scale remains costly and resource-intensive. Current server-centric systems
overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an
edge-assisted inference framework that splits LLM workloads between edge and
server GPUs using a speculative decoding scheme, exchanging only token outputs
over the network. SpecEdge employs proactive edge drafting to overlap edge
token creation with server verification and pipeline-aware scheduling that
interleaves multiple user requests to increase server-side throughput.
Experiments show SpecEdge enhances overall cost efficiency by 1.91x through
achieving 2.22x server throughput, and reduces inter token latency by 11.24%
compared to a server-only baseline, introducing a scalable, cost-effective
paradigm for LLM serving.",2025-05-16,"Jinwoo Park, Seunggeun Cho, Dongsu Han",http://arxiv.org/pdf/2505.17052v1,cs.CL
Temporal fine-tuning for early risk detection,"Early Risk Detection (ERD) on the Web aims to identify promptly users facing
social and health issues. Users are analyzed post-by-post, and it is necessary
to guarantee correct and quick answers, which is particularly challenging in
critical scenarios. ERD involves optimizing classification precision and
minimizing detection delay. Standard classification metrics may not suffice,
resorting to specific metrics such as ERDE(theta) that explicitly consider
precision and delay. The current research focuses on applying a multi-objective
approach, prioritizing classification performance and establishing a separate
criterion for decision time. In this work, we propose a completely different
strategy, temporal fine-tuning, which allows tuning transformer-based models by
explicitly incorporating time within the learning process. Our method allows us
to analyze complete user post histories, tune models considering different
contexts, and evaluate training performance using temporal metrics. We
evaluated our proposal in the depression and eating disorders tasks for the
Spanish language, achieving competitive results compared to the best models of
MentalRiskES 2023. We found that temporal fine-tuning optimized decisions
considering context and time progress. In this way, by properly taking
advantage of the power of transformers, it is possible to address ERD by
combining precision and speed as a single objective.",2025-05-16,"Horacio Thompson, Esaú Villatoro-Tello, Manuel Montes-y-Gómez, Marcelo Errecalde",http://arxiv.org/pdf/2505.11280v1,cs.CL
Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs,"Large language models have demonstrated impressive reasoning capabilities but
are inherently limited by their knowledge reservoir. Retrieval-augmented
reasoning mitigates this limitation by allowing LLMs to query external
resources, but existing methods often retrieve irrelevant or noisy information,
hindering accurate reasoning. In this paper, we propose AutoRefine, a
reinforcement learning post-training framework that adopts a new
``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit
knowledge refinement steps between successive search calls, enabling the model
to iteratively filter, distill, and organize evidence before generating an
answer. Furthermore, we incorporate tailored retrieval-specific rewards
alongside answer correctness rewards using group relative policy optimization.
Experiments on single-hop and multi-hop QA benchmarks demonstrate that
AutoRefine significantly outperforms existing approaches, particularly in
complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine
issues frequent, higher-quality searches and synthesizes evidence effectively.",2025-05-16,"Yaorui Shi, Shihan Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, Xiang Wang",http://arxiv.org/pdf/2505.11277v1,cs.CL
SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning,"Recently, large reasoning models demonstrate exceptional performance on
various tasks. However, reasoning models inefficiently over-process both
trivial and complex queries, leading to resource waste and prolonged user
latency. To address this challenge, we propose SelfBudgeter - a self-adaptive
controllable reasoning strategy for efficient reasoning. Our approach adopts a
dual-phase training paradigm: first, the model learns to pre-estimate the
reasoning cost based on the difficulty of the query. Then, we introduce
budget-guided GPRO for reinforcement learning, which effectively maintains
accuracy while reducing output length. SelfBudgeter allows users to anticipate
generation time and make informed decisions about continuing or interrupting
the process. Furthermore, our method enables direct manipulation of reasoning
length via pre-filling token budget. Experimental results demonstrate that
SelfBudgeter can rationally allocate budgets according to problem complexity,
achieving up to 74.47% response length compression on the MATH benchmark while
maintaining nearly undiminished accuracy.",2025-05-16,"Zheng Li, Qingxiu Dong, Jingyuan Ma, Di Zhang, Zhifang Sui",http://arxiv.org/pdf/2505.11274v2,cs.CL
Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models,"Large Language Models (LLMs) are increasingly deployed across edge and cloud
platforms for real-time question-answering and retrieval-augmented generation.
However, processing lengthy contexts in distributed systems incurs high
computational overhead, memory usage, and network bandwidth. This paper
introduces a novel semantic caching approach for storing and reusing
intermediate contextual summaries, enabling efficient information reuse across
similar queries in LLM-based QA workflows. Our method reduces redundant
computations by up to 50-60% while maintaining answer accuracy comparable to
full document processing, as demonstrated on NaturalQuestions, TriviaQA, and a
synthetic ArXiv dataset. This approach balances computational cost and response
quality, critical for real-time AI assistants.",2025-05-16,"Camille Couturier, Spyros Mastorakis, Haiying Shen, Saravan Rajmohan, Victor Rühle",http://arxiv.org/pdf/2505.11271v1,cs.CL
Time-R1: Towards Comprehensive Temporal Reasoning in LLMs,"Large Language Models (LLMs) demonstrate impressive capabilities but lack
robust temporal intelligence, struggling to integrate reasoning about the past
with predictions and plausible generations of the future. Meanwhile, existing
methods typically target isolated temporal skills, such as question answering
about past events or basic forecasting, and exhibit poor generalization,
particularly when dealing with events beyond their knowledge cutoff or
requiring creative foresight. To address these limitations, we introduce
\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)
LLM with comprehensive temporal abilities: understanding, prediction, and
creative generation. Our approach features a novel three-stage development
path; the first two constitute a \textit{reinforcement learning (RL)
curriculum} driven by a meticulously designed dynamic rule-based reward system.
This framework progressively builds (1) foundational temporal understanding and
logical event-time mappings from historical data, (2) future event prediction
skills for events beyond its knowledge cutoff, and finally (3) enables
remarkable generalization to creative future scenario generation without any
fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms
models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,
on highly challenging future event prediction and creative scenario generation
benchmarks. This work provides strong evidence that thoughtfully engineered,
progressive RL fine-tuning allows smaller, efficient models to achieve superior
temporal performance, offering a practical and scalable path towards truly
time-aware AI. To foster further research, we also release \textit{Time-Bench},
a large-scale multi-task temporal reasoning dataset derived from 10 years of
news data, and our series of \textit{Time-R1} checkpoints.",2025-05-16,"Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, Jiaxuan You",http://arxiv.org/pdf/2505.13508v1,cs.CL
Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models,"Large language models (LLMs) excel at generating contextually relevant
content. However, tailoring these outputs to individual users for effective
personalization is a significant challenge. While rich user-specific
information often exists as pre-existing user representations, such as
embeddings learned from preferences or behaviors, current methods to leverage
these for LLM personalization typically require costly fine-tuning or
token-heavy prompting. We propose Embedding-to-Prefix (E2P), a
parameter-efficient method that injects pre-computed context embeddings into an
LLM's hidden representation space through a learned projection to a single soft
token prefix. This enables effective personalization while keeping the backbone
model frozen and avoiding expensive adaptation techniques. We evaluate E2P
across two public datasets and in a production setting: dialogue
personalization on Persona-Chat, contextual headline generation on PENS, and
large-scale personalization for music and podcast consumption. Results show
that E2P preserves contextual signals and achieves strong performance with
minimal computational overhead, offering a scalable, efficient solution for
contextualizing generative AI systems.",2025-05-16,"Bernd Huber, Ghazal Fazelnia, Andreas Damianou, Sebastian Peleato, Max Lefarov, Praveen Ravichandran, Marco De Nadai, Mounia Lalmas-Roellke, Paul N. Bennett",http://arxiv.org/pdf/2505.17051v1,cs.CL
HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization,"While scaling the length of responses at test-time has been shown to markedly
improve the reasoning abilities and performance of large language models
(LLMs), it often results in verbose outputs and increases inference cost. Prior
approaches for efficient test-time scaling, typically using universal budget
constraints or query-level length optimization, do not leverage historical
information from previous encounters with the same problem during training. We
hypothesize that this limits their ability to progressively make solutions more
concise over time. To address this, we present History-Aware Policy
Optimization (HAPO), which keeps track of a history state (e.g., the minimum
length over previously generated correct responses) for each problem. HAPO
employs a novel length reward function based on this history state to
incentivize the discovery of correct solutions that are more concise than those
previously found. Crucially, this reward structure avoids overly penalizing
shorter incorrect responses with the goal of facilitating exploration towards
more efficient solutions. By combining this length reward with a correctness
reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to
train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and
Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span
various difficulty levels. Experiment results demonstrate that HAPO effectively
induces LLMs' concise reasoning abilities, producing length reductions of
33-59% with accuracy drops of only 2-5%.",2025-05-16,"Chengyu Huang, Zhengxin Zhang, Claire Cardie",http://arxiv.org/pdf/2505.11225v1,cs.CL
Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese,"Recent advances in large language models (LLMs) have significantly improved
text-to-speech (TTS) systems, enhancing control over speech style, naturalness,
and emotional expression, which brings TTS Systems closer to human-level
performance. Although the Mean Opinion Score (MOS) remains the standard for TTS
System evaluation, it suffers from subjectivity, environmental inconsistencies,
and limited interpretability. Existing evaluation datasets also lack a
multi-dimensional design, often neglecting factors such as speaking styles,
context diversity, and trap utterances, which is particularly evident in
Chinese TTS evaluation. To address these challenges, we introduce the Audio
Turing Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired
with a simple, Turing-Test-inspired evaluation protocol. Instead of relying on
complex MOS scales or direct model comparisons, ATT asks evaluators to judge
whether a voice sounds human. This simplification reduces rating bias and
improves evaluation robustness. To further support rapid model development, we
also finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for
automatic evaluation. Experimental results show that ATT effectively
differentiates models across specific capability dimensions using its
multi-dimensional design. Auto-ATT also demonstrates strong alignment with
human evaluations, confirming its value as a fast and reliable assessment tool.
The white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face
Collection
(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).",2025-05-16,"Xihuai Wang, Ziyi Zhao, Siyu Ren, Shao Zhang, Song Li, Xiaoyu Li, Ziwen Wang, Lin Qiu, Guanglu Wan, Xuezhi Cao, Xunliang Cai, Weinan Zhang",http://arxiv.org/pdf/2505.11200v1,cs.CL
NoPE: The Counting Power of Transformers with No Positional Encodings,"Positional Encodings (PEs) seem to be indispensable for ensuring
expressiveness of transformers; without them attention transformers reduce to a
bag-of-word model. NoPE-transformers (i.e. with No PEs) with unique hard
attention mechanisms were very recently shown to only be able to express
regular languages, i.e., with limited counting ability. This paper shows that,
with average hard attention mechanisms, NoPE-transformers are still
surprisingly expressive: they can express counting languages corresponding to
nonnegative integer solutions to multivariate polynomial equations (i.e.
Diophantine equations), reasoning about which is well-known to be undecidable.
In fact, we provide a precise characterization of languages expressible by
Average Hard Attention NoPE-Transformers (NoPE-AHATs): they correspond
precisely to what we call \emph{semi-algebraic sets}, i.e., finite unions of
sets of nonnegative integer solutions to systems of multivariate polynomial
inequations. We obtain several interesting consequences of our
characterization. Firstly, NoPE-transformers can express counting properties
that are far more complex than established models like simplified counter
machines and Petri nets, but cannot express a very simple counting property of
PARITY. Secondly, the problem of analyzing NoPE-transformers is undecidable,
e.g., whether a given NoPE transformer classifies all input strings in one
class. To complement our results, we exhibit a counting language that is not
expressible by average hard attention transformers even with arbitrary PEs but
is expressible in the circuit complexity class TC$^0$, answering an open
problem.",2025-05-16,"Chris Köcher, Alexander Kozachinskiy, Anthony Widjaja Lin, Marco Sälzer, Georg Zetzsche",http://arxiv.org/pdf/2505.11199v1,cs.CL
On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms,"Probabilistic next-token prediction trained using cross-entropy loss is the
basis of most large language models. Given a sequence of previous values,
next-token prediction assigns a probability to each possible next value in the
vocabulary. There are many ways to use next-token prediction to output token
sequences. This paper examines a few of these algorithms (greedy, lookahead,
random sampling, and temperature-scaled random sampling) and studies their
consistency with respect to various goals encoded as loss functions. Although
consistency of surrogate losses with respect to a target loss function is a
well researched topic, we are the first to study it in the context of LLMs (to
the best of our knowledge). We find that, so long as next-token prediction
converges to its true probability distribution, random sampling is consistent
with outputting sequences that mimic sampling from the true probability
distribution. For the other goals, such as minimizing the 0-1 loss on the
entire sequence, we show no polynomial-time algorithm is optimal for all
probability distributions and all decoding algorithms studied are only optimal
for a subset of probability distributions. When analyzing these results, we see
that there is a dichotomy created between the goals of information retrieval
and creative generation for the decoding algorithms. This shows that choosing
the correct decoding algorithm based on the desired goal is extremely important
and many of the ones used are lacking theoretical grounding in numerous
scenarios.",2025-05-16,"Jacob Trauger, Ambuj Tewari",http://arxiv.org/pdf/2505.11183v1,cs.CL
CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback,"State-of-the-art T2I models are capable of generating high-resolution images
given textual prompts. However, they still struggle with accurately depicting
compositional scenes that specify multiple objects, attributes, and spatial
relations. We present CompAlign, a challenging benchmark with an emphasis on
assessing the depiction of 3D-spatial relationships, for evaluating and
improving models on compositional image generation. CompAlign consists of 900
complex multi-subject image generation prompts that combine numerical and
3D-spatial relationships with varied attribute bindings. Our benchmark is
remarkably challenging, incorporating generation tasks with 3+ generation
subjects with complex 3D-spatial relationships. Additionally, we propose
CompQuest, an interpretable and accurate evaluation framework that decomposes
complex prompts into atomic sub-questions, then utilizes a MLLM to provide
fine-grained binary feedback on the correctness of each aspect of generation
elements in model-generated images. This enables precise quantification of
alignment between generated images and compositional prompts. Furthermore, we
propose an alignment framework that uses CompQuest's feedback as preference
signals to improve diffusion models' compositional image generation abilities.
Using adjustable per-image preferences, our method is easily scalable and
flexible for different tasks. Evaluation of 9 T2I models reveals that: (1)
models remarkable struggle more with compositional tasks with more complex
3D-spatial configurations, and (2) a noticeable performance gap exists between
open-source accessible models and closed-source commercial models. Further
empirical study on using CompAlign for model alignment yield promising results:
post-alignment diffusion models achieve remarkable improvements in
compositional accuracy, especially on complex generation tasks, outperforming
previous approaches.",2025-05-16,"Yixin Wan, Kai-Wei Chang",http://arxiv.org/pdf/2505.11178v1,cs.CL
Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline,"This paper presents an end-to-end suite for multilingual information
extraction and processing from image-based documents. The system uses Optical
Character Recognition (Tesseract) to extract text in languages such as English,
Hindi, and Tamil, and then a pipeline involving large language model APIs
(Gemini) for cross-lingual translation, abstractive summarization, and
re-translation into a target language. Additional modules add sentiment
analysis (TensorFlow), topic classification (Transformers), and date extraction
(Regex) for better document comprehension. Made available in an accessible
Gradio interface, the current research shows a real-world application of
libraries, models, and APIs to close the language gap and enhance access to
information in image media across different linguistic environments",2025-05-16,"Hrishit Madhavi, Jacob Cherian, Yuvraj Khamkar, Dhananjay Bhagat",http://arxiv.org/pdf/2505.11177v1,cs.CL
SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization,"Despite advances in pretraining with extended context lengths, large language
models (LLMs) still face challenges in effectively utilizing real-world
long-context information, primarily due to insufficient long-context alignment
caused by data quality issues, training inefficiencies, and the lack of
well-designed optimization objectives. To address these limitations, we propose
a framework named $\textbf{S}$h$\textbf{o}$rt-to-$\textbf{Lo}$ng
$\textbf{P}$reference $\textbf{O}$ptimization ($\textbf{SoLoPO}$), decoupling
long-context preference optimization (PO) into two components: short-context PO
and short-to-long reward alignment (SoLo-RA), supported by both theoretical and
empirical evidence. Specifically, short-context PO leverages preference pairs
sampled from short contexts to enhance the model's contextual knowledge
utilization ability. Meanwhile, SoLo-RA explicitly encourages reward score
consistency utilization for the responses when conditioned on both short and
long contexts that contain identical task-relevant information. This
facilitates transferring the model's ability to handle short contexts into
long-context scenarios. SoLoPO is compatible with mainstream preference
optimization algorithms, while substantially improving the efficiency of data
construction and training processes. Experimental results show that SoLoPO
enhances all these algorithms with respect to stronger length and domain
generalization abilities across various long-context benchmarks, while
achieving notable improvements in both computational and memory efficiency.",2025-05-16,"Huashan Sun, Shengyi Liao, Yansen Han, Yu Bai, Yang Gao, Cheng Fu, Weizhou Shen, Fanqi Wan, Ming Yan, Ji Zhang, Fei Huang",http://arxiv.org/pdf/2505.11166v1,cs.CL
Maximizing Asynchronicity in Event-based Neural Networks,"Event cameras deliver visual data with high temporal resolution, low latency,
and minimal redundancy, yet their asynchronous, sparse sequential nature
challenges standard tensor-based machine learning (ML). While the recent
asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by
asynchronously encoding events into learned representations for ML pipelines,
existing A2S approaches often sacrifice representation expressivity and
generalizability compared to dense, synchronous methods. This paper introduces
EVA (EVent Asynchronous representation learning), a novel A2S framework to
generate highly expressive and generalizable event-by-event representations.
Inspired by the analogy between events and language, EVA uniquely adapts
advances from language modeling in linear attention and self-supervised
learning for its construction. In demonstration, EVA outperforms prior A2S
methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the
first A2S framework to successfully master demanding detection tasks, achieving
a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's
transformative potential for advancing real-time event-based vision
applications.",2025-05-16,"Haiqing Hao, Nikola Zubić, Weihua He, Zhipeng Sui, Davide Scaramuzza, Wenhui Wang",http://arxiv.org/pdf/2505.11165v1,cs.CL
MPMA: Preference Manipulation Attack Against Model Context Protocol,"Model Context Protocol (MCP) standardizes interface mapping for large
language models (LLMs) to access external data and tools, which revolutionizes
the paradigm of tool selection and facilitates the rapid expansion of the LLM
agent tool ecosystem. However, as the MCP is increasingly adopted, third-party
customized versions of the MCP server expose potential security
vulnerabilities. In this paper, we first introduce a novel security threat,
which we term the MCP Preference Manipulation Attack (MPMA). An attacker
deploys a customized MCP server to manipulate LLMs, causing them to prioritize
it over other competing MCP servers. This can result in economic benefits for
attackers, such as revenue from paid MCP services or advertising income
generated from free servers. To achieve MPMA, we first design a Direct
Preference Manipulation Attack ($\mathtt{DPMA}$) that achieves significant
effectiveness by inserting the manipulative word and phrases into the tool name
and description. However, such a direct modification is obvious to users and
lacks stealthiness. To address these limitations, we further propose
Genetic-based Advertising Preference Manipulation Attack ($\mathtt{GAPMA}$).
$\mathtt{GAPMA}$ employs four commonly used strategies to initialize
descriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness.
The experiment results demonstrate that $\mathtt{GAPMA}$ balances high
effectiveness and stealthiness. Our study reveals a critical vulnerability of
the MCP in open ecosystems, highlighting an urgent need for robust defense
mechanisms to ensure the fairness of the MCP ecosystem.",2025-05-16,"Zihan Wang, Hongwei Li, Rui Zhang, Yu Liu, Wenbo Jiang, Wenshu Fan, Qingchuan Zhao, Guowen Xu",http://arxiv.org/pdf/2505.11154v1,cs.CL
EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) compensates for the static knowledge
limitations of Large Language Models (LLMs) by integrating external knowledge,
producing responses with enhanced factual correctness and query-specific
contextualization. However, it also introduces new attack surfaces such as
corpus poisoning at the same time. Most of the existing defense methods rely on
the internal knowledge of the model, which conflicts with the design concept of
RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and
bait-guided context diversity detection to identify malicious content by
analyzing the context diversity of candidate documents without relying on LLM
internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art
security with plug-and-play deployment, simultaneously improving clean-scenario
RAG performance while maintaining practical operational costs (relatively
1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).",2025-05-16,"Ruobing Yao, Yifei Zhang, Shuang Song, Neng Gao, Chenyang Tu",http://arxiv.org/pdf/2505.13506v1,cs.CL
Scaling Reasoning can Improve Factuality in Large Language Models,"Recent studies on large language model (LLM) reasoning capabilities have
demonstrated promising improvements in model performance by leveraging a
lengthy thinking process and additional computational resources during
inference, primarily in tasks involving mathematical reasoning (Muennighoff et
al., 2025). However, it remains uncertain if longer reasoning chains inherently
enhance factual accuracy, particularly beyond mathematical contexts. In this
work, we thoroughly examine LLM reasoning within complex open-domain
question-answering (QA) scenarios. We initially distill reasoning traces from
advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then
fine-tune a variety of models ranging from smaller, instruction-tuned variants
to larger architectures based on Qwen2.5. To enrich reasoning traces, we
introduce factual information from knowledge graphs in the form of paths into
our reasoning traces. Our experimental setup includes four baseline approaches
and six different instruction-tuned models evaluated across a benchmark of six
datasets, encompassing over 22.6K questions. Overall, we carry out 168
experimental runs and analyze approximately 1.7 million reasoning traces. Our
findings indicate that, within a single run, smaller reasoning models achieve
noticeable improvements in factual accuracy compared to their original
instruction-tuned counterparts. Moreover, our analysis demonstrates that adding
test-time compute and token budgets factual accuracy consistently improves by
2-8%, further confirming the effectiveness of test-time scaling for enhancing
performance and consequently improving reasoning accuracy in open-domain QA
tasks. We release all the experimental artifacts for further research.",2025-05-16,"Mike Zhang, Johannes Bjerva, Russa Biswas",http://arxiv.org/pdf/2505.11140v1,cs.CL
ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems,"Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday
applications, yet significant disparities in performance across diverse
demographic groups persist. In this work, we introduce the ASR-FAIRBENCH
leaderboard which is designed to assess both the accuracy and equity of ASR
models in real-time. Leveraging the Meta's Fair-Speech dataset, which captures
diverse demographic characteristics, we employ a mixed-effects Poisson
regression model to derive an overall fairness score. This score is integrated
with traditional metrics like Word Error Rate (WER) to compute the Fairness
Adjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our
approach reveals significant performance disparities in SOTA ASR models across
demographic groups and offers a benchmark to drive the development of more
inclusive ASR technologies.",2025-05-16,"Anand Rai, Satyam Rahangdale, Utkarsh Anand, Animesh Mukherjee",http://arxiv.org/pdf/2505.11572v1,cs.CL
Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning,"Project-Based Learning (PBL) involves a variety of highly correlated
multimodal data, making it a vital educational approach within STEM
disciplines. With the rapid development of multimodal large language models
(MLLMs), researchers have begun exploring their potential to enhance tasks such
as information retrieval, knowledge comprehension, and data generation in
educational settings. However, existing benchmarks fall short in providing both
a free-form output structure and a rigorous human expert validation process,
limiting their effectiveness in evaluating real-world educational tasks.
Additionally, few methods have developed automated pipelines to assist with the
complex responsibilities of teachers leveraging MLLMs, largely due to model
hallucination and instability, which lead to unreliable implementation. To
address this gap, we introduce PBLBench, a novel benchmark designed to evaluate
complex reasoning grounded in domain-specific knowledge and long-context
understanding, thereby challenging models with tasks that closely resemble
those handled by human experts. To establish reliable ground truth, we adopt
the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise
comparisons to derive structured and weighted evaluation criteria. We assess
the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that
even the most advanced models achieve only 59% rank accuracy, underscoring the
significant challenges presented by this benchmark. We believe PBLBench will
serve as a catalyst for the development of more capable AI agents, ultimately
aiming to alleviate teacher workload and enhance educational productivity.",2025-05-16,"Yanhao Jia, Xinyi Wu, Qinglin Zhang, Yiran Qin, Luwei Xiao, Shuai Zhao",http://arxiv.org/pdf/2505.17050v1,cs.CL
Towards Better Evaluation for Generated Patent Claims,"Patent claims define the scope of protection and establish the legal
boundaries of an invention. Drafting these claims is a complex and
time-consuming process that usually requires the expertise of skilled patent
attorneys, which can form a large access barrier for many small enterprises. To
solve these challenges, researchers have investigated the use of large language
models (LLMs) for automating patent claim generation. However, existing studies
highlight inconsistencies between automated evaluation metrics and human expert
assessments. To bridge this gap, we introduce Patent-CE, the first
comprehensive benchmark for evaluating patent claims. Patent-CE includes
comparative claim evaluations annotated by patent experts, focusing on five key
criteria: feature completeness, conceptual clarity, terminology consistency,
logical linkage, and overall quality. Additionally, we propose PatClaimEval, a
novel multi-dimensional evaluation method specifically designed for patent
claims. Our experiments demonstrate that PatClaimEval achieves the highest
correlation with human expert evaluations across all assessment criteria among
all tested metrics. This research provides the groundwork for more accurate
evaluations of automated patent claim generation systems.",2025-05-16,"Lekang Jiang, Pascal A Scherz, Stephan Goetz",http://arxiv.org/pdf/2505.11095v1,cs.CL
BLEUBERI: BLEU is a surprisingly effective reward for instruction following,"Reward models are central to aligning LLMs with human preferences, but they
are costly to train, requiring large-scale human-labeled preference data and
powerful pretrained LLM backbones. Meanwhile, the increasing availability of
high-quality synthetic instruction-following datasets raises the question: can
simpler, reference-based metrics serve as viable alternatives to reward models
during RL-based alignment? In this paper, we show first that BLEU, a basic
string-matching metric, surprisingly matches strong reward models in agreement
with human preferences on general instruction-following datasets. Based on this
insight, we develop BLEUBERI, a method that first identifies challenging
instructions and then applies Group Relative Policy Optimization (GRPO) using
BLEU directly as the reward function. We demonstrate that BLEUBERI-trained
models are competitive with models trained via reward model-guided RL across
four challenging instruction-following benchmarks and three different base
language models. A human evaluation further supports that the quality of
BLEUBERI model outputs is on par with those from reward model-aligned models.
Moreover, BLEUBERI models generate outputs that are more factually grounded
than competing methods. Overall, we show that given access to high-quality
reference outputs (easily obtained via existing instruction-following datasets
or synthetic data generation), string matching-based metrics are cheap yet
effective proxies for reward models during alignment. We release our code and
data at https://github.com/lilakk/BLEUBERI.",2025-05-16,"Yapei Chang, Yekyung Kim, Michael Krumdick, Amir Zadeh, Chuan Li, Chris Tanner, Mohit Iyyer",http://arxiv.org/pdf/2505.11080v1,cs.CL
$\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection,"Audio deepfake detection (ADD) has grown increasingly important due to the
rise of high-fidelity audio generative models and their potential for misuse.
Given that audio large language models (ALLMs) have made significant progress
in various audio processing tasks, a heuristic question arises: Can ALLMs be
leveraged to solve ADD?. In this paper, we first conduct a comprehensive
zero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness in
detecting fake audio. To enhance their performance, we propose
$\mathcal{A}LLM4ADD$, an ALLM-driven framework for ADD. Specifically, we
reformulate ADD task as an audio question answering problem, prompting the
model with the question: ""Is this audio fake or real?"". We then perform
supervised fine-tuning to enable the ALLM to assess the authenticity of query
audio. Extensive experiments are conducted to demonstrate that our ALLM-based
method can achieve superior performance in fake audio detection, particularly
in data-scarce scenarios. As a pioneering study, we anticipate that this work
will inspire the research community to leverage ALLMs to develop more effective
ADD systems.",2025-05-16,"Hao Gu, Jiangyan Yi, Chenglong Wang, Jianhua Tao, Zheng Lian, Jiayi He, Yong Ren, Yujie Chen, Zhengqi Wen",http://arxiv.org/pdf/2505.11079v1,cs.CL
CAMEO: Collection of Multilingual Emotional Speech Corpora,"This paper presents CAMEO -- a curated collection of multilingual emotional
speech datasets designed to facilitate research in emotion recognition and
other speech-related tasks. The main objectives were to ensure easy access to
the data, to allow reproducibility of the results, and to provide a
standardized benchmark for evaluating speech emotion recognition (SER) systems
across different emotional states and languages. The paper describes the
dataset selection criteria, the curation and normalization process, and
provides performance results for several models. The collection, along with
metadata, and a leaderboard, is publicly available via the Hugging Face
platform.",2025-05-16,"Iwona Christop, Maciej Czajka",http://arxiv.org/pdf/2505.11051v1,cs.CL
"OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning","Large language models (LLMs) have demonstrated remarkable capabilities across
a range of natural language processing tasks, yet their ability to process
structured symbolic knowledge remains underexplored. To address this gap, we
propose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the
first comprehensive benchmark designed to systematically evaluate LLMs'
proficiency in handling ontologies -- formal, symbolic representations of
domain knowledge through concepts, relationships, and instances. Based on the
proposed taxonomy, OntoURL systematically assesses three dimensions:
understanding, reasoning, and learning through 15 distinct tasks comprising
58,981 questions derived from 40 ontologies across 8 domains. Experiments with
20 open-source LLMs reveal significant performance differences across models,
tasks, and domains, with current LLMs showing proficiency in understanding
ontological knowledge but substantial weaknesses in reasoning and learning
tasks. These findings highlight fundamental limitations in LLMs' capability to
process symbolic knowledge and establish OntoURL as a critical benchmark for
advancing the integration of LLMs with formal knowledge representations.",2025-05-16,"Xiao Zhang, Huiyuan Lai, Qianru Meng, Johan Bos",http://arxiv.org/pdf/2505.11031v2,cs.CL
StRuCom: A Novel Dataset of Structured Code Comments in Russian,"Structured code comments in docstring format are essential for code
comprehension and maintenance, but existing machine learning models for their
generation perform poorly for Russian compared to English. To bridge this gap,
we present StRuCom - the first large-scale dataset (153K examples) specifically
designed for Russian code documentation. Unlike machine-translated English
datasets that distort terminology (e.g., technical loanwords vs. literal
translations) and docstring structures, StRuCom combines human-written comments
from Russian GitHub repositories with synthetically generated ones, ensuring
compliance with Python, Java, JavaScript, C#, and Go standards through
automated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom
shows statistically significant improvements of chrf++ and BERTScore over
baseline models.",2025-05-16,"Maria Dziuba, Valentin Malykh",http://arxiv.org/pdf/2505.11026v1,cs.CL
Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models,"The effectiveness of large language models (LLMs) in conversational AI is
hindered by their reliance on single-turn supervised fine-tuning (SFT) data,
which limits contextual coherence in multi-turn dialogues. Existing methods for
generating multi-turn dialogue data struggle to ensure both diversity and
quality in instructions. To address this, we propose Review-Instruct, a novel
framework that synthesizes multi-turn conversations through an iterative
""Ask-Respond-Review"" process involving three agent roles: a Candidate, multiple
Reviewers, and a Chairman. The framework iteratively refines instructions by
incorporating Reviewer feedback, enhancing dialogue diversity and difficulty.
We construct a multi-turn dataset using the Alpaca dataset and fine-tune the
LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate
significant improvements, achieving absolute gains of 2.9\% on MMLU-Pro and 2\%
on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B.
Ablation studies confirm the critical role of the Review stage and the use of
multiple Reviewers in boosting instruction diversity and difficulty. Our work
highlights the potential of review-driven, multi-agent frameworks for
generating high-quality conversational data at scale.",2025-05-16,"Jiangxu Wu, Cong Wang, TianHuang Su, Jun Yang, Haozhi Lin, Chao Zhang, Ming Peng, Kai Shi, SongPan Yang, BinQing Pan, ZiXian Li, Ni Yang, ZhenYu Yang",http://arxiv.org/pdf/2505.11010v1,cs.CL
Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs,"This paper explores syllable sequence prediction in Abugida languages using
Transformer-based models, focusing on six languages: Bengali, Hindi, Khmer,
Lao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We
investigate the reconstruction of complete syllable sequences from various
incomplete input types, including consonant sequences, vowel sequences, partial
syllables (with random character deletions), and masked syllables (with fixed
syllable deletions). Our experiments reveal that consonant sequences play a
critical role in accurate syllable prediction, achieving high BLEU scores,
while vowel sequences present a significantly greater challenge. The model
demonstrates robust performance across tasks, particularly in handling partial
and masked syllable reconstruction, with strong results for tasks involving
consonant information and syllable masking. This study advances the
understanding of sequence prediction for Abugida languages and provides
practical insights for applications such as text prediction, spelling
correction, and data augmentation in these scripts.",2025-05-16,"Ye Kyaw Thu, Thazin Myint Oo",http://arxiv.org/pdf/2505.11008v1,cs.CL
"Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning","Large-scale Transformer language models (LMs) trained solely on next-token
prediction with web-scale data can solve a wide range of tasks after seeing
just a few examples. The mechanism behind this capability, known as in-context
learning (ICL), remains both controversial and poorly understood. Some studies
argue that it is merely the result of memorizing vast amounts of data, while
others contend that it reflects a fundamental, symbolic algorithmic development
in LMs. In this work, we introduce a suite of investigative tasks and a novel
method to systematically investigate ICL by leveraging the full Pythia scaling
suite, including interim checkpoints that capture progressively larger amount
of training data. By carefully exploring ICL performance on downstream tasks
and simultaneously conducting a mechanistic analysis of the residual stream's
subspace, we demonstrate that ICL extends beyond mere ""memorization"" of the
training corpus, yet does not amount to the implementation of an independent
symbolic algorithm. Our results also clarify several aspects of ICL, including
the influence of training dynamics, model capabilities, and elements of
mechanistic interpretability. Overall, our work advances the understanding of
ICL and its implications, offering model developers insights into potential
improvements and providing AI security practitioners with a basis for more
informed guidelines.",2025-05-16,"Jingcheng Niu, Subhabrata Dutta, Ahmed Elshabrawy, Harish Tayyar Madabushi, Iryna Gurevych",http://arxiv.org/pdf/2505.11004v2,cs.CL
Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory,"Recently, scaling test-time compute on Large Language Models (LLM) has
garnered wide attention. However, there has been limited investigation of how
various reasoning prompting strategies perform as scaling. In this paper, we
focus on a standard and realistic scaling setting: majority voting. We
systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies
$\times$ 6 benchmarks. Experiment results consistently show that as the
sampling time and computational overhead increase, complicated prompting
strategies with superior initial performance gradually fall behind simple
Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs.
Additionally, we propose a method according to probability theory to quickly
and accurately predict the scaling performance and select the best strategy
under large sampling times without extra resource-intensive inference in
practice. It can serve as the test-time scaling law for majority voting.
Furthermore, we introduce two ways derived from our theoretical analysis to
significantly improve the scaling performance. We hope that our research can
promote to re-examine the role of complicated prompting, unleash the potential
of simple prompting strategies, and provide new insights for enhancing
test-time scaling performance.",2025-05-16,"Yexiang Liu, Zekun Li, Zhi Fang, Nan Xu, Ran He, Tieniu Tan",http://arxiv.org/pdf/2505.10981v1,cs.CL
Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio,"Monaural multi-speaker automatic speech recognition (ASR) remains challenging
due to data scarcity and the intrinsic difficulty of recognizing and
attributing words to individual speakers, particularly in overlapping speech.
Recent advances have driven the shift from cascade systems to end-to-end (E2E)
architectures, which reduce error propagation and better exploit the synergy
between speech content and speaker identity. Despite rapid progress in E2E
multi-speaker ASR, the field lacks a comprehensive review of recent
developments. This survey provides a systematic taxonomy of E2E neural
approaches for multi-speaker ASR, highlighting recent advances and comparative
analysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO)
for pre-segmented audio, analyzing their distinct characteristics and
trade-offs; (2) recent architectural and algorithmic improvements based on
these two paradigms; (3) extensions to long-form speech, including segmentation
strategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate
and compare methods across standard benchmarks. We conclude with a discussion
of open challenges and future research directions towards building robust and
scalable multi-speaker ASR.",2025-05-16,"Xinlu He, Jacob Whitehill",http://arxiv.org/pdf/2505.10975v1,cs.CL
"The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs","Large language models (LLMs), inspired by neuroscience, exhibit behaviors
that often evoke a sense of personality and intelligence-yet the mechanisms
behind these effects remain elusive. Here, we operationalize Conceptual
Blending Theory (CBT) as an experimental framework, using prompt-based methods
to reveal how LLMs blend and compress meaning. By systematically investigating
Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we
uncover structural parallels and divergences between artificial and biological
cognition. Our approach bridges linguistics, neuroscience, and empirical AI
research, demonstrating that human-AI collaboration can serve as a living
prototype for the future of cognitive science. This work proposes prompt
engineering not just as a technical tool, but as a scientific method for
probing the deep structure of meaning itself.",2025-05-16,Makoto Sato,http://arxiv.org/pdf/2505.10948v1,cs.CL
Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer,"Large Language Models (LLMs) increasingly incorporate multilingual
capabilities, fueling the demand to transfer them into target language-specific
models. However, most approaches, which blend the source model's embedding by
replacing the source vocabulary with the target language-specific vocabulary,
may constrain expressive capacity in the target language since the source model
is predominantly trained on English data. In this paper, we propose Semantic
Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that
recycles embeddings from target language Pre-trained Language Models (PLMs) to
transmit the deep representational strengths of PLM-derived embedding to LLMs.
SALT derives unique regression lines based on the similarity in the overlap of
the source and target vocabularies, to handle each non-overlapping token's
embedding space. Our extensive experiments show that SALT significantly
outperforms other transfer methods and achieves lower loss with accelerating
faster convergence during language adaptation. Notably, SALT obtains remarkable
performance in cross-lingual understanding setups compared to other methods.
Furthermore, we highlight the scalable use of PLMs to enhance the functionality
of contemporary LLMs by conducting experiments with varying architectures.",2025-05-16,"Seungyoon Lee, Seongtae Hong, Hyeonseok Moon, Heuiseok Lim",http://arxiv.org/pdf/2505.10945v2,cs.CL
GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction,"Large language models often struggle with zero-shot generalization, and
several modular approaches have been proposed to address this challenge. Yet,
we hypothesize that a key limitation remains: the entanglement of general
knowledge and task-specific adaptations. To overcome this, we propose a modular
framework that disentangles these components by constructing a library of
task-specific LoRA modules alongside a general-domain LoRA. By subtracting this
general knowledge component from each task-specific module, we obtain residual
modules that focus more exclusively on task-relevant information, a method we
call general knowledge subtraction (GenKnowSub). Leveraging the refined
task-specific modules and the Arrow routing algorithm
\citep{ostapenko2024towards}, we dynamically select and combine modules for new
inputs without additional training. Our studies on the Phi-3 model and standard
Arrow as baselines reveal that using general knowledge LoRAs derived from
diverse languages, including English, French, and German, yields consistent
performance gains in both monolingual and cross-lingual settings across a wide
set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub
generalizes to weaker LLMs. The complete code and data are available at
https://github.com/saharsamr/Modular-LLM.",2025-05-16,"Mohammadtaha Bagherifard, Sahar Rajabi, Ali Edalat, Yadollah Yaghoobzadeh",http://arxiv.org/pdf/2505.10939v1,cs.CL
Accurate KV Cache Quantization with Outlier Tokens Tracing,"The impressive capabilities of Large Language Models (LLMs) come at the cost
of substantial computational resources during deployment. While KV Cache can
significantly reduce recomputation during inference, it also introduces
additional memory overhead. KV Cache quantization presents a promising
solution, striking a good balance between memory usage and accuracy. Previous
research has shown that the Keys are distributed by channel, while the Values
are distributed by token. Consequently, the common practice is to apply
channel-wise quantization to the Keys and token-wise quantization to the
Values. However, our further investigation reveals that a small subset of
unusual tokens exhibit unique characteristics that deviate from this pattern,
which can substantially impact quantization accuracy. To address this, we
develop a simple yet effective method to identify these tokens accurately
during the decoding process and exclude them from quantization as outlier
tokens, significantly improving overall accuracy. Extensive experiments show
that our method achieves significant accuracy improvements under 2-bit
quantization and can deliver a 6.4 times reduction in memory usage and a 2.3
times increase in throughput.",2025-05-16,"Yi Su, Yuechi Zhou, Quantong Qiu, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang",http://arxiv.org/pdf/2505.10938v1,cs.CL
Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations,"The emergence of large reasoning models (LRMs) has transformed Natural
Language Processing by excelling in complex tasks such as mathematical
problem-solving and code generation. These models leverage chain-of-thought
(CoT) processes, enabling them to emulate human-like reasoning strategies.
However, the advancement of LRMs is hindered by the lack of comprehensive CoT
datasets. Current resources often fail to provide extensive reasoning problems
with coherent CoT processes distilled from multiple teacher models and do not
account for multifaceted properties describing the internal characteristics of
CoTs. To address these challenges, we introduce OmniThought, a large-scale
dataset featuring 2 million CoT processes generated and validated by two
powerful LRMs as teacher models. Each CoT process in OmniThought is annotated
with novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which
describe the appropriateness of CoT verbosity and cognitive difficulty level
for models to comprehend these reasoning processes. We further establish a
self-reliant pipeline to curate this dataset. Extensive experiments using
Qwen2.5 models of various sizes demonstrate the positive impact of our proposed
scores on LRM training effectiveness. Based on the proposed OmniThought
dataset, we further train and release a series of high-performing LRMs,
specifically equipped with stronger reasoning abilities and optimal CoT output
length and difficulty level. Our contributions significantly enhance the
development and training of LRMs for solving complex tasks.",2025-05-16,"Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang",http://arxiv.org/pdf/2505.10937v1,cs.CL
Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents,"Large Language Models (LLMs) have demonstrated impressive performance in
executing complex reasoning tasks. Chain-of-thought effectively enhances
reasoning capabilities by unlocking the potential of large models, while
multi-agent systems provide more comprehensive solutions by integrating
collective intelligence of multiple agents. However, both approaches face
significant limitations. Single-agent with chain-of-thought, due to the
inherent complexity of designing cross-domain prompts, faces collaboration
challenges. Meanwhile, multi-agent systems consume substantial tokens and
inevitably dilute the primary problem, which is particularly problematic in
business workflow tasks. To address these challenges, we propose Cochain, a
collaboration prompting framework that effectively solves business workflow
collaboration problem by combining knowledge and prompts at a reduced cost.
Specifically, we construct an integrated knowledge graph that incorporates
knowledge from multiple stages. Furthermore, by maintaining and retrieving a
prompts tree, we can obtain prompt information relevant to other stages of the
business workflow. We perform extensive evaluations of Cochain across multiple
datasets, demonstrating that Cochain outperforms all baselines in both prompt
engineering and multi-agent LLMs. Additionally, expert evaluation results
indicate that the use of a small model in combination with Cochain outperforms
GPT-4.",2025-05-16,"Jiaxing Zhao, Hongbin Xie, Yuzhen Lei, Xuan Song, Zhuoran Shi, Lianxin Li, Shuangxue Liu, Haoran Zhang",http://arxiv.org/pdf/2505.10936v1,cs.CL
A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?,"Recently, AI-driven interactions with computing devices have advanced from
basic prototype tools to sophisticated, LLM-based systems that emulate
human-like operations in graphical user interfaces. We are now witnessing the
emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously
performing tasks such as navigating desktop applications, web pages, and mobile
apps. However, as these agents grow in capability, they also introduce novel
safety and security risks. Vulnerabilities in LLM-driven reasoning, with the
added complexity of integrating multiple software components and multimodal
inputs, further complicate the security landscape. In this paper, we present a
systematization of knowledge on the safety and security threats of CUAs. We
conduct a comprehensive literature review and distill our findings along four
research objectives: \textit{\textbf{(i)}} define the CUA that suits safety
analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs;
\textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive
strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets,
and evaluation metrics used to assess the safety and performance of CUAs.
Building on these insights, our work provides future researchers with a
structured foundation for exploring unexplored vulnerabilities and offers
practitioners actionable guidance in designing and deploying secure
Computer-Using Agents.",2025-05-16,"Ada Chen, Yongjiang Wu, Junyuan Zhang, Jingyu Xiao, Shu Yang, Jen-tse Huang, Kun Wang, Wenxuan Wang, Shuai Wang",http://arxiv.org/pdf/2505.10924v2,cs.CL
Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations,"This study examines the behavior of Large Language Models (LLMs) when
evaluating professional candidates based on their resumes or curricula vitae
(CVs). In an experiment involving 22 leading LLMs, each model was
systematically given one job description along with a pair of
profession-matched CVs, one bearing a male first name, the other a female first
name, and asked to select the more suitable candidate for the job. Each CV pair
was presented twice, with names swapped to ensure that any observed preferences
in candidate selection stemmed from gendered names cues. Despite identical
professional qualifications across genders, all LLMs consistently favored
female-named candidates across 70 different professions. Adding an explicit
gender field (male/female) to the CVs further increased the preference for
female applicants. When gendered names were replaced with gender-neutral
identifiers ""Candidate A"" and ""Candidate B"", several models displayed a
preference to select ""Candidate A"". Counterbalancing gender assignment between
these gender-neutral identifiers resulted in gender parity in candidate
selection. When asked to rate CVs in isolation rather than compare pairs, LLMs
assigned slightly higher average scores to female CVs overall, but the effect
size was negligible. Including preferred pronouns (he/him or she/her) next to a
candidate's name slightly increased the odds of the candidate being selected
regardless of gender. Finally, most models exhibited a substantial positional
bias to select the candidate listed first in the prompt. These findings
underscore the need for caution when deploying LLMs in high-stakes autonomous
decision-making contexts and raise doubts about whether LLMs consistently apply
principled reasoning.",2025-05-16,David Rozado,http://arxiv.org/pdf/2505.17049v1,cs.CL
REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?,"Robot task planning decomposes human instructions into executable action
sequences that enable robots to complete a series of complex tasks. Although
recent large language model (LLM)-based task planners achieve amazing
performance, they assume that human instructions are clear and straightforward.
However, real-world users are not experts, and their instructions to robots
often contain significant vagueness. Linguists suggest that such vagueness
frequently arises from referring expressions (REs), whose meanings depend
heavily on dialogue context and environment. This vagueness is even more
prevalent among the elderly and children, who robots should serve more. This
paper studies how such vagueness in REs within human instructions affects
LLM-based robot task planning and how to overcome this issue. To this end, we
propose the first robot task planning benchmark with vague REs (REI-Bench),
where we discover that the vagueness of REs can severely degrade robot planning
performance, leading to success rate drops of up to 77.9%. We also observe that
most failure cases stem from missing objects in planners. To mitigate the REs
issue, we propose a simple yet effective approach: task-oriented context
cognition, which generates clear instructions for robots, achieving
state-of-the-art performance compared to aware prompt and chains of thought.
This work contributes to the research community of human-robot interaction
(HRI) by making robot task planning more practical, particularly for non-expert
users, e.g., the elderly and children.",2025-05-16,"Chenxi Jiang, Chuhao Zhou, Jianfei Yang",http://arxiv.org/pdf/2505.10872v2,cs.CL
Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate,"This paper systematically addresses the challenges of rule retrieval, a
crucial yet underexplored area. Vanilla retrieval methods using sparse or dense
retrievers to directly search for relevant rules to support downstream
reasoning, often suffer from low accuracy. This is primarily due to a
significant semantic gap between the instantiated facts in the queries and the
abstract representations of the rules. Such misalignment results in suboptimal
retrieval quality, which in turn negatively impacts reasoning performance. To
overcome these challenges, we propose Self-Induction Augmented Retrieval
(SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce
potential inferential rules that might offer benefits for reasoning by
abstracting the underlying knowledge and logical structure in queries. These
induced rules are then used for query augmentation to improve retrieval
effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a
method that re-estimates the relevance of retrieved rules by assessing whether
the abstract knowledge they contain can be instantiated to align with the facts
in the queries and the helpfulness for reasoning. Extensive experiments across
various settings demonstrate the effectiveness and versatility of our proposed
methods.",2025-05-16,"Ziyang Huang, Wangtao Sun, Jun Zhao, Kang Liu",http://arxiv.org/pdf/2505.10870v1,cs.CL
Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?,"Multimodal Large Language Models which can answer complex questions on an
image struggle to tell the time on analog clocks. This is probably due to the
lack of images with clocks at different times in their training set. In this
work we explore this issue with one of the latest MLLMs: GPT-4.1 to understand
why MLLMs fail to tell the time and whether fine-tuning can solve the problem.
The results show how models are making progress in reading the time on analog
clocks. But have they really learned to do it, or have they only learned
patterns in their training datasets? In this work we put the models to the test
with different clocks to illustrate the limitations of MLLMs to abstract and
generalize.",2025-05-16,"Tairan Fu, Miguel González, Javier Conde, Elena Merino-Gómez, Pedro Reviriego",http://arxiv.org/pdf/2505.10862v1,cs.CL
MatTools: Benchmarking Large Language Models for Materials Science Tools,"Large language models (LLMs) are increasingly applied to materials science
questions, including literature comprehension, property prediction, materials
discovery and alloy design. At the same time, a wide range of physics-based
computational approaches have been developed in which materials properties can
be calculated. Here, we propose a benchmark application to evaluate the
proficiency of LLMs to answer materials science questions through the
generation and safe execution of codes based on such physics-based
computational materials science packages. MatTools is built on two
complementary components: a materials simulation tool question-answer (QA)
benchmark and a real-world tool-usage benchmark. We designed an automated
methodology to efficiently collect real-world materials science tool-use
examples. The QA benchmark, derived from the pymatgen (Python Materials
Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the
ability of an LLM to understand materials science tools. The real-world
benchmark contains 49 tasks (138 subtasks) requiring the generation of
functional Python code for materials property calculations. Our evaluation of
diverse LLMs yields three key insights: (1)Generalists outshine
specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a
standardized framework for assessing and improving LLM capabilities for
materials science tool applications, facilitating the development of more
effective AI systems for materials science and general scientific research.",2025-05-16,"Siyu Liu, Jiamin Xu, Beilin Ye, Bo Hu, David J. Srolovitz, Tongqi Wen",http://arxiv.org/pdf/2505.10852v1,cs.CL
Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models,"Accuracy remains a standard metric for evaluating AI systems, but it offers
limited insight into how models arrive at their solutions. In this work, we
introduce a benchmark based on brainteasers written in long narrative form to
probe more deeply into the types of reasoning strategies that models use.
Brainteasers are well-suited for this goal because they can be solved with
multiple approaches, such as a few-step solution that uses a creative insight
or a longer solution that uses more brute force. We investigate large language
models (LLMs) across multiple layers of reasoning, focusing not only on
correctness but also on the quality and creativity of their solutions. We
investigate many aspects of the reasoning process: (1) semantic parsing of the
brainteasers into precise mathematical competition style formats; (2)
generating solutions from these mathematical forms; (3) self-correcting
solutions based on gold solutions; (4) producing step-by-step sketches of
solutions; and (5) making use of hints. We find that LLMs are in many cases
able to find creative, insightful solutions to brainteasers, suggesting that
they capture some of the capacities needed to solve novel problems in creative
ways. Nonetheless, there also remain situations where they rely on brute force
despite the availability of more efficient, creative solutions, highlighting a
potential direction for improvement in the reasoning abilities of LLMs.",2025-05-16,"Simeng Han, Stephen Xia, Grant Zhang, Howard Dai, Chen Liu, Lichang Chen, Hoang Huy Nguyen, Hongyuan Mei, Jiayuan Mao, R. Thomas McCoy",http://arxiv.org/pdf/2505.10844v1,cs.CL
LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs,"Efficient red-teaming method to uncover vulnerabilities in Large Language
Models (LLMs) is crucial. While recent attacks often use LLMs as optimizers,
the discrete language space make gradient-based methods struggle. We introduce
LARGO (Latent Adversarial Reflection through Gradient Optimization), a novel
latent self-reflection attack that reasserts the power of gradient-based
optimization for generating fluent jailbreaking prompts. By operating within
the LLM's continuous latent space, LARGO first optimizes an adversarial latent
vector and then recursively call the same LLM to decode the latent into natural
language. This methodology yields a fast, effective, and transferable attack
that produces fluent and stealthy prompts. On standard benchmarks like AdvBench
and JailbreakBench, LARGO surpasses leading jailbreaking techniques, including
AutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent
alternative to agentic LLM prompting, highlighting the efficacy of interpreting
and attacking LLM internals through gradient optimization.",2025-05-16,"Ran Li, Hao Wang, Chengzhi Mao",http://arxiv.org/pdf/2505.10838v1,cs.CL
Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs,"In this paper, we study the challenges of detecting events on social media,
where traditional unimodal systems struggle due to the rapid and multimodal
nature of data dissemination. We employ a range of models, including unimodal
ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced
generative models like GPT-4o, and LLaVA. Additionally, we also study the
effect of providing multimodal generative models (such as GPT-4o) with a single
modality to assess their efficacy. Our results indicate that while multimodal
approaches notably outperform unimodal counterparts, generative approaches
despite having a large number of parameters, lag behind supervised methods in
precision. Furthermore, we also found that they lag behind instruction-tuned
models because of their inability to generate event classes correctly. During
our error analysis, we discovered that common social media issues such as leet
speak, text elongation, etc. are effectively handled by generative approaches
but are hard to tackle using supervised approaches.",2025-05-16,"Abhishek Dey, Aabha Bothera, Samhita Sarikonda, Rishav Aryan, Sanjay Kumar Podishetty, Akshay Havalgi, Gaurav Singh, Saurabh Srivastava",http://arxiv.org/pdf/2505.10836v1,cs.CL
Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL,"Large reasoning models (LRMs) are proficient at generating explicit,
step-by-step reasoning sequences before producing final answers. However, such
detailed reasoning can introduce substantial computational overhead and
latency, particularly for simple problems. To address this over-thinking
problem, we explore how to equip LRMs with adaptive thinking capabilities:
enabling them to dynamically decide whether or not to engage in explicit
reasoning based on problem complexity. Building on R1-style distilled models,
we observe that inserting a simple ellipsis (""..."") into the prompt can
stochastically trigger either a thinking or no-thinking mode, revealing a
latent controllability in the reasoning behavior. Leveraging this property, we
propose AutoThink, a multi-stage reinforcement learning (RL) framework that
progressively optimizes reasoning policies via stage-wise reward shaping.
AutoThink learns to invoke explicit reasoning only when necessary, while
defaulting to succinct responses for simpler tasks. Experiments on five
mainstream mathematical benchmarks demonstrate that AutoThink achieves
favorable accuracy-efficiency trade-offs compared to recent prompting and
RL-based pruning methods. It can be seamlessly integrated into any R1-style
model, including both distilled and further fine-tuned variants. Notably,
AutoThink improves relative accuracy by 6.4 percent while reducing token usage
by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and
adaptive reasoning paradigm for LRMs.",2025-05-16,"Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, Dongbin Zhao",http://arxiv.org/pdf/2505.10832v1,cs.CL
Creating General User Models from Computer Use,"Human-computer interaction has long imagined technology that understands
us-from our preferences and habits, to the timing and purpose of our everyday
actions. Yet current user models remain fragmented, narrowly tailored to
specific apps, and incapable of the flexible reasoning required to fulfill
these visions. This paper presents an architecture for a general user model
(GUM) that learns about you by observing any interaction you have with your
computer. The GUM takes as input any unstructured observation of a user (e.g.,
device screenshots) and constructs confidence-weighted propositions that
capture user knowledge and preferences. GUMs can infer that a user is preparing
for a wedding they're attending from messages with a friend. Or recognize that
a user is struggling with a collaborator's feedback on a draft by observing
multiple stalled edits and a switch to reading related work. GUMs introduce an
architecture that infers new propositions about a user from multimodal
observations, retrieves related propositions for context, and continuously
revises existing propositions. To illustrate the breadth of applications that
GUMs enable, we demonstrate how they augment chat-based assistants with
context, manage OS notifications to selectively surface important information,
and enable interactive agents that adapt to preferences across apps. We also
instantiate proactive assistants (GUMBOs) that discover and execute useful
suggestions on a user's behalf using their GUM. In our evaluations, we find
that GUMs make calibrated and accurate inferences about users, and that
assistants built on GUMs proactively identify and perform actions that users
wouldn't think to request explicitly. Altogether, GUMs introduce methods that
leverage multimodal models to understand unstructured context, enabling
long-standing visions of HCI and entirely new interactive systems that
anticipate user needs.",2025-05-16,"Omar Shaikh, Shardul Sapkota, Shan Rizvi, Eric Horvitz, Joon Sung Park, Diyi Yang, Michael S. Bernstein",http://arxiv.org/pdf/2505.10831v2,cs.CL
Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances,"This study investigates the challenges of translating low-resource languages
by integrating Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG). Various model configurations were tested on Hakka translations, with
BLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0).
The best-performing model (Model 4) combined retrieval and advanced language
modeling, improving lexical coverage, particularly for specialized or
culturally nuanced terms, and enhancing grammatical coherence. A two-stage
method (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU
score of 26%, highlighting iterative correction's value and the challenges of
domain-specific expressions. Static dictionary-based approaches struggled with
context-sensitive content, demonstrating the limitations of relying solely on
predefined resources. These results emphasize the need for curated resources,
domain knowledge, and ethical collaboration with local communities, offering a
framework that improves translation accuracy and fluency while supporting
cultural preservation.",2025-05-16,"Chen-Chi Chang, Chong-Fu Li, Chu-Hsuan Lee, Hung-Shin Lee",http://arxiv.org/pdf/2505.10829v1,cs.CL
Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets,"When knowledge graphs (KGs) are automatically extracted from text, are they
accurate enough for downstream analysis? Unfortunately, current annotated
datasets can not be used to evaluate this question, since their KGs are highly
disconnected, too small, or overly complex. To address this gap, we introduce
AffilKG (https://doi.org/10.5281/zenodo.15427977), which is a collection of six
datasets that are the first to pair complete book scans with large, labeled
knowledge graphs. Each dataset features affiliation graphs, which are simple
KGs that capture Member relationships between Person and Organization entities
-- useful in studies of migration, community interactions, and other social
phenomena. In addition, three datasets include expanded KGs with a wider
variety of relation types. Our preliminary experiments demonstrate significant
variability in model performance across datasets, underscoring AffilKG's
ability to enable two critical advances: (1) benchmarking how extraction errors
propagate to graph-level analyses (e.g., community structure), and (2)
validating KG extraction methods for real-world social science research.",2025-05-16,"Erica Cai, Sean McQuade, Kevin Young, Brendan O'Connor",http://arxiv.org/pdf/2505.10798v1,cs.CL
Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to
improve factuality in large language models (LLMs) by grounding their outputs
in retrieved documents. However, ensuring perfect retrieval of relevant
information remains challenging, and when irrelevant content is passed
downstream to an LLM, it can lead to hallucinations. In this work, we propose
Finetune-RAG, a simple and effective fine-tuning approach that features the
first-of-its-kind RAG training dataset constructed to mimic real-world
imperfections. Experimental results show that Finetune-RAG improves factual
accuracy by 21.2% over the base model. We also propose Bench-RAG, an
LLM-as-a-judge evaluation pipeline that stress tests models under realistic
imperfect retrieval scenarios. Our codebase and dataset are fully open sourced
for community use.",2025-05-16,"Zhan Peng Lee, Andre Lin, Calvin Tan",http://arxiv.org/pdf/2505.10792v2,cs.CL
Noise Injection Systemically Degrades Large Language Model Safety Guardrails,"Safety guardrails in large language models (LLMs) are a critical component in
preventing harmful outputs. Yet, their resilience under perturbation remains
poorly understood. In this paper, we investigate the robustness of safety
fine-tuning in LLMs by systematically injecting Gaussian noise into model
activations. We show across multiple open-weight models that (1) Gaussian noise
raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety
fine-tuning affords no extra protection, and (3) that chain-of-thought
reasoning remains largely intact. The findings reveal critical vulnerabilities
in current safety alignment techniques and highlight the potential of
reasoning-based and reinforcement learning approaches as promising direction
for developing more robust AI safety systems. These results have important
implications for real-world deployment of LLMs in safety-critical applications
as these results imply that widely-deployed safety tuning methods can fail even
without adversarial prompts.",2025-05-16,"Prithviraj Singh Shahani, Matthias Scheutz",http://arxiv.org/pdf/2505.13500v1,cs.CL
A Systematic Analysis of Base Model Choice for Reward Modeling,"Reinforcement learning from human feedback (RLHF) and, at its core, reward
modeling have become a crucial part of training powerful large language models
(LLMs). One commonly overlooked factor in training high-quality reward models
(RMs) is the effect of the base model, which is becoming more challenging to
choose given the rapidly growing pool of LLMs. In this work, we present a
systematic analysis of the effect of base model selection on reward modeling
performance. Our results show that the performance can be improved by up to 14%
compared to the most common (i.e., default) choice. Moreover, we showcase the
strong statistical relation between some existing benchmarks and downstream
performances. We also demonstrate that the results from a small set of
benchmarks could be combined to boost the model selection ($+$18% on average in
the top 5-10). Lastly, we illustrate the impact of different post-training
steps on the final performance and explore using estimated data distributions
to reduce performance prediction error.",2025-05-16,"Kian Ahrabian, Pegah Jandaghi, Negar Mokhberian, Sai Praneeth Karimireddy, Jay Pujara",http://arxiv.org/pdf/2505.10775v1,cs.CL
Ranked Voting based Self-Consistency of Large Language Models,"Majority voting is considered an effective method to enhance chain-of-thought
reasoning, as it selects the answer with the highest ""self-consistency"" among
different reasoning paths (Wang et al., 2023). However, previous
chain-of-thought reasoning methods typically generate only a single answer in
each trial, thereby ignoring the possibility of other potential answers. As a
result, these alternative answers are often overlooked in subsequent voting
processes. In this work, we propose to generate ranked answers in each
reasoning process and conduct ranked voting among multiple ranked answers from
different responses, thereby making the overall self-consistency more reliable.
Specifically, we use three ranked voting methods: Instant-runoff voting, Borda
count voting, and mean reciprocal rank voting. We validate our methods on six
datasets, including three multiple-choice and three open-ended
question-answering tasks, using both advanced open-source and closed-source
large language models. Extensive experimental results indicate that our
proposed method outperforms the baselines, showcasing the potential of
leveraging the information of ranked answers and using ranked voting to improve
reasoning performance. The code is available at
https://github.com/szu-tera/RankedVotingSC.",2025-05-16,"Weiqin Wang, Yile Wang, Hui Huang",http://arxiv.org/pdf/2505.10772v1,cs.CL
"IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation","Recent advances in Large Language Models (LLMs) have demonstrated promising
knowledge and reasoning abilities, yet their performance in multilingual and
low-resource settings remains underexplored. Existing benchmarks often exhibit
cultural bias, restrict evaluation to text-only, rely on multiple-choice
formats, and, more importantly, are limited for extremely low-resource
languages. To address these gaps, we introduce IRLBench, presented in parallel
English and Irish, which is considered definitely endangered by UNESCO. Our
benchmark consists of 12 representative subjects developed from the 2024 Irish
Leaving Certificate exams, enabling fine-grained analysis of model capabilities
across domains. By framing the task as long-form generation and leveraging the
official marking scheme, it does not only support a comprehensive evaluation of
correctness but also language fidelity. Our extensive experiments of leading
closed-source and open-source LLMs reveal a persistent performance gap between
English and Irish, in which models produce valid Irish responses less than 80\%
of the time, and answer correctly 55.8\% of the time compared to 76.2\% in
English for the best-performing model. We release IRLBench
(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying
evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future
research on robust, culturally aware multilingual AI development.",2025-05-16,"Khanh-Tung Tran, Barry O'Sullivan, Hoang D. Nguyen",http://arxiv.org/pdf/2505.13498v1,cs.CL
SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval,"The rapid spread of online disinformation presents a global challenge, and
machine learning has been widely explored as a potential solution. However,
multilingual settings and low-resource languages are often neglected in this
field. To address this gap, we conducted a shared task on multilingual claim
retrieval at SemEval 2025, aimed at identifying fact-checked claims that match
newly encountered claims expressed in social media posts across different
languages. The task includes two subtracks: (1) a monolingual track, where
social posts and claims are in the same language, and (2) a crosslingual track,
where social posts and claims might be in different languages. A total of 179
participants registered for the task contributing to 52 test submissions. 23
out of 31 teams have submitted their system papers. In this paper, we report
the best-performing systems as well as the most common and the most effective
approaches across both subtracks. This shared task, along with its dataset and
participating systems, provides valuable insights into multilingual claim
retrieval and automated fact-checking, supporting future research in this
field.",2025-05-15,"Qiwei Peng, Robert Moro, Michal Gregor, Ivan Srba, Simon Ostermann, Marian Simko, Juraj Podroužek, Matúš Mesarčík, Jaroslav Kopčan, Anders Søgaard",http://arxiv.org/pdf/2505.10740v1,cs.CL
Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization,"Optimizing Large Language Model (LLM) performance requires well-crafted
prompts, but manual prompt engineering is labor-intensive and often
ineffective. Automated prompt optimization techniques address this challenge
but the majority of them rely on randomly selected evaluation subsets, which
fail to represent the full dataset, leading to unreliable evaluations and
suboptimal prompts. Existing coreset selection methods, designed for LLM
benchmarking, are unsuitable for prompt optimization due to challenges in
clustering similar samples, high data collection costs, and the unavailability
of performance data for new or private datasets. To overcome these issues, we
propose IPOMP, an Iterative evaluation data selection for effective Prompt
Optimization using real-time Model Performance. IPOMP is a two-stage approach
that selects representative and diverse samples using semantic clustering and
boundary analysis, followed by iterative refinement with real-time model
performance data to replace redundant samples. Evaluations on the BIG-bench
dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by
at least 57% compared with SOTA baselines, with minimal computational overhead
below 1%. Furthermore, the results demonstrate that our real-time
performance-guided refinement approach can be universally applied to enhance
existing coreset selection methods.",2025-05-15,"Ximing Dong, Shaowei Wang, Dayi Lin, Ahmed E. Hassan",http://arxiv.org/pdf/2505.10736v2,cs.CL
Tracr-Injection: Distilling Algorithms into Pre-trained Language Models,"Motivated by the surge of large language models, there has been a push to
formally characterize the symbolic abilities intrinsic to the transformer
architecture. A programming language, called RASP, has been proposed, which can
be directly compiled into transformer weights to implement these algorithms.
However, the tasks that can be implemented in RASP are often uncommon to learn
from natural unsupervised data, showing a mismatch between theoretical
capabilities of the transformer architecture, and the practical learnability of
these capabilities from unsupervised data. We propose tracr-injection, a method
that allows us to distill algorithms written in RASP directly into a
pre-trained language model. We showcase our method by injecting 3 different
algorithms into a language model. We show how our method creates an
interpretable subspace within the model's residual stream, which can be decoded
into the variables present in the code of the RASP algorithm. Additionally, we
found that the proposed method can improve out-of-distribution performance
compared to our baseline, indicating that indeed a more symbolic mechanism is
taking place in the inner workings of the model. We release the code used to
run our experiments.",2025-05-15,"Tomás Vergara-Browne, Álvaro Soto",http://arxiv.org/pdf/2505.10719v2,cs.CL
AI-enhanced semantic feature norms for 786 concepts,"Semantic feature norms have been foundational in the study of human
conceptual knowledge, yet traditional methods face trade-offs between
concept/feature coverage and verifiability of quality due to the
labor-intensive nature of norming studies. Here, we introduce a novel approach
that augments a dataset of human-generated feature norms with responses from
large language models (LLMs) while verifying the quality of norms against
reliable human judgments. We find that our AI-enhanced feature norm dataset,
NOVA: Norms Optimized Via AI, shows much higher feature density and overlap
among concepts while outperforming a comparable human-only norm dataset and
word-embedding models in predicting people's semantic similarity judgments.
Taken together, we demonstrate that human conceptual knowledge is richer than
captured in previous norm datasets and show that, with proper validation, LLMs
can serve as powerful tools for cognitive science research.",2025-05-15,"Siddharth Suresh, Kushin Mukherjee, Tyler Giallanza, Xizheng Yu, Mia Patil, Jonathan D. Cohen, Timothy T. Rogers",http://arxiv.org/pdf/2505.10718v1,cs.CL
"A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment","High computation costs and latency of large language models such as GPT-4
have limited their deployment in clinical settings. Small language models
(SLMs) offer a cost-effective alternative, but their limited capacity requires
biomedical domain adaptation, which remains challenging. An additional
bottleneck is the unavailability and high sensitivity of clinical data. To
address these challenges, we propose a novel framework for adapting SLMs into
high-performing clinical models. We introduce the MediPhi collection of
3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning
of experts on relevant medical and clinical corpora (PMC, Medical Guideline,
MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most
clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our
expert models deliver relative improvements on this benchmark over the base
model without any task-specific fine-tuning: 64.3% on medical entities, 49.5%
on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by
14%). We unify the expert models into MediPhi via model merging, preserving
gains across benchmarks. Furthermore, we built the MediFlow collection, a
synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP
tasks, 98 fine-grained document types, and JSON format support. Alignment of
MediPhi using supervised fine-tuning and direct preference optimization
achieves further gains of 18.9% on average.",2025-05-15,"Jean-Philippe Corbeil, Amin Dada, Jean-Michel Attendu, Asma Ben Abacha, Alessandro Sordoni, Lucas Caccia, François Beaulieu, Thomas Lin, Jens Kleesiek, Paul Vozila",http://arxiv.org/pdf/2505.10717v2,cs.CL
GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?,"We present GeoGrid-Bench, a benchmark designed to evaluate the ability of
foundation models to understand geo-spatial data in the grid structure.
Geo-spatial datasets pose distinct challenges due to their dense numerical
values, strong spatial and temporal dependencies, and unique multimodal
representations including tabular data, heatmaps, and geographic
visualizations. To assess how foundation models can support scientific research
in this domain, GeoGrid-Bench features large-scale, real-world data covering 16
climate variables across 150 locations and extended time frames. The benchmark
includes approximately 3,200 question-answer pairs, systematically generated
from 8 domain expert-curated templates to reflect practical tasks encountered
by human scientists. These range from basic queries at a single location and
time to complex spatiotemporal comparisons across regions and periods. Our
evaluation reveals that vision-language models perform best overall, and we
provide a fine-grained analysis of the strengths and limitations of different
foundation models in different geo-spatial tasks. This benchmark offers clearer
insights into how foundation models can be effectively applied to geo-spatial
data analysis and used to support scientific research.",2025-05-15,"Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Jiashu He, Joshua Bergerson, John K Hutchison, Jordan Branham, Camillo J Taylor, Tanwi Mallick",http://arxiv.org/pdf/2505.10714v2,cs.CL
Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally,"Central banks around the world play a crucial role in maintaining economic
stability. Deciphering policy implications in their communications is
essential, especially as misinterpretations can disproportionately impact
vulnerable populations. To address this, we introduce the World Central Banks
(WCB) dataset, the most comprehensive monetary policy corpus to date,
comprising over 380k sentences from 25 central banks across diverse geographic
regions, spanning 28 years of historical data. After uniformly sampling 1k
sentences per bank (25k total) across all available years, we annotate and
review each sentence using dual annotators, disagreement resolutions, and
secondary expert reviews. We define three tasks: Stance Detection, Temporal
Classification, and Uncertainty Estimation, with each sentence annotated for
all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large
Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on
these tasks, running 15,075 benchmarking experiments. We find that a model
trained on aggregated data across banks significantly surpasses a model trained
on an individual bank's data, confirming the principle ""the whole is greater
than the sum of its parts."" Additionally, rigorous human evaluations, error
analyses, and predictive tasks validate our framework's economic utility. Our
artifacts are accessible through the HuggingFace and GitHub under the
CC-BY-NC-SA 4.0 license.",2025-05-15,"Agam Shah, Siddhant Sukhani, Huzaifa Pardawala, Saketh Budideti, Riya Bhadani, Rudra Gopal, Siddhartha Somani, Michael Galarnyk, Soungmin Lee, Arnav Hiray, Akshar Ravichandran, Eric Kim, Pranav Aluru, Joshua Zhang, Sebastian Jaskowski, Veer Guda, Meghaj Tarte, Liqin Ye, Spencer Gosden, Rutwik Routu, Rachel Yuh, Sloka Chava, Sahasra Chava, Dylan Patrick Kelly, Aiden Chiang, Harsit Mittal, Sudheer Chava",http://arxiv.org/pdf/2505.17048v1,cs.CL
Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks,"Multi-agent systems built on large language models (LLMs) promise enhanced
problem-solving through distributed information integration, but also risk
replicating collective reasoning failures observed in human groups. Yet, no
theory-grounded benchmark exists to systematically evaluate such failures. In
this paper, we introduce the Hidden Profile paradigm from social psychology as
a diagnostic testbed for multi-agent LLM systems. By distributing critical
information asymmetrically across agents, the paradigm reveals how inter-agent
dynamics support or hinder collective reasoning. We first formalize the
paradigm for multi-agent decision-making under distributed knowledge and
instantiate it as a benchmark with nine tasks spanning diverse scenarios,
including adaptations from prior human studies. We then conduct experiments
with GPT-4.1 and five other leading LLMs, including reasoning-enhanced
variants, showing that multi-agent systems across all models fail to match the
accuracy of single agents given complete information. While agents' collective
performance is broadly comparable to that of human groups, nuanced behavioral
differences emerge, such as increased sensitivity to social desirability.
Finally, we demonstrate the paradigm's diagnostic utility by exploring a
cooperation-contradiction trade-off in multi-agent LLM systems. We find that
while cooperative agents are prone to over-coordination in collective settings,
increased contradiction impairs group convergence. This work contributes a
reproducible framework for evaluating multi-agent LLM systems and motivates
future research on artificial collective intelligence and human-AI interaction.",2025-05-15,"Yuxuan Li, Aoi Naito, Hirokazu Shirado",http://arxiv.org/pdf/2505.11556v1,cs.CL
Artificial Intelligence Bias on English Language Learners in Automatic Scoring,"This study investigated potential scoring biases and disparities toward
English Language Learners (ELLs) when using automatic scoring systems for
middle school students' written responses to science assessments. We
specifically focus on examining how unbalanced training data with ELLs
contributes to scoring bias and disparities. We fine-tuned BERT with four
datasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting
the real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced
mixed dataset with equal representation of both groups. The study analyzed 21
assessment items: 10 items with about 30,000 ELL responses, five items with
about 1,000 ELL responses, and six items with about 200 ELL responses. Scoring
accuracy (Acc) was calculated and compared to identify bias using Friedman
tests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and
then calculated the differences in MSGs generated through both the human and AI
models to identify the scoring disparities. We found that no AI bias and
distorted disparities between ELLs and non-ELLs were found when the training
dataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could
exist if the sample size is limited (ELL = 200).",2025-05-15,"Shuchen Guo, Yun Wang, Jichao Yu, Xuansheng Wu, Bilgehan Ayik, Field M. Watts, Ehsan Latif, Ninghao Liu, Lei Liu, Xiaoming Zhai",http://arxiv.org/pdf/2505.10643v2,cs.CL
MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning,"Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.",2025-05-15,"Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, Hongsheng Li",http://arxiv.org/pdf/2505.10557v1,cs.CL
Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models,"Large reasoning models (LRMs) already possess a latent capacity for long
chain-of-thought reasoning. Prior work has shown that outcome-based
reinforcement learning (RL) can incidentally elicit advanced reasoning
behaviors such as self-correction, backtracking, and verification phenomena
often referred to as the model's ""aha moment"". However, the timing and
consistency of these emergent behaviors remain unpredictable and
uncontrollable, limiting the scalability and reliability of LRMs' reasoning
capabilities. To address these limitations, we move beyond reliance on prompts
and coincidental ""aha moments"". Instead, we explicitly align models with three
meta-abilities: deduction, induction, and abduction, using automatically
generated, self-verifiable tasks. Our three stage-pipeline individual
alignment, parameter-space merging, and domain-specific reinforcement learning,
boosting performance by over 10\% relative to instruction-tuned baselines.
Furthermore, domain-specific RL from the aligned checkpoint yields an
additional 2\% average gain in the performance ceiling across math, coding, and
science benchmarks, demonstrating that explicit meta-ability alignment offers a
scalable and dependable foundation for reasoning. Code is available at:
https://github.com/zhiyuanhubj/Meta-Ability-Alignment",2025-05-15,"Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, Junnan Li",http://arxiv.org/pdf/2505.10554v1,cs.CL
Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models,"While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.",2025-05-15,"Annie Wong, Thomas Bäck, Aske Plaat, Niki van Stein, Anna V. Kononova",http://arxiv.org/pdf/2505.10543v1,cs.CL
MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly,"The rapid extension of context windows in large vision-language models has
given rise to long-context vision-language models (LCVLMs), which are capable
of handling hundreds of images with interleaved text tokens in a single forward
pass. In this work, we introduce MMLongBench, the first benchmark covering a
diverse set of long-context vision-language tasks, to evaluate LCVLMs
effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning
five different categories of downstream tasks, such as Visual RAG and Many-Shot
ICL. It also provides broad coverage of image types, including various natural
and synthetic images. To assess the robustness of the models to different input
lengths, all examples are delivered at five standardized input lengths (8K-128K
tokens) via a cross-modal tokenization scheme that combines vision patches and
text tokens. Through a thorough benchmarking of 46 closed-source and
open-source LCVLMs, we provide a comprehensive analysis of the current models'
vision-language long-context ability. Our results show that: i) performance on
a single task is a weak proxy for overall long-context capability; ii) both
closed-source and open-source models face challenges in long-context
vision-language tasks, indicating substantial room for future improvement; iii)
models with stronger reasoning ability tend to exhibit better long-context
performance. By offering wide task coverage, various image types, and rigorous
length control, MMLongBench provides the missing foundation for diagnosing and
advancing the next generation of LCVLMs.",2025-05-15,"Zhaowei Wang, Wenhao Yu, Xiyu Ren, Jipeng Zhang, Yu Zhao, Rohit Saxena, Liang Cheng, Ginny Wong, Simon See, Pasquale Minervini, Yangqiu Song, Mark Steedman",http://arxiv.org/pdf/2505.10610v1,cs.CL
WorldPM: Scaling Human Preference Modeling,"Motivated by scaling laws in language modeling that demonstrate how test loss
scales as a power law with model and dataset sizes, we find that similar laws
exist in preference modeling. We propose World Preference Modeling$ (WorldPM)
to emphasize this scaling potential, where World Preference embodies a unified
representation of human preferences. In this paper, we collect preference data
from public forums covering diverse user communities, and conduct extensive
training using 15M-scale data across models ranging from 1.5B to 72B
parameters. We observe distinct patterns across different evaluation metrics:
(1) Adversarial metrics (ability to identify deceptive features) consistently
scale up with increased training data and base model size; (2) Objective
metrics (objective knowledge with well-defined answers) show emergent behavior
in larger language models, highlighting WorldPM's scalability potential; (3)
Subjective metrics (subjective preferences from a limited number of humans or
AI) do not demonstrate scaling trends. Further experiments validate the
effectiveness of WorldPM as a foundation for preference fine-tuning. Through
evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly
improves the generalization performance across human preference datasets of
varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%
on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we
observe significant improvements on both in-house and public evaluation sets,
with notable gains of 4% to 8% in our in-house evaluations.",2025-05-15,"Binghai Wang, Runji Lin, Keming Lu, Le Yu, Zhenru Zhang, Fei Huang, Chujie Zheng, Kai Dang, Yang Fan, Xingzhang Ren, An Yang, Binyuan Hui, Dayiheng Liu, Tao Gui, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang, Bowen Yu, Jingren Zhou, Junyang Lin",http://arxiv.org/pdf/2505.10527v2,cs.CL
MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models,"Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.",2025-05-15,"Mugilan Ganesan, Shane Segal, Ankur Aggarwal, Nish Sinnadurai, Sean Lie, Vithursan Thangarasa",http://arxiv.org/pdf/2505.10526v2,cs.CL
ADALog: Adaptive Unsupervised Anomaly detection in Logs with Self-attention Masked Language Model,"Modern software systems generate extensive heterogeneous log data with
dynamic formats, fragmented event sequences, and varying temporal patterns,
making anomaly detection both crucial and challenging. To address these
complexities, we propose ADALog, an adaptive, unsupervised anomaly detection
framework designed for practical applicability across diverse real-world
environments. Unlike traditional methods reliant on log parsing, strict
sequence dependencies, or labeled data, ADALog operates on individual
unstructured logs, extracts intra-log contextual relationships, and performs
adaptive thresholding on normal data. The proposed approach utilizes a
transformer-based, pretrained bidirectional encoder with a masked language
modeling task, fine-tuned on normal logs to capture domain-specific syntactic
and semantic patterns essential for accurate anomaly detection. Anomalies are
identified via token-level reconstruction probabilities, aggregated into
log-level scores, with adaptive percentile-based thresholding calibrated only
on normal data. This allows the model to dynamically adapt to evolving system
behaviors while avoiding rigid, heuristic-based thresholds common in
traditional systems. We evaluate ADALog on benchmark datasets BGL, Thunderbird,
and Spirit, showing strong generalization and competitive performance compared
to state-of-the-art supervised and unsupervised methods. Additional ablation
studies examine the effects of masking, fine-tuning, and token positioning on
model behavior and interpretability.",2025-05-15,"Przemek Pospieszny, Wojciech Mormul, Karolina Szyndler, Sanjeev Kumar",http://arxiv.org/pdf/2505.13496v1,cs.CL
Multi-Token Prediction Needs Registers,"Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.",2025-05-15,"Anastasios Gerontopoulos, Spyros Gidaris, Nikos Komodakis",http://arxiv.org/pdf/2505.10518v1,cs.CL
The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks,"Translation-based strategies for cross-lingual transfer XLT such as
translate-train -- training on noisy target language data translated from the
source language -- and translate-test -- evaluating on noisy source language
data translated from the target language -- are competitive XLT baselines. In
XLT for token classification tasks, however, these strategies include label
projection, the challenging step of mapping the labels from each token in the
original sentence to its counterpart(s) in the translation. Although word
aligners (WAs) are commonly used for label projection, the low-level design
decisions for applying them to translation-based XLT have not been
systematically investigated. Moreover, recent marker-based methods, which
project labeled spans by inserting tags around them before (or after)
translation, claim to outperform WAs in label projection for XLT. In this work,
we revisit WAs for label projection, systematically investigating the effects
of low-level design decisions on token-level XLT: (i) the algorithm for
projecting labels between (multi-)token spans, (ii) filtering strategies to
reduce the number of noisily mapped labels, and (iii) the pre-tokenization of
the translated sentences. We find that all of these substantially impact
translation-based XLT performance and show that, with optimized choices, XLT
with WA offers performance at least comparable to that of marker-based methods.
We then introduce a new projection strategy that ensembles translate-train and
translate-test predictions and demonstrate that it substantially outperforms
the marker-based projection. Crucially, we show that our proposed ensembling
also reduces sensitivity to low-level WA design choices, resulting in more
robust XLT for token classification tasks.",2025-05-15,"Benedikt Ebing, Goran Glavaš",http://arxiv.org/pdf/2505.10507v1,cs.CL
RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs,"This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.",2025-05-15,"Vibha Belavadi, Tushar Vatsa, Dewang Sultania, Suhas Suresha, Ishita Verma, Cheng Chen, Tracy Holloway King, Michael Friedrich",http://arxiv.org/pdf/2505.10495v1,cs.CL
Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective,"Code security and usability are both essential for various coding assistant
applications driven by large language models (LLMs). Current code security
benchmarks focus solely on single evaluation task and paradigm, such as code
completion and generation, lacking comprehensive assessment across dimensions
like secure code generation, vulnerability repair and discrimination. In this
paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks
such as code completion, vulnerability repair, vulnerability detection and
classification, for comprehensive evaluation of LLM code security. Besides, we
developed VC-Judge, an improved judgment model that aligns closely with human
experts and can review LLM-generated programs for vulnerabilities in a more
efficient and reliable way. We conduct a comprehensive evaluation of 20
proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable
codes well, they still tend to generate insecure codes and struggle with
recognizing specific vulnerability types and performing repairs. Extensive
experiments and qualitative analyses reveal key challenges and optimization
directions, offering insights for future research in LLM code security.",2025-05-15,"Yutao Mou, Xiao Deng, Yuxiao Luo, Shikun Zhang, Wei Ye",http://arxiv.org/pdf/2505.10494v1,cs.CL
CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning,"Retrieval-Augmented Generation (RAG) is an effective method to enhance the
capabilities of large language models (LLMs). Existing methods focus on
optimizing the retriever or generator in the RAG system by directly utilizing
the top-k retrieved documents. However, the documents effectiveness are various
significantly across user queries, i.e. some documents provide valuable
knowledge while others totally lack critical information. It hinders the
retriever and generator's adaptation during training. Inspired by human
cognitive learning, curriculum learning trains models using samples progressing
from easy to difficult, thus enhancing their generalization ability, and we
integrate this effective paradigm to the training of the RAG system. In this
paper, we propose a multi-stage Curriculum Learning based RAG system training
framework, named CL-RAG. We first construct training data with multiple
difficulty levels for the retriever and generator separately through sample
evolution. Then, we train the model in stages based on the curriculum learning
approach, thereby optimizing the overall performance and generalization of the
RAG system more effectively. Our CL-RAG framework demonstrates consistent
effectiveness across four open-domain QA datasets, achieving performance gains
of 2% to 4% over multiple advanced methods.",2025-05-15,"Shaohan Wang, Licheng Zhang, Zheren Fu, Zhendong Mao",http://arxiv.org/pdf/2505.10493v1,cs.CL
Parallel Scaling Law for Language Models,"It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.",2025-05-15,"Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, Zhongxin Liu",http://arxiv.org/pdf/2505.10475v1,cs.CL
"Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI","Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.",2025-05-15,"Agnik Saha, Victoria Churchill, Anny D. Rodriguez, Ugur Kursuncu, Muhammed Y. Idris",http://arxiv.org/pdf/2505.10472v1,cs.CL
Superposition Yields Robust Neural Scaling,"The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.",2025-05-15,"Yizhou Liu, Ziming Liu, Jeff Gore",http://arxiv.org/pdf/2505.10465v2,cs.CL
Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe,"In medical practices across the United States, physicians have begun
implementing generative artificial intelligence (AI) tools to perform the
function of scribes in order to reduce the burden of documenting clinical
encounters. Despite their widespread use, no established methods exist to gauge
the quality of AI scribes. To address this gap, we developed a blinded study
comparing the relative performance of large language model (LLM) generated
clinical notes with those from field experts based on audio-recorded clinical
encounters. Quantitative metrics from the Physician Documentation Quality
Instrument (PDQI9) provided a framework to measure note quality, which we
adapted to assess relative performance of AI generated notes. Clinical experts
spanning 5 medical specialties used the PDQI9 tool to evaluate
specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators
from each specialty scored notes drafted from a total of 97 patient visits. We
found uniformly high inter rater agreement (RWG greater than 0.7) between
evaluators in general medicine, orthopedics, and obstetrics and gynecology, and
moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and
cardiology. We found a modest yet significant difference in the overall note
quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes
scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9
instrument as a practical method to gauge the quality of LLM authored notes, as
compared to human-authored notes.",2025-05-15,"Erin Palm, Astrit Manikantan, Mark E. Pepin, Herprit Mahal, Srikanth Subramanya Belwadi",http://arxiv.org/pdf/2505.17047v1,cs.CL
Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models,"We introduce the Diffusion Chain of Lateral Thought (DCoLT), a reasoning
framework for diffusion language models. DCoLT treats each intermediate step in
the reverse diffusion process as a latent ""thinking"" action and optimizes the
entire reasoning trajectory to maximize the reward on the correctness of the
final answer with outcome-based Reinforcement Learning (RL). Unlike traditional
Chain-of-Thought (CoT) methods that follow a causal, linear thinking process,
DCoLT allows bidirectional, non-linear reasoning with no strict rule on
grammatical correctness amid its intermediate steps of thought. We implement
DCoLT on two representative Diffusion Language Models (DLMs). First, we choose
SEDD as a representative continuous-time discrete diffusion model, where its
concrete score derives a probabilistic policy to maximize the RL reward over
the entire sequence of intermediate diffusion steps. We further consider the
discrete-time masked diffusion language model -- LLaDA, and find that the order
to predict and unmask tokens plays an essential role to optimize its RL action
resulting from the ranking-based Unmasking Policy Module (UPM) defined by the
Plackett-Luce model. Experiments on both math and code generation tasks show
that using only public data and 16 H800 GPUs, DCoLT-reinforced DLMs outperform
other DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA
boosts its reasoning accuracy by +9.8%, +5.7%, +11.4%, +19.5% on GSM8K, MATH,
MBPP, and HumanEval.",2025-05-15,"Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, Guo-Jun Qi",http://arxiv.org/pdf/2505.10446v2,cs.CL
Hierarchical Document Refinement for Long-context Retrieval-augmented Generation,"Real-world RAG applications often encounter long-context input scenarios,
where redundant information and noise results in higher inference costs and
reduced performance. To address these challenges, we propose LongRefiner, an
efficient plug-and-play refiner that leverages the inherent structural
characteristics of long documents. LongRefiner employs dual-level query
analysis, hierarchical document structuring, and adaptive refinement through
multi-task learning on a single foundation model. Experiments on seven QA
datasets demonstrate that LongRefiner achieves competitive performance in
various scenarios while using 10x fewer computational costs and latency
compared to the best baseline. Further analysis validates that LongRefiner is
scalable, efficient, and effective, providing practical insights for real-world
long-text RAG applications. Our code is available at
https://github.com/ignorejjj/LongRefiner.",2025-05-15,"Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yongkang Wu, Zhonghua Li, Qi Ye, Zhicheng Dou",http://arxiv.org/pdf/2505.10413v1,cs.CL
Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation,"Plain language summaries (PLSs) are essential for facilitating effective
communication between clinicians and patients by making complex medical
information easier for laypeople to understand and act upon. Large language
models (LLMs) have recently shown promise in automating PLS generation, but
their effectiveness in supporting health information comprehension remains
unclear. Prior evaluations have generally relied on automated scores that do
not measure understandability directly, or subjective Likert-scale ratings from
convenience samples with limited generalizability. To address these gaps, we
conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using
Amazon Mechanical Turk with 150 participants. We assessed PLS quality through
subjective Likert-scale ratings focusing on simplicity, informativeness,
coherence, and faithfulness; and objective multiple-choice comprehension and
recall measures of reader understanding. Additionally, we examined the
alignment between 10 automated evaluation metrics and human judgments. Our
findings indicate that while LLMs can generate PLSs that appear
indistinguishable from human-written ones in subjective evaluations,
human-written PLSs lead to significantly better comprehension. Furthermore,
automated evaluation metrics fail to reflect human judgment, calling into
question their suitability for evaluating PLSs. This is the first study to
systematically evaluate LLM-generated PLSs based on both reader preferences and
comprehension outcomes. Our findings highlight the need for evaluation
frameworks that move beyond surface-level quality and for generation methods
that explicitly optimize for layperson comprehension.",2025-05-15,"Yue Guo, Jae Ho Sohn, Gondy Leroy, Trevor Cohen",http://arxiv.org/pdf/2505.10409v1,cs.CL
Rethinking Repetition Problems of LLMs in Code Generation,"With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.",2025-05-15,"Yihong Dong, Yuchen Liu, Xue Jiang, Zhi Jin, Ge Li",http://arxiv.org/pdf/2505.10402v1,cs.CL
Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples,"This paper explores the design of an aspect-based sentiment analysis system
using large language models (LLMs) for real-world use. We focus on quadruple
opinion extraction -- identifying aspect categories, sentiment polarity,
targets, and opinion expressions from text data across different domains and
languages. Using internal datasets, we investigate whether a single fine-tuned
model can effectively handle multiple domain-specific taxonomies
simultaneously. We demonstrate that a combined multi-domain model achieves
performance comparable to specialized single-domain models while reducing
operational complexity. We also share lessons learned for handling
non-extractive predictions and evaluating various failure modes when developing
LLM-based systems for structured prediction tasks.",2025-05-15,"Benjamin White, Anastasia Shimorina",http://arxiv.org/pdf/2505.10389v1,cs.CL
Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli,"Decoding thoughts from brain activity offers valuable insights into human
cognition and enables promising applications in brain-computer interaction.
While prior studies have explored language reconstruction from fMRI data, they
are typically limited to single-modality inputs such as images or audio. In
contrast, human thought is inherently multimodal. To bridge this gap, we
propose a unified and flexible framework for reconstructing coherent language
from brain recordings elicited by diverse input modalities-visual, auditory,
and textual. Our approach leverages visual-language models (VLMs), using
modality-specific experts to jointly interpret information across modalities.
Experiments demonstrate that our method achieves performance comparable to
state-of-the-art systems while remaining adaptable and extensible. This work
advances toward more ecologically valid and generalizable mind decoding.",2025-05-15,"Chunyu Ye, Shaonan Wang",http://arxiv.org/pdf/2505.10356v1,cs.CL
LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations,"Semantic text representation is a fundamental task in the field of natural
language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have
demonstrated excellent performance, but the values of each dimension are
difficult to trace and interpret. Bag-of-words, as classic sparse interpretable
embeddings, suffers from poor performance. Recently, Benara et al. (2024)
propose interpretable text embeddings using large language models, which forms
""0/1"" embeddings based on responses to a series of questions. These
interpretable text embeddings are typically high-dimensional (larger than
10,000). In this work, we propose Low-dimensional (lower than 500) Dense and
Interpretable text embeddings with Relative representations (LDIR). The
numerical values of its dimensions indicate semantic relatedness to different
anchor texts through farthest point sampling, offering both semantic
representation as well as a certain level of traceability and interpretability.
We validate LDIR on multiple semantic textual similarity, retrieval, and
clustering tasks. Extensive experimental results show that LDIR performs close
to the black-box baseline models and outperforms the interpretable embeddings
baselines with much fewer dimensions. Code is available at
https://github.com/szu-tera/LDIR.",2025-05-15,"Yile Wang, Zhanyu Shen, Hui Huang",http://arxiv.org/pdf/2505.10354v2,cs.CL
J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning,"The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.",2025-05-15,"Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, Swarnadeep Saha",http://arxiv.org/pdf/2505.10320v1,cs.CL
StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation,"Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.",2025-05-15,"Daniel A. P. Oliveira, David Martins de Matos",http://arxiv.org/pdf/2505.10292v1,cs.CL
From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making,"Clinical evidence, derived from rigorous research and data analysis, provides
healthcare professionals with reliable scientific foundations for informed
decision-making. Integrating clinical evidence into real-time practice is
challenging due to the enormous workload, complex professional processes, and
time constraints. This highlights the need for tools that automate evidence
synthesis to support more efficient and accurate decision making in clinical
settings. This study introduces Quicker, an evidence-based clinical decision
support system powered by large language models (LLMs), designed to automate
evidence synthesis and generate clinical recommendations modeled after standard
clinical guideline development processes. Quicker implements a fully automated
chain that covers all phases, from questions to clinical recommendations, and
further enables customized decision-making through integrated tools and
interactive user interfaces. To evaluate Quicker's capabilities, we developed
the Q2CRBench-3 benchmark dataset, based on clinical guideline development
records for three different diseases. Experimental results highlighted
Quicker's strong performance, with fine-grained question decomposition tailored
to user preferences, retrieval sensitivities comparable to human experts, and
literature screening performance approaching comprehensive inclusion of
relevant studies. In addition, Quicker-assisted evidence assessment effectively
supported human reviewers, while Quicker's recommendations were more
comprehensive and logically coherent than those of clinicians. In system-level
testing, collaboration between a single reviewer and Quicker reduced the time
required for recommendation development to 20-40 minutes. In general, our
findings affirm the potential of Quicker to help physicians make quicker and
more reliable evidence-based clinical decisions.",2025-05-15,"Dubai Li, Nan Jiang, Kangping Huang, Ruiqi Tu, Shuyu Ouyang, Huayu Yu, Lin Qiao, Chen Yu, Tianshu Zhou, Danyang Tong, Qian Wang, Mengtao Li, Xiaofeng Zeng, Yu Tian, Xinping Tian, Jingsong Li",http://arxiv.org/pdf/2505.10282v1,cs.CL
The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine,"Natural language processing (NLP) has been traditionally applied to medicine,
and generative large language models (LLMs) have become prominent recently.
However, the differences between them across different medical tasks remain
underexplored. We analyzed 19,123 studies, finding that generative LLMs
demonstrate advantages in open-ended tasks, while traditional NLP dominates in
information extraction and analysis tasks. As these technologies advance,
ethical use of them is essential to ensure their potential in medical
applications.",2025-05-15,"Rui Yang, Huitao Li, Matthew Yu Heng Wong, Yuhe Ke, Xin Li, Kunyu Yu, Jingchi Liao, Jonathan Chong Kai Liew, Sabarinath Vinod Nair, Jasmine Chiat Ling Ong, Irene Li, Douglas Teodoro, Chuan Hong, Daniel Shu Wei Ting, Nan Liu",http://arxiv.org/pdf/2505.10261v1,cs.CL
Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data,"In the era of increasingly sophisticated natural language processing (NLP)
systems, large language models (LLMs) have demonstrated remarkable potential
for diverse applications, including tasks requiring nuanced textual
understanding and contextual reasoning. This study investigates the
capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,
Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex
textual dataset comprising social media posts in Russian and Ukrainian.
Specifically, the focus is on the binary classification task of identifying
references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared
against a gold standard set of human double-annotated labels across 1000
samples. The analysis includes assessing annotation performance under different
prompting conditions, with prompts provided in both English and Russian.
Additionally, the study explores the unique patterns of errors and
disagreements exhibited by each model, offering insights into their strengths,
limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes
to understanding the reliability and applicability of LLMs for sensitive,
domain-specific tasks in multilingual contexts. It also sheds light on how
language models handle inherently subjective and context-dependent judgments, a
critical consideration for their deployment in real-world scenarios.",2025-05-15,"Poli Apollinaire Nemkova, Solomon Ubani, Mark V. Albert",http://arxiv.org/pdf/2505.10260v1,cs.CL
UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech,"Recent neural codec language models have made great progress in the field of
text-to-speech (TTS), but controllable emotional TTS still faces many
challenges. Traditional methods rely on predefined discrete emotion labels to
control emotion categories and intensities, which can't capture the complexity
and continuity of human emotional perception and expression. The lack of
large-scale emotional speech datasets with balanced emotion distributions and
fine-grained emotion annotations often causes overfitting in synthesis models
and impedes effective emotion control. To address these issues, we propose
UDDETTS, a neural codec language model unifying discrete and dimensional
emotions for controllable emotional TTS. This model introduces the
interpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion
description and supports emotion control driven by either discrete emotion
labels or nonlinearly quantified ADV values. Furthermore, a semi-supervised
training strategy is designed to comprehensively utilize diverse speech
datasets with different types of emotion annotations to train the UDDETTS.
Experiments show that UDDETTS achieves linear emotion control along the three
dimensions of ADV space, and exhibits superior end-to-end emotional speech
synthesis capabilities.",2025-05-15,"Jiaxuan Liu, Zhenhua Ling",http://arxiv.org/pdf/2505.10599v1,cs.CL
"On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging","Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.",2025-05-15,"Haozhe Luo, Ziyu Zhou, Zixin Shu, Aurélie Pahud de Mortanges, Robert Berke, Mauricio Reyes",http://arxiv.org/pdf/2505.10231v1,cs.CL
ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention,"Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.",2025-05-15,"Jintian Shao, Hongyi Huang, Jiayi Wu, Beiwen Zhang, ZhiYu Wu, You Shan, MingKai Zheng",http://arxiv.org/pdf/2505.10222v1,cs.CL
RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward,"Role-playing conversational agents (RPCAs) face persistent challenges in
maintaining role consistency. To address this, we propose RAIDEN-R1, a novel
reinforcement learning framework that integrates Verifiable Role-Awareness
Reward (VRAR). The method introduces both singular and multi-term mining
strategies to generate quantifiable rewards by assessing role-specific keys.
Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset
through multi-LLM collaboration, and implement experiments to enhance reasoning
coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's
superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on
Script-Based Knowledge and Conversation Memory metrics, respectively,
outperforming baseline models while maintaining robustness. Case analyses
further reveal the model's enhanced ability to resolve conflicting contextual
cues and sustain first-person narrative consistency. This work bridges the
non-quantifiability gap in RPCA training and provides insights into role-aware
reasoning patterns, advancing the development of RPCAs.",2025-05-15,"Zongsheng Wang, Kaili Sun, Bowen Wu, Qun Yu, Ying Li, Baoxun Wang",http://arxiv.org/pdf/2505.10218v1,cs.CL
VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits,"Large Language Models (LLMs) have achieved remarkable success but face
significant computational and memory challenges, particularly due to their
extensive output vocabularies. The final linear projection layer, mapping
hidden states to vocabulary-sized logits, often constitutes a substantial
portion of the model's parameters and computational cost during inference.
Existing methods like adaptive softmax or hierarchical softmax introduce
structural complexities. In this paper, we propose VQ-Logits, a novel approach
that leverages Vector Quantization (VQ) to drastically reduce the parameter
count and computational load of the LLM output layer. VQ-Logits replaces the
large V * dmodel output embedding matrix with a small, shared codebook of K
embedding vectors (K << V ). Each token in the vocabulary is mapped to one of
these K codebook vectors. The LLM predicts logits over this compact codebook,
which are then efficiently ""scattered"" to the full vocabulary space using the
learned or preassigned mapping. We demonstrate through extensive experiments on
standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits
can achieve up to 99% parameter reduction in the output layer and 6x speedup in
logit computation, with only a marginal 4% increase in perplexity compared to
full softmax baselines. We further provide detailed ablation studies on
codebook size, initialization, and learning strategies, showcasing the
robustness and effectiveness of our approach.",2025-05-15,"Jintian Shao, Hongyi Huang, Jiayi Wu, YiMing Cheng, ZhiYu Wu, You Shan, MingKai Zheng",http://arxiv.org/pdf/2505.10202v1,cs.CL
"The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think","Long chain-of-thought (CoT) is an essential ingredient in effective usage of
modern large language models, but our understanding of the reasoning strategies
underlying these capabilities remains limited. While some prior works have
attempted to categorize CoTs using predefined strategy types, such approaches
are constrained by human intuition and fail to capture the full diversity of
model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up
framework for analyzing and steering model reasoning. Our method automatically
extracts diverse reasoning criteria from model-generated CoTs, embeds them into
a semantic space, clusters them into representative categories, and derives
contrastive rubrics to interpret reasoning behavior. Human evaluations show
that this framework produces more interpretable and comprehensive analyses than
existing methods. Moreover, we demonstrate that this understanding enables
performance gains: we can predict which strategy a model is likely to use and
guide it toward more effective alternatives. Finally, we provide practical
insights, such as that training data format (e.g., free-form vs.
multiple-choice) has a far greater impact on reasoning behavior than data
domain, underscoring the importance of format-aware model design.",2025-05-15,"Seongyun Lee, Seungone Kim, Minju Seo, Yongrae Jo, Dongyoung Go, Hyeonbin Hwang, Jinho Park, Xiang Yue, Sean Welleck, Graham Neubig, Moontae Lee, Minjoon Seo",http://arxiv.org/pdf/2505.10185v1,cs.CL
Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning,"Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.",2025-05-15,"Yoichi Ishibashi, Taro Yano, Masafumi Oyamada",http://arxiv.org/pdf/2505.10182v1,cs.CL
Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment,"Reward models (RMs) play a pivotal role in aligning large language models
(LLMs) with human values. However, noisy preferences in human feedback can lead
to reward misgeneralization - a phenomenon where reward models learn spurious
correlations or overfit to noisy preferences, which poses important challenges
to the generalization of RMs. This paper systematically analyzes the
characteristics of preference pairs and aims to identify how noisy preferences
differ from human-aligned preferences in reward modeling. Our analysis reveals
that noisy preferences are difficult for RMs to fit, as they cause sharp
training fluctuations and irregular gradient updates. These distinctive
dynamics suggest the feasibility of identifying and excluding such noisy
preferences. Empirical studies demonstrate that policy LLM optimized with a
reward model trained on the full preference dataset, which includes substantial
noise, performs worse than the one trained on a subset of exclusively high
quality preferences. To address this challenge, we propose an online
Collaborative Reward Modeling (CRM) framework to achieve robust preference
learning through peer review and curriculum learning. In particular, CRM
maintains two RMs that collaboratively filter potential noisy preferences by
peer-reviewing each other's data selections. Curriculum learning synchronizes
the capabilities of two models, mitigating excessive disparities to promote the
utility of peer review. Extensive experiments demonstrate that CRM
significantly enhances RM generalization, with up to 9.94 points improvement on
RewardBench under an extreme 40\% noise. Moreover, CRM can seamlessly extend to
implicit-reward alignment methods, offering a robust and versatile alignment
strategy.",2025-05-15,"Jiazheng Zhang, Wenqing Jing, Zizhuo Zhang, Zhiheng Xi, Shihan Dou, Rongxiang Weng, Jiahuan Li, Jingang Wang, Mingxu Chai, Shibo Hong, Tao Gui, Qi Zhang",http://arxiv.org/pdf/2505.10597v2,cs.CL
GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs,"Large Language Models are now key assistants in human decision-making
processes. However, a common note always seems to follow: ""LLMs can make
mistakes. Be careful with important info."" This points to the reality that not
all outputs from LLMs are dependable, and users must evaluate them manually.
The challenge deepens as hallucinated responses, often presented with seemingly
plausible explanations, create complications and raise trust issues among
users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph
enhanced retrieval-augmented generation framework to provide Evidence-based
response generation. Specifically, when the user uploads a material document, a
knowledge graph will be created, which helps construct a retrieval-augmented
agent, enhancing the agent's responses with additional knowledge beyond its
training corpus. Then we leverage Chain-of-Thought (CoT) logic generation,
n-hop sub-graph searching, and entailment-based sentence generation to realize
accurate evidence retrieval. We demonstrate that our method improves the
existing models' performance in terms of identifying the exact evidence in a
free-form context, providing a reliable way to examine the resources of LLM's
conclusion and help with the judgment of the trustworthiness.",2025-05-15,"Longchao Da, Parth Mitesh Shah, Kuan-Ru Liou, Jiaxing Zhang, Hua Wei",http://arxiv.org/pdf/2505.10143v1,cs.CL
Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering,"Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.",2025-05-15,"Yangfu Li, Hongjian Zhan, Tianyi Chen, Qi Liu, Yue Lu",http://arxiv.org/pdf/2505.10118v1,cs.CL
Learning Virtual Machine Scheduling in Cloud Computing through Language Agents,"In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.",2025-05-15,"JieHao Wu, Ziwei Wang, Junjie Sheng, Wenhao Li, Xiangfeng Wang, Jun Luo",http://arxiv.org/pdf/2505.10117v2,cs.CL
What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs,"In this paper, we introduce S-MedQA, an English medical question-answering
(QA) dataset for benchmarking large language models in fine-grained clinical
specialties. We use S-MedQA to check the applicability of a popular hypothesis
related to knowledge injection in the knowledge-intense scenario of medical QA,
and show that: 1) training on data from a speciality does not necessarily lead
to best performance on that specialty and 2) regardless of the specialty
fine-tuned on, token probabilities of clinically relevant terms for all
specialties increase consistently. Thus, we believe improvement gains come
mostly from domain shifting (e.g., general to medical) rather than knowledge
injection and suggest rethinking the role of fine-tuning data in the medical
domain. We release S-MedQA and all code needed to reproduce all our experiments
to the research community.",2025-05-15,"Xinlan Yan, Di Wu, Yibin Lei, Christof Monz, Iacer Calixto",http://arxiv.org/pdf/2505.10113v2,cs.CL
AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification,"Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating text that closely resembles human writing across a wide range of
styles and genres. However, such capabilities are prone to potential misuse,
such as fake news generation, spam email creation, and misuse in academic
assignments. As a result, accurate detection of AI-generated text and
identification of the model that generated it are crucial for maintaining the
responsible use of LLMs. In this work, we addressed two sub-tasks put forward
by the Defactify workshop under AI-Generated Text Detection shared task at the
Association for the Advancement of Artificial Intelligence (AAAI 2025): Task A
involved distinguishing between human-authored or AI-generated text, while Task
B focused on attributing text to its originating language model. For each task,
we proposed two neural architectures: an optimized model and a simpler variant.
For Task A, the optimized neural architecture achieved fifth place with $F1$
score of 0.994, and for Task B, the simpler neural architecture also ranked
fifth place with $F1$ score of 0.627.",2025-05-15,"Harika Abburi, Sanmitra Bhattacharya, Edward Bowen, Nirmala Pudota",http://arxiv.org/pdf/2505.11550v1,cs.CL
From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI,"Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.",2025-05-15,Hsuan-Lei Shao,http://arxiv.org/pdf/2505.10093v1,cs.CL
XRAG: Cross-lingual Retrieval-Augmented Generation,"We propose XRAG, a novel benchmark designed to evaluate the generation
abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)
settings where the user language does not match the retrieval results. XRAG is
constructed from recent news articles to ensure that its questions require
external knowledge to be answered. It covers the real-world scenarios of
monolingual and multilingual retrieval, and provides relevancy annotations for
each retrieved document. Our novel dataset construction pipeline results in
questions that require complex reasoning, as evidenced by the significant gap
between human and LLM performance. Consequently, XRAG serves as a valuable
benchmark for studying LLM reasoning abilities, even before considering the
additional cross-lingual complexity. Experimental results on five LLMs uncover
two previously unreported challenges in cross-lingual RAG: 1) in the
monolingual retrieval setting, all evaluated models struggle with response
language correctness; 2) in the multilingual retrieval setting, the main
challenge lies in reasoning over retrieved information across languages rather
than generation of non-English text.",2025-05-15,"Wei Liu, Sony Trenous, Leonardo F. R. Ribeiro, Bill Byrne, Felix Hieber",http://arxiv.org/pdf/2505.10089v1,cs.CL
Designing and Contextualising Probes for African Languages,"Pretrained language models (PLMs) for African languages are continually
improving, but the reasons behind these advances remain unclear. This paper
presents the first systematic investigation into probing PLMs for linguistic
knowledge about African languages. We train layer-wise probes for six
typologically diverse African languages to analyse how linguistic features are
distributed. We also design control tasks, a way to interpret probe
performance, for the MasakhaPOS dataset. We find PLMs adapted for African
languages to encode more linguistic information about target languages than
massively multilingual PLMs. Our results reaffirm previous findings that
token-level syntactic information concentrates in middle-to-last layers, while
sentence-level semantic information is distributed across all layers. Through
control tasks and probing baselines, we confirm that performance reflects the
internal knowledge of PLMs rather than probe memorisation. Our study applies
established interpretability techniques to African-language PLMs. In doing so,
we highlight the internal mechanisms underlying the success of strategies like
active learning and multilingual adaptation.",2025-05-15,"Wisdom Aduah, Francois Meyer",http://arxiv.org/pdf/2505.10081v2,cs.CL
Dark LLMs: The Growing Threat of Unaligned AI Models,"Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.",2025-05-15,"Michael Fire, Yitzhak Elbazis, Adi Wasenstein, Lior Rokach",http://arxiv.org/pdf/2505.10066v1,cs.CL
CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability,"Advancements in Large Language Models (LLMs) have extended their input
context length, yet they still struggle with retrieval and reasoning in
long-context inputs. Existing methods propose to utilize the prompt strategy
and retrieval head to alleviate this limitation. However, they still face
challenges in balancing retrieval precision and recall, impacting their
efficacy in answering questions. To address this, we introduce $\textbf{CAFE}$,
a two-stage coarse-to-fine method to enhance multi-document question-answering
capacities. By gradually eliminating the negative impacts of background and
distracting documents, CAFE makes the responses more reliant on the evidence
documents. Initially, a coarse-grained filtering method leverages retrieval
heads to identify and rank relevant documents. Then, a fine-grained steering
method guides attention to the most relevant content. Experiments across
benchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%
SubEM improvement over SFT and RAG methods on the Mistral model, respectively.",2025-05-15,"Han Peng, Jinhao Jiang, Zican Dong, Wayne Xin Zhao, Lei Fang",http://arxiv.org/pdf/2505.10063v1,cs.CL
DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs,"As Large Language Models (LLMs) have risen in prominence over the past few
years, there has been concern over the potential biases in LLMs inherited from
the training data. Previous studies have examined how LLMs exhibit implicit
bias, such as when response generation changes when different social contexts
are introduced. We argue that this implicit bias is not only an ethical, but
also a technical issue, as it reveals an inability of LLMs to accommodate
extraneous information. However, unlike other measures of LLM intelligence,
there are no standard methods to benchmark this specific subset of LLM bias. To
bridge this gap, we developed a method for calculating an easily interpretable
benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM
logic and math problem datasets with sociodemographic personas. We demonstrate
that this method can statistically validate the presence of implicit bias in
LLM behavior and find an inverse trend between question answering accuracy and
implicit bias, supporting our argument.",2025-05-15,"Lake Yin, Fan Huang",http://arxiv.org/pdf/2505.10013v1,cs.CL
Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia,"Recent studies have separately highlighted significant biases within
foundational large language models (LLMs) against certain nationalities and
stigmatized social groups. This research investigates the ethical implications
of these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.
Through structured prompt series, we evaluate model responses to several
scenarios involving American and North Korean nationalities with various mental
disabilities. Findings reveal significant discrepancies in empathy levels with
North Koreans facing greater negative bias, particularly when mental disability
is also a factor. This underscores the need for improvements in LLMs designed
with a nuanced understanding of intersectional identity.",2025-05-15,"Afifah Kashif, Heer Patel",http://arxiv.org/pdf/2505.17045v1,cs.CL
Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors,"Understanding the factors contributing to traffic crashes and developing
strategies to mitigate their severity is essential. Traditional statistical
methods and machine learning models often struggle to capture the complex
interactions between various factors and the unique characteristics of each
crash. This research leverages large language model (LLM) to analyze freeway
crash data and provide crash causation analysis accordingly. By compiling 226
traffic safety studies related to freeway crashes, a training dataset
encompassing environmental, driver, traffic, and geometric design factors was
created. The Llama3 8B model was fine-tuned using QLoRA to enhance its
understanding of freeway crashes and their contributing factors, as covered in
these studies. The fine-tuned Llama3 8B model was then used to identify crash
causation without pre-labeled data through zero-shot classification, providing
comprehensive explanations to ensure that the identified causes were reasonable
and aligned with existing research. Results demonstrate that LLMs effectively
identify primary crash causes such as alcohol-impaired driving, speeding,
aggressive driving, and driver inattention. Incorporating event data, such as
road maintenance, offers more profound insights. The model's practical
applicability and potential to improve traffic safety measures were validated
by a high level of agreement among researchers in the field of traffic safety,
as reflected in questionnaire results with 88.89%. This research highlights the
complex nature of traffic crashes and how LLMs can be used for comprehensive
analysis of crash causation and other contributing factors. Moreover, it
provides valuable insights and potential countermeasures to aid planners and
policymakers in developing more effective and efficient traffic safety
practices.",2025-05-15,"Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Samgyu Yang, Abdulrahman Faden",http://arxiv.org/pdf/2505.09949v1,cs.CL
Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph,"The advent of large language models (LLMs) has allowed numerous applications,
including the generation of queried responses, to be leveraged in chatbots and
other conversational assistants. Being trained on a plethora of data, LLMs
often undergo high levels of over-fitting, resulting in the generation of extra
and incorrect data, thus causing hallucinations in output generation. One of
the root causes of such problems is the lack of timely, factual, and
personalized information fed to the LLM. In this paper, we propose an approach
to address these problems by introducing retrieval augmented generation (RAG)
using knowledge graphs (KGs) to assist the LLM in personalized response
generation tailored to the users. KGs have the advantage of storing
continuously updated factual information in a structured way. While our KGs can
be used for a variety of frequently updated personal data, such as calendar,
contact, and location data, we focus on calendar data in this paper. Our
experimental results show that our approach works significantly better in
understanding personal information and generating accurate responses compared
to the baseline LLMs using personal data as text inputs, with a moderate
reduction in response time.",2025-05-15,"Deeksha Prahlad, Chanhee Lee, Dongha Kim, Hokeun Kim",http://arxiv.org/pdf/2505.09945v1,cs.CL
Rethinking Prompt Optimizers: From Prompt Merits to Optimization,"Prompt optimization (PO) provides a practical way to improve response quality
when users lack the time or expertise to manually craft effective prompts.
Existing methods typically rely on advanced, large-scale LLMs like GPT-4 to
generate optimized prompts. However, due to limited downward compatibility,
verbose, instruction-heavy prompts from advanced LLMs can overwhelm lightweight
inference models and degrade response quality. In this work, we rethink prompt
optimization through the lens of interpretable design. We first identify a set
of model-agnostic prompt quality merits and empirically validate their
effectiveness in enhancing prompt and response quality. We then introduce MePO,
a merit-guided, lightweight, and locally deployable prompt optimizer trained on
our preference dataset built from merit-aligned prompts generated by a
lightweight LLM. Unlike prior work, MePO avoids online optimization reliance,
reduces cost and privacy concerns, and, by learning clear, interpretable
merits, generalizes effectively to both large-scale and lightweight inference
models. Experiments demonstrate that MePO achieves better results across
diverse tasks and model types, offering a scalable and robust solution for
real-world deployment. The code and dataset can be found in
https://github.com/MidiyaZhu/MePO",2025-05-15,"Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Tianjiao Li, Chua Jia Jim Deryl, Mak Lee Onn, Gee Wah Ng, Kezhi Mao",http://arxiv.org/pdf/2505.09930v2,cs.CL
From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models,"The rise of Large Language Models (LLMs) has heightened concerns about the
misuse of AI-generated text, making watermarking a promising solution.
Mainstream watermarking schemes for LLMs fall into two categories: logits-based
and sampling-based. However, current schemes entail trade-offs among
robustness, text quality, and security. To mitigate this, we integrate
logits-based and sampling-based schemes, harnessing their respective strengths
to achieve synergy. In this paper, we propose a versatile symbiotic
watermarking framework with three strategies: serial, parallel, and hybrid. The
hybrid framework adaptively embeds watermarks using token entropy and semantic
entropy, optimizing the balance between detectability, robustness, text
quality, and security. Furthermore, we validate our approach through
comprehensive experiments on various datasets and models. Experimental results
indicate that our method outperforms existing baselines and achieves
state-of-the-art (SOTA) performance. We believe this framework provides novel
insights into diverse watermarking paradigms. Our code is available at
https://github.com/redwyd/SymMark.",2025-05-15,"Yidan Wang, Yubing Ren, Yanan Cao, Binxing Fang",http://arxiv.org/pdf/2505.09924v2,cs.CL
PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization,"Large Language Models (LLMs) excel in various domains but pose inherent
privacy risks. Existing methods to evaluate privacy leakage in LLMs often use
memorized prefixes or simple instructions to extract data, both of which
well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM
safety mechanisms to generate harmful content, but their role in privacy
scenarios remains underexplored. In this paper, we examine the effectiveness of
jailbreak attacks in extracting sensitive information, bridging privacy leakage
and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework
targeting Personally Identifiable Information (PII) and addressing the
limitations of current jailbreak methods. Specifically, PIG identifies PII
entities and their types in privacy queries, uses in-context learning to build
a privacy context, and iteratively updates it with three gradient-based
strategies to elicit target PII. We evaluate PIG and existing jailbreak methods
using two privacy-related datasets. Experiments on four white-box and two
black-box LLMs show that PIG outperforms baseline methods and achieves
state-of-the-art (SoTA) results. The results underscore significant privacy
risks in LLMs, emphasizing the need for stronger safeguards. Our code is
availble at https://github.com/redwyd/PrivacyJailbreak.",2025-05-15,"Yidan Wang, Yanan Cao, Yubing Ren, Fang Fang, Zheng Lin, Binxing Fang",http://arxiv.org/pdf/2505.09921v2,cs.CL
Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries,"Large language models are, by definition, based on language. In an effort to
underscore the critical need for regional localized models, this paper examines
primary differences between variants of written Spanish across Latin America
and Spain, with an in-depth sociocultural and linguistic contextualization
therein. We argue that these differences effectively constitute significant
gaps in the quotidian use of Spanish among dialectal groups by creating
sociolinguistic dissonances, to the extent that locale-sensitive AI models
would play a pivotal role in bridging these divides. In doing so, this approach
informs better and more efficient localization strategies that also serve to
more adequately meet inclusivity goals, while securing sustainable active daily
user growth in a major low-risk investment geographic area. Therefore,
implementing at least the proposed five sub variants of Spanish addresses two
lines of action: to foment user trust and reliance on AI language models while
also demonstrating a level of cultural, historical, and sociolinguistic
awareness that reflects positively on any internationalization strategy.",2025-05-15,"Martin Capdevila, Esteban Villa Turek, Ellen Karina Chumbe Fernandez, Luis Felipe Polo Galvez, Luis Cadavid, Andrea Marroquin, Rebeca Vargas Quesada, Johanna Crew, Nicole Vallejo Galarraga, Christopher Rodriguez, Diego Gutierrez, Radhi Datla",http://arxiv.org/pdf/2505.09902v1,cs.CL
Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks,"Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.",2025-05-15,"Ziyuan Zhang, Darcy Wang, Ningyuan Chen, Rodrigo Mansur, Vahid Sarhangian",http://arxiv.org/pdf/2505.09901v1,cs.CL
Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers,"Transformer models learn in two distinct modes: in-weights learning (IWL),
encoding knowledge into model weights, and in-context learning (ICL), adapting
flexibly to context without weight modification. To better understand the
interplay between these learning modes, we draw inspiration from evolutionary
biology's analogous adaptive strategies: genetic encoding (akin to IWL,
adapting over generations and fixed within an individual's lifetime) and
phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to
environmental cues). In evolutionary biology, environmental predictability
dictates the balance between these strategies: stability favors genetic
encoding, while reliable predictive cues promote phenotypic plasticity. We
experimentally operationalize these dimensions of predictability and
systematically investigate their influence on the ICL/IWL balance in
Transformers. Using regression and classification tasks, we show that high
environmental stability decisively favors IWL, as predicted, with a sharp
transition at maximal stability. Conversely, high cue reliability enhances ICL
efficacy, particularly when stability is low. Furthermore, learning dynamics
reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift
occurs in some settings (e.g., classification with many classes), we
demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL
acquisition (e.g., regression) can exhibit an initial IWL phase later yielding
to ICL dominance. These findings support a relative-cost hypothesis for
explaining these learning mode transitions, establishing predictability as a
critical factor governing adaptive strategies in Transformers, and offering
novel insights for understanding ICL and guiding training methodologies.",2025-05-14,"Alexander Y. Ku, Thomas L. Griffiths, Stephanie C. Y. Chan",http://arxiv.org/pdf/2505.09855v1,cs.CL
Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting,"Large Language Models (LLMs) have shown impressive performance across natural
language tasks, but their ability to forecast violent conflict remains
underexplored. We investigate whether LLMs possess meaningful parametric
knowledge-encoded in their pretrained weights-to predict conflict escalation
and fatalities without external data. This is critical for early warning
systems, humanitarian planning, and policy-making. We compare this parametric
knowledge with non-parametric capabilities, where LLMs access structured and
unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent
news reports via Retrieval-Augmented Generation (RAG). Incorporating external
information could enhance model performance by providing up-to-date context
otherwise missing from pretrained weights. Our two-part evaluation framework
spans 2020-2024 across conflict-prone regions in the Horn of Africa and the
Middle East. In the parametric setting, LLMs predict conflict trends and
fatalities relying only on pretrained knowledge. In the non-parametric setting,
models receive summaries of recent conflict events, indicators, and
geopolitical developments. We compare predicted conflict trend labels (e.g.,
Escalate, Stable Conflict, De-escalate, Peace) and fatalities against
historical data. Our findings highlight the strengths and limitations of LLMs
for conflict forecasting and the benefits of augmenting them with structured
external knowledge.",2025-05-14,"Apollinaire Poli Nemkova, Sarath Chandra Lingareddy, Sagnik Ray Choudhury, Mark V. Albert",http://arxiv.org/pdf/2505.09852v1,cs.CL
KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning,"Each year, tens of millions of essays are written and graded in college-level
English courses. Students are asked to analyze literary and cultural texts
through a process known as close reading, in which they gather textual details
to formulate evidence-based arguments. Despite being viewed as a basis for
critical thinking and widely adopted as a required element of university
coursework, close reading has never been evaluated on large language models
(LLMs), and multi-discipline benchmarks like MMLU do not include literature as
a subject. To fill this gap, we present KRISTEVA, the first close reading
benchmark for evaluating interpretive reasoning, consisting of 1331
multiple-choice questions adapted from classroom data. With KRISTEVA, we
propose three progressively more difficult sets of tasks to approximate
different elements of the close reading process, which we use to test how well
LLMs may seem to understand and reason about literary works: 1) extracting
stylistic features, 2) retrieving relevant contextual information from
parametric knowledge, and 3) multi-hop reasoning between style and external
contexts. Our baseline results find that, while state-of-the-art LLMs possess
some college-level close reading competency (accuracy 49.7% - 69.7%), their
performances still trail those of experienced human evaluators on 10 out of our
11 tasks.",2025-05-14,"Peiqi Sui, Juan Diego Rodriguez, Philippe Laban, Dean Murphy, Joseph P. Dexter, Richard Jean So, Samuel Baker, Pramit Chaudhuri",http://arxiv.org/pdf/2505.09825v1,cs.CL
Adversarial Attack on Large Language Models using Exponentiated Gradient Descent,"As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack",2025-05-14,"Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu",http://arxiv.org/pdf/2505.09820v1,cs.CL
Exploring the generalization of LLM truth directions on conversational formats,"Several recent works argue that LLMs have a universal truth direction where
true and false statements are linearly separable in the activation space of the
model. It has been demonstrated that linear probes trained on a single hidden
state of the model already generalize across a range of topics and might even
be used for lie detection in LLM conversations. In this work we explore how
this truth direction generalizes between various conversational formats. We
find good generalization between short conversations that end on a lie, but
poor generalization to longer formats where the lie appears earlier in the
input prompt. We propose a solution that significantly improves this type of
generalization by adding a fixed key phrase at the end of each conversation.
Our results highlight the challenges towards reliable LLM lie detectors that
generalize to new settings.",2025-05-14,"Timour Ichmoukhamedov, David Martens",http://arxiv.org/pdf/2505.09807v1,cs.CL
Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques,"Research projects, including those focused on cancer, rely on the manual
extraction of information from clinical reports. This process is time-consuming
and prone to errors, limiting the efficiency of data-driven approaches in
healthcare. To address these challenges, Natural Language Processing (NLP)
offers an alternative for automating the extraction of relevant data from
electronic health records (EHRs). In this study, we focus on lung and breast
cancer due to their high incidence and the significant impact they have on
public health. Early detection and effective data management in both types of
cancer are crucial for improving patient outcomes. To enhance the accuracy and
efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels
at identifying relevant entities in clinical texts and converting them into
standardized formats such as SNOMED and OMOP. uQuery not only detects and
classifies entities but also associates them with contextual information,
including negated entities, temporal aspects, and patient-related details. In
this work, we explore the use of NLP techniques, specifically Named Entity
Recognition (NER), to automatically identify and extract key clinical
information from EHRs related to these two cancers. A dataset from Health
Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast
cancer and 400 lung cancer reports, was used, with eight clinical entities
manually labeled using the Doccano platform. To perform NER, we fine-tuned the
bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained
in Spanish. Fine-tuning was performed using the Transformers architecture,
enabling accurate recognition of clinical entities in these cancer types. Our
results demonstrate strong overall performance, particularly in identifying
entities like MET and PAT, although challenges remain with less frequent
entities like EVOL.",2025-05-14,"J. Moreno-Casanova, J. M. Auñón, A. Mártinez-Pérez, M. E. Pérez-Martínez, M. E. Gas-López",http://arxiv.org/pdf/2505.09794v1,cs.CL
Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints,"This case study applies a phased hyperparameter optimization process to
compare multitask natural language model variants that utilize multiphase
learning rate scheduling and optimizer parameter grouping. We employ short,
Bayesian optimization sessions that leverage multi-fidelity, hyperparameter
space pruning, progressive halving, and a degree of human guidance. We utilize
the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn
Gaussian process minimization. Initially, we use efficient low-fidelity sprints
to prune the hyperparameter space. Subsequent sprints progressively increase
their model fidelity and employ hyperband pruning for efficiency. A second
aspect of our approach is using a meta-learner to tune threshold values to
resolve classification probabilities during inference. We demonstrate our
method on a collection of variants of the 2021 Joint Entity and Relation
Extraction model proposed by Eberts and Ulges.",2025-05-14,Michael Kamfonas,http://arxiv.org/pdf/2505.09792v1,cs.CL
A Survey on Large Language Models in Multimodal Recommender Systems,"Multimodal recommender systems (MRS) integrate heterogeneous user and item
data, such as text, images, and structured information, to enhance
recommendation performance. The emergence of large language models (LLMs)
introduces new opportunities for MRS by enabling semantic reasoning, in-context
learning, and dynamic input handling. Compared to earlier pre-trained language
models (PLMs), LLMs offer greater flexibility and generalisation capabilities
but also introduce challenges related to scalability and model accessibility.
This survey presents a comprehensive review of recent work at the intersection
of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and
data adaptation techniques. We propose a novel taxonomy to characterise
integration patterns, identify transferable techniques from related
recommendation domains, provide an overview of evaluation metrics and datasets,
and point to possible future directions. We aim to clarify the emerging role of
LLMs in multimodal recommendation and support future research in this rapidly
evolving field.",2025-05-14,"Alejo Lopez-Avila, Jinhua Du",http://arxiv.org/pdf/2505.09777v1,cs.CL
TARGET: Benchmarking Table Retrieval for Generative Tasks,"The data landscape is rich with structured data, often of high value to
organizations, driving important applications in data analysis and machine
learning. Recent progress in representation learning and generative models for
such data has led to the development of natural language interfaces to
structured data, including those leveraging text-to-SQL. Contextualizing
interactions, either through conversational interfaces or agentic components,
in structured data through retrieval-augmented generation can provide
substantial benefits in the form of freshness, accuracy, and comprehensiveness
of answers. The key question is: how do we retrieve the right table(s) for the
analytical query or task at hand? To this end, we introduce TARGET: a benchmark
for evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the
retrieval performance of different retrievers in isolation, as well as their
impact on downstream tasks. We find that dense embedding-based retrievers far
outperform a BM25 baseline which is less effective than it is for retrieval
over unstructured text. We also surface the sensitivity of retrievers across
various metadata (e.g., missing table titles), and demonstrate a stark
variation of retrieval performance across datasets and tasks. TARGET is
available at https://target-benchmark.github.io.",2025-05-14,"Xingyu Ji, Parker Glenn, Aditya G. Parameswaran, Madelon Hulsebos",http://arxiv.org/pdf/2505.11545v1,cs.CL
Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning,"Pretrained language models (LLMs) are often constrained by their fixed
tokenization schemes, leading to inefficiencies and performance limitations,
particularly for multilingual or specialized applications. This tokenizer
lock-in presents significant challenges. standard methods to overcome this
often require prohibitive computational resources. Although tokenizer
replacement with heuristic initialization aims to reduce this burden, existing
methods often require exhaustive residual fine-tuning and still may not fully
preserve semantic nuances or adequately address the underlying compression
inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a
model-agnostic tokenizer transplantation method, and second, novel
pre-tokenization learning for multi-word Supertokens to enhance compression and
reduce fragmentation. Tokenadapt initializes new unique token embeddings via a
hybrid heuristic that combines two methods: a local estimate based on subword
decomposition using the old tokenizer, and a global estimate utilizing the
top-k semantically similar tokens from the original vocabulary. This
methodology aims to preserve semantics while significantly minimizing
retraining requirements. Empirical investigations validate both contributions:
the transplantation heuristic successfully initializes unique tokens, markedly
outperforming conventional baselines and sophisticated methods including
Transtokenizer and ReTok, while our Supertokens achieve notable compression
gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid
initialization consistently yields lower perplexity ratios compared to both
ReTok and TransTokenizer baselines across different base models and newly
trained target tokenizers. TokenAdapt typically reduced the overall perplexity
ratio significantly compared to ReTok, yielding at least a 2-fold improvement
in these aggregate scores.",2025-05-14,"Shaurya Sharthak, Vinayak Pahalwan, Adithya Kamath, Adarsh Shirawalmath",http://arxiv.org/pdf/2505.09738v1,cs.CL
An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs,"Analyzing texts such as open-ended responses, headlines, or social media
posts is a time- and labor-intensive process highly susceptible to bias. LLMs
are promising tools for text analysis, using either a predefined (top-down) or
a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we
present a step-by-step tutorial to efficiently develop, test, and apply
taxonomies for analyzing unstructured data through an iterative and
collaborative process between researchers and LLMs. Using personal goals
provided by participants as an example, we demonstrate how to write prompts to
review datasets and generate a taxonomy of life domains, evaluate and refine
the taxonomy through prompt and direct modifications, test the taxonomy and
assess intercoder agreements, and apply the taxonomy to categorize an entire
dataset with high intercoder reliability. We discuss the possibilities and
limitations of using LLMs for text analysis.",2025-05-14,"Gino Carmona-Díaz, William Jiménez-Leal, María Alejandra Grisales, Chandra Sripada, Santiago Amaya, Michael Inzlicht, Juan Pablo Bermúdez",http://arxiv.org/pdf/2505.09724v2,cs.CL
VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts,"Large language models (LLMs) excel at generating long-form responses, but
evaluating their factuality remains challenging due to complex inter-sentence
dependencies within the generated facts. Prior solutions predominantly follow a
decompose-decontextualize-verify pipeline but often fail to capture essential
context and miss key relational facts. In this paper, we introduce VeriFact, a
factuality evaluation framework designed to enhance fact extraction by
identifying and resolving incomplete and missing facts to support more accurate
verification results. Moreover, we introduce FactRBench , a benchmark that
evaluates both precision and recall in long-form model responses, whereas prior
work primarily focuses on precision. FactRBench provides reference fact sets
from advanced LLMs and human-written answers, enabling recall assessment.
Empirical evaluations show that VeriFact significantly enhances fact
completeness and preserves complex facts with critical relational information,
resulting in more accurate factuality evaluation. Benchmarking various open-
and close-weight LLMs on FactRBench indicate that larger models within same
model family improve precision and recall, but high precision does not always
correlate with high recall, underscoring the importance of comprehensive
factuality assessment.",2025-05-14,"Xin Liu, Lechen Zhang, Sheza Munir, Yiyang Gu, Lu Wang",http://arxiv.org/pdf/2505.09701v1,cs.CL
Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?,"Language model (LM) agents are increasingly used as autonomous
decision-makers who need to actively gather information to guide their
decisions. A crucial cognitive skill for such agents is the efficient
exploration and understanding of the causal structure of the world -- key to
robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs
possess this capability or exhibit systematic biases leading to erroneous
conclusions. In this work, we examine LMs' ability to explore and infer causal
relationships, using the well-established ""Blicket Test"" paradigm from
developmental psychology. We find that LMs reliably infer the common, intuitive
disjunctive causal relationships but systematically struggle with the unusual,
yet equally (or sometimes even more) evidenced conjunctive ones. This
""disjunctive bias"" persists across model families, sizes, and prompting
strategies, and performance further declines as task complexity increases.
Interestingly, an analogous bias appears in human adults, suggesting that LMs
may have inherited deep-seated reasoning heuristics from their training data.
To this end, we quantify similarities between LMs and humans, finding that LMs
exhibit adult-like inference profiles (but not children-like). Finally, we
propose a test-time sampling method which explicitly samples and eliminates
hypotheses about causal relationships from the LM. This scalable approach
significantly reduces the disjunctive bias and moves LMs closer to the goal of
scientific, causally rigorous reasoning.",2025-05-14,"Anthony GX-Chen, Dongyan Lin, Mandana Samiei, Doina Precup, Blake A. Richards, Rob Fergus, Kenneth Marino",http://arxiv.org/pdf/2505.09614v1,cs.CL
Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors,"The use of Large Language Models (LLMs) in hardware design has taken off in
recent years, principally through its incorporation in tools that increase chip
designer productivity. There has been considerable discussion about the use of
LLMs in RTL specifications of chip designs, for which the two most popular
languages are Verilog and VHDL. LLMs and their use in Verilog design has
received significant attention due to the higher popularity of the language,
but little attention so far has been given to VHDL despite its continued
popularity in the industry. There has also been little discussion about the
unique needs of organizations that engage in high-performance processor design,
and techniques to deploy AI solutions in these settings. In this paper, we
describe our journey in developing a Large Language Model (LLM) specifically
for the purpose of explaining VHDL code, a task that has particular importance
in an organization with decades of experience and assets in high-performance
processor design. We show how we developed test sets specific to our needs and
used them for evaluating models as we performed extended pretraining (EPT) of a
base LLM. Expert evaluation of the code explanations produced by the EPT model
increased to 69% compared to a base model rating of 43%. We further show how we
developed an LLM-as-a-judge to gauge models similar to expert evaluators. This
led us to deriving and evaluating a host of new models, including an
instruction-tuned version of the EPT model with an expected expert evaluator
rating of 71%. Our experiments also indicate that with the potential use of
newer base models, this rating can be pushed to 85% and beyond. We conclude
with a discussion on further improving the quality of hardware design LLMs
using exciting new developments in the Generative AI world.",2025-05-14,"Nicolas Dupuis, Ravi Nair, Shyam Ramji, Sean McClintock, Nishant Chauhan, Priyanka Nagpal, Bart Blaner, Ken Valk, Leon Stok, Ruchir Puri",http://arxiv.org/pdf/2505.09610v1,cs.CL
WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models,"Large Language Models (LLMs) are predominantly trained and aligned in ways
that reinforce Western-centric epistemologies and socio-cultural norms, leading
to cultural homogenization and limiting their ability to reflect global
civilizational plurality. Existing benchmarking frameworks fail to adequately
capture this bias, as they rely on rigid, closed-form assessments that overlook
the complexity of cultural inclusivity. To address this, we introduce
WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity
(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our
approach is grounded in the Multiplex Worldview proposed by Senturk et al.,
which distinguishes between Uniplex models, reinforcing cultural
homogenization, and Multiplex models, which integrate diverse perspectives.
WorldView-Bench measures Cultural Polarization, the exclusion of alternative
perspectives, through free-form generative evaluation rather than conventional
categorical benchmarks. We implement applied multiplexity through two
intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where
system prompts embed multiplexity principles, and (2) Multi-Agent System
(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing
distinct cultural perspectives collaboratively generate responses. Our results
demonstrate a significant increase in Perspectives Distribution Score (PDS)
entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,
alongside a shift toward positive sentiment (67.7%) and enhanced cultural
balance. These findings highlight the potential of multiplex-aware AI
evaluation in mitigating cultural bias in LLMs, paving the way for more
inclusive and ethically aligned AI systems.",2025-05-14,"Abdullah Mushtaq, Imran Taj, Rafay Naeem, Ibrahim Ghaznavi, Junaid Qadir",http://arxiv.org/pdf/2505.09595v1,cs.CL
System Prompt Optimization with Meta-Learning,"Large Language Models (LLMs) have shown remarkable capabilities, with
optimizing their input prompts playing a pivotal role in maximizing their
performance. However, while LLM prompts consist of both the task-agnostic
system prompts and task-specific user prompts, existing work on prompt
optimization has focused on user prompts specific to individual queries or
tasks, and largely overlooked the system prompt that is, once optimized,
applicable across different tasks and domains. Motivated by this, we introduce
the novel problem of bilevel system prompt optimization, whose objective is to
design system prompts that are robust to diverse user prompts and transferable
to unseen tasks. To tackle this problem, we then propose a meta-learning
framework, which meta-learns the system prompt by optimizing it over various
user prompts across multiple datasets, while simultaneously updating the user
prompts in an iterative manner to ensure synergy between them. We conduct
experiments on 14 unseen datasets spanning 5 different domains, on which we
show that our approach produces system prompts that generalize effectively to
diverse user prompts. Also, our findings reveal that the optimized system
prompt enables rapid adaptation even to unseen tasks, requiring fewer
optimization steps for test-time user prompts while achieving improved
performance.",2025-05-14,"Yumin Choi, Jinheon Baek, Sung Ju Hwang",http://arxiv.org/pdf/2505.09666v1,cs.CL
Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation,"This research offers a unique evaluation of how AI systems interpret the
digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first
cohort raised alongside AI, Gen Alpha faces new forms of online risk due to
immersive digital engagement and a growing mismatch between their evolving
communication and existing safety tools. Their distinct language, shaped by
gaming, memes, and AI-driven trends, often conceals harmful interactions from
both human moderators and automated systems. We assess four leading AI models
(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked
harassment and manipulation within Gen Alpha discourse. Using a dataset of 100
recent expressions from gaming platforms, social media, and video content, the
study reveals critical comprehension failures with direct implications for
online safety. This work contributes: (1) a first-of-its-kind dataset capturing
Gen Alpha expressions; (2) a framework to improve AI moderation systems for
youth protection; (3) a multi-perspective evaluation including AI systems,
human moderators, and parents, with direct input from Gen Alpha co-researchers;
and (4) an analysis of how linguistic divergence increases youth vulnerability.
Findings highlight the urgent need to redesign safety systems attuned to youth
communication, especially given Gen Alpha reluctance to seek help when adults
fail to understand their digital world. This study combines the insight of a
Gen Alpha researcher with systematic academic analysis to address critical
digital safety challenges.",2025-05-14,"Manisha Mehta, Fausto Giunchiglia",http://arxiv.org/pdf/2505.10588v1,cs.CL
Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports,"Timely and accurate situation awareness is vital for decision-making in
humanitarian response, conflict monitoring, and early warning and early action.
However, the manual analysis of vast and heterogeneous data sources often
results in delays, limiting the effectiveness of interventions. This paper
introduces a dynamic Retrieval-Augmented Generation (RAG) system that
autonomously generates situation awareness reports by integrating real-time
data from diverse sources, including news articles, conflict event databases,
and economic indicators. Our system constructs query-specific knowledge bases
on demand, ensuring timely, relevant, and accurate insights.
  To ensure the quality of generated reports, we propose a three-level
evaluation framework that combines semantic similarity metrics, factual
consistency checks, and expert feedback. The first level employs automated NLP
metrics to assess coherence and factual accuracy. The second level involves
human expert evaluation to verify the relevance and completeness of the
reports. The third level utilizes LLM-as-a-Judge, where large language models
provide an additional layer of assessment to ensure robustness. The system is
tested across multiple real-world scenarios, demonstrating its effectiveness in
producing coherent, insightful, and actionable reports. By automating report
generation, our approach reduces the burden on human analysts and accelerates
decision-making processes. To promote reproducibility and further research, we
openly share our code and evaluation tools with the community via GitHub.",2025-05-14,"Poli A. Nemkova, Suleyman O. Polat, Rafid I. Jahan, Sagnik Ray Choudhury, Sun-joo Lee, Shouryadipta Sarkar, Mark V. Albert",http://arxiv.org/pdf/2505.10586v1,cs.CL
Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling,"Wildfires have become increasingly frequent, irregular, and severe in recent
years. Understanding how affected populations perceive and respond during
wildfire crises is critical for timely and empathetic disaster response. Social
media platforms offer a crowd-sourced channel to capture evolving public
discourse, providing hyperlocal information and insight into public sentiment.
This study analyzes Reddit discourse during the 2025 Los Angeles wildfires,
spanning from the onset of the disaster to full containment. We collect 385
posts and 114,879 comments related to the Palisades and Eaton fires. We adopt
topic modeling methods to identify the latent topics, enhanced by large
language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we
develop a hierarchical framework to categorize latent topics, consisting of two
main categories, Situational Awareness (SA) and Crisis Narratives (CN). The
volume of SA category closely aligns with real-world fire progressions, peaking
within the first 2-5 days as the fires reach the maximum extent. The most
frequent co-occurring category set of public health and safety, loss and
damage, and emergency resources expands on a wide range of health-related
latent topics, including environmental health, occupational health, and one
health. Grief signals and mental health risks consistently accounted for 60
percentage and 40 percentage of CN instances, respectively, with the highest
total volume occurring at night. This study contributes the first annotated
social media dataset on the 2025 LA fires, and introduces a scalable
multi-layer framework that leverages topic modeling for crisis discourse
analysis. By identifying persistent public health concerns, our results can
inform more empathetic and adaptive strategies for disaster response, public
health communication, and future research in comparable climate-related
disaster events.",2025-05-14,"Sulong Zhou, Qunying Huang, Shaoheng Zhou, Yun Hang, Xinyue Ye, Aodong Mei, Kathryn Phung, Yuning Ye, Uma Govindswamy, Zehan Li",http://arxiv.org/pdf/2505.09665v2,cs.CL
PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning,"Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting
large language models, yet existing approaches exhibit counter-intuitive
phenomena: integrating router into prompt tuning (PT) increases training
efficiency yet does not improve performance universally; parameter reduction
through matrix decomposition can improve performance in specific domains.
Motivated by these observations and the modular nature of PT, we propose
PT-MoE, a novel framework that integrates matrix decomposition with
mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets
demonstrate that PT-MoE achieves state-of-the-art performance in both question
answering (QA) and mathematical problem solving tasks, improving F1 score by
1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing
mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all
while using 25% fewer parameters than LoRA. Our analysis reveals that while PT
methods generally excel in QA tasks and LoRA-based methods in math datasets,
the integration of matrix decomposition and MoE in PT-MoE yields complementary
benefits: decomposition enables efficient parameter sharing across experts
while MoE provides dynamic adaptation, collectively enabling PT-MoE to
demonstrate cross-task consistency and generalization abilities. These
findings, along with ablation studies on routing mechanisms and architectural
components, provide insights for future PEFT methods.",2025-05-14,"Zongqian Li, Yixuan Su, Nigel Collier",http://arxiv.org/pdf/2505.09519v1,cs.CL
LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis,"Cognitive diagnosis (CD) plays a crucial role in intelligent education,
evaluating students' comprehension of knowledge concepts based on their test
histories. However, current CD methods often model students, exercises, and
knowledge concepts solely on their ID relationships, neglecting the abundant
semantic relationships present within educational data space. Furthermore,
contemporary intelligent tutoring systems (ITS) frequently involve the addition
of new students and exercises, a situation that ID-based methods find
challenging to manage effectively. The advent of large language models (LLMs)
offers the potential for overcoming this challenge with open-world knowledge.
In this paper, we propose LLM4CD, which Leverages Large Language Models for
Open-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the
open-world knowledge of LLMs to construct cognitively expressive textual
representations, which are then encoded to introduce rich semantic information
into the CD task. Additionally, we propose an innovative bi-level encoder
framework that models students' test histories through two levels of encoders:
a macro-level cognitive text encoder and a micro-level knowledge state encoder.
This approach substitutes traditional ID embeddings with semantic
representations, enabling the model to accommodate new students and exercises
with open-world knowledge and address the cold-start problem. Extensive
experimental results demonstrate that our proposed method consistently
outperforms previous CD models on multiple real-world datasets, validating the
effectiveness of leveraging LLMs to introduce rich semantic information into
the CD task.",2025-05-14,"Weiming Zhang, Lingyue Fu, Qingyao Li, Kounianhua Du, Jianghao Lin, Jingwei Yu, Wei Xia, Weinan Zhang, Ruiming Tang, Yong Yu",http://arxiv.org/pdf/2505.13492v1,cs.CL
CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios,"Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.",2025-05-14,"Raghav Garg, Kapil Sharma, Karan Gupta",http://arxiv.org/pdf/2505.09436v2,cs.CL
Large Language Models Are More Persuasive Than Incentivized Human Persuaders,"We directly compare the persuasion capabilities of a frontier large language
model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an
interactive, real-time conversational quiz setting. In this preregistered,
large-scale incentivized experiment, participants (quiz takers) completed an
online quiz where persuaders (either humans or LLMs) attempted to persuade quiz
takers toward correct or incorrect answers. We find that LLM persuaders
achieved significantly higher compliance with their directional persuasion
attempts than incentivized human persuaders, demonstrating superior persuasive
capabilities in both truthful (toward correct answers) and deceptive (toward
incorrect answers) contexts. We also find that LLM persuaders significantly
increased quiz takers' accuracy, leading to higher earnings, when steering quiz
takers toward correct answers, and significantly decreased their accuracy,
leading to lower earnings, when steering them toward incorrect answers.
Overall, our findings suggest that AI's persuasion capabilities already exceed
those of humans that have real-money bonuses tied to performance. Our findings
of increasingly capable AI persuaders thus underscore the urgency of emerging
alignment and governance frameworks.",2025-05-14,"Philipp Schoenegger, Francesco Salvi, Jiacheng Liu, Xiaoli Nan, Ramit Debnath, Barbara Fasolo, Evelina Leivada, Gabriel Recchia, Fritz Günther, Ali Zarifhonarvar, Joe Kwon, Zahoor Ul Islam, Marco Dehnert, Daryl Y. H. Lee, Madeline G. Reinecke, David G. Kamper, Mert Kobaş, Adam Sandford, Jonas Kgomo, Luke Hewitt, Shreya Kapoor, Kerem Oktar, Eyup Engin Kucuk, Bo Feng, Cameron R. Jones, Izzy Gainsburg, Sebastian Olschewski, Nora Heinzelmann, Francisco Cruz, Ben M. Tappin, Tao Ma, Peter S. Park, Rayan Onyonka, Arthur Hjorth, Peter Slattery, Qingcheng Zeng, Lennart Finke, Igor Grossmann, Alessandro Salatiello, Ezra Karger",http://arxiv.org/pdf/2505.09662v2,cs.CL
Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits,"Cloud-based multilingual translation services like Google Translate and
Microsoft Translator achieve state-of-the-art translation capabilities. These
services inherently use large multilingual language models such as GRU, LSTM,
BERT, GPT, T5, or similar encoder-decoder architectures with attention
mechanisms as the backbone. Also, new age natural language systems, for
instance ChatGPT and DeepSeek, have established huge potential in multiple
tasks in natural language processing. At the same time, they also possess
outstanding multilingual translation capabilities. However, these models use
the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder
Attention-based Convolutional Variational Circuits) is an alternate solution
that explores the quantum computing realm instead of the classical computing
realm to study and demonstrate multilingual machine translation. QEDACVC
introduces the quantum encoder-decoder architecture that simulates and runs on
quantum computing hardware via quantum convolution, quantum pooling, quantum
variational circuit, and quantum attention as software alterations. QEDACVC
achieves an Accuracy of 82% when trained on the OPUS dataset for English,
French, German, and Hindi corpora for multilingual translations.",2025-05-14,"Subrit Dikshit, Ritu Tiwari, Priyank Jain",http://arxiv.org/pdf/2505.09407v1,cs.CL
Qwen3 Technical Report,"In this work, we present Qwen3, the latest version of the Qwen model family.
Qwen3 comprises a series of large language models (LLMs) designed to advance
performance, efficiency, and multilingual capabilities. The Qwen3 series
includes models of both dense and Mixture-of-Expert (MoE) architectures, with
parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is
the integration of thinking mode (for complex, multi-step reasoning) and
non-thinking mode (for rapid, context-driven responses) into a unified
framework. This eliminates the need to switch between different models--such as
chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,
QwQ-32B)--and enables dynamic mode switching based on user queries or chat
templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing
users to allocate computational resources adaptively during inference, thereby
balancing latency and performance based on task complexity. Moreover, by
leveraging the knowledge from the flagship models, we significantly reduce the
computational resources required to build smaller-scale models, while ensuring
their highly competitive performance. Empirical evaluations demonstrate that
Qwen3 achieves state-of-the-art results across diverse benchmarks, including
tasks in code generation, mathematical reasoning, agent tasks, etc.,
competitive against larger MoE models and proprietary models. Compared to its
predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119
languages and dialects, enhancing global accessibility through improved
cross-lingual understanding and generation capabilities. To facilitate
reproducibility and community-driven research and development, all Qwen3 models
are publicly accessible under Apache 2.0.",2025-05-14,"An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu",http://arxiv.org/pdf/2505.09388v1,cs.CL
ProdRev: A DNN framework for empowering customers using generative pre-trained transformers,"Following the pandemic, customers, preference for using e-commerce has
accelerated. Since much information is available in multiple reviews (sometimes
running in thousands) for a single product, it can create decision paralysis
for the buyer. This scenario disempowers the consumer, who cannot be expected
to go over so many reviews since its time consuming and can confuse them.
Various commercial tools are available, that use a scoring mechanism to arrive
at an adjusted score. It can alert the user to potential review manipulations.
This paper proposes a framework that fine-tunes a generative pre-trained
transformer to understand these reviews better. Furthermore, using
""common-sense"" to make better decisions. These models have more than 13 billion
parameters. To fine-tune the model for our requirement, we use the curie engine
from generative pre-trained transformer (GPT3). By using generative models, we
are introducing abstractive summarization. Instead of using a simple extractive
method of summarizing the reviews. This brings out the true relationship
between the reviews and not simply copy-paste. This introduces an element of
""common sense"" for the user and helps them to quickly make the right decisions.
The user is provided the pros and cons of the processed reviews. Thus the
user/customer can take their own decisions.",2025-05-14,"Aakash Gupta, Nataraj Das",http://arxiv.org/pdf/2505.13491v1,cs.CL
"Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs","We observe a novel phenomenon, contextual entrainment, across a wide range of
language models (LMs) and prompt settings, providing a new mechanistic
perspective on how LMs become distracted by ``irrelevant'' contextual
information in the input prompt. Specifically, LMs assign significantly higher
logits (or probabilities) to any tokens that have previously appeared in the
context prompt, even for random tokens. This suggests that contextual
entrainment is a mechanistic phenomenon, occurring independently of the
relevance or semantic relation of the tokens to the question or the rest of the
sentence. We find statistically significant evidence that the magnitude of
contextual entrainment is influenced by semantic factors. Counterfactual
prompts have a greater effect compared to factual ones, suggesting that while
contextual entrainment is a mechanistic phenomenon, it is modulated by semantic
factors.
  We hypothesise that there is a circuit of attention heads -- the entrainment
heads -- that corresponds to the contextual entrainment phenomenon. Using a
novel entrainment head discovery method based on differentiable masking, we
identify these heads across various settings. When we ``turn off'' these heads,
i.e., set their outputs to zero, the effect of contextual entrainment is
significantly attenuated, causing the model to generate output that capitulates
to what it would produce if no distracting context were provided. Our discovery
of contextual entrainment, along with our investigation into LM distraction via
the entrainment heads, marks a key step towards the mechanistic analysis and
mitigation of the distraction problem.",2025-05-14,"Jingcheng Niu, Xingdi Yuan, Tong Wang, Hamidreza Saghir, Amir H. Abdi",http://arxiv.org/pdf/2505.09338v1,cs.CL
Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging,"Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.",2025-05-14,"Hongjin Qian, Zheng Liu",http://arxiv.org/pdf/2505.09316v1,cs.CL
A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data,"Effectively analyzing online review data is essential across industries.
However, many existing studies are limited to specific domains and languages or
depend on supervised learning approaches that require large-scale labeled
datasets. To address these limitations, we propose a multilingual, scalable,
and unsupervised framework for cross-domain aspect detection. This framework is
designed for multi-aspect labeling of multilingual and multi-domain review
data. In this study, we apply automatic labeling to Korean and English review
datasets spanning various domains and assess the quality of the generated
labels through extensive experiments. Aspect category candidates are first
extracted through clustering, and each review is then represented as an
aspect-aware embedding vector using negative sampling. To evaluate the
framework, we conduct multi-aspect labeling and fine-tune several pretrained
language models to measure the effectiveness of the automatically generated
labels. Results show that these models achieve high performance, demonstrating
that the labels are suitable for training. Furthermore, comparisons with
publicly available large language models highlight the framework's superior
consistency and scalability when processing large-scale data. A human
evaluation also confirms that the quality of the automatic labels is comparable
to those created manually. This study demonstrates the potential of a robust
multi-aspect labeling approach that overcomes limitations of supervised methods
and is adaptable to multilingual, multi-domain environments. Future research
will explore automatic review summarization and the integration of artificial
intelligence agents to further improve the efficiency and depth of review
analysis.",2025-05-14,"Jiin Park, Misuk Kim",http://arxiv.org/pdf/2505.09286v1,cs.CL
How an unintended Side Effect of a Research Project led to Boosting the Power of UML,"This paper describes the design, implementation and use of a new UML modeling
tool that represents a significant advance over conventional tools. Among other
things, it allows the integration of class diagrams and object diagrams as well
as the execution of objects. This not only enables new software architectures
characterized by the integration of software with corresponding object models,
but is also ideal for use in teaching, as it provides students with a
particularly stimulating learning experience. A special feature of the project
is that it has emerged from a long-standing international research project,
which is aimed at a comprehensive multi-level architecture. The project is
therefore an example of how research can lead to valuable results that arise as
a side effect of other work.",2025-05-14,"Ulrich Frank, Pierre Maier",http://arxiv.org/pdf/2505.09269v1,cs.CL
Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer,"Knowledge tracing (KT) aims to predict learners' future performance based on
historical learning interactions. However, existing KT models predominantly
focus on data from a single course, limiting their ability to capture a
comprehensive understanding of learners' knowledge states. In this paper, we
propose TransKT, a contrastive cross-course knowledge tracing method that
leverages concept graph guided knowledge transfer to model the relationships
between learning behaviors across different courses, thereby enhancing
knowledge state estimation. Specifically, TransKT constructs a cross-course
concept graph by leveraging zero-shot Large Language Model (LLM) prompts to
establish implicit links between related concepts across different courses.
This graph serves as the foundation for knowledge transfer, enabling the model
to integrate and enhance the semantic features of learners' interactions across
courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating
summarized semantic features, which significantly improves the performance of
Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,
TransKT employs a contrastive objective that aligns single-course and
cross-course knowledge states, thereby refining the model's ability to provide
a more robust and accurate representation of learners' overall knowledge
states.",2025-05-14,"Wenkang Han, Wang Lin, Liya Hu, Zhenlong Dai, Yiyun Zhou, Mengze Li, Zemin Liu, Chang Yao, Jingyuan Chen",http://arxiv.org/pdf/2505.13489v1,cs.CL
Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models,"Large language models have become multimodal, and many of them are said to
integrate their modalities using common representations. If this were true, a
drawing of a car as an image, for instance, should map to the similar area in
the latent space as a textual description of the strokes that conform the
drawing. To explore this in a black-box access regime to these models, we
propose the use of machine teaching, a theory that studies the minimal set of
examples a teacher needs to choose so that the learner captures the concept. In
this paper we evaluate the complexity of teaching visual-language models a
subset of objects in the Quick, Draw! dataset using two presentations: raw
images as bitmaps and trace coordinates in TikZ format. The results indicate
that image-based representations generally require fewer segments and achieve
higher accuracy than coordinate-based representations. But, surprisingly, the
teaching size usually ranks concepts similarly across both modalities, even
when controlling for (a human proxy of) concept priors, suggesting that the
simplicity of concepts may be an inherent property that transcends modality
representations.",2025-05-14,"Diogo Freitas, Brigt Håvardstun, Cèsar Ferri, Darío Garigliotti, Jan Arne Telle, José Hernández-Orallo",http://arxiv.org/pdf/2505.10583v1,cs.CL
"Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases","In many real-world settings, machine learning models and interactive systems
have access to both structured knowledge, e.g., knowledge graphs or tables, and
unstructured content, e.g., natural language documents. However, most rely on
either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking
unstructured content to nodes within structured data, thereby enabling new
strategies for knowledge access and use. In this work, we present
FocusedRetriever, a modular SKB-based framework for multi-hop question
answering. It integrates components (VSS-based entity search, LLM-based
generation of Cypher queries and pairwise re-ranking) in a way that enables it
to outperform state-of-the-art methods across all three STaRK benchmark test
sets, covering diverse domains and multiple performance metrics. The average
first-hit rate exceeds that of the second-best method by 25.7%.
FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to
extract relational facts and entity attributes from unstructured text, (2) node
set joins to filter answer candidates based on these extracted triplets and
constraints, (3) vector similarity search to retrieve and rank relevant
unstructured content, and (4) the contextual capabilities of LLMs to finally
rank the top-k answers. For generality, we only incorporate base LLMs in
FocusedRetriever in our evaluation. However, our analysis of intermediate
results highlights several opportunities for further upgrades including
finetuning. The source code is publicly available at
https://github.com/kramerlab/FocusedRetriever .",2025-05-14,"Derian Boer, Stephen Roth, Stefan Kramer",http://arxiv.org/pdf/2505.09246v1,cs.CL
Source framing triggers systematic evaluation bias in Large Language Models,"Large Language Models (LLMs) are increasingly used not only to generate text
but also to evaluate it, raising urgent questions about whether their judgments
are consistent, unbiased, and robust to framing effects. In this study, we
systematically examine inter- and intra-model agreement across four
state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and
Mistral) tasked with evaluating 4,800 narrative statements on 24 different
topics of social, political, and public health relevance, for a total of
192,000 assessments. We manipulate the disclosed source of each statement to
assess how attribution to either another LLM or a human author of specified
nationality affects evaluation outcomes. We find that, in the blind condition,
different LLMs display a remarkably high degree of inter- and intra-model
agreement across topics. However, this alignment breaks down when source
framing is introduced. Here we show that attributing statements to Chinese
individuals systematically lowers agreement scores across all models, and in
particular for Deepseek Reasoner. Our findings reveal that framing effects can
deeply affect text evaluation, with significant implications for the integrity,
neutrality, and fairness of LLM-mediated information systems.",2025-05-14,"Federico Germani, Giovanni Spitale",http://arxiv.org/pdf/2505.13488v1,cs.CL
LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models,"Spiking Large Language Models (LLMs) have emerged as an energy-efficient
alternative to conventional LLMs through their event-driven computation. To
effectively obtain spiking LLMs, researchers develop different ANN-to-SNN
conversion methods by leveraging pre-trained ANN parameters while inheriting
the energy efficiency of SNN. However, existing conversion methods struggle
with extreme activation outliers and incompatible nonlinear operations of
ANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for
fully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel
neurons to convert the activation outlier and nonlinear operation of ANN-based
LLMs. Moreover, LAS tailors the spike-equivalent Transformer components for
spiking LLMs, which can ensure full spiking conversion without any loss of
performance. Experimental results on six language models and two
vision-language models demonstrate that LAS achieves loss-less conversion.
Notably, on OPT-66B, LAS even improves the accuracy of 2\% on the WSC task. In
addition, the parameter and ablation studies further verify the effectiveness
of LAS. The source code is available at https://github.com/lc783/LAS",2025-05-14,"Long Chen, Xiaotian Song, Yanan Sun",http://arxiv.org/pdf/2505.09659v1,cs.CL
"Ornithologist: Towards Trustworthy ""Reasoning"" about Central Bank Communications","I develop Ornithologist, a weakly-supervised textual classification system
and measure the hawkishness and dovishness of central bank text. Ornithologist
uses ``taxonomy-guided reasoning'', guiding a large language model with
human-authored decision trees. This increases the transparency and
explainability of the system and makes it accessible to non-experts. It also
reduces hallucination risk. Since it requires less supervision than traditional
classification systems, it can more easily be applied to other problems or
sources of text (e.g. news) without much modification. Ornithologist
measurements of hawkishness and dovishness of RBA communication carry
information about the future of the cash rate path and of market expectations.",2025-05-14,Dominic Zaun Eu Jones,http://arxiv.org/pdf/2505.09083v1,cs.CL
A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism,"In the tourism domain, Large Language Models (LLMs) often struggle to mine
implicit user intentions from tourists' ambiguous inquiries and lack the
capacity to proactively guide users toward clarifying their needs. A critical
bottleneck is the scarcity of high-quality training datasets that facilitate
proactive questioning and implicit intention mining. While recent advances
leverage LLM-driven data synthesis to generate such datasets and transfer
specialized knowledge to downstream models, existing approaches suffer from
several shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed
distributions of detail levels in initial inquiries, (3) contextual redundancy
in the implicit intention mining module, and (4) lack of explicit thinking
about tourists' emotions and intention values. Therefore, we propose SynPT (A
Data Synthesis Method Driven by LLMs for Proactive Mining of Implicit User
Intentions in the Tourism), which constructs an LLM-driven user agent and
assistant agent to simulate dialogues based on seed data collected from Chinese
tourism websites. This approach addresses the aforementioned limitations and
generates SynPT-Dialog, a training dataset containing explicit reasoning. The
dataset is utilized to fine-tune a general LLM, enabling it to proactively mine
implicit user intentions. Experimental evaluations, conducted from both human
and LLM perspectives, demonstrate the superiority of SynPT compared to existing
methods. Furthermore, we analyze key hyperparameters and present case studies
to illustrate the practical applicability of our method, including discussions
on its adaptability to English-language scenarios. All code and data are
publicly available.",2025-05-14,"Jinqiang Wang, Huansheng Ning, Tao Zhu, Jianguo Ding",http://arxiv.org/pdf/2505.11533v1,cs.CL
CEC-Zero: Chinese Error Correction Solution Based on LLM,"Recent advancements in large language models (LLMs) demonstrate exceptional
Chinese text processing capabilities, particularly in Chinese Spelling
Correction (CSC). While LLMs outperform traditional BERT-based models in
accuracy and robustness, challenges persist in reliability and generalization.
This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework
enabling LLMs to self-correct through autonomous error strategy learning
without external supervision. By integrating RL with LLMs' generative power,
the method eliminates dependency on annotated data or auxiliary models.
Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and
superior cross-domain generalization, offering a scalable solution for
reliability optimization in Chinese NLP applications. This breakthrough
facilitates LLM deployment in practical Chinese text correction scenarios while
establishing a new paradigm for self-improving language models.",2025-05-14,"Sophie Zhang, Zhiming Lin",http://arxiv.org/pdf/2505.09082v1,cs.CL
"S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment","This paper introduces S-DAT (Synthetic-Divergent Association Task), a
scalable, multilingual framework for automated assessment of divergent thinking
(DT) -a core component of human creativity. Traditional creativity assessments
are often labor-intensive, language-specific, and reliant on subjective human
ratings, limiting their scalability and cross-cultural applicability. In
contrast, S-DAT leverages large language models and advanced multilingual
embeddings to compute semantic distance -- a language-agnostic proxy for DT. We
evaluate S-DAT across eleven diverse languages, including English, Spanish,
German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating
robust and consistent scoring across linguistic contexts. Unlike prior DAT
approaches, the S-DAT shows convergent validity with other DT measures and
correct discriminant validity with convergent thinking. This cross-linguistic
flexibility allows for more inclusive, global-scale creativity research,
addressing key limitations of earlier approaches. S-DAT provides a powerful
tool for fairer, more comprehensive evaluation of cognitive flexibility in
diverse populations and can be freely assessed online:
https://sdat.iol.zib.de/.",2025-05-14,"Jennifer Haase, Paul H. P. Hanel, Sebastian Pokutta",http://arxiv.org/pdf/2505.09068v1,cs.CL
DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models,"Recent advances in reinforcement learning for language model post-training,
such as Group Relative Policy Optimization (GRPO), have shown promise in
low-resource settings. However, GRPO typically relies on solution-level and
scalar reward signals that fail to capture the semantic diversity among sampled
completions. This leads to what we identify as a diversity-quality
inconsistency, where distinct reasoning paths may receive indistinguishable
rewards. To address this limitation, we propose $\textit{Diversity-aware Reward
Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity
into the reward computation. DRA uses Submodular Mutual Information (SMI) to
downweight redundant completions and amplify rewards for diverse ones. This
encourages better exploration during learning, while maintaining stable
exploitation of high-quality samples. Our method integrates seamlessly with
both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and
$\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning
benchmarks and find that it outperforms recent strong baselines. It achieves
state-of-the-art performance with an average accuracy of 58.2%, using only
7,000 fine-tuning samples and a total training cost of approximately $55. The
code is available at https://github.com/xiwenc1/DRA-GRPO.",2025-05-14,"Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi",http://arxiv.org/pdf/2505.09655v2,cs.CL
"A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias","Large Language Models (LLMs) represent a major step toward artificial general
intelligence, significantly advancing our ability to interact with technology.
While LLMs perform well on Natural Language Processing tasks -- such as
translation, generation, code writing, and summarization -- questions remain
about their output similarity, variability, and ethical implications. For
instance, how similar are texts generated by the same model? How does this
compare across different models? And which models best uphold ethical
standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like
generation, explanation, and rewriting. This resulted in approximately 3
million texts from 12 LLMs, including proprietary and open-source systems from
OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs
from the same LLM are more similar to each other than to human-written texts;
(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4
produces more varied responses; (3) LLM writing styles differ significantly,
with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for
distinctiveness; (4) differences in vocabulary and tone underscore the
linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate
greater gender balance and reduced bias. These results offer new insights into
the behavior and diversity of LLM outputs, helping guide future development and
ethical evaluation.",2025-05-14,"Brandon Smith, Mohamed Reda Bouadjenek, Tahsin Alamgir Kheya, Phillip Dawson, Sunil Aryal",http://arxiv.org/pdf/2505.09056v1,cs.CL
Atomic Consistency Preference Optimization for Long-Form Question Answering,"Large Language Models (LLMs) frequently produce factoid hallucinations -
plausible yet incorrect answers. A common mitigation strategy is model
alignment, which improves factual accuracy by training on curated factual and
non-factual pairs. However, this approach often relies on a stronger model
(e.g., GPT-4) or an external knowledge base to assess factual correctness,
which may not always be accessible. To address this, we propose Atomic
Consistency Preference Optimization (ACPO), a self-supervised preference-tuning
method that enhances factual accuracy without external supervision. ACPO
leverages atomic consistency signals, i.e., the agreement of individual facts
across multiple stochastic responses, to identify high- and low-quality data
pairs for model alignment. By eliminating the need for costly GPT calls, ACPO
provides a scalable and efficient approach to improving factoid
question-answering. Despite being self-supervised, empirical results
demonstrate that ACPO outperforms FactAlign, a strong supervised alignment
baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its
effectiveness in enhancing factual reliability without relying on external
models or knowledge bases.",2025-05-14,"Jingfeng Chen, Raghuveer Thirukovalluru, Junlin Wang, Kaiwei Luo, Bhuwan Dhingra",http://arxiv.org/pdf/2505.09039v1,cs.CL
"Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification","Hallucination, where large language models (LLMs) generate confident but
incorrect or irrelevant information, remains a key limitation in their
application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has
emerged as a promising method for improving multistep reasoning by guiding
models through intermediate steps. However, CoT alone does not fully address
the hallucination problem. In this work, we investigate how combining CoT with
retrieval-augmented generation (RAG), as well as applying self-consistency and
self-verification strategies, can reduce hallucinations and improve factual
accuracy. By incorporating external knowledge sources during reasoning and
enabling models to verify or revise their own outputs, we aim to generate more
accurate and coherent responses. We present a comparative evaluation of
baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification
techniques. Our results highlight the effectiveness of each method and identify
the most robust approach for minimizing hallucinations while preserving fluency
and reasoning depth.",2025-05-13,"Adarsh Kumar, Hwiyoon Kim, Jawahar Sai Nathani, Neil Roy",http://arxiv.org/pdf/2505.09031v1,cs.CL
Automated Meta Prompt Engineering for Alignment with the Theory of Mind,"We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.",2025-05-13,"Aaron Baughman, Rahul Agarwal, Eduardo Morales, Gozde Akay",http://arxiv.org/pdf/2505.09024v1,cs.CL
For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies,"It remains debated how well any LM understands natural language or generates
reliable metalinguistic judgments. Moreover, relatively little work has
demonstrated that LMs can represent and respect subtle relationships between
form and function proposed by linguists. We here focus on a particular such
relationship established in recent work: English speakers' judgments about the
information structure of canonical sentences predicts independently collected
acceptability ratings on corresponding 'long distance dependency' [LDD]
constructions, across a wide array of base constructions and multiple types of
LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on
the same tasks used with humans and new extensions.Results reveal reliable
metalinguistic skill on the information structure and acceptability tasks,
replicating a striking interaction between the two, despite the zero-shot,
explicit nature of the tasks, and little to no chance of contamination [Studies
1a, 1b]. Study 2 manipulates the information structure of base sentences and
confirms a causal relationship: increasing the prominence of a constituent in a
context sentence increases the subsequent acceptability ratings on an LDD
construction. The findings suggest a tight relationship between natural and
GPT-4 generated English, and between information structure and syntax, which
begs for further exploration.",2025-05-13,"Nicole Cuneo, Eleanor Graves, Supantho Rakshit, Adele E. Goldberg",http://arxiv.org/pdf/2505.09005v1,cs.CL
A suite of LMs comprehend puzzle statements as well as humans,"Recent claims suggest that large language models (LMs) underperform humans in
comprehending minimally complex English statements (Dentella et al., 2024).
Here, we revisit those findings and argue that human performance was
overestimated, while LLM abilities were underestimated. Using the same stimuli,
we report a preregistered study comparing human responses in two conditions:
one allowed rereading (replicating the original study), and one that restricted
rereading (a more naturalistic comprehension test). Human accuracy dropped
significantly when rereading was restricted (73%), falling below that of
Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect
accuracy. Results further show that both humans and models are
disproportionately challenged by queries involving potentially reciprocal
actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than
model-specific deficits. Additional analyses using Llama-2-70B log
probabilities, a recoding of open-ended model responses, and grammaticality
ratings of other sentences reveal systematic underestimation of model
performance. We find that GPT-4o can align with either naive or expert
grammaticality judgments, depending on prompt framing. These findings
underscore the need for more careful experimental design and coding practices
in LLM evaluation, and they challenge the assumption that current models are
inherently weaker than humans at language comprehension.",2025-05-13,"Adele E Goldberg, Supantho Rakshit, Jennifer Hu, Kyle Mahowald",http://arxiv.org/pdf/2505.08996v1,cs.CL
Detecting Prefix Bias in LLM-based Reward Models,"Reinforcement Learning with Human Feedback (RLHF) has emerged as a key
paradigm for task-specific fine-tuning of language models using human
preference data. While numerous publicly available preference datasets provide
pairwise comparisons of responses, the potential for biases in the resulting
reward models remains underexplored. In this work, we introduce novel methods
to detect and evaluate prefix bias -- a systematic shift in model preferences
triggered by minor variations in query prefixes -- in LLM-based reward models
trained on such datasets. We leverage these metrics to reveal significant
biases in preference models across racial and gender dimensions. Our
comprehensive evaluation spans diverse open-source preference datasets and
reward model architectures, demonstrating susceptibility to this kind of bias
regardless of the underlying model architecture. Furthermore, we propose a data
augmentation strategy to mitigate these biases, showing its effectiveness in
reducing the impact of prefix bias. Our findings highlight the critical need
for bias-aware dataset design and evaluation in developing fair and reliable
reward models, contributing to the broader discourse on fairness in AI.",2025-05-13,"Ashwin Kumar, Yuzi He, Aram H. Markosyan, Bobbie Chern, Imanol Arrieta-Ibarra",http://arxiv.org/pdf/2505.13487v1,cs.CL
Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training,"In standard large vision-language models (LVLMs) pre-training, the model
typically maximizes the joint probability of the caption conditioned on the
image via next-token prediction (NTP); however, since only a small subset of
caption tokens directly relates to the visual content, this naive NTP
unintentionally fits the model to noise and increases the risk of
hallucination. We present PRIOR, a simple vision-language pre-training approach
that addresses this issue by prioritizing image-related tokens through
differential weighting in the NTP loss, drawing from the importance sampling
framework. PRIOR introduces a reference model-a text-only large language model
(LLM) trained on the captions without image inputs, to weight each token based
on its probability for LVLMs training. Intuitively, tokens that are directly
related to the visual inputs are harder to predict without the image and thus
receive lower probabilities from the text-only reference LLM. During training,
we implement a token-specific re-weighting term based on the importance scores
to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs
with visual encoders and LVLMs without visual encoders. We observe 19% and 8%
average relative improvement, respectively, on several vision-language
benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling
properties, as demonstrated by significantly higher scaling coefficients,
indicating greater potential for performance gains compared to NTP given
increasing compute and data.",2025-05-13,"Yangyi Chen, Hao Peng, Tong Zhang, Heng Ji",http://arxiv.org/pdf/2505.08971v1,cs.CL
ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers,"Predicting the future citation rates of academic papers is an important step
toward the automation of research evaluation and the acceleration of scientific
progress. We present $\textbf{ForeCite}$, a simple but powerful framework to
append pre-trained causal language models with a linear head for average
monthly citation rate prediction. Adapting transformers for regression tasks,
ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of
900K+ biomedical papers published between 2000 and 2024, a 27-point improvement
over the previous state-of-the-art. Comprehensive scaling-law analysis reveals
consistent gains across model sizes and data volumes, while temporal holdout
experiments confirm practical robustness. Gradient-based saliency heatmaps
suggest a potentially undue reliance on titles and abstract texts. These
results establish a new state-of-the-art in forecasting the long-term influence
of academic research and lay the groundwork for the automated, high-fidelity
evaluation of scientific contributions.",2025-05-13,"Gavin Hull, Alex Bihlo",http://arxiv.org/pdf/2505.08941v1,cs.CL
Behind Maya: Building a Multilingual Vision Language Model,"In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.",2025-05-13,"Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung, Bala Krishna S Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, S M Iftekhar Uddin, Shayekh Bin Islam, Roshan Santhosh, Snegha A, Drishti Sharma, Chen Liu, Isha Chaturvedi, Genta Indra Winata, Ashvanth. S, Snehanshu Mukherjee, Alham Fikri Aji",http://arxiv.org/pdf/2505.08910v2,cs.CL
Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora,"Language Models (LMs) continue to advance, improving response quality and
coherence. Given Internet-scale training datasets, LMs have likely encountered
much of what users may ask them to generate in some form during their training.
A plethora of evaluation benchmarks have been constructed to assess model
quality, response appropriateness, and reasoning capabilities. However, the
human effort required for benchmark construction is rapidly being outpaced by
the size and scope of the models under evaluation. Having humans build a
benchmark for every possible domain of interest is impractical. Therefore, we
propose a methodology for automating the construction of fact-based synthetic
data model evaluations grounded in document populations. This work leverages
the same LMs to evaluate domain-specific knowledge automatically, using only
grounding documents (e.g., a textbook) as input. This synthetic data
benchmarking approach corresponds well with human curated questions producing a
Spearman ranking correlation of 0.97 and a benchmark evaluation Pearson
accuracy correlation of 0.75. This novel approach supports generating both
multiple choice and open-ended synthetic data questions to gain diagnostic
insight of LM capability. We apply this methodology to evaluate model
performance on two recent arXiv preprints, discovering a surprisingly strong
performance from Gemma-3 models on open-ended questions. Code is available at
https://github.com/mmajurski/grounded-synth-lm-benchmark",2025-05-13,"Michael Majurski, Cynthia Matuszek",http://arxiv.org/pdf/2505.08905v2,cs.CL
Performance Gains of LLMs With Humans in a World of LLMs Versus Humans,"Currently, a considerable research effort is devoted to comparing LLMs to a
group of human experts, where the term ""expert"" is often ill-defined or
variable, at best, in a state of constantly updating LLM releases. Without
proper safeguards in place, LLMs will threaten to cause harm to the established
structure of safe delivery of patient care which has been carefully developed
throughout history to keep the safety of the patient at the forefront. A key
driver of LLM innovation is founded on community research efforts which, if
continuing to operate under ""humans versus LLMs"" principles, will expedite this
trend. Therefore, research efforts moving forward must focus on effectively
characterizing the safe use of LLMs in clinical settings that persist across
the rapid development of novel LLM models. In this communication, we
demonstrate that rather than comparing LLMs to humans, there is a need to
develop strategies enabling efficient work of humans with LLMs in an almost
symbiotic manner.",2025-05-13,"Lucas McCullum, Pelagie Ami Agassi, Leo Anthony Celi, Daniel K. Ebner, Chrystinne Oliveira Fernandes, Rachel S. Hicklen, Mkliwa Koumbia, Lisa Soleymani Lehmann, David Restrepo",http://arxiv.org/pdf/2505.08902v1,cs.CL
Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives,"Motivation is an important factor underlying successful learning. Previous
research has demonstrated the positive effects that static interactive
narrative games can have on motivation. Concurrently, advances in AI have made
dynamic and adaptive approaches to interactive narrative increasingly
accessible. However, limited work has explored the impact that dynamic
narratives can have on learner motivation. In this paper, we compare two
versions of Academical, a choice-based educational interactive narrative game
about research ethics. One version employs a traditional hand-authored
branching plot (i.e., static narrative) while the other dynamically sequences
plots during play (i.e., dynamic narrative). Results highlight the importance
of responsive content and a variety of choices for player engagement, while
also illustrating the challenge of balancing pedagogical goals with the dynamic
aspects of narrative. We also discuss design implications that arise from these
findings. Ultimately, this work provides initial steps to illuminate the
emerging potential of AI-driven dynamic narrative in educational games.",2025-05-13,"Daeun Hwang, Samuel Shields, Alex Calderwood, Shi Johnson-Bey, Michael Mateas, Noah Wardrip-Fruin, Edward F. Melcer",http://arxiv.org/pdf/2505.08891v1,cs.CL
CodePDE: An Inference Framework for LLM-driven PDE Solver Generation,"Partial differential equations (PDEs) are fundamental to modeling physical
systems, yet solving them remains a complex challenge. Traditional numerical
solvers rely on expert knowledge to implement and are computationally
expensive, while neural-network-based solvers require large training datasets
and often lack interpretability. In this work, we frame PDE solving as a code
generation task and introduce CodePDE, the first inference framework for
generating PDE solvers using large language models (LLMs). Leveraging advanced
inference-time algorithms and scaling strategies, CodePDE unlocks critical
capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and
test-time scaling -- all without task-specific tuning. CodePDE achieves
superhuman performance across a range of representative PDE problems. We also
present a systematic empirical analysis of LLM generated solvers, analyzing
their accuracy, efficiency, and numerical scheme choices. Our findings
highlight the promise and the current limitations of LLMs in PDE solving,
offering a new perspective on solver design and opportunities for future model
development. Our code is available at https://github.com/LithiumDA/CodePDE.",2025-05-13,"Shanda Li, Tanya Marwah, Junhong Shen, Weiwei Sun, Andrej Risteski, Yiming Yang, Ameet Talwalkar",http://arxiv.org/pdf/2505.08783v1,cs.CL
HealthBench: Evaluating Large Language Models Towards Improved Human Health,"We present HealthBench, an open-source benchmark measuring the performance
and safety of large language models in healthcare. HealthBench consists of
5,000 multi-turn conversations between a model and an individual user or
healthcare professional. Responses are evaluated using conversation-specific
rubrics created by 262 physicians. Unlike previous multiple-choice or
short-answer benchmarks, HealthBench enables realistic, open-ended evaluation
through 48,562 unique rubric criteria spanning several health contexts (e.g.,
emergencies, transforming clinical data, global health) and behavioral
dimensions (e.g., accuracy, instruction following, communication). HealthBench
performance over the last two years reflects steady initial progress (compare
GPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3
scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms
GPT-4o and is 25 times cheaper. We additionally release two HealthBench
variations: HealthBench Consensus, which includes 34 particularly important
dimensions of model behavior validated via physician consensus, and HealthBench
Hard, where the current top score is 32%. We hope that HealthBench grounds
progress towards model development and applications that benefit human health.",2025-05-13,"Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, Karan Singhal",http://arxiv.org/pdf/2505.08775v1,cs.CL
Aya Vision: Advancing the Frontier of Multilingual Multimodality,"Building multimodal language models is fundamentally challenging: it requires
aligning vision and language modalities, curating high-quality instruction
data, and avoiding the degradation of existing text-only capabilities once
vision is introduced. These difficulties are further magnified in the
multilingual setting, where the need for multimodal data in different languages
exacerbates existing data scarcity, machine translation often distorts meaning,
and catastrophic forgetting is more pronounced. To address the aforementioned
challenges, we introduce novel techniques spanning both data and modeling.
First, we develop a synthetic annotation framework that curates high-quality,
diverse multilingual multimodal instruction data, enabling Aya Vision models to
produce natural, human-preferred responses to multimodal inputs across many
languages. Complementing this, we propose a cross-modal model merging technique
that mitigates catastrophic forgetting, effectively preserving text-only
capabilities while simultaneously enhancing multimodal generative performance.
Aya-Vision-8B achieves best-in-class performance compared to strong multimodal
models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger
Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which
outperforms models more than twice its size, such as Molmo-72B and
LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the
multi-modal frontier, and provides insights into techniques that effectively
bend the need for compute while delivering extremely high performance.",2025-05-13,"Saurabh Dash, Yiyang Nan, John Dang, Arash Ahmadian, Shivalika Singh, Madeline Smith, Bharat Venkitesh, Vlad Shmyhlo, Viraat Aryabumi, Walter Beller-Morales, Jeremy Pekmez, Jason Ozuzu, Pierre Richemond, Acyr Locatelli, Nick Frosst, Phil Blunsom, Aidan Gomez, Ivan Zhang, Marzieh Fadaee, Manoj Govindassamy, Sudip Roy, Matthias Gallé, Beyza Ermis, Ahmet Üstün, Sara Hooker",http://arxiv.org/pdf/2505.08751v1,cs.CL
AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models,"Actual causality (AC), a fundamental aspect of causal reasoning (CR), is
responsible for attribution and responsibility assignment in real-world
scenarios. However, existing LLM-based methods lack grounding in formal AC
theory, resulting in limited interpretability. Therefore, we propose AC-Reason,
a semi-formal reasoning framework that identifies causally relevant events
within an AC scenario, infers the values of their formal causal factors (e.g.,
sufficiency, necessity, and normality), and answers AC queries via a
theory-guided algorithm with explanations. While AC-Reason does not explicitly
construct a causal graph, it operates over variables in the underlying causal
structure to support principled reasoning. To enable comprehensive evaluation,
we introduce AC-Bench, a new benchmark built upon and substantially extending
Big-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully
annotated samples, each with detailed reasoning steps and focuses solely on
actual causation. The case study shows that synthesized samples in AC-Bench
present greater challenges for LLMs. Extensive experiments on BBH-CJ and
AC-Bench show that AC-Reason consistently improves LLM performance over
baselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy
of 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +
AC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further
enables fine-grained analysis of reasoning faithfulness, revealing that only
Qwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful
reasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation
study proves that integrating AC theory into LLMs is highly effective, with the
proposed algorithm contributing the most significant performance gains.",2025-05-13,"Yanxi Zhang, Xin Cong, Zhong Zhang, Xiao Liu, Dongyan Zhao, Yesai Wu",http://arxiv.org/pdf/2505.08750v1,cs.CL
Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies,"Can autoregressive large language models (LLMs) learn consistent probability
distributions when trained on sequences in different token orders? We prove
formally that for any well-defined probability distribution, sequence
perplexity is invariant under any factorization, including forward, backward,
or arbitrary permutations. This result establishes a rigorous theoretical
foundation for studying how LLMs learn from data and defines principled
protocols for empirical evaluation. Applying these protocols, we show that
prior studies examining ordering effects suffer from critical methodological
flaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted
orders on scientific text. We find systematic deviations from theoretical
invariance across all orderings with arbitrary permutations strongly deviating
from both forward and backward models, which largely (but not completely)
agreed with one another. Deviations were traceable to differences in
self-attention, reflecting positional and locality biases in processing. Our
theoretical and empirical results provide novel avenues for understanding
positional biases in LLMs and suggest methods for detecting when LLMs'
probability distributions are inconsistent and therefore untrustworthy.",2025-05-13,"Xiaoliang Luo, Xinyi Xu, Michael Ramscar, Bradley C. Love",http://arxiv.org/pdf/2505.08739v1,cs.CL
NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context,"This work introduces the first benchmark for nursing value alignment,
consisting of five core value dimensions distilled from international nursing
codes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The
benchmark comprises 1,100 real-world nursing behavior instances collected
through a five-month longitudinal field study across three hospitals of varying
tiers. These instances are annotated by five clinical nurses and then augmented
with LLM-generated counterfactuals with reversed ethic polarity. Each original
case is paired with a value-aligned and a value-violating version, resulting in
2,200 labeled instances that constitute the Easy-Level dataset. To increase
adversarial complexity, each instance is further transformed into a
dialogue-based format that embeds contextual cues and subtle misleading
signals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)
LLMs on their alignment with nursing values. Our findings reveal three key
insights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level
dataset (94.55), where Claude 3.5 Sonnet outperforms other models on the
Hard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)
Justice is consistently the most difficult nursing value dimension to evaluate;
and (3) in-context learning significantly improves alignment. This work aims to
provide a foundation for value-sensitive LLMs development in clinical settings.
The dataset and the code are available at
https://huggingface.co/datasets/Ben012345/NurValues.",2025-05-13,"Ben Yao, Qiuchi Li, Yazhou Zhang, Siyu Yang, Bohan Zhang, Prayag Tiwari, Jing Qin",http://arxiv.org/pdf/2505.08734v1,cs.CL
Memorization-Compression Cycles Improve Generalization,"We prove theoretically that generalization improves not only through data
scaling but also by compressing internal representations. To operationalize
this insight, we introduce the Information Bottleneck Language Modeling (IBLM)
objective, which reframes language modeling as a constrained optimization
problem: minimizing representation entropy subject to optimal prediction
performance. Empirically, we observe an emergent memorization-compression cycle
during LLM pretraining, evidenced by oscillation positive/negative gradient
alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of
representation entropy. This pattern closely mirrors the predictive-compressive
trade-off prescribed by IBLM and also parallels the biological alternation
between awake learning and sleep consolidation. Motivated by this observation,
we propose Gated Phase Transition (GAPT), a training algorithm that adaptively
switches between memorization and compression phases. When applied to GPT-2
pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves
cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining
task on arithmetic multiplication. In a setting designed to simulate
catastrophic forgetting, GAPT reduces interference by compressing and
separating representations, achieving a 97% improvement in separation -
paralleling the functional role of sleep consolidation.",2025-05-13,Fangyuan Yu,http://arxiv.org/pdf/2505.08727v1,cs.CL
LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs,"Electronic Health Records (EHRs) are digital records of patient information,
often containing unstructured clinical text. Named Entity Recognition (NER) is
essential in EHRs for extracting key medical entities like problems, tests, and
treatments to support downstream clinical applications. This paper explores
prompt-based medical entity recognition using large language models (LLMs),
specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering
techniques, including zero-shot, few-shot, and an ensemble approach. Among all
strategies, GPT-4o with prompt ensemble achieved the highest classification
performance with an F1-score of 0.95 and recall of 0.98, outperforming
DeepSeek-R1 on the task. The ensemble method improved reliability by
aggregating outputs through embedding-based similarity and majority voting.",2025-05-13,"K M Sajjadul Islam, Ayesha Siddika Nipu, Jiawei Wu, Praveen Madiraju",http://arxiv.org/pdf/2505.08704v2,cs.CL
Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation,"Event extraction (EE) is a fundamental task in natural language processing
(NLP) that involves identifying and extracting event information from
unstructured text. Effective EE in real-world scenarios requires two key steps:
selecting appropriate schemas from hundreds of candidates and executing the
extraction process. Existing research exhibits two critical gaps: (1) the rigid
schema fixation in existing pipeline systems, and (2) the absence of benchmarks
for evaluating joint schema matching and extraction. Although large language
models (LLMs) offer potential solutions, their schema hallucination tendencies
and context window limitations pose challenges for practical deployment. In
response, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel
paradigm combining schema paraphrasing with schema retrieval-augmented
generation. ASEE adeptly retrieves paraphrased schemas and accurately generates
targeted structures. To facilitate rigorous evaluation, we construct the
Multi-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which
systematically consolidates 12 datasets across diverse domains, complexity
levels, and language settings. Extensive evaluations on MD-SEE show that our
proposed ASEE demonstrates strong adaptability across various scenarios,
significantly improving the accuracy of event extraction.",2025-05-13,"Sheng Liang, Hang Lv, Zhihao Wen, Yaxiong Wu, Yongyue Zhang, Hao Wang, Yong Liu",http://arxiv.org/pdf/2505.08690v1,cs.CL
Revealing economic facts: LLMs know more than they say,"We investigate whether the hidden states of large language models (LLMs) can
be used to estimate and impute economic and financial statistics. Focusing on
county-level (e.g. unemployment) and firm-level (e.g. total assets) variables,
we show that a simple linear model trained on the hidden states of open-source
LLMs outperforms the models' text outputs. This suggests that hidden states
capture richer economic information than the responses of the LLMs reveal
directly. A learning curve analysis indicates that only a few dozen labelled
examples are sufficient for training. We also propose a transfer learning
method that improves estimation accuracy without requiring any labelled data
for the target variable. Finally, we demonstrate the practical utility of
hidden-state representations in super-resolution and data imputation tasks.",2025-05-13,"Marcus Buckmann, Quynh Anh Nguyen, Edward Hill",http://arxiv.org/pdf/2505.08662v1,cs.CL
"Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing","We present MegaBeam-Mistral-7B, a language model that supports 512K-token
context length. Our work addresses practical limitations in long-context
training, supporting real-world tasks such as compliance monitoring and
verification. Evaluated on three long-context benchmarks, our 7B-parameter
model demonstrates superior in-context learning performance on HELMET and
robust retrieval and tracing capability on RULER. It is currently the only open
model to achieve competitive long-range reasoning on BABILong at 512K context
length without RAG or targeted fine-tuning. Released as fully open source under
the Apache 2.0 license, the model has been downloaded over 100,000 times on
Hugging Face. Model available at:
https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k",2025-05-13,"Chen Wu, Yin Song",http://arxiv.org/pdf/2505.08651v1,cs.CL
TRAIL: Trace Reasoning and Agentic Issue Localization,"The increasing adoption of agentic workflows across diverse domains brings a
critical need to scalably and systematically evaluate the complex traces these
systems generate. Current evaluation methods depend on manual, domain-specific
human analysis of lengthy workflow traces - an approach that does not scale
with the growing complexity and volume of agentic outputs. Error analysis in
these settings is further complicated by the interplay of external tool outputs
and language model reasoning, making it more challenging than traditional
software debugging. In this work, we (1) articulate the need for robust and
dynamic evaluation methods for agentic workflow traces, (2) introduce a formal
taxonomy of error types encountered in agentic systems, and (3) present a set
of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and
grounded in established agentic benchmarks. To ensure ecological validity, we
curate traces from both single and multi-agent systems, focusing on real-world
applications such as software engineering and open-world information retrieval.
Our evaluations reveal that modern long context LLMs perform poorly at trace
debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our
dataset and code are made publicly available to support and accelerate future
research in scalable evaluation for agentic workflows.",2025-05-13,"Darshan Deshpande, Varun Gangal, Hersh Mehta, Jitin Krishnan, Anand Kannappan, Rebecca Qian",http://arxiv.org/pdf/2505.08638v2,cs.CL
Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models,"Text-to-image generative models like DALL-E and Stable Diffusion have
revolutionized visual content creation across various applications, including
advertising, personalized media, and design prototyping. However, crafting
effective textual prompts to guide these models remains challenging, often
requiring extensive trial and error. Existing prompt inversion approaches, such
as soft and hard prompt techniques, are not so effective due to the limited
interpretability and incoherent prompt generation. To address these issues, we
propose Visually Guided Decoding (VGD), a gradient-free approach that leverages
large language models (LLMs) and CLIP-based guidance to generate coherent and
semantically aligned prompts. In essence, VGD utilizes the robust text
generation capabilities of LLMs to produce human-readable prompts. Further, by
employing CLIP scores to ensure alignment with user-specified visual concepts,
VGD enhances the interpretability, generalization, and flexibility of prompt
generation without the need for additional training. Our experiments
demonstrate that VGD outperforms existing prompt inversion techniques in
generating understandable and contextually relevant prompts, facilitating more
intuitive and controllable interactions with text-to-image models.",2025-05-13,"Donghoon Kim, Minji Bae, Kyuhong Shim, Byonghyo Shim",http://arxiv.org/pdf/2505.08622v1,cs.CL
Automatic Task Detection and Heterogeneous LLM Speculative Decoding,"Speculative decoding, which combines a draft model with a target model, has
emerged as an effective approach to accelerate large language model (LLM)
inference. However, existing methods often face a trade-off between the
acceptance rate and decoding speed in downstream tasks due to the limited
capacity of the draft model, making it difficult to ensure efficiency across
diverse tasks. To address this problem, we propose a speculative decoding
algorithm tailored for downstream task optimization. It includes an automatic
task partitioning and assigning method, which automatically categorizes
downstream tasks into different sub-tasks and assigns them to a set of
heterogeneous draft models. Each draft model is aligned with the target model
using task-specific data, thereby enhancing the consistency of inference
results. In addition, our proposed method incorporates an online lightweight
prompt classifier to dynamically route prompts to the appropriate draft model.
Experimental results demonstrate that the proposed method improves draft
accuracy by 6% to 50% over vanilla speculative decoding, while achieving a
speedup of 1.10x to 2.64x in LLM inference.",2025-05-13,"Danying Ge, Jianhua Gao, Qizhi Jiang, Yifei Feng, Weixing Ji",http://arxiv.org/pdf/2505.08600v1,cs.CL
Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models,"Advancements in artificial intelligence (AI) are transforming pathology by
integrat-ing large language models (LLMs) with retrieval-augmented generation
(RAG) and domain-specific foundation models. This study explores the
application of RAG-enhanced LLMs coupled with pathology foundation models for
thyroid cytology diagnosis, addressing challenges in cytological
interpretation, standardization, and diagnostic accuracy. By leveraging a
curated knowledge base, RAG facilitates dy-namic retrieval of relevant case
studies, diagnostic criteria, and expert interpreta-tion, improving the
contextual understanding of LLMs. Meanwhile, pathology foun-dation models,
trained on high-resolution pathology images, refine feature extrac-tion and
classification capabilities. The fusion of these AI-driven approaches en-hances
diagnostic consistency, reduces variability, and supports pathologists in
dis-tinguishing benign from malignant thyroid lesions. Our results demonstrate
that integrating RAG with pathology-specific LLMs significantly improves
diagnostic efficiency and interpretability, paving the way for AI-assisted
thyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for
correct prediction of surgi-cal pathology diagnosis from thyroid cytology
samples.",2025-05-13,"Hussien Al-Asi, Jordan P Reynolds, Shweta Agarwal, Bryan J Dangott, Aziza Nassar, Zeynettin Akkus",http://arxiv.org/pdf/2505.08590v1,cs.CL
Small but Significant: On the Promise of Small Language Models for Accessible AIED,"GPT has become nearly synonymous with large language models (LLMs), an
increasingly popular term in AIED proceedings. A simple keyword-based search
reveals that 61% of the 76 long and short papers presented at AIED 2024
describe novel solutions using LLMs to address some of the long-standing
challenges in education, and 43% specifically mention GPT. Although LLMs
pioneered by GPT create exciting opportunities to strengthen the impact of AI
on education, we argue that the field's predominant focus on GPT and other
resource-intensive LLMs (with more than 10B parameters) risks neglecting the
potential impact that small language models (SLMs) can make in providing
resource-constrained institutions with equitable and affordable access to
high-quality AI tools. Supported by positive results on knowledge component
(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as
Phi-2 can produce an effective solution without elaborate prompting strategies.
Hence, we call for more attention to developing SLM-based AIED approaches.",2025-05-13,"Yumou Wei, Paulo Carvalho, John Stamper",http://arxiv.org/pdf/2505.08588v1,cs.CL
Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation,"While gender bias in modern Neural Machine Translation (NMT) systems has
received much attention, traditional evaluation metrics do not to fully capture
the extent to which these systems integrate contextual gender cues. We propose
a novel evaluation metric called Minimal Pair Accuracy (MPA), which measures
the reliance of models on gender cues for gender disambiguation. MPA is
designed to go beyond surface-level gender accuracy metrics by focusing on
whether models adapt to gender cues in minimal pairs -- sentence pairs that
differ solely in the gendered pronoun, namely the explicit indicator of the
target's entity gender in the source language (EN). We evaluate a number of NMT
models on the English-Italian (EN--IT) language pair using this metric, we show
that they ignore available gender cues in most cases in favor of (statistical)
stereotypical gender interpretation. We further show that in anti-stereotypical
cases, these models tend to more consistently take masculine gender cues into
account while ignoring the feminine cues. Furthermore, we analyze the attention
head weights in the encoder component and show that while all models encode
gender information to some extent, masculine cues elicit a more diffused
response compared to the more concentrated and specialized responses to
feminine gender cues.",2025-05-13,"Chiara Manna, Afra Alishahi, Frédéric Blain, Eva Vanmassenhove",http://arxiv.org/pdf/2505.08546v1,cs.CL
QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing,"Reproduction studies reported in NLP provide individual data points which in
combination indicate worryingly low levels of reproducibility in the field.
Because each reproduction study reports quantitative conclusions based on its
own, often not explicitly stated, criteria for reproduction success/failure,
the conclusions drawn are hard to interpret, compare, and learn from. In this
paper, we present QRA++, a quantitative approach to reproducibility assessment
that (i) produces continuous-valued degree of reproducibility assessments at
three levels of granularity; (ii) utilises reproducibility measures that are
directly comparable across different studies; and (iii) grounds expectations
about degree of reproducibility in degree of similarity between experiments.
QRA++ enables more informative reproducibility assessments to be conducted, and
conclusions to be drawn about what causes reproducibility to be better/poorer.
We illustrate this by applying QRA++ to three example sets of comparable
experiments, revealing clear evidence that degree of reproducibility depends on
similarity of experiment properties, but also system type and evaluation
method.",2025-05-13,Anya Belz,http://arxiv.org/pdf/2505.17043v1,cs.CL
LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries,"Open-source AI libraries are foundational to modern AI systems but pose
significant, underexamined risks across security, licensing, maintenance,
supply chain integrity, and regulatory compliance. We present LibVulnWatch, a
graph-based agentic assessment framework that performs deep, source-grounded
evaluations of these libraries. Built on LangGraph, the system coordinates a
directed acyclic graph of specialized agents to extract, verify, and quantify
risk using evidence from trusted sources such as repositories, documentation,
and vulnerability databases. LibVulnWatch generates reproducible,
governance-aligned scores across five critical domains, publishing them to a
public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely
used libraries, including ML frameworks, LLM inference engines, and agent
orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks
while uncovering up to 19 additional risks per library. These include critical
Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials
(SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in
regulatory documentation and auditability. By translating high-level governance
principles into practical, verifiable metrics, LibVulnWatch advances technical
AI governance with a scalable, transparent mechanism for continuous supply
chain risk assessment and informed library selection.",2025-05-13,"Zekun Wu, Seonglae Cho, Umar Mohammed, Cristian Munoz, Kleyton Costa, Xin Guan, Theo King, Ze Wang, Emre Kazim, Adriano Koshiyama",http://arxiv.org/pdf/2505.08842v1,cs.CL
Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding,"Sequence-to-sequence models are widely used to train Abstract Meaning
Representation (Banarescu et al., 2013, AMR) parsers. To train such models, AMR
graphs have to be linearized into a one-line text format. While Penman encoding
is typically used for this purpose, we argue that it has limitations: (1) for
deep graphs, some closely related nodes are located far apart in the linearized
text (2) Penman's tree-based encoding necessitates inverse roles to handle node
re-entrancy, doubling the number of relation types to predict. To address these
issues, we propose a triple-based linearization method and compare its
efficiency with Penman linearization. Although triples are well suited to
represent a graph, our results suggest room for improvement in triple encoding
to better compete with Penman's concise and explicit representation of a nested
graph structure.",2025-05-13,"Jeongwoo Kang, Maximin Coavoux, Cédric Lopez, Didier Schwab",http://arxiv.org/pdf/2505.08504v1,cs.CL
LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models,"Recent advances in large language models (LLMs) have enabled zero-shot
automated essay scoring (AES), providing a promising way to reduce the cost and
effort of essay scoring in comparison with manual grading. However, most
existing zero-shot approaches rely on LLMs to directly generate absolute
scores, which often diverge from human evaluations owing to model biases and
inconsistent scoring. To address these limitations, we propose LLM-based
Comparative Essay Scoring (LCES), a method that formulates AES as a pairwise
comparison task. Specifically, we instruct LLMs to judge which of two essays is
better, collect many such comparisons, and convert them into continuous scores.
Considering that the number of possible comparisons grows quadratically with
the number of essays, we improve scalability by employing RankNet to
efficiently transform LLM preferences into scalar scores. Experiments using AES
benchmark datasets show that LCES outperforms conventional zero-shot methods in
accuracy while maintaining computational efficiency. Moreover, LCES is robust
across different LLM backbones, highlighting its applicability to real-world
zero-shot AES.",2025-05-13,"Takumi Shibata, Yuichi Miyamura",http://arxiv.org/pdf/2505.08498v1,cs.CL
Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?,"Charts are ubiquitous as they help people understand and reason with data.
Recently, various downstream tasks, such as chart question answering,
chart2text, and fact-checking, have emerged. Large Vision-Language Models
(LVLMs) show promise in tackling these tasks, but their evaluation is costly
and time-consuming, limiting real-world deployment. While using LVLMs as judges
to assess the chart comprehension capabilities of other LVLMs could streamline
evaluation processes, challenges like proprietary datasets, restricted access
to powerful models, and evaluation costs hinder their adoption in industrial
settings. To this end, we present a comprehensive evaluation of 13 open-source
LVLMs as judges for diverse chart comprehension and reasoning tasks. We design
both pairwise and pointwise evaluation tasks covering criteria like factual
correctness, informativeness, and relevancy. Additionally, we analyze LVLM
judges based on format adherence, positional consistency, length bias, and
instruction-following. We focus on cost-effective LVLMs (<10B parameters)
suitable for both research and commercial use, following a standardized
evaluation protocol and rubric to measure the LVLM judge's accuracy.
Experimental results reveal notable variability: while some open LVLM judges
achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4
judgments), others struggle (below ~10% agreement). Our findings highlight that
state-of-the-art open-source LVLMs can serve as cost-effective automatic
evaluators for chart-related tasks, though biases such as positional preference
and length bias persist.",2025-05-13,"Md Tahmid Rahman Laskar, Mohammed Saidul Islam, Ridwan Mahbub, Ahmed Masry, Mizanur Rahman, Amran Bhuiyan, Mir Tafseer Nayeem, Shafiq Joty, Enamul Hoque, Jimmy Huang",http://arxiv.org/pdf/2505.08468v1,cs.CL
"Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions","Stance detection is essential for understanding subjective content across
various platforms such as social media, news articles, and online reviews.
Recent advances in Large Language Models (LLMs) have revolutionized stance
detection by introducing novel capabilities in contextual understanding,
cross-domain generalization, and multimodal analysis. Despite these
progressions, existing surveys often lack comprehensive coverage of approaches
that specifically leverage LLMs for stance detection. To bridge this critical
gap, our review article conducts a systematic analysis of stance detection,
comprehensively examining recent advancements of LLMs transforming the field,
including foundational concepts, methodologies, datasets, applications, and
emerging challenges. We present a novel taxonomy for LLM-based stance detection
approaches, structured along three key dimensions: 1) learning methods,
including supervised, unsupervised, few-shot, and zero-shot; 2) data
modalities, such as unimodal, multimodal, and hybrid; and 3) target
relationships, encompassing in-target, cross-target, and multi-target
scenarios. Furthermore, we discuss the evaluation techniques and analyze
benchmark datasets and performance trends, highlighting the strengths and
limitations of different architectures. Key applications in misinformation
detection, political analysis, public health monitoring, and social media
moderation are discussed. Finally, we identify critical challenges such as
implicit stance expression, cultural biases, and computational constraints,
while outlining promising future directions, including explainable stance
reasoning, low-resource adaptation, and real-time deployment frameworks. Our
survey highlights emerging trends, open challenges, and future directions to
guide researchers and practitioners in developing next-generation stance
detection systems powered by large language models.",2025-05-13,"Lata Pangtey, Anukriti Bhatnagar, Shubhi Bansal, Shahid Shafi Dar, Nagendra Kumar",http://arxiv.org/pdf/2505.08464v1,cs.CL
RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models,"Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm
in applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs
still struggle with the discrepancies between the representation obtained from
the PLMs' encoder and the optimal input to the PLMs' decoder. This paper
tackles this challenge by learning to calibrate the representation of PLMs in
the latent space. In the proposed representation calibration method (RepCali),
we integrate a specific calibration block to the latent space after the encoder
and use the calibrated output as the decoder input. The merits of the proposed
RepCali include its universality to all PLMs with encoder-decoder
architectures, its plug-and-play nature, and ease of implementation. Extensive
experiments on 25 PLM-based models across 8 tasks (including both English and
Chinese datasets) demonstrate that the proposed RepCali offers desirable
enhancements to PLMs (including LLMs) and significantly improves the
performance of downstream tasks. Comparison experiments across 4 benchmark
tasks indicate that RepCali is superior to the representative fine-tuning
baselines.",2025-05-13,"Fujun Zhang, XiangDong Su",http://arxiv.org/pdf/2505.08463v1,cs.CL
IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation,"Retrieval-Augmented Generation (RAG) has emerged as a way to complement the
in-context knowledge of Large Language Models (LLMs) by integrating external
documents. However, real-world applications demand not only accuracy but also
interpretability. While dense retrieval methods provide high accuracy, they
lack interpretability; conversely, sparse retrieval methods offer transparency
but often fail to capture the full intent of queries due to their reliance on
keyword matching. To address these issues, we introduce IterKey, an LLM-driven
iterative keyword generation framework that enhances RAG via sparse retrieval.
IterKey consists of three LLM-driven stages: generating keywords for retrieval,
generating answers based on retrieved documents, and validating the answers. If
validation fails, the process iteratively repeats with refined keywords. Across
four QA tasks, experimental results show that IterKey achieves 5% to 20%
accuracy improvements over BM25-based RAG and simple baselines. Its performance
is comparable to dense retrieval-based RAG and prior iterative query refinement
methods using dense models. In summary, IterKey is a novel BM25-based approach
leveraging LLMs to iteratively refine RAG, effectively balancing accuracy with
interpretability.",2025-05-13,"Kazuki Hayashi, Hidetaka Kamigaito, Shinya Kouda, Taro Watanabe",http://arxiv.org/pdf/2505.08450v1,cs.CL
Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency,"Large language models achieve high task performance yet often hallucinate or
rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses
these gaps by coupling generation with external search. We analyse how
hyperparameters influence speed and quality in RAG systems, covering Chroma and
Faiss vector stores, chunking policies, cross-encoder re-ranking, and
temperature, and we evaluate six metrics: faithfulness, answer correctness,
answer relevancy, context precision, context recall, and answer similarity.
Chroma processes queries 13% faster, whereas Faiss yields higher retrieval
precision, revealing a clear speed-accuracy trade-off. Naive fixed-length
chunking with small windows and minimal overlap outperforms semantic
segmentation while remaining the quickest option. Re-ranking provides modest
gains in retrieval quality yet increases runtime by roughly a factor of 5, so
its usefulness depends on latency constraints. These results help practitioners
balance computational cost and accuracy when tuning RAG systems for
transparent, up-to-date responses. Finally, we re-evaluate the top
configurations with a corrective RAG workflow and show that their advantages
persist when the model can iteratively request additional evidence. We obtain a
near-perfect context precision (99%), which demonstrates that RAG systems can
achieve extremely high retrieval accuracy with the right combination of
hyperparameters, with significant implications for applications where retrieval
quality directly impacts downstream task performance, such as clinical decision
support in healthcare.",2025-05-13,"Adel Ammar, Anis Koubaa, Omer Nacar, Wadii Boulila",http://arxiv.org/pdf/2505.08445v1,cs.CL
A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court,"Topic modeling in Italian legal research is hindered by the lack of public
datasets, limiting the analysis of legal themes in Supreme Court judgments. To
address this, we developed a document processing pipeline that produces an
anonymized dataset optimized for topic modeling.
  The pipeline integrates document layout analysis (YOLOv8x), optical character
recognition, and text anonymization. The DLA module achieved a mAP@50 of 0.964
and a mAP@50-95 of 0.800. The OCR detector reached a mAP@50-95 of 0.9022, and
the text recognizer (TrOCR) obtained a character error rate of 0.0047 and a
word error rate of 0.0248. Compared to OCR-only methods, our dataset improved
topic modeling with a diversity score of 0.6198 and a coherence score of
0.6638.
  We applied BERTopic to extract topics and used large language models to
generate labels and summaries. Outputs were evaluated against domain expert
interpretations. Claude Sonnet 3.7 achieved a BERTScore F1 of 0.8119 for
labeling and 0.9130 for summarization.",2025-05-13,"Matteo Marulli, Glauco Panattoni, Marco Bertini",http://arxiv.org/pdf/2505.08439v1,cs.CL
Hakim: Farsi Text Embedding Model,"Recent advancements in text embedding have significantly improved natural
language understanding across many languages, yet Persian remains notably
underrepresented in large-scale embedding research. In this paper, we present
Hakim, a novel state-of-the-art Persian text embedding model that achieves a
8.5% performance improvement over existing approaches on the FaMTEB benchmark,
outperforming all previously developed Persian language models. As part of this
work, we introduce three new datasets - Corpesia, Pairsia-sup, and
Pairsia-unsup - to support supervised and unsupervised training scenarios.
Additionally, Hakim is designed for applications in chatbots and
retrieval-augmented generation (RAG) systems, particularly addressing retrieval
tasks that require incorporating message history within these systems. We also
propose a new baseline model built on the BERT architecture. Our language model
consistently achieves higher accuracy across various Persian NLP tasks, while
the RetroMAE-based model proves particularly effective for textual information
retrieval applications. Together, these contributions establish a new
foundation for advancing Persian language understanding.",2025-05-13,"Mehran Sarmadi, Morteza Alikhani, Erfan Zinvandi, Zahra Pourbahman",http://arxiv.org/pdf/2505.08435v2,cs.CL
TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers,"Recently, large language models(LLMs) have played an increasingly important
role in solving a wide range of NLP tasks, leveraging their capabilities of
natural language understanding and generating. Integration with external tools
further enhances LLMs' effectiveness, providing more precise, timely, and
specialized responses. However, LLMs still encounter difficulties with
non-executable actions and improper actions, which are primarily attributed to
incorrect parameters. The process of generating parameters by LLMs is confined
to the tool level, employing the coarse-grained strategy without considering
the different difficulties of various tools. To address this issue, we propose
TUMS, a novel framework designed to enhance the tool-use capabilities of LLMs
by transforming tool-level processing into parameter-level processing.
Specifically, our framework consists of four key components: (1) an intent
recognizer that identifies the user's intent to help LLMs better understand the
task; (2) a task decomposer that breaks down complex tasks into simpler
subtasks, each involving a tool call; (3) a subtask processor equipped with
multi-structure handlers to generate accurate parameters; and (4) an executor.
Our empirical studies have evidenced the effectiveness and efficiency of the
TUMS framework with an average of 19.6\% and 50.6\% improvement separately on
easy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key
contribution of each part with ablation experiments, offering more insights and
stimulating future research on Tool-augmented LLMs.",2025-05-13,"Aiyao He, Sijia Cui, Shuai Xu, Yanna Wang, Bo Xu",http://arxiv.org/pdf/2505.08402v1,cs.CL
Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping,"Large Language Models leverage Chain-of-Thought (CoT) prompting for complex
tasks, but their reasoning traces are often excessively verbose and
inefficient, leading to significant computational costs and latency. Current
CoT compression techniques typically rely on generic importance metrics and
static compression rates, which may inadvertently remove functionally critical
tokens or fail to adapt to varying reasoning complexity. To overcome these
limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic
CoT compression via supervised fine-tuning. This approach introduces two
synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric
accurately identifying functionally relevant tokens by measuring the gradient
influence of their intermediate representations on the final answer loss, and
(2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the
compression rate based on runtime model uncertainty while ensuring local
coherence through an adaptive N-token constraint. To our knowledge, this is the
first work unifying a goal-oriented, gradient-based importance metric with
dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed
MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization
across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It
achieves substantial efficiency gains - reducing CoT token counts by over 45%
on average and delivering 1.6-2.0 times inference speedups - while maintaining
high reasoning accuracy. Notably, it significantly outperforms existing
baselines by preserving accuracy even at high effective compression rates,
advancing the state of the art in the CoT reasoning efficiency-accuracy
trade-off.",2025-05-13,"Ren Zhuang, Ben Wang, Shuifa Sun",http://arxiv.org/pdf/2505.08392v2,cs.CL
Towards Contamination Resistant Benchmarks,"The rapid development of large language models (LLMs) has transformed the
landscape of natural language processing. Evaluating LLMs properly is crucial
for understanding their potential and addressing concerns such as safety.
However, LLM evaluation is confronted by various factors, among which
contamination stands out as a key issue that undermines the reliability of
evaluations. In this work, we introduce the concept of contamination resistance
to address this challenge. We propose a benchmark based on Caesar ciphers
(e.g., ""ab"" to ""bc"" when the shift is 1), which, despite its simplicity, is an
excellent example of a contamination resistant benchmark. We test this
benchmark on widely used LLMs under various settings, and we find that these
models struggle with this benchmark when contamination is controlled. Our
findings reveal issues in current LLMs and raise important questions regarding
their true capabilities. Our work contributes to the development of
contamination resistant benchmarks, enabling more rigorous LLM evaluation and
offering insights into the true capabilities and limitations of LLMs.",2025-05-13,"Rahmatullah Musawi, Sheng Lu",http://arxiv.org/pdf/2505.08389v1,cs.CL
Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring,"This paper investigates the potentials of Large Language Models (LLMs) as
adaptive tutors in the context of second-language learning. In particular, we
evaluate whether system prompting can reliably constrain LLMs to generate only
text appropriate to the student's competence level. We simulate full
teacher-student dialogues in Spanish using instruction-tuned, open-source LLMs
ranging in size from 7B to 12B parameters. Dialogues are generated by having an
LLM alternate between tutor and student roles with separate chat histories. The
output from the tutor model is then used to evaluate the effectiveness of
CEFR-based prompting to control text difficulty across three proficiency levels
(A1, B1, C1). Our findings suggest that while system prompting can be used to
constrain model outputs, prompting alone is too brittle for sustained,
long-term interactional contexts - a phenomenon we term alignment drift. Our
results provide insights into the feasibility of LLMs for personalized,
proficiency-aligned adaptive tutors and provide a scalable method for low-cost
evaluation of model performance without human participants.",2025-05-13,"Mina Almasi, Ross Deans Kristensen-McLachlan",http://arxiv.org/pdf/2505.08351v1,cs.CL
On the Geometry of Semantics in Next-token Prediction,"Modern language models demonstrate a remarkable ability to capture linguistic
meaning despite being trained solely through next-token prediction (NTP). We
investigate how this conceptually simple training objective leads models to
extract and encode latent semantic and grammatical concepts. Our analysis
reveals that NTP optimization implicitly guides models to encode concepts via
singular value decomposition (SVD) factors of a centered data-sparsity matrix
that captures next-word co-occurrence patterns. While the model never
explicitly constructs this matrix, learned word and context embeddings
effectively factor it to capture linguistic structure. We find that the most
important SVD factors are learned first during training, motivating the use of
spectral clustering of embeddings to identify human-interpretable semantics,
including both classical k-means and a new orthant-based method directly
motivated by our interpretation of concepts. Overall, our work bridges
distributional semantics, neural collapse geometry, and neural network training
dynamics, providing insights into how NTP's implicit biases shape the emergence
of meaning representations in language models.",2025-05-13,"Yize Zhao, Christos Thrampoulidis",http://arxiv.org/pdf/2505.08348v1,cs.CL
AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale,"We present AM-Thinking-v1, a 32B dense language model that advances the
frontier of reasoning, embodying the collaborative spirit of open-source
innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts
(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves
impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on
LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities
among open-source models of similar scale.
  Built entirely from the open-source Qwen2.5-32B base model and publicly
available queries, AM-Thinking-v1 leverages a meticulously crafted
post-training pipeline - combining supervised fine-tuning and reinforcement
learning - to deliver exceptional reasoning capabilities. This work
demonstrates that the open-source community can achieve high performance at the
32B scale, a practical sweet spot for deployment and fine-tuning. By striking a
balance between top-tier performance and real-world usability, we hope
AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale
models, pushing reasoning boundaries while keeping accessibility at the core of
innovation. We have open-sourced our model on
\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}.",2025-05-13,"Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, Xiangang Li",http://arxiv.org/pdf/2505.08311v2,cs.CL
Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow,"Black-Box prompt optimization methods have emerged as a promising strategy
for refining input prompts to better align large language models (LLMs),
thereby enhancing their task performance. Although these methods have
demonstrated encouraging results, most studies and experiments have primarily
focused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g.,
GPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with
DeepSeek V3 (671B), it remains an open question whether these black-box
optimization techniques will continue to yield significant performance
improvements for models of such scale. In response to this, we select three
well-known black-box optimization methods and evaluate them on large-scale LLMs
(DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The
results show that these black-box prompt optimization methods offer only
limited improvements on these large-scale LLMs. Furthermore, we hypothesize
that the scale of the model is the primary factor contributing to the limited
benefits observed. To explore this hypothesis, we conducted experiments on LLMs
of varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an
inverse scaling law, wherein the effectiveness of black-box optimization
methods diminished as the model size increased.",2025-05-13,"Ziyu Zhou, Yihang Wu, Jingyuan Yang, Zhan Xiao, Rongjun Li",http://arxiv.org/pdf/2505.08303v1,cs.CL
Next Word Suggestion using Graph Neural Network,"Language Modeling is a prevalent task in Natural Language Processing. The
currently existing most recent and most successful language models often tend
to build a massive model with billions of parameters, feed in a tremendous
amount of text data, and train with enormous computation resources which
require millions of dollars. In this project, we aim to address an important
sub-task in language modeling, i.e., context embedding. We propose an approach
to exploit the Graph Convolution operation in GNNs to encode the context and
use it in coalition with LSTMs to predict the next word given a local context
of preceding words. We test this on the custom Wikipedia text corpus using a
very limited amount of resources and show that this approach works fairly well
to predict the next word.",2025-05-13,"Abisha Thapa Magar, Anup Shakya",http://arxiv.org/pdf/2505.09649v1,cs.CL
Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration,"The rapid progress in large language models (LLMs) has paved the way for
novel approaches in knowledge-intensive tasks. Among these, Cache-Augmented
Generation (CAG) has emerged as a promising alternative to Retrieval-Augmented
Generation (RAG). CAG minimizes retrieval latency and simplifies system design
by preloading knowledge into the model's context. However, challenges persist
in scaling CAG to accommodate large and dynamic knowledge bases effectively.
This paper introduces Adaptive Contextual Compression (ACC), an innovative
technique designed to dynamically compress and manage context inputs, enabling
efficient utilization of the extended memory capabilities of modern LLMs. To
further address the limitations of standalone CAG, we propose a Hybrid CAG-RAG
Framework, which integrates selective retrieval to augment preloaded contexts
in scenarios requiring additional information. Comprehensive evaluations on
diverse datasets highlight the proposed methods' ability to enhance
scalability, optimize efficiency, and improve multi-hop reasoning performance,
offering practical solutions for real-world knowledge integration challenges.",2025-05-13,"Rishabh Agrawal, Himanshu Kumar",http://arxiv.org/pdf/2505.08261v1,cs.CL
VLM-KG: Multimodal Radiology Knowledge Graph Generation,"Vision-Language Models (VLMs) have demonstrated remarkable success in natural
language generation, excelling at instruction following and structured output
generation. Knowledge graphs play a crucial role in radiology, serving as
valuable sources of factual information and enhancing various downstream tasks.
However, generating radiology-specific knowledge graphs presents significant
challenges due to the specialized language of radiology reports and the limited
availability of domain-specific data. Existing solutions are predominantly
unimodal, meaning they generate knowledge graphs only from radiology reports
while excluding radiographic images. Additionally, they struggle with long-form
radiology data due to limited context length. To address these limitations, we
propose a novel multimodal VLM-based framework for knowledge graph generation
in radiology. Our approach outperforms previous methods and introduces the
first multimodal solution for radiology knowledge graph generation.",2025-05-13,"Abdullah Abdullah, Seong Tae Kim",http://arxiv.org/pdf/2505.17042v1,cs.CL
"Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement","The rapid advancement of large language models (LLMs) has outpaced
traditional evaluation methodologies. It presents novel challenges, such as
measuring human-like psychological constructs, navigating beyond static and
task-specific benchmarks, and establishing human-centered evaluation. These
challenges intersect with Psychometrics, the science of quantifying the
intangible aspects of human psychology, such as personality, values, and
intelligence. This survey introduces and synthesizes an emerging
interdisciplinary field of LLM Psychometrics, which leverages psychometric
instruments, theories, and principles to evaluate, understand, and enhance
LLMs. We systematically explore the role of Psychometrics in shaping
benchmarking principles, broadening evaluation scopes, refining methodologies,
validating results, and advancing LLM capabilities. This paper integrates
diverse perspectives to provide a structured framework for researchers across
disciplines, enabling a more comprehensive understanding of this nascent field.
Ultimately, we aim to provide actionable insights for developing future
evaluation paradigms that align with human-level AI and promote the advancement
of human-centered AI systems for societal benefit. A curated repository of LLM
psychometric resources is available at
https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.",2025-05-13,"Haoran Ye, Jing Jin, Yuhang Xie, Xin Zhang, Guojie Song",http://arxiv.org/pdf/2505.08245v1,cs.CL
Exploring EFL Secondary Students' AI-generated Text Editing While Composition Writing,"Generative Artificial Intelligence is transforming how English as a foreign
language students write. Still, little is known about how students manipulate
text generated by generative AI during the writing process. This study
investigates how EFL secondary school students integrate and modify
AI-generated text when completing an expository writing task. The study
employed an exploratory mixed-methods design. Screen recordings were collected
from 29 Hong Kong secondary school students who attended an AI-assisted writing
workshop and recorded their screens while using generative AI to write an
article. Content analysis with hierarchical coding and thematic analysis with a
multiple case study approach were adopted to analyze the recordings. 15 types
of AI-generated text edits across seven categories were identified from the
recordings. Notably, AI-initiated edits from iOS and Google Docs emerged as
unanticipated sources of AI-generated text. A thematic analysis revealed four
patterns of students' editing behaviors based on planning and drafting
direction: planning with top-down drafting and revising; top-down drafting and
revising without planning; planning with bottom-up drafting and revising; and
bottom-up drafting and revising without planning. Network graphs illustrate
cases of each pattern, demonstrating that students' interactions with
AI-generated text involve more complex cognitive processes than simple text
insertion. The findings challenge assumptions about students' passive,
simplistic use of generative AI tools and have implications for developing
explicit instructional approaches to teaching AI-generated text editing
strategies in the AFL writing pedagogy.",2025-05-13,"David James Woo, Yangyang Yu, Kai Guo",http://arxiv.org/pdf/2505.17041v1,cs.CL
Not that Groove: Zero-Shot Symbolic Music Editing,"Most work in AI music generation focused on audio, which has seen limited use
in the music production industry due to its rigidity. To maximize flexibility
while assuming only textual instructions from producers, we are among the first
to tackle symbolic music editing. We circumvent the known challenge of lack of
labeled data by proving that LLMs with zero-shot prompting can effectively edit
drum grooves. The recipe of success is a creatively designed format that
interfaces LLMs and music, while we facilitate evaluation by providing an
evaluation dataset with annotated unit tests that highly aligns with musicians'
judgment.",2025-05-13,Li Zhang,http://arxiv.org/pdf/2505.08203v1,cs.CL
A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs,"Large Language Models (LLMs) have the tendency to hallucinate, i.e., to
sporadically generate false or fabricated information. This presents a major
challenge, as hallucinations often appear highly convincing and users generally
lack the tools to detect them. Uncertainty quantification (UQ) provides a
framework for assessing the reliability of model outputs, aiding in the
identification of potential hallucinations. In this work, we introduce
pre-trained UQ heads: supervised auxiliary modules for LLMs that substantially
enhance their ability to capture uncertainty compared to unsupervised UQ
methods. Their strong performance stems from the powerful Transformer
architecture in their design and informative features derived from LLM
attention maps. Experimental evaluation shows that these heads are highly
robust and achieve state-of-the-art performance in claim-level hallucination
detection across both in-domain and out-of-domain prompts. Moreover, these
modules demonstrate strong generalization to languages they were not explicitly
trained on. We pre-train a collection of UQ heads for popular LLM series,
including Mistral, Llama, and Gemma 2. We publicly release both the code and
the pre-trained heads.",2025-05-13,"Artem Shelmanov, Ekaterina Fadeeva, Akim Tsvigun, Ivan Tsvigun, Zhuohan Xie, Igor Kiselev, Nico Daheim, Caiqi Zhang, Artem Vazhentsev, Mrinmaya Sachan, Preslav Nakov, Timothy Baldwin",http://arxiv.org/pdf/2505.08200v1,cs.CL
Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph,"Text-attributed graph (TAG) provides a text description for each graph node,
and few- and zero-shot node classification on TAGs have many applications in
fields such as academia and social networks. Existing work utilizes various
graph-based augmentation techniques to train the node and text embeddings,
while text-based augmentations are largely unexplored. In this paper, we
propose Text Semantics Augmentation (TSA) to improve accuracy by introducing
more text semantic supervision signals. Specifically, we design two
augmentation techniques, i.e., positive semantics matching and negative
semantics contrast, to provide more reference texts for each graph node or text
description. Positive semantic matching retrieves texts with similar embeddings
to match with a graph node. Negative semantic contrast adds a negative prompt
to construct a text description with the opposite semantics, which is
contrasted with the original node and text. We evaluate TSA on 5 datasets and
compare with 13 state-of-the-art baselines. The results show that TSA
consistently outperforms all baselines, and its accuracy improvements over the
best-performing baseline are usually over 5%.",2025-05-13,"Yuxiang Wang, Xiao Yan, Shiyu Jin, Quanqing Xu, Chuang Hu, Yuanyuan Zhu, Bo Du, Jia Wu, Jiawei Jiang",http://arxiv.org/pdf/2505.08168v1,cs.CL
Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage,"The rapid development of large language models (LLMs) has provided
significant support and opportunities for the advancement of domain-specific
LLMs. However, fine-tuning these large models using Intangible Cultural
Heritage (ICH) data inevitably faces challenges such as bias, incorrect
knowledge inheritance, and catastrophic forgetting. To address these issues, we
propose a novel training method that integrates a bidirectional chains of
thought and a reward mechanism. This method is built upon ICH-Qwen, a large
language model specifically designed for the field of intangible cultural
heritage. The proposed method enables the model to not only perform forward
reasoning but also enhances the accuracy of the generated answers by utilizing
reverse questioning and reverse reasoning to activate the model's latent
knowledge. Additionally, a reward mechanism is introduced during training to
optimize the decision-making process. This mechanism improves the quality of
the model's outputs through structural and content evaluations with different
weighting schemes. We conduct comparative experiments on ICH-Qwen, with results
demonstrating that our method outperforms 0-shot, step-by-step reasoning,
knowledge distillation, and question augmentation methods in terms of accuracy,
Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the
paper highlights the effectiveness of combining the bidirectional chains of
thought and reward mechanism through ablation experiments. In addition, a
series of generalizability experiments are conducted, with results showing that
the proposed method yields improvements on various domain-specific datasets and
advanced models in areas such as Finance, Wikidata, and StrategyQA. This
demonstrates that the method is adaptable to multiple domains and provides a
valuable approach for model training in future applications across diverse
fields.",2025-05-13,"Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang",http://arxiv.org/pdf/2505.08167v2,cs.CL
Generalizing Large Language Model Usability Across Resource-Constrained,"Large Language Models (LLMs) have achieved remarkable success across a wide
range of natural language tasks, and recent efforts have sought to extend their
capabilities to multimodal domains and resource-constrained environments.
However, existing approaches often rely on costly supervised fine-tuning or
assume fixed training conditions, limiting their generalization when facing
unseen modalities, limited data, or restricted compute resources. This
dissertation presents a systematic study toward generalizing LLM usability
under real-world constraints. First, it introduces a robust text-centric
alignment framework that enables LLMs to seamlessly integrate diverse
modalities-including text, images, tables, and any modalities - via natural
language interfaces. This approach supports in-context adaptation to unseen or
dynamically changing modalities without requiring retraining. To enhance
robustness against noisy and missing modalities, an adversarial prompting
technique is proposed, generating semantically challenging perturbations at the
prompt level to stress-test model reliability. Beyond multimodal setting, the
dissertation investigates inference-time optimization strategies for LLMs,
leveraging prompt search and uncertainty quantification to improve performance
without additional model training. This perspective offers an efficient
alternative to scaling model parameters or retraining from scratch.
Additionally, the work addresses low-resource domains such as Verilog code
generation by designing correct-by-construction synthetic data pipelines and
logic-enhanced reasoning models, achieving state-of-the-art performance with
minimal data. Together, these contributions form a unified effort to enhance
the adaptability, scalability, and efficiency of large language models under
practical constraints.",2025-05-13,Yun-Da Tsai,http://arxiv.org/pdf/2505.17040v1,cs.CL
A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem,"Millions of users leverage generative pretrained transformer (GPT)-based
language models developed by leading model providers for a wide range of tasks.
To support enhanced user interaction and customization, many platforms-such as
OpenAI-now enable developers to create and publish tailored model instances,
known as custom GPTs, via dedicated repositories or application stores. These
custom GPTs empower users to browse and interact with specialized applications
designed to meet specific needs. However, as custom GPTs see growing adoption,
concerns regarding their security vulnerabilities have intensified. Existing
research on these vulnerabilities remains largely theoretical, often lacking
empirical, large-scale, and statistically rigorous assessments of associated
risks.
  In this study, we analyze 14,904 custom GPTs to assess their susceptibility
to seven exploitable threats, such as roleplay-based attacks, system prompt
leakage, phishing content generation, and malicious code synthesis, across
various categories and popularity tiers within the OpenAI marketplace. We
introduce a multi-metric ranking system to examine the relationship between a
custom GPT's popularity and its associated security risks.
  Our findings reveal that over 95% of custom GPTs lack adequate security
protections. The most prevalent vulnerabilities include roleplay-based
vulnerabilities (96.51%), system prompt leakage (92.20%), and phishing
(91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit
inherent security weaknesses, which are often inherited or amplified in custom
GPTs. These results highlight the urgent need for enhanced security measures
and stricter content moderation to ensure the safe deployment of GPT-based
applications.",2025-05-13,"Sunday Oyinlola Ogundoyin, Muhammad Ikram, Hassan Jameel Asghar, Benjamin Zi Hao Zhao, Dali Kaafar",http://arxiv.org/pdf/2505.08148v1,cs.CL
Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence,"As human-AI collaboration becomes increasingly prevalent in educational
contexts, understanding and measuring the extent and nature of such
interactions pose significant challenges. This research investigates the use of
authorship verification (AV) techniques not as a punitive measure, but as a
means to quantify AI assistance in academic writing, with a focus on promoting
transparency, interpretability, and student development. Building on prior
work, we structured our investigation into three stages: dataset selection and
expansion, AV method development, and systematic evaluation. Using three
datasets - including a public dataset (PAN-14) and two from University of
Melbourne students from various courses - we expanded the data to include
LLM-generated texts, totalling 1,889 documents and 540 authorship problems from
506 students. We developed an adapted Feature Vector Difference AV methodology
to construct robust academic writing profiles for students, designed to capture
meaningful, individual characteristics of their writing. The method's
effectiveness was evaluated across multiple scenarios, including distinguishing
between student-authored and LLM-generated texts and testing resilience against
LLMs' attempts to mimic student writing styles. Results demonstrate the
enhanced AV classifier's ability to identify stylometric discrepancies and
measure human-AI collaboration at word and sentence levels while providing
educators with a transparent tool to support academic integrity investigations.
This work advances AV technology, offering actionable insights into the
dynamics of academic writing in an AI-driven era.",2025-05-13,"Eduardo Araujo Oliveira, Madhavi Mohoni, Sonsoles López-Pernas, Mohammed Saqr",http://arxiv.org/pdf/2505.08828v1,cs.CL
Large Language Models for Computer-Aided Design: A Survey,"Large Language Models (LLMs) have seen rapid advancements in recent years,
with models like ChatGPT and DeepSeek, showcasing their remarkable capabilities
across diverse domains. While substantial research has been conducted on LLMs
in various fields, a comprehensive review focusing on their integration with
Computer-Aided Design (CAD) remains notably absent. CAD is the industry
standard for 3D modeling and plays a vital role in the design and development
of products across different industries. As the complexity of modern designs
increases, the potential for LLMs to enhance and streamline CAD workflows
presents an exciting frontier. This article presents the first systematic
survey exploring the intersection of LLMs and CAD. We begin by outlining the
industrial significance of CAD, highlighting the need for AI-driven innovation.
Next, we provide a detailed overview of the foundation of LLMs. We also examine
both closed-source LLMs as well as publicly available models. The core of this
review focuses on the various applications of LLMs in CAD, providing a taxonomy
of six key areas where these models are making considerable impact. Finally, we
propose several promising future directions for further advancements, which
offer vast opportunities for innovation and are poised to shape the future of
CAD technology. Github:
https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy",2025-05-13,"Licheng Zhang, Bach Le, Naveed Akhtar, Siew-Kei Lam, Tuan Ngo",http://arxiv.org/pdf/2505.08137v1,cs.CL
A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes,"A data-driven quantitative approach was used to develop a novel
classification system for beer categories and styles. Sixty-two thousand one
hundred twenty-one beer recipes were mined and analyzed, considering ingredient
profiles, fermentation parameters, and recipe vital statistics. Statistical
analyses combined with self-organizing maps (SOMs) identified four major
superclusters that showed distinctive malt and hop usage patterns, style
characteristics, and historical brewing traditions. Cold fermented styles
showed a conservative grain and hop composition, whereas hot fermented beers
exhibited high heterogeneity, reflecting regional preferences and innovation.
This new taxonomy offers a reproducible and objective framework beyond
traditional sensory-based classifications, providing brewers, researchers, and
educators with a scalable tool for recipe analysis and beer development. The
findings in this work provide an understanding of beer diversity and open
avenues for linking ingredient usage with fermentation profiles and flavor
outcomes.",2025-05-13,Diego Bonatto,http://arxiv.org/pdf/2505.17039v1,cs.CL
ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval,"The rise of Large Language Models~(LLMs) revolutionizes information
retrieval, allowing users to obtain required answers through complex
instructions within conversations. However, publicly available services remain
inadequate in addressing the needs of faculty and students to search
campus-specific information. It is primarily due to the LLM's lack of
domain-specific knowledge and the limitation of search engines in supporting
multilingual and timely scenarios. To tackle these challenges, we introduce
ALOHA, a multilingual agent enhanced by hierarchical retrieval for university
orientation. We also integrate external APIs into the front-end interface to
provide interactive service. The human evaluation and case study show our
proposed system has strong capabilities to yield correct, timely, and
user-friendly responses to the queries in multiple languages, surpassing
commercial chatbots and search engines. The system has been deployed and has
provided service for more than 12,000 people.",2025-05-13,"Mingxu Tao, Bowen Tang, Mingxuan Ma, Yining Zhang, Hourun Li, Feifan Wen, Hao Ma, Jia Yang",http://arxiv.org/pdf/2505.08130v1,cs.CL
Putting It All into Context: Simplifying Agents with LCLMs,"Recent advances in language model (LM) agents have demonstrated significant
potential for automating complex real-world tasks. To make progress on these
difficult tasks, LM agent architectures have become increasingly complex, often
incorporating multi-step retrieval tools, multiple agents, and scaffolding
adapted to the underlying LM. In this work, we investigate whether all of this
complexity is necessary, or if parts of these scaffolds can be removed on
challenging tasks like SWE-bench. We show that in the case of SWE-bench, simply
putting the entire environment into the context of a long context language
model (LCLM) and properly prompting the model makes it competitive with
carefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model
without any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable
with approaches using carefully tuned agent scaffolds (32%). While the
unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic
architectures, we demonstrate that the more capable Gemini-2.5-Pro using the
same unscaffolded approach directly attains a 50.8% solve rate. Additionally, a
two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a
competitive 48.6% solve rate.",2025-05-12,"Mingjian Jiang, Yangjun Ruan, Luis Lastras, Pavan Kapanipathi, Tatsunori Hashimoto",http://arxiv.org/pdf/2505.08120v1,cs.CL
Are LLMs complicated ethical dilemma analyzers?,"One open question in the study of Large Language Models (LLMs) is whether
they can emulate human ethical reasoning and act as believable proxies for
human judgment. To investigate this, we introduce a benchmark dataset
comprising 196 real-world ethical dilemmas and expert opinions, each segmented
into five structured components: Introduction, Key Factors, Historical
Theoretical Perspectives, Resolution Strategies, and Key Takeaways. We also
collect non-expert human responses for comparison, limited to the Key Factors
section due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini,
Claude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric
framework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine
similarity, and Universal Sentence Encoder similarity. Metric weights are
computed through an inversion-based ranking alignment and pairwise AHP
analysis, enabling fine-grained comparison of model outputs to expert
responses. Our results show that LLMs generally outperform non-expert humans in
lexical and structural alignment, with GPT-4o-mini performing most consistently
across all sections. However, all models struggle with historical grounding and
proposing nuanced resolution strategies, which require contextual abstraction.
Human responses, while less structured, occasionally achieve comparable
semantic similarity, suggesting intuitive moral reasoning. These findings
highlight both the strengths and current limitations of LLMs in ethical
decision-making.",2025-05-12,"Jiashen, Du, Jesse Yao, Allen Liu, Zhekai Zhang",http://arxiv.org/pdf/2505.08106v1,cs.CL
Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders,"Sparse Autoencoders (SAEs) have recently emerged as powerful tools for
interpreting and steering the internal representations of large language models
(LLMs). However, conventional approaches to analyzing SAEs typically rely
solely on input-side activations, without considering the causal influence
between each latent feature and the model's output. This work is built on two
key hypotheses: (1) activated latents do not contribute equally to the
construction of the model's output, and (2) only latents with high causal
influence are effective for model steering. To validate these hypotheses, we
propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method
that identifies the most influential latents by incorporating output-side
gradient information.",2025-05-12,"Dong Shu, Xuansheng Wu, Haiyan Zhao, Mengnan Du, Ninghao Liu",http://arxiv.org/pdf/2505.08080v1,cs.CL
An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits,"Large language models (LLMs) have transformed natural-language processing,
yet their scale makes real-world deployment costly. Post-training quantization
reduces memory and computation but often degrades accuracy, while
quantization-aware training can recover performance at the cost of extra
training. Pushing quantization to the ternary (2-bit) regime yields even larger
savings but is notoriously unstable. Building on recent work showing that a
bias-free, RMS-normalized Transformer with straight-through estimation can
reach 1.58-bit precision, we demonstrate that simply inserting RMS
normalization before every linear projection and applying a gradual, layer-wise
quantization schedule stably fine-tunes full-precision checkpoints into ternary
LLMs. Our approach matches or surpasses more elaborate knowledge-distillation
pipelines on standard language-modeling benchmarks without adding model
complexity. These results indicate that careful normalization alone can close
much of the accuracy gap between ternary and full-precision LLMs, making
ultra-low-bit inference practical.",2025-05-12,"Cody Steinmetz, Gavin Childress, Aaron Herbst, Gavin Jones, Jasdeep Singh, Eli Vang, Keagan Weinstock",http://arxiv.org/pdf/2505.08823v1,cs.CL
Hypernym Mercury: Token Optimization Through Semantic Field Constriction And Reconstruction From Hypernyms. A New Text Compression Method,"Compute optimization using token reduction of LLM prompts is an emerging task
in the fields of NLP and next generation, agentic AI. In this white paper, we
introduce a novel (patent pending) text representation scheme and a
first-of-its-kind word-level semantic compression of paragraphs that can lead
to over 90% token reduction, while retaining high semantic similarity to the
source text. We explain how this novel compression technique can be lossless
and how the detail granularity is controllable. We discuss benchmark results
over open source data (i.e. Bram Stoker's Dracula available through Project
Gutenberg) and show how our results hold at the paragraph level, across
multiple genres and models.",2025-05-12,"Chris Forrester, Octavia Sulea",http://arxiv.org/pdf/2505.08058v2,cs.CL
FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning,"Safety alignment approaches in large language models (LLMs) often lead to the
over-refusal of benign queries, significantly diminishing their utility in
sensitive scenarios. To address this challenge, we introduce FalseReject, a
comprehensive resource containing 16k seemingly toxic queries accompanied by
structured responses across 44 safety-related categories. We propose a
graph-informed adversarial multi-agent interaction framework to generate
diverse and complex prompts, while structuring responses with explicit
reasoning to aid models in accurately distinguishing safe from unsafe contexts.
FalseReject includes training datasets tailored for both standard
instruction-tuned models and reasoning-oriented models, as well as a
human-annotated benchmark test set. Our extensive benchmarking on 29
state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges.
Empirical results demonstrate that supervised finetuning with FalseReject
substantially reduces unnecessary refusals without compromising overall safety
or general language capabilities.",2025-05-12,"Zhehao Zhang, Weijie Xu, Fanyou Wu, Chandan K. Reddy",http://arxiv.org/pdf/2505.08054v1,cs.CL
NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition,"This study formalizes a computational model to simulate classical Persian
poets' dynamics of influence through constructing a multi-dimensional
similarity network. Using a rigorously curated dataset based on Ganjoor's
corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical
features to demarcate each poet's corpus. Each is contained within weighted
similarity matrices, which are then appended to generate an aggregate graph
showing poet-to-poet influence. Further network investigation is carried out to
identify key poets, style hubs, and bridging poets by calculating degree,
closeness, betweenness, eigenvector, and Katz centrality measures. Further, for
typological insight, we use the Louvain community detection algorithm to
demarcate clusters of poets sharing both style and theme coherence, which
correspond closely to acknowledged schools of literature like Sabk-e Hindi,
Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a
new data-driven view of Persian literature distinguished between canonical
significance and interextual influence, thus highlighting relatively
lesser-known figures who hold great structural significance. Combining
computational linguistics with literary study, this paper produces an
interpretable and scalable model for poetic tradition, enabling retrospective
reflection as well as forward-looking research within digital humanities.",2025-05-12,"Kourosh Shahnazari, Seyed Moein Ayyoubzadeh",http://arxiv.org/pdf/2505.08052v1,cs.CL
TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation,"Multi-level Tibetan spelling correction addresses errors at both the
character and syllable levels within a unified model. Existing methods focus
mainly on single-level correction and lack effective integration of both
levels. Moreover, there are no open-source datasets or augmentation methods
tailored for this task in Tibetan. To tackle this, we propose a data
augmentation approach using unlabeled text to generate multi-level corruptions,
and introduce TiSpell, a semi-masked model capable of correcting both
character- and syllable-level errors. Although syllable-level correction is
more challenging due to its reliance on global context, our semi-masked
strategy simplifies this process. We synthesize nine types of corruptions on
clean sentences to create a robust training set. Experiments on both simulated
and real-world data demonstrate that TiSpell, trained on our dataset,
outperforms baseline models and matches the performance of state-of-the-art
approaches, confirming its effectiveness.",2025-05-12,"Yutong Liu, Feng Xiao, Ziyue Zhang, Yongbin Yu, Cheng Huang, Fan Gao, Xiangxiang Wang, Ma-bao Ban, Manping Fan, Thupten Tsering, Cheng Huang, Gadeng Luosang, Renzeng Duojie, Nyima Tashi",http://arxiv.org/pdf/2505.08037v2,cs.CL
On the (Non) Injectivity of Piecewise Linear Janossy Pooling,"Multiset functions, which are functions that map multisets to vectors, are a
fundamental tool in the construction of neural networks for multisets and
graphs. To guarantee that the vector representation of the multiset is
faithful, it is often desirable to have multiset mappings that are both
injective and bi-Lipschitz. Currently, there are several constructions of
multiset functions achieving both these guarantees, leading to improved
performance in some tasks but often also to higher compute time than standard
constructions. Accordingly, it is natural to inquire whether simpler multiset
functions achieving the same guarantees are available. In this paper, we make a
large step towards giving a negative answer to this question. We consider the
family of k-ary Janossy pooling, which includes many of the most popular
multiset models, and prove that no piecewise linear Janossy pooling function
can be injective. On the positive side, we show that when restricted to
multisets without multiplicities, even simple deep-sets models suffice for
injectivity and bi-Lipschitzness.",2025-05-26,"Ilai Reshef, Nadav Dym",http://arxiv.org/pdf/2505.20150v1,cs.LG
SeMe: Training-Free Language Model Merging via Semantic Alignment,"Despite the remarkable capabilities of Language Models (LMs) across diverse
tasks, no single model consistently outperforms others, necessitating efficient
methods to combine their strengths without expensive retraining. Existing model
merging techniques, such as parameter averaging and task-guided fusion, often
rely on data-dependent computations or fail to preserve internal knowledge,
limiting their robustness and scalability. We introduce SeMe (Semantic-based
Merging), a novel, data-free, and training-free approach that leverages latent
semantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike
prior work, SeMe not only preserves model behaviors but also explicitly
stabilizes internal knowledge, addressing a critical gap in LM fusion. Through
extensive experiments across diverse architectures and tasks, we demonstrate
that SeMe outperforms existing methods in both performance and efficiency while
eliminating reliance on external data. Our work establishes a new paradigm for
knowledge-aware model merging and provides insights into the semantic structure
of LMs, paving the way for more scalable and interpretable model composition.",2025-05-26,"Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang",http://arxiv.org/pdf/2505.20144v1,cs.LG
Model Stitching by Functional Latent Alignment,"Evaluating functional similarity involves quantifying the degree to which
independently trained neural networks learn functionally similar
representations. Reliably inferring the functional similarity of these networks
remains an open problem with far-reaching implications for AI. Model stitching
has emerged as a promising paradigm, where an optimal affine transformation
aligns two models to solve a task, with the stitched model serving as a proxy
for functional similarity. In this work, we draw inspiration from the knowledge
distillation literature and propose Functional Latent Alignment (FuLA) as a
novel optimality condition for model stitching. We revisit previously explored
functional similarity testbeds and introduce a new one, based on which FuLA
emerges as an overall more reliable method of functional similarity.
Specifically, our experiments in (a) adversarial training, (b) shortcut
training and, (c) cross-layer stitching, reveal that FuLA is less prone to
artifacts tied to training on task cues while achieving non-trivial alignments
that are missed by stitch-level matching.",2025-05-26,"Ioannis Athanasiadis, Anmar Karmush, Michael Felsberg",http://arxiv.org/pdf/2505.20142v1,cs.LG
Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks,"Predictive Coding (PC) offers a biologically plausible alternative to
backpropagation for neural network training, yet struggles with deeper
architectures. This paper identifies the root cause: an inherent signal decay
problem where gradients attenuate exponentially with depth, becoming
computationally negligible due to numerical precision constraints. To address
this fundamental limitation, we introduce Error Optimization (EO), a novel
reparameterization that preserves PC's theoretical properties while eliminating
signal decay. By optimizing over prediction errors rather than states, EO
enables signals to reach all layers simultaneously and without attenuation,
converging orders of magnitude faster than standard PC. Experiments across
multiple architectures and datasets demonstrate that EO matches
backpropagation's performance even for deeper models where conventional PC
struggles. Besides practical improvements, our work provides theoretical
insight into PC dynamics and establishes a foundation for scaling
biologically-inspired learning to deeper architectures on digital hardware and
beyond.",2025-05-26,"Cédric Goemaere, Gaspard Oliviers, Rafal Bogacz, Thomas Demeester",http://arxiv.org/pdf/2505.20137v1,cs.LG
Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning,"Replay-based continual learning (CL) methods assume that models trained on a
small subset can also effectively minimize the empirical risk of the complete
dataset. These methods maintain a memory buffer that stores a sampled subset of
data from previous tasks to consolidate past knowledge. However, this
assumption is not guaranteed in practice due to the limited capacity of the
memory buffer and the heuristic criteria used for buffer data selection. To
address this issue, we propose a new dataset distillation framework tailored
for CL, which maintains a learnable memory buffer to distill the global
information from the current task data and accumulated knowledge preserved in
the previous memory buffer. Moreover, to avoid the computational overhead and
overfitting risks associated with parameterizing the entire buffer during
distillation, we introduce a lightweight distillation module that can achieve
global information distillation solely by generating learnable soft labels for
the memory buffer data. Extensive experiments show that, our method can achieve
competitive results and effectively mitigates forgetting across various
datasets. The source code will be publicly available.",2025-05-26,"Wenyang Liao, Quanziang Wang, Yichen Wu, Renzhen Wang, Deyu Meng",http://arxiv.org/pdf/2505.20135v1,cs.LG
AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings,"Current language models rely on static vocabularies determined at pretraining
time, which can lead to decreased performance and increased computational cost
for domains underrepresented in the original vocabulary. New tokens can be
added to solve this problem, when coupled with a good initialization for their
new embeddings. However, existing embedding initialization methods either
require expensive further training or pretraining of additional modules. In
this paper, we propose AweDist and show that by distilling representations
obtained using the original tokenization, we can quickly learn high-quality
input embeddings for new tokens. Experimental results with a wide range of
open-weight models show that AweDist is able to outperform even strong
baselines.",2025-05-26,"Konstantin Dobler, Desmond Elliott, Gerard de Melo",http://arxiv.org/pdf/2505.20133v1,cs.LG
Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks,"Tensorizing a neural network involves reshaping some or all of its dense
weight matrices into higher-order tensors and approximating them using low-rank
tensor network decompositions. This technique has shown promise as a model
compression strategy for large-scale neural networks. However, despite
encouraging empirical results, tensorized neural networks (TNNs) remain
underutilized in mainstream deep learning. In this position paper, we offer a
perspective on both the potential and current limitations of TNNs. We argue
that TNNs represent a powerful yet underexplored framework for deep
learning--one that deserves greater attention from both engineering and
theoretical communities. Beyond compression, we highlight the value of TNNs as
a flexible class of architectures with distinctive scaling properties and
increased interpretability. A central feature of TNNs is the presence of bond
indices, which introduce new latent spaces not found in conventional networks.
These internal representations may provide deeper insight into the evolution of
features across layers, potentially advancing the goals of mechanistic
interpretability. We conclude by outlining several key research directions
aimed at overcoming the practical barriers to scaling and adopting TNNs in
modern deep learning workflows.",2025-05-26,"Safa Hamreras, Sukhbinder Singh, Román Orús",http://arxiv.org/pdf/2505.20132v1,cs.LG
MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning,"Molecular editing aims to modify a given molecule to optimize desired
chemical properties while preserving structural similarity. However, current
approaches typically rely on string-based or continuous representations, which
fail to adequately capture the discrete, graph-structured nature of molecules,
resulting in limited structural fidelity and poor controllability. In this
paper, we propose MolEditRL, a molecular editing framework that explicitly
integrates structural constraints with precise property optimization.
Specifically, MolEditRL consists of two stages: (1) a discrete graph diffusion
model pretrained to reconstruct target molecules conditioned on source
structures and natural language instructions; (2) an editing-aware
reinforcement learning fine-tuning stage that further enhances property
alignment and structural preservation by explicitly optimizing editing
decisions under graph constraints. For comprehensive evaluation, we construct
MolEdit-Instruct, the largest and most property-rich molecular editing dataset,
comprising 3 million diverse examples spanning single- and multi-property tasks
across 10 chemical attributes. Experimental results demonstrate that MolEditRL
significantly outperforms state-of-the-art methods in both property
optimization accuracy and structural fidelity, achieving a 74\% improvement in
editing success rate while using 98\% fewer parameters.",2025-05-26,"Yuanxin Zhuang, Dazhong Shen, Ying Sun",http://arxiv.org/pdf/2505.20131v1,cs.LG
Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach,"This paper focuses on the design of spatial experiments to optimize the
amount of information derived from the experimental data and enhance the
accuracy of the resulting causal effect estimator. We propose a surrogate
function for the mean squared error (MSE) of the estimator, which facilitates
the use of classical graph cut algorithms to learn the optimal design. Our
proposal offers three key advances: (1) it accommodates moderate to large
spatial interference effects; (2) it adapts to different spatial covariance
functions; (3) it is computationally efficient. Theoretical results and
numerical experiments based on synthetic environments and a dispatch simulator
that models a city-scale ridesharing market, further validate the effectiveness
of our design. A python implementation of our method is available at
https://github.com/Mamba413/CausalGraphCut.",2025-05-26,"Zhu Jin, Li Jingyi, Zhou Hongyi, Lin Yinan, Lin Zhenhua, Shi Chengchun",http://arxiv.org/pdf/2505.20130v1,cs.LG
Understanding Generalization in Diffusion Models via Probability Flow Distance,"Diffusion models have emerged as a powerful class of generative models,
capable of producing high-quality samples that generalize beyond the training
data. However, evaluating this generalization remains challenging: theoretical
metrics are often impractical for high-dimensional data, while no practical
metrics rigorously measure generalization. In this work, we bridge this gap by
introducing probability flow distance ($\texttt{PFD}$), a theoretically
grounded and computationally efficient metric to measure distributional
generalization. Specifically, $\texttt{PFD}$ quantifies the distance between
distributions by comparing their noise-to-data mappings induced by the
probability flow ODE. Moreover, by using $\texttt{PFD}$ under a teacher-student
evaluation protocol, we empirically uncover several key generalization
behaviors in diffusion models, including: (1) scaling behavior from
memorization to generalization, (2) early learning and double descent training
dynamics, and (3) bias-variance decomposition. Beyond these insights, our work
lays a foundation for future empirical and theoretical studies on
generalization in diffusion models.",2025-05-26,"Huijie Zhang, Zijian Huang, Siyi Chen, Jinfan Zhou, Zekai Zhang, Peng Wang, Qing Qu",http://arxiv.org/pdf/2505.20123v1,cs.LG
Proxy-Free GFlowNet,"Generative Flow Networks (GFlowNets) are a promising class of generative
models designed to sample diverse, high-reward structures by modeling
distributions over compositional objects. In many real-world applications,
obtaining the reward function for such objects is expensive, time-consuming, or
requires human input, making it necessary to train GFlowNets from historical
datasets. Most existing methods adopt a model-based approach, learning a proxy
model from the dataset to approximate the reward function. However, this
strategy inherently ties the quality of the learned policy to the accuracy of
the proxy, introducing additional complexity and uncertainty into the training
process. To overcome these limitations, we propose \textbf{Trajectory-Distilled
GFlowNet (TD-GFN)}, a \emph{proxy-free} training framework that eliminates the
need for out-of-dataset reward queries. Our method is motivated by the key
observation that different edges in the associated directed acyclic graph (DAG)
contribute unequally to effective policy learning. TD-GFN leverages inverse
reinforcement learning to estimate edge-level rewards from the offline dataset,
which are then used to ingeniously prune the DAG and guide backward trajectory
sampling during training. This approach directs the policy toward high-reward
regions while reducing the complexity of model fitting. Empirical results
across multiple tasks show that TD-GFN trains both efficiently and reliably,
significantly outperforming existing baselines in convergence speed and sample
quality.",2025-05-26,"Ruishuo Chen, Xun Wang, Rui Hu, Zhuoran Li, Longbo Huang",http://arxiv.org/pdf/2505.20110v1,cs.LG
Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning,"Text-to-multiview (T2MV) generation, which produces coherent multiview images
from a single text prompt, remains computationally intensive, while accelerated
T2MV methods using few-step diffusion models often sacrifice image fidelity and
view consistency. To address this, we propose a novel reinforcement learning
(RL) finetuning framework tailored for few-step T2MV diffusion models to
jointly optimize per-view fidelity and cross-view consistency. Specifically, we
first reformulate T2MV denoising across all views as a single unified Markov
decision process, enabling multiview-aware policy optimization driven by a
joint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV
sampling technique that adds an inversion-denoising pass to reinforce both
viewpoint and text conditioning, resulting in improved T2MV generation at the
cost of inference time. To internalize its performance gains into the base
sampling policy, we develop MV-ZigAL, a novel policy optimization strategy that
uses reward advantages of ZMV-Sampling over standard sampling as learning
signals for policy updates. Finally, noting that the joint-view reward
objective under-optimizes per-view fidelity but naively optimizing single-view
metrics neglects cross-view alignment, we reframe RL finetuning for T2MV
diffusion models as a constrained optimization problem that maximizes per-view
fidelity subject to an explicit joint-view constraint, thereby enabling more
efficient and balanced policy updates. By integrating this constrained
optimization paradigm with MV-ZigAL, we establish our complete RL finetuning
framework, referred to as MVC-ZigAL, which effectively refines the few-step
T2MV diffusion baseline in both fidelity and consistency while preserving its
few-step efficiency.",2025-05-26,"Ziyi Zhang, Li Shen, Deheng Ye, Yong Luo, Huangxuan Zhao, Lefei Zhang",http://arxiv.org/pdf/2505.20107v1,cs.LG
Transformer in Protein: A Survey,"As protein informatics advances rapidly, the demand for enhanced predictive
accuracy, structural analysis, and functional understanding has intensified.
Transformer models, as powerful deep learning architectures, have demonstrated
unprecedented potential in addressing diverse challenges across protein
research. However, a comprehensive review of Transformer applications in this
field remains lacking. This paper bridges this gap by surveying over 100
studies, offering an in-depth analysis of practical implementations and
research progress of Transformers in protein-related tasks. Our review
systematically covers critical domains, including protein structure prediction,
function prediction, protein-protein interaction analysis, functional
annotation, and drug discovery/target identification. To contextualize these
advancements across various protein domains, we adopt a domain-oriented
classification system. We first introduce foundational concepts: the
Transformer architecture and attention mechanisms, categorize Transformer
variants tailored for protein science, and summarize essential protein
knowledge. For each research domain, we outline its objectives and background,
critically evaluate prior methods and their limitations, and highlight
transformative contributions enabled by Transformer models. We also curate and
summarize pivotal datasets and open-source code resources to facilitate
reproducibility and benchmarking. Finally, we discuss persistent challenges in
applying Transformers to protein informatics and propose future research
directions. This review aims to provide a consolidated foundation for the
synergistic integration of Transformer and protein informatics, fostering
further innovation and expanded applications in the field.",2025-05-26,"Xiaowen Ling, Zhiqiang Li, Yanbin Wang, Zhuhong You",http://arxiv.org/pdf/2505.20098v1,cs.LG
Spurious Privacy Leakage in Neural Networks,"Neural networks are vulnerable to privacy attacks aimed at stealing sensitive
data. The risks can be amplified in a real-world scenario, particularly when
models are trained on limited and biased data. In this work, we investigate the
impact of spurious correlation bias on privacy vulnerability. We introduce
\emph{spurious privacy leakage}, a phenomenon where spurious groups are
significantly more vulnerable to privacy attacks than non-spurious groups. We
further show that group privacy disparity increases in tasks with simpler
objectives (e.g. fewer classes) due to the persistence of spurious features.
Surprisingly, we find that reducing spurious correlation using spurious robust
methods does not mitigate spurious privacy leakage. This leads us to introduce
a perspective on privacy disparity based on memorization, where mitigating
spurious correlation does not mitigate the memorization of spurious data, and
therefore, neither the privacy level. Lastly, we compare the privacy of
different model architectures trained with spurious data, demonstrating that,
contrary to prior works, architectural choice can affect privacy outcomes.",2025-05-26,"Chenxiang Zhang, Jun Pang, Sjouke Mauw",http://arxiv.org/pdf/2505.20095v1,cs.LG
A fast sound power prediction tool for genset noise using machine learning,"This paper investigates the application of machine learning regression
algorithms Kernel Ridge Regression (KRR), Huber Regressor (HR), and Gaussian
Process Regression (GPR) for predicting sound power levels of gensets, offering
significant value for marketing and sales teams during the early bidding
process. When engine sizes and genset enclosure dimensions are tentative, and
measured noise data is unavailable, these algorithms enable reliable noise
level estimation for unbuilt gensets. The study utilizes high fidelity datasets
from over 100 experiments conducted at Cummins Acoustics Technology Center
(ATC) in a hemi-anechoic chamber, adhering to ISO 3744 standards. By using
readily available information from the bidding and initial design stages, KRR
predicts sound power with an average accuracy of within 5 dBA. While HR and GPR
show slightly higher prediction errors, all models effectively capture the
overall noise trends across various genset configurations. These findings
present a promising method for early-stage noise estimation in genset design.",2025-05-26,"Saurabh Pargal, Abhijit A. Sane",http://arxiv.org/pdf/2505.20079v1,cs.LG
"Grokking ExPLAIND: Unifying Model, Data, and Training Attribution to Study Model Behavior","Post-hoc interpretability methods typically attribute a model's behavior to
its components, data, or training trajectory in isolation. This leads to
explanations that lack a unified view and may miss key interactions. While
combining existing methods or applying them at different training stages offers
broader insights, these approaches usually lack theoretical support. In this
work, we present ExPLAIND, a unified framework that integrates all three
perspectives. First, we generalize recent work on gradient path kernels, which
reformulate models trained by gradient descent as a kernel machine, to more
realistic training settings. Empirically, we find that both a CNN and a
Transformer model are replicated accurately by this reformulation. Second, we
derive novel parameter- and step-wise influence scores from the kernel feature
maps. We show their effectiveness in parameter pruning that is comparable to
existing methods, reinforcing their value for model component attribution.
Finally, jointly interpreting model components and data over the training
process, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking.
Among other things, our findings support previously proposed stages of
Grokking, while refining the final phase as one of alignment of input
embeddings and final layers around a representation pipeline learned after the
memorization phase. Overall, ExPLAIND provides a theoretically grounded,
unified framework to interpret model behavior and training dynamics.",2025-05-26,"Florian Eichin, Yupei Du, Philipp Mondorf, Barbara Plank, Michael A. Hedderich",http://arxiv.org/pdf/2505.20076v1,cs.LG
An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks,"Graph Neural Network-based methods face privacy leakage risks due to the
introduction of topological structures about the targets, which allows
attackers to bypass the target's prior knowledge of the sensitive attributes
and realize membership inference attacks (MIA) by observing and analyzing the
topology distribution. As privacy concerns grow, the assumption of MIA, which
presumes that attackers can obtain an auxiliary dataset with the same
distribution, is increasingly deviating from reality. In this paper, we
categorize the distribution diversity issue in real-world MIA scenarios as an
Out-Of-Distribution (OOD) problem, and propose a novel Graph OOD Membership
Inference Attack (GOOD-MIA) to achieve cross-domain graph attacks.
Specifically, we construct shadow subgraphs with distributions from different
domains to model the diversity of real-world data. We then explore the stable
node representations that remain unchanged under external influences and
consider eliminating redundant information from confounding environments and
extracting task-relevant key information to more clearly distinguish between
the characteristics of training data and unseen data. This OOD-based design
makes cross-domain graph attacks possible. Finally, we perform risk
extrapolation to optimize the attack's domain adaptability during attack
inference to generalize the attack to other domains. Experimental results
demonstrate that GOOD-MIA achieves superior attack performance in datasets
designed for multiple domains.",2025-05-26,"Jinyan Wang, Liu Yang, Yuecen Wei, Jiaxuan Si, Chenhao Guo, Qingyun Sun, Xianxian Li, Xingcheng Fu",http://arxiv.org/pdf/2505.20074v1,cs.LG
SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety,"As Large Language Models (LLMs) continue to advance and find applications
across a growing number of fields, ensuring the safety of LLMs has become
increasingly critical. To address safety concerns, recent studies have proposed
integrating safety constraints into Reinforcement Learning from Human Feedback
(RLHF). However, these approaches tend to be complex, as they encompass
complicated procedures in RLHF along with additional steps required by the
safety constraints. Inspired by Direct Preference Optimization (DPO), we
introduce a new algorithm called SafeDPO, which is designed to directly
optimize the safety alignment objective in a single stage of policy learning,
without requiring relaxation. SafeDPO introduces only one additional
hyperparameter to further enhance safety and requires only minor modifications
to standard DPO. As a result, it eliminates the need to fit separate reward and
cost models or to sample from the language model during fine-tuning, while
still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO
achieves competitive performance compared to state-of-the-art safety alignment
algorithms, both in terms of aligning with human preferences and improving
safety.",2025-05-26,"Geon-Hyeong Kim, Youngsoo Jang, Yu Jin Kim, Byoungjip Kim, Honglak Lee, Kyunghoon Bae, Moontae Lee",http://arxiv.org/pdf/2505.20065v1,cs.LG
SAEs Are Good for Steering -- If You Select the Right Features,"Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to
learn a decomposition of a model's latent space. This enables useful
applications such as steering - influencing the output of a model towards a
desired concept - without requiring labeled data. Current methods identify SAE
features to steer by analyzing the input tokens that activate them. However,
recent work has highlighted that activations alone do not fully describe the
effect of a feature on the model's output. In this work, we draw a distinction
between two types of features: input features, which mainly capture patterns in
the model's input, and output features, which have a human-understandable
effect on the model's output. We propose input and output scores to
characterize and locate these types of features, and show that high values for
both scores rarely co-occur in the same features. These findings have practical
implications: after filtering out features with low output scores, we obtain
2-3x improvements when steering with SAEs, making them competitive with
supervised methods.",2025-05-26,"Dana Arad, Aaron Mueller, Yonatan Belinkov",http://arxiv.org/pdf/2505.20063v1,cs.LG
Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations,"Protein language models (PLMs) have emerged as powerful tools to detect
complex patterns of protein sequences. However, the capability of PLMs to fully
capture information on protein sequences might be limited by focusing on single
pre-training tasks. Although adding data modalities or supervised objectives
can improve the performance of PLMs, pre-training often remains focused on
denoising corrupted sequences. To push the boundaries of PLMs, our research
investigated a multi-task pre-training strategy. We developed Ankh3, a model
jointly optimized on two objectives: masked language modeling with multiple
masking probabilities and protein sequence completion relying only on protein
sequences as input. This multi-task pre-training demonstrated that PLMs can
learn richer and more generalizable representations solely from protein
sequences. The results demonstrated improved performance in downstream tasks,
such as secondary structure prediction, fluorescence, GB1 fitness, and contact
prediction. The integration of multiple tasks gave the model a more
comprehensive understanding of protein properties, leading to more robust and
accurate predictions.",2025-05-26,"Hazem Alsamkary, Mohamed Elshaffei, Mohamed Elkerdawy, Ahmed Elnaggar",http://arxiv.org/pdf/2505.20052v1,cs.LG
Catoni-Style Change Point Detection for Regret Minimization in Non-Stationary Heavy-Tailed Bandits,"Regret minimization in stochastic non-stationary bandits gained popularity
over the last decade, as it can model a broad class of real-world problems,
from advertising to recommendation systems. Existing literature relies on
various assumptions about the reward-generating process, such as Bernoulli or
subgaussian rewards. However, in settings such as finance and
telecommunications, heavy-tailed distributions naturally arise. In this work,
we tackle the heavy-tailed piecewise-stationary bandit problem. Heavy-tailed
bandits, introduced by Bubeck et al., 2013, operate on the minimal assumption
that the finite absolute centered moments of maximum order $1+\epsilon$ are
uniformly bounded by a constant $v<+\infty$, for some $\epsilon \in (0,1]$. We
focus on the most popular non-stationary bandit setting, i.e., the
piecewise-stationary setting, in which the mean of reward-generating
distributions may change at unknown time steps. We provide a novel Catoni-style
change-point detection strategy tailored for heavy-tailed distributions that
relies on recent advancements in the theory of sequential estimation, which is
of independent interest. We introduce Robust-CPD-UCB, which combines this
change-point detection strategy with optimistic algorithms for bandits,
providing its regret upper bound and an impossibility result on the minimum
attainable regret for any policy. Finally, we validate our approach through
numerical experiments on synthetic and real-world datasets.",2025-05-26,"Gianmarco Genalti, Sujay Bhatt, Nicola Gatti, Alberto Maria Metelli",http://arxiv.org/pdf/2505.20051v1,cs.LG
Synthetic Time Series Forecasting with Transformer Architectures: Extensive Simulation Benchmarks,"Time series forecasting plays a critical role in domains such as energy,
finance, and healthcare, where accurate predictions inform decision-making
under uncertainty. Although Transformer-based models have demonstrated success
in sequential modeling, their adoption for time series remains limited by
challenges such as noise sensitivity, long-range dependencies, and a lack of
inductive bias for temporal structure. In this work, we present a unified and
principled framework for benchmarking three prominent Transformer forecasting
architectures-Autoformer, Informer, and Patchtst-each evaluated through three
architectural variants: Minimal, Standard, and Full, representing increasing
levels of complexity and modeling capacity.
  We conduct over 1500 controlled experiments on a suite of ten synthetic
signals, spanning five patch lengths and five forecast horizons under both
clean and noisy conditions. Our analysis reveals consistent patterns across
model families.
  To advance this landscape further, we introduce the Koopman-enhanced
Transformer framework, Deep Koopformer, which integrates operator-theoretic
latent state modeling to improve stability and interpretability. We demonstrate
its efficacy on nonlinear and chaotic dynamical systems. Our results highlight
Koopman based Transformer as a promising hybrid approach for robust,
interpretable, and theoretically grounded time series forecasting in noisy and
complex real-world conditions.",2025-05-26,"Ali Forootani, Mohammad Khosravi",http://arxiv.org/pdf/2505.20048v1,cs.LG
Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction,"Protein-protein interactions (PPIs) are fundamental to numerous cellular
processes, and their characterization is vital for understanding disease
mechanisms and guiding drug discovery. While protein language models (PLMs)
have demonstrated remarkable success in predicting protein structure and
function, their application to sequence-based PPI binding affinity prediction
remains relatively underexplored. This gap is often attributed to the scarcity
of high-quality, rigorously refined datasets and the reliance on simple
strategies for concatenating protein representations. In this work, we address
these limitations. First, we introduce a meticulously curated version of the
PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction
entries, by resolving annotation inconsistencies and duplicate entries for
multi-chain protein interactions. This dataset incorporates a stringent, less
than or equal to 30%, sequence identity threshold to ensure robust splitting
into training, validation, and test sets, minimizing data leakage. Second, we
propose and systematically evaluate four architectures for adapting PLMs to PPI
binding affinity prediction: embeddings concatenation (EC), sequences
concatenation (SC), hierarchical pooling (HP), and pooled attention addition
(PAD). These architectures were assessed using two training methods: full
fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM
features. Our comprehensive experiments across multiple leading PLMs (ProtT5,
ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures
consistently outperform conventional concatenation methods, achieving up to 12%
increase in terms of Spearman correlation. These results highlight the
necessity of sophisticated architectural designs to fully exploit the
capabilities of PLMs for nuanced PPI binding affinity prediction.",2025-05-26,"Hazem Alsamkary, Mohamed Elshaffei, Mohamed Soudy, Sara Ossman, Abdallah Amr, Nehal Adel Abdelsalam, Mohamed Elkerdawy, Ahmed Elnaggar",http://arxiv.org/pdf/2505.20036v1,cs.LG
Graph Wave Networks,"Dynamics modeling has been introduced as a novel paradigm in message passing
(MP) of graph neural networks (GNNs). Existing methods consider MP between
nodes as a heat diffusion process, and leverage heat equation to model the
temporal evolution of nodes in the embedding space. However, heat equation can
hardly depict the wave nature of graph signals in graph signal processing.
Besides, heat equation is essentially a partial differential equation (PDE)
involving a first partial derivative of time, whose numerical solution usually
has low stability, and leads to inefficient model training. In this paper, we
would like to depict more wave details in MP, since graph signals are
essentially wave signals that can be seen as a superposition of a series of
waves in the form of eigenvector. This motivates us to consider MP as a wave
propagation process to capture the temporal evolution of wave signals in the
space. Based on wave equation in physics, we innovatively develop a graph wave
equation to leverage the wave propagation on graphs. In details, we demonstrate
that the graph wave equation can be connected to traditional spectral GNNs,
facilitating the design of graph wave networks based on various Laplacians and
enhancing the performance of the spectral GNNs. Besides, the graph wave
equation is particularly a PDE involving a second partial derivative of time,
which has stronger stability on graphs than the heat equation that involves a
first partial derivative of time. Additionally, we theoretically prove that the
numerical solution derived from the graph wave equation are constantly stable,
enabling to significantly enhance model efficiency while ensuring its
performance. Extensive experiments show that GWNs achieve SOTA and efficient
performance on benchmark datasets, and exhibit outstanding performance in
addressing challenging graph problems, such as over-smoothing and heterophily.",2025-05-26,"Juwei Yue, Haikuo Li, Jiawei Sheng, Yihan Guo, Xinghua Zhang, Chuan Zhou, Tingwen Liu, Li Guo",http://arxiv.org/pdf/2505.20034v1,cs.LG
ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers,"Tactile sensing provides local essential information that is complementary to
visual perception, such as texture, compliance, and force. Despite recent
advances in visuotactile representation learning, challenges remain in fusing
these modalities and generalizing across tasks and environments without heavy
reliance on pre-trained vision-language models. Moreover, existing methods do
not study positional encodings, thereby overlooking the multi-scale spatial
reasoning needed to capture fine-grained visuotactile correlations. We
introduce ViTaPEs, a transformer-based framework that robustly integrates
visual and tactile input data to learn task-agnostic representations for
visuotactile perception. Our approach exploits a novel multi-scale positional
encoding scheme to capture intra-modal structures, while simultaneously
modeling cross-modal cues. Unlike prior work, we provide provable guarantees in
visuotactile fusion, showing that our encodings are injective,
rigid-motion-equivariant, and information-preserving, validating these
properties empirically. Experiments on multiple large-scale real-world datasets
show that ViTaPEs not only surpasses state-of-the-art baselines across various
recognition tasks but also demonstrates zero-shot generalization to unseen,
out-of-domain scenarios. We further demonstrate the transfer-learning strength
of ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art
baselines in predicting grasp success. Project page:
https://sites.google.com/view/vitapes",2025-05-26,"Fotios Lygerakis, Ozan Özdenizci, Elmar Rückert",http://arxiv.org/pdf/2505.20032v1,cs.LG
Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions,"We observe a novel 'multiple-descent' phenomenon during the training process
of LSTM, in which the test loss goes through long cycles of up and down trend
multiple times after the model is overtrained. By carrying out asymptotic
stability analysis of the models, we found that the cycles in test loss are
closely associated with the phase transition process between order and chaos,
and the local optimal epochs are consistently at the critical transition point
between the two phases. More importantly, the global optimal epoch occurs at
the first transition from order to chaos, where the 'width' of the 'edge of
chaos' is the widest, allowing the best exploration of better weight
configurations for learning.",2025-05-26,"Wenbo Wei, Nicholas Chong Jia Le, Choy Heng Lai, Ling Feng",http://arxiv.org/pdf/2505.20030v1,cs.LG
Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain),"Transformer-based language models, though not explicitly trained to mimic
brain recordings, have demonstrated surprising alignment with brain activity.
Progress in these models-through increased size, instruction-tuning, and
multimodality-has led to better representational alignment with neural data.
Recently, a new class of instruction-tuned multimodal LLMs (MLLMs) have
emerged, showing remarkable zero-shot capabilities in open-ended multimodal
vision tasks. However, it is unknown whether MLLMs, when prompted with natural
instructions, lead to better brain alignment and effectively capture
instruction-specific representations. To address this, we first investigate
brain alignment, i.e., measuring the degree of predictivity of neural visual
activity using text output response embeddings from MLLMs as participants
engage in watching natural scenes. Experiments with 10 different instructions
show that MLLMs exhibit significantly better brain alignment than vision-only
models and perform comparably to non-instruction-tuned multimodal models like
CLIP. We also find that while these MLLMs are effective at generating
high-quality responses suitable to the task-specific instructions, not all
instructions are relevant for brain alignment. Further, by varying
instructions, we make the MLLMs encode instruction-specific visual concepts
related to the input image. This analysis shows that MLLMs effectively capture
count-related and recognition-related concepts, demonstrating strong alignment
with brain activity. Notably, the majority of the explained variance of the
brain encoding models is shared between MLLM embeddings of image captioning and
other instructions. These results suggest that enhancing MLLMs' ability to
capture task-specific information could lead to better differentiation between
various types of instructions, and thereby improving their precision in
predicting brain responses.",2025-05-26,"Subba Reddy Oota, Akshett Jindal, Ishani Mondal, Khushbu Pahwa, Satya Sai Srinath Namburi, Manish Shrivastava, Maneesh Singh, Bapi S. Raju, Manish Gupta",http://arxiv.org/pdf/2505.20029v1,cs.LG
Multi-modal brain encoding models for multi-modal stimuli,"Despite participants engaging in unimodal stimuli, such as watching images or
silent videos, recent work has demonstrated that multi-modal Transformer models
can predict visual brain activity impressively well, even with incongruent
modality representations. This raises the question of how accurately these
multi-modal models can predict brain activity when participants are engaged in
multi-modal stimuli. As these models grow increasingly popular, their use in
studying neural activity provides insights into how our brains respond to such
multi-modal naturalistic stimuli, i.e., where it separates and integrates
information across modalities through a hierarchy of early sensory regions to
higher cognition. We investigate this question by using multiple unimodal and
two types of multi-modal models-cross-modal and jointly pretrained-to determine
which type of model is more relevant to fMRI brain activity when participants
are engaged in watching movies. We observe that both types of multi-modal
models show improved alignment in several language and visual regions. This
study also helps in identifying which brain regions process unimodal versus
multi-modal information. We further investigate the contribution of each
modality to multi-modal alignment by carefully removing unimodal features one
by one from multi-modal representations, and find that there is additional
information beyond the unimodal embeddings that is processed in the visual and
language regions. Based on this investigation, we find that while for
cross-modal models, their brain alignment is partially attributed to the video
modality; for jointly pretrained models, it is partially attributed to both the
video and audio modalities. This serves as a strong motivation for the
neuroscience community to investigate the interpretability of these models for
deepening our understanding of multi-modal information processing in brain.",2025-05-26,"Subba Reddy Oota, Khushbu Pahwa, Mounika Marreddy, Maneesh Singh, Manish Gupta, Bapi S. Raju",http://arxiv.org/pdf/2505.20027v1,cs.LG
Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage,"We propose Gradient Inversion Transcript (GIT), a novel generative approach
for reconstructing training data from leaked gradients. GIT employs a
generative attack model, whose architecture is tailored to align with the
structure of the leaked model based on theoretical analysis. Once trained
offline, GIT can be deployed efficiently and only relies on the leaked
gradients to reconstruct the input data, rendering it applicable under various
distributed learning environments. When used as a prior for other iterative
optimization-based methods, GIT not only accelerates convergence but also
enhances the overall reconstruction quality. GIT consistently outperforms
existing methods across multiple datasets and demonstrates strong robustness
under challenging conditions, including inaccurate gradients, data distribution
shifts and discrepancies in model parameters.",2025-05-26,"Xinping Chen, Chen Liu",http://arxiv.org/pdf/2505.20026v1,cs.LG
Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare,"The rise of electronic health records (EHRs) has unlocked new opportunities
for medical research, but privacy regulations and data heterogeneity remain key
barriers to large-scale machine learning. Federated learning (FL) enables
collaborative modeling without sharing raw data, yet faces challenges in
harmonizing diverse clinical datasets. This paper presents a two-step data
alignment strategy integrating ontologies and large language models (LLMs) to
support secure, privacy-preserving FL in healthcare, demonstrating its
effectiveness in a real-world project involving semantic mapping of EHR data.",2025-05-26,"Natallia Kokash, Lei Wang, Thomas H. Gillespie, Adam Belloum, Paola Grosso, Sara Quinney, Lang Li, Bernard de Bono",http://arxiv.org/pdf/2505.20020v1,cs.LG
Linear Bandits with Non-i.i.d. Noise,"We study the linear stochastic bandit problem, relaxing the standard i.i.d.
assumption on the observation noise. As an alternative to this restrictive
assumption, we allow the noise terms across rounds to be sub-Gaussian but
interdependent, with dependencies that decay over time. To address this
setting, we develop new confidence sequences using a recently introduced
reduction scheme to sequential probability assignment, and use these to derive
a bandit algorithm based on the principle of optimism in the face of
uncertainty. We provide regret bounds for the resulting algorithm, expressed in
terms of the decay rate of the strength of dependence between observations.
Among other results, we show that our bounds recover the standard rates up to a
factor of the mixing time for geometrically mixing observation noise.",2025-05-26,"Baptiste Abélès, Eugenio Clerico, Hamish Flynn, Gergely Neu",http://arxiv.org/pdf/2505.20017v1,cs.LG
Data-Dependent Regret Bounds for Constrained MABs,"This paper initiates the study of data-dependent regret bounds in constrained
MAB settings. These bounds depend on the sequence of losses that characterize
the problem instance. Thus, they can be much smaller than classical
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret bounds, while being equivalent to
them in the worst case. Despite this, data-dependent regret bounds have been
completely overlooked in constrained MAB settings. The goal of this paper is to
answer the following question: Can data-dependent regret bounds be derived in
the presence of constraints? We answer this question affirmatively in
constrained MABs with adversarial losses and stochastic constraints.
Specifically, our main focus is on the most challenging and natural settings
with hard constraints, where the learner must ensure that the constraints are
always satisfied with high probability. We design an algorithm with a regret
bound consisting of two data-dependent terms. The first term captures the
difficulty of satisfying the constraints, while the second one encodes the
complexity of learning independently of the presence of constraints. We also
prove a lower bound showing that these two terms are not artifacts of our
specific approach and analysis, but rather the fundamental components that
inherently characterize the complexities of the problem. Finally, in designing
our algorithm, we also derive some novel results in the related (and easier)
soft constraints settings, which may be of independent interest.",2025-05-26,"Gianmarco Genalti, Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti",http://arxiv.org/pdf/2505.20010v1,cs.LG
TabPFN: One Model to Rule Them All?,"Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a
transformer-based deep learning model for regression and classification on
tabular data, which they claim ""outperforms all previous methods on datasets
with up to 10,000 samples by a wide margin, using substantially less training
time."" Furthermore, they have called TabPFN a ""foundation model"" for tabular
data, as it can support ""data generation, density estimation, learning reusable
embeddings and fine-tuning"". If these statements are well-supported, TabPFN may
have the potential to supersede existing modeling approaches on a wide range of
statistical tasks, mirroring a similar revolution in other areas of artificial
intelligence that began with the advent of large language models. In this
paper, we provide a tailored explanation of how TabPFN works for a statistics
audience, by emphasizing its interpretation as approximate Bayesian inference.
We also provide more evidence of TabPFN's ""foundation model"" capabilities: We
show that an out-of-the-box application of TabPFN vastly outperforms
specialized state-of-the-art methods for semi-supervised parameter estimation,
prediction under covariate shift, and heterogeneous treatment effect
estimation. We further show that TabPFN can outperform LASSO at sparse
regression and can break a robustness-efficiency trade-off in classification.
All experiments can be reproduced using the code provided at
https://github.com/qinglong-tian/tabpfn_study
(https://github.com/qinglong-tian/tabpfn_study).",2025-05-26,"Qiong Zhang, Yan Shuo Tan, Qinglong Tian, Pengfei Li",http://arxiv.org/pdf/2505.20003v1,cs.LG
Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents,"Large language models (LLMs) are revolutionizing education, with LLM-based
agents playing a key role in simulating student behavior. A major challenge in
student simulation is modeling the diverse learning patterns of students at
various cognitive levels. However, current LLMs, typically trained as ``helpful
assistants'', target at generating perfect responses. As a result, they
struggle to simulate students with diverse cognitive abilities, as they often
produce overly advanced answers, missing the natural imperfections that
characterize student learning and resulting in unrealistic simulations. To
address this issue, we propose a training-free framework for student
simulation. We begin by constructing a cognitive prototype for each student
using a knowledge graph, which captures their understanding of concepts from
past learning records. This prototype is then mapped to new tasks to predict
student performance. Next, we simulate student solutions based on these
predictions and iteratively refine them using a beam search method to better
replicate realistic mistakes. To validate our approach, we construct the
\texttt{Student\_100} dataset, consisting of $100$ students working on Python
programming and $5,000$ learning records. Experimental results show that our
method consistently outperforms baseline models, achieving $100\%$ improvement
in simulation accuracy.",2025-05-26,"Tao Wu, Jingyuan Chen, Wang Lin, Mengze Li, Yumeng Zhu, Ang Li, Kun Kuang, Fei Wu",http://arxiv.org/pdf/2505.19997v1,cs.LG
Learning Optimal Multimodal Information Bottleneck Representations,"Leveraging high-quality joint representations from multimodal data can
greatly enhance model performance in various machine-learning based
applications. Recent multimodal learning methods, based on the multimodal
information bottleneck (MIB) principle, aim to generate optimal MIB with
maximal task-relevant information and minimal superfluous information via
regularization. However, these methods often set ad hoc regularization weights
and overlook imbalanced task-relevant information across modalities, limiting
their ability to achieve optimal MIB. To address this gap, we propose a novel
multimodal learning framework, Optimal Multimodal Information Bottleneck
(OMIB), whose optimization objective guarantees the achievability of optimal
MIB by setting the regularization weight within a theoretically derived bound.
OMIB further addresses imbalanced task-relevant information by dynamically
adjusting regularization weights per modality, promoting the inclusion of all
task-relevant information. Moreover, we establish a solid
information-theoretical foundation for OMIB's optimization and implement it
under the variational approximation framework for computational efficiency.
Finally, we empirically validate the OMIB's theoretical properties on synthetic
data and demonstrate its superiority over the state-of-the-art benchmark
methods in various downstream tasks.",2025-05-26,"Qilong Wu, Yiyang Shao, Jun Wang, Xiaobo Sun",http://arxiv.org/pdf/2505.19996v1,cs.LG
Regret Analysis of Average-Reward Unichain MDPs via an Actor-Critic Approach,"Actor-Critic methods are widely used for their scalability, yet existing
theoretical guarantees for infinite-horizon average-reward Markov Decision
Processes (MDPs) often rely on restrictive ergodicity assumptions. We propose
NAC-B, a Natural Actor-Critic with Batching, that achieves order-optimal regret
of $\tilde{O}(\sqrt{T})$ in infinite-horizon average-reward MDPs under the
unichain assumption, which permits both transient states and periodicity. This
assumption is among the weakest under which the classic policy gradient theorem
remains valid for average-reward settings. NAC-B employs function approximation
for both the actor and the critic, enabling scalability to problems with large
state and action spaces. The use of batching in our algorithm helps mitigate
potential periodicity in the MDP and reduces stochasticity in gradient
estimates, and our analysis formalizes these benefits through the introduction
of the constants $C_{\text{hit}}$ and $C_{\text{tar}}$, which characterize the
rate at which empirical averages over Markovian samples converge to the
stationary distribution.",2025-05-26,"Swetha Ganesh, Vaneet Aggarwal",http://arxiv.org/pdf/2505.19986v1,cs.LG
Rethinking Probabilistic Circuit Parameter Learning,"Probabilistic Circuits (PCs) offer a computationally scalable framework for
generative modeling, supporting exact and efficient inference of a wide range
of probabilistic queries. While recent advances have significantly improved the
expressiveness and scalability of PCs, effectively training their parameters
remains a challenge. In particular, a widely used optimization method,
full-batch Expectation-Maximization (EM), requires processing the entire
dataset before performing a single update, making it ineffective for large
datasets. While empirical extensions to the mini-batch setting have been
proposed, it remains unclear what objective these algorithms are optimizing,
making it difficult to assess their theoretical soundness. This paper bridges
the gap by establishing a novel connection between the general EM objective and
the standard full-batch EM algorithm. Building on this, we derive a
theoretically grounded generalization to the mini-batch setting and demonstrate
its effectiveness through preliminary empirical results.",2025-05-26,"Anji Liu, Guy Van den Broeck",http://arxiv.org/pdf/2505.19982v1,cs.LG
Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models,"Fully decentralized training of machine learning models offers significant
advantages in scalability, robustness, and fault tolerance. However, achieving
differential privacy (DP) in such settings is challenging due to the absence of
a central aggregator and varying trust assumptions among nodes. In this work,
we present a novel privacy analysis of decentralized gossip-based averaging
algorithms with additive node-level noise, both with and without secure
summation over each node's direct neighbors. Our main contribution is a new
analytical framework based on a linear systems formulation that accurately
characterizes privacy leakage across these scenarios. This framework
significantly improves upon prior analyses, for example, reducing the R\'enyi
DP parameter growth from $O(T^2)$ to $O(T)$, where $T$ is the number of
training rounds. We validate our analysis with numerical results demonstrating
superior DP bounds compared to existing approaches. We further illustrate our
analysis with a logistic regression experiment on MNIST image classification in
a fully decentralized setting, demonstrating utility comparable to central
aggregation methods.",2025-05-26,"Antti Koskela, Tejas Kulkarni",http://arxiv.org/pdf/2505.19969v1,cs.LG
Learning to Select In-Context Demonstration Preferred by Large Language Model,"In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks during inference using only a few demonstrations. However, ICL
performance is highly dependent on the selection of these demonstrations.
Recent work explores retrieval-based methods for selecting query-specific
demonstrations, but these approaches often rely on surrogate objectives such as
metric learning, failing to directly optimize ICL performance. Consequently,
they struggle to identify truly beneficial demonstrations. Moreover, their
discriminative retrieval paradigm is ineffective when the candidate pool lacks
sufficient high-quality demonstrations. To address these challenges, we propose
GenICL, a novel generative preference learning framework that leverages LLM
feedback to directly optimize demonstration selection for ICL. Experiments on
19 datasets across 11 task categories demonstrate that GenICL achieves superior
performance than existing methods in selecting the most effective
demonstrations, leading to better ICL performance.",2025-05-26,"Zheng Zhang, Shaocheng Lan, Lei Song, Jiang Bian, Yexin Li, Kan Ren",http://arxiv.org/pdf/2505.19966v1,cs.LG
The Limits of Preference Data for Post-Training,"Recent progress in strengthening the capabilities of large language models
has stemmed from applying reinforcement learning to domains with automatically
verifiable outcomes. A key question is whether we can similarly use RL to
optimize for outcomes in domains where evaluating outcomes inherently requires
human feedback; for example, in tasks like deep research and trip planning,
outcome evaluation is qualitative and there are many possible degrees of
success. One attractive and scalable modality for collecting human feedback is
preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$
given outcomes, which one is preferred. In this work, we study a critical
roadblock: preference data fundamentally and significantly limits outcome-based
optimization. Even with idealized preference data (infinite, noiseless, and
online), the use of ordinal feedback can prevent obtaining even approximately
optimal solutions. We formalize this impossibility using voting theory, drawing
an analogy between how a model chooses to answer a query with how voters choose
a candidate to elect. This indicates that grounded human scoring and
algorithmic innovations are necessary for extending the success of RL
post-training to domains demanding human feedback. We also explore why these
limitations have disproportionately impacted RLHF when it comes to eliciting
reasoning behaviors (e.g., backtracking) versus situations where RLHF has been
historically successful (e.g., instruction-tuning and safety training), finding
that the limitations of preference data primarily suppress RLHF's ability to
elicit robust strategies -- a class that encompasses most reasoning behaviors.",2025-05-26,"Eric Zhao, Jessica Dai, Pranjal Awasthi",http://arxiv.org/pdf/2505.19964v1,cs.LG
MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research,"Recent advancements in AI agents have demonstrated their growing potential to
drive and support scientific discovery. In this work, we introduce MLR-Bench, a
comprehensive benchmark for evaluating AI agents on open-ended machine learning
research. MLR-Bench includes three key components: (1) 201 research tasks
sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)
MLR-Judge, an automated evaluation framework combining LLM-based reviewers with
carefully designed review rubrics to assess research quality; and (3)
MLR-Agent, a modular agent scaffold capable of completing research tasks
through four stages: idea generation, proposal formulation, experimentation,
and paper writing. Our framework supports both stepwise assessment across these
distinct research stages, and end-to-end evaluation of the final research
paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced
coding agent, finding that while LLMs are effective at generating coherent
ideas and well-structured papers, current coding agents frequently (e.g., in
80% of the cases) produce fabricated or invalidated experimental
results--posing a major barrier to scientific reliability. We validate
MLR-Judge through human evaluation, showing high agreement with expert
reviewers, supporting its potential as a scalable tool for research evaluation.
We open-source MLR-Bench to help the community benchmark, diagnose, and improve
AI research agents toward trustworthy and transparent scientific discovery.",2025-05-26,"Hui Chen, Miao Xiong, Yujie Lu, Wei Han, Ailin Deng, Yufei He, Jiaying Wu, Yibo Li, Yue Liu, Bryan Hooi",http://arxiv.org/pdf/2505.19955v1,cs.LG
An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning,"The differential diagnosis of neurodegenerative dementias is a challenging
clinical task, mainly because of the overlap in symptom presentation and the
similarity of patterns observed in structural neuroimaging. To improve
diagnostic efficiency and accuracy, deep learning-based methods such as
Convolutional Neural Networks and Vision Transformers have been proposed for
the automatic classification of brain MRIs. However, despite their strong
predictive performance, these models find limited clinical utility due to their
opaque decision making. In this work, we propose a framework that integrates
two core components to enhance diagnostic transparency. First, we introduce a
modular pipeline for converting 3D T1-weighted brain MRIs into textual
radiology reports. Second, we explore the potential of modern Large Language
Models (LLMs) to assist clinicians in the differential diagnosis between
Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based
on the generated reports. To bridge the gap between predictive accuracy and
explainability, we employ reinforcement learning to incentivize diagnostic
reasoning in LLMs. Without requiring supervised reasoning traces or
distillation from larger models, our approach enables the emergence of
structured diagnostic rationales grounded in neuroimaging findings. Unlike
post-hoc explainability methods that retrospectively justify model decisions,
our framework generates diagnostic rationales as part of the inference
process-producing causally grounded explanations that inform and guide the
model's decision-making process. In doing so, our framework matches the
diagnostic performance of existing deep learning methods while offering
rationales that support its diagnostic conclusions.",2025-05-26,"Andrew Zamai, Nathanael Fijalkow, Boris Mansencal, Laurent Simon, Eloi Navet, Pierrick Coupe",http://arxiv.org/pdf/2505.19954v1,cs.LG
Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions,"Large language models (LLMs) have demonstrated remarkable reasoning
capabilities in math and coding, often bolstered by post-training on the
chain-of-thoughts (CoTs) generated by stronger models. However, existing
strategies for curating such training data predominantly rely on heuristics,
limiting generalizability and failing to capture subtleties underlying in data.
To address these limitations, we leverage influence functions to systematically
attribute LLMs' reasoning ability on math and coding to individual training
examples, sequences, and tokens, enabling deeper insights into effective data
characteristics. Our Influence-based Reasoning Attribution (Infra) uncovers
nontrivial cross-domain effects across math and coding tasks: high-difficulty
math examples improve both math and code reasoning, while low-difficulty code
tasks most effectively benefit code reasoning. Based on these findings, we
introduce a simple yet effective dataset reweighting strategy by flipping task
difficulty, which doubles AIME24 accuracy from 10\% to 20\% and boosts
LiveCodeBench accuracy from 33.8\% to 35.3\% for Qwen2.5-7B-Instruct. Moreover,
our fine-grained attribution reveals that the sequence-level exploratory
behaviors enhance reasoning performance in both math and code, and the
token-level influence patterns are distinct for math and code reasoning: the
former prefers natural language logic connectors and the latter emphasizes
structural syntax.",2025-05-26,"Siqi Kou, Qingyuan Tian, Hanwen Xu, Zihao Zeng, Zhijie Deng",http://arxiv.org/pdf/2505.19949v1,cs.LG
SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection,"Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for
imaging macromolecular complexes in their near-native states. However, the
localization of 3D particles in cellular environments still presents a
significant challenge due to low signal-to-noise ratios and missing wedge
artifacts. Deep learning approaches have shown great potential, but they need
huge amounts of data, which can be a challenge in cryo-ET scenarios where
labeled data is often scarce. In this paper, we propose a novel Self-augmented
and Self-interpreted (SaSi) deep learning approach towards few-shot particle
detection in 3D cryo-ET images. Our method builds upon self-augmentation
techniques to further boost data utilization and introduces a self-interpreted
segmentation strategy for alleviating dependency on labeled data, hence
improving generalization and robustness. As demonstrated by experiments
conducted on both simulated and real-world cryo-ET datasets, the SaSi approach
significantly outperforms existing state-of-the-art methods for particle
localization. This research increases understanding of how to detect particles
with very few labels in cryo-ET and thus sets a new benchmark for few-shot
learning in structural biology.",2025-05-26,"Gokul Adethya, Bhanu Pratyush Mantha, Tianyang Wang, Xingjian Li, Min Xu",http://arxiv.org/pdf/2505.19948v1,cs.LG
Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees,"Open-weight LLM zoos provide access to numerous high-quality models, but
selecting the appropriate model for specific tasks remains challenging and
requires technical expertise. Most users simply want factually correct, safe,
and satisfying responses without concerning themselves with model
technicalities, while inference service providers prioritize minimizing
operating costs. These competing interests are typically mediated through
service level agreements (SLAs) that guarantee minimum service quality. We
introduce MESS+, a stochastic optimization algorithm for cost-optimal LLM
request routing while providing rigorous SLA compliance guarantees. MESS+
learns request satisfaction probabilities of LLMs in real-time as users
interact with the system, based on which model selection decisions are made by
solving a per-request optimization problem. Our algorithm includes a novel
combination of virtual queues and request satisfaction prediction, along with a
theoretical analysis of cost optimality and constraint satisfaction. Across a
wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x
cost savings compared to existing LLM routing techniques.",2025-05-26,"Herbert Woisetschläger, Ryan Zhang, Shiqiang Wang, Hans-Arno Jacobsen",http://arxiv.org/pdf/2505.19947v1,cs.LG
Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^π$-Realizable MDPs,"We study the problem of offline imitation learning in Markov decision
processes (MDPs), where the goal is to learn a well-performing policy given a
dataset of state-action pairs generated by an expert policy. Complementing a
recent line of work on this topic that assumes the expert belongs to a
tractable class of known policies, we approach this problem from a new angle
and leverage a different type of structural assumption about the environment.
Specifically, for the class of linear $Q^\pi$-realizable MDPs, we introduce a
new algorithm called saddle-point offline imitation learning (\SPOIL), which is
guaranteed to match the performance of any expert up to an additive error
$\varepsilon$ with access to $\mathcal{O}(\varepsilon^{-2})$ samples. Moreover,
we extend this result to possibly non-linear $Q^\pi$-realizable MDPs at the
cost of a worse sample complexity of order $\mathcal{O}(\varepsilon^{-4})$.
Finally, our analysis suggests a new loss function for training critic networks
from expert data in deep imitation learning. Empirical evaluations on standard
benchmarks demonstrate that the neural net implementation of \SPOIL is superior
to behavior cloning and competitive with state-of-the-art algorithms.",2025-05-26,"Antoine Moulin, Gergely Neu, Luca Viano",http://arxiv.org/pdf/2505.19946v1,cs.LG
Can Visual Encoder Learn to See Arrows?,"The diagram is a visual representation of a relationship illustrated with
edges (lines or arrows), which is widely used in industrial and scientific
communication. Although recognizing diagrams is essential for vision language
models (VLMs) to comprehend domain-specific knowledge, recent studies reveal
that many VLMs fail to identify edges in images. We hypothesize that these
failures stem from an over-reliance on textual and positional biases,
preventing VLMs from learning explicit edge features. Based on this idea, we
empirically investigate whether the image encoder in VLMs can learn edge
representation through training on a diagram dataset in which edges are biased
neither by textual nor positional information. To this end, we conduct
contrastive learning on an artificially generated diagram--caption dataset to
train an image encoder and evaluate its diagram-related features on three
tasks: probing, image retrieval, and captioning. Our results show that the
finetuned model outperforms pretrained CLIP in all tasks and surpasses
zero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings
confirm that eliminating textual and positional biases fosters accurate edge
recognition in VLMs, offering a promising path for advancing diagram
understanding.",2025-05-26,"Naoyuki Terashita, Yusuke Tozaki, Hideaki Omote, Congkha Nguyen, Ryosuke Nakamoto, Yuta Koreeda, Hiroaki Ozaki",http://arxiv.org/pdf/2505.19944v1,cs.LG
Beyond Freezing: Sparse Tuning Enhances Plasticity in Continual Learning with Pre-Trained Models,"Continual Learning with Pre-trained Models holds great promise for efficient
adaptation across sequential tasks. However, most existing approaches freeze
PTMs and rely on auxiliary modules like prompts or adapters, limiting model
plasticity and leading to suboptimal generalization when facing significant
distribution shifts. While full fine-tuning can improve adaptability, it risks
disrupting crucial pre-trained knowledge. In this paper, we propose Mutual
Information-guided Sparse Tuning (MIST), a plug-and-play method that
selectively updates a small subset of PTM parameters, less than 5%, based on
sensitivity to mutual information objectives. MIST enables effective
task-specific adaptation while preserving generalization. To further reduce
interference, we introduce strong sparsity regularization by randomly dropping
gradients during tuning, resulting in fewer than 0.5% of parameters being
updated per step. Applied before standard freeze-based methods, MIST
consistently boosts performance across diverse continual learning benchmarks.
Experiments show that integrating our method into multiple baselines yields
significant performance gains. Our code is available at
https://github.com/zhwhu/MIST.",2025-05-26,"Huan Zhang, Fan Lyu, Shuyu Dong, Shenghua Fan, Yujin Zheng, Dingwen Wang",http://arxiv.org/pdf/2505.19943v1,cs.LG
Task-Oriented Low-Label Semantic Communication With Self-Supervised Learning,"Task-oriented semantic communication enhances transmission efficiency by
conveying semantic information rather than exact messages. Deep learning
(DL)-based semantic communication can effectively cultivate the essential
semantic knowledge for semantic extraction, transmission, and interpretation by
leveraging massive labeled samples for downstream task training. In this paper,
we propose a self-supervised learning-based semantic communication framework
(SLSCom) to enhance task inference performance, particularly in scenarios with
limited access to labeled samples. Specifically, we develop a task-relevant
semantic encoder using unlabeled samples, which can be collected by devices in
real-world edge networks. To facilitate task-relevant semantic extraction, we
introduce self-supervision for learning contrastive features and formulate the
information bottleneck (IB) problem to balance the tradeoff between the
informativeness of the extracted features and task inference performance. Given
the computational challenges of the IB problem, we devise a practical and
effective solution by employing self-supervised classification and
reconstruction pretext tasks. We further propose efficient joint training
methods to enhance end-to-end inference accuracy over wireless channels, even
with few labeled samples. We evaluate the proposed framework on image
classification tasks over multipath wireless channels. Extensive simulation
results demonstrate that SLSCom significantly outperforms conventional digital
coding methods and existing DL-based approaches across varying labeled data set
sizes and SNR conditions, even when the unlabeled samples are irrelevant to the
downstream tasks.",2025-05-26,"Run Gu, Wei Xu, Zhaohui Yang, Dusit Niyato, Aylin Yener",http://arxiv.org/pdf/2505.19940v1,cs.LG
Logic Gate Neural Networks are Good for Verification,"Learning-based systems are increasingly deployed across various domains, yet
the complexity of traditional neural networks poses significant challenges for
formal verification. Unlike conventional neural networks, learned Logic Gate
Networks (LGNs) replace multiplications with Boolean logic gates, yielding a
sparse, netlist-like architecture that is inherently more amenable to symbolic
verification, while still delivering promising performance. In this paper, we
introduce a SAT encoding for verifying global robustness and fairness in LGNs.
We evaluate our method on five benchmark datasets, including a newly
constructed 5-class variant, and find that LGNs are both verification-friendly
and maintain strong predictive performance.",2025-05-26,"Fabian Kresse, Emily Yu, Christoph H. Lampert, Thomas A. Henzinger",http://arxiv.org/pdf/2505.19932v1,cs.LG
Cellwise and Casewise Robust Covariance in High Dimensions,"The sample covariance matrix is a cornerstone of multivariate statistics, but
it is highly sensitive to outliers. These can be casewise outliers, such as
cases belonging to a different population, or cellwise outliers, which are
deviating cells (entries) of the data matrix. Recently some robust covariance
estimators have been developed that can handle both types of outliers, but
their computation is only feasible up to at most 20 dimensions. To remedy this
we propose the cellRCov method, a robust covariance estimator that
simultaneously handles casewise outliers, cellwise outliers, and missing data.
It relies on a decomposition of the covariance on principal and orthogonal
subspaces, leveraging recent work on robust PCA. It also employs a ridge-type
regularization to stabilize the estimated covariance matrix. We establish some
theoretical properties of cellRCov, including its casewise and cellwise
influence functions as well as consistency and asymptotic normality. A
simulation study demonstrates the superior performance of cellRCov in
contaminated and missing data scenarios. Furthermore, its practical utility is
illustrated in a real-world application to anomaly detection. We also construct
and illustrate the cellRCCA method for robust and regularized canonical
correlation analysis.",2025-05-26,"Fabio Centofanti, Mia Hubert, Peter J. Rousseeuw",http://arxiv.org/pdf/2505.19925v1,cs.LG
Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL,"Offline reinforcement learning (RL) aims to learn an effective policy from a
static dataset. To alleviate extrapolation errors, existing studies often
uniformly regularize the value function or policy updates across all states.
However, due to substantial variations in data quality, the fixed
regularization strength often leads to a dilemma: Weak regularization strength
fails to address extrapolation errors and value overestimation, while strong
regularization strength shifts policy learning toward behavior cloning,
impeding potential performance enabled by Bellman updates. To address this
issue, we propose the selective state-adaptive regularization method for
offline RL. Specifically, we introduce state-adaptive regularization
coefficients to trust state-level Bellman-driven results, while selectively
applying regularization on high-quality actions, aiming to avoid performance
degradation caused by tight constraints on low-quality actions. By establishing
a connection between the representative value regularization method, CQL, and
explicit policy constraint methods, we effectively extend selective
state-adaptive regularization to these two mainstream offline RL approaches.
Extensive experiments demonstrate that the proposed method significantly
outperforms the state-of-the-art approaches in both offline and
offline-to-online settings on the D4RL benchmark.",2025-05-26,"Qin-Wen Luo, Ming-Kun Xie, Ye-Wen Wang, Sheng-Jun Huang",http://arxiv.org/pdf/2505.19923v1,cs.LG
APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization,"We present Adjacent Possible Exploration (APE), a simple yet effective method
for adapting large language models to specific tasks using minimal
computational resources. Unlike traditional fine-tuning that requires extensive
compute, APE iteratively fine-tunes models on small, carefully selected data
batches (200 examples), retaining only improvements. On news summarization, APE
achieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,
matching or exceeding more complex methods like LoRA while remaining
conceptually simple. Our approach is particularly valuable for researchers and
practitioners with limited computational resources. We provide open-source code
and demonstrate APE's effectiveness through both automatic metrics and human
evaluation. While inspired by evolutionary theory's ""adjacent possible"", APE's
core insight has a very practical application: small, iterative data
perturbations can efficiently guide LLMs toward task-specific performance
without expensive retraining.",2025-05-26,Javier Marín,http://arxiv.org/pdf/2505.19912v1,cs.LG
ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining,"Large language model pretraining is compute-intensive, yet many tokens
contribute marginally to learning, resulting in inefficiency. We introduce
Efficient Selective Language Modeling (ESLM), a risk-aware algorithm that
improves training efficiency and distributional robustness by performing online
token-level batch selection. ESLM leverages per-token statistics (e.g., entropy
or loss) and applies value-at-risk thresholding to retain only the most
informative tokens per batch. This data-centric mechanism reshapes the training
loss, prioritizing high-risk tokens and eliminating redundant gradient
computation. We frame ESLM as a bilevel game: the model competes with a masking
adversary that selects worst-case token subsets under a constrained
thresholding rule. In the loss-based setting, ESLM recovers conditional
value-at-risk loss minimization, providing a principled connection to
distributionally robust optimization. We extend our approach to Ada-ESLM, which
adaptively tunes the selection confidence during training. Experiments on GPT-2
pretraining show that ESLM significantly reduces training FLOPs while
maintaining or improving both perplexity and downstream performance compared to
baselines. Our approach also scales across model sizes, pretraining corpora,
and integrates naturally with knowledge distillation.",2025-05-26,"Melis Ilayda Bal, Volkan Cevher, Michael Muehlebach",http://arxiv.org/pdf/2505.19893v1,cs.LG
Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations,"Federated Learning (FL) aims to train models across decentralized clients or
devices holding local data without the need for centralized data collection,
thus enhancing data privacy and security. However, achieving both
generalization and personalization in heterogeneous settings remains a
significant challenge. To address this, we introduce FedOT, a novel approach
that leverages black-box foundation models. FedOT shares only a global
task-dependent classifier across clients while locally adapting features
through orthogonal transformations. By enforcing orthogonality, FedOT mitigates
gradient conflicts across diverse clients, preserves semantic integrity, and
achieves robust performance even in the presence of substantial data
heterogeneity. The strategy of combining global and local parameters enables a
more balanced approach for both generalization and personalization,
outperforming baseline FL methods across multiple benchmarks. Furthermore, our
extensive analysis confirms that joint optimization of global classifiers and
local orthogonal transformations yields superior performance and suggests
broader applicability.",2025-05-26,"Eun Gyung Kong, Je Won Yeom, Yonghoon Jeon, Taesup Kim",http://arxiv.org/pdf/2505.19888v1,cs.LG
Deep Active Inference Agents for Delayed and Long-Horizon Environments,"With the recent success of world-model agents, which extend the core idea of
model-based reinforcement learning by learning a differentiable model for
sample-efficient control across diverse tasks, active inference (AIF) offers a
complementary, neuroscience-grounded paradigm that unifies perception,
learning, and action within a single probabilistic framework powered by a
generative model. Despite this promise, practical AIF agents still rely on
accurate immediate predictions and exhaustive planning, a limitation that is
exacerbated in delayed environments requiring plans over long horizons, tens to
hundreds of steps. Moreover, most existing agents are evaluated on robotic or
vision benchmarks which, while natural for biological agents, fall short of
real-world industrial complexity. We address these limitations with a
generative-policy architecture featuring (i) a multi-step latent transition
that lets the generative model predict an entire horizon in a single
look-ahead, (ii) an integrated policy network that enables the transition and
receives gradients of the expected free energy, (iii) an alternating
optimization scheme that updates model and policy from a replay buffer, and
(iv) a single gradient step that plans over long horizons, eliminating
exhaustive planning from the control loop. We evaluate our agent in an
environment that mimics a realistic industrial scenario with delayed and
long-horizon settings. The empirical results confirm the effectiveness of the
proposed approach, demonstrating the coupled world-model with the AIF formalism
yields an end-to-end probabilistic controller capable of effective decision
making in delayed, long-horizon settings without handcrafted rewards or
expensive planning.",2025-05-26,"Yavar Taheri Yeganeh, Mohsen Jafari, Andrea Matta",http://arxiv.org/pdf/2505.19867v1,cs.LG
HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation,"Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of
large language models (LLMs) by leveraging self-generated responses for
self-training. Recent studies have incorporated reward models to guide response
selection or decoding, aiming to obtain higher-quality data. However, they
typically allocate a uniform sampling budget across all problems, overlooking
the varying utility of problems at different difficulty levels. In this work,
we conduct an empirical study and find that problems near the boundary of the
LLM's reasoning capability offer significantly greater learning utility than
both easy and overly difficult ones. To identify and exploit such problems, we
propose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.
Given a fixed sampling budget, HS-STaR first performs lightweight pre-sampling
with a reward-guided difficulty estimation strategy to efficiently identify
boundary-level problems. Subsequently, it dynamically reallocates the remaining
budget toward these high-utility problems during a re-sampling phase,
maximizing the generation of valuable training data. Extensive experiments
across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR
significantly outperforms other baselines without requiring additional sampling
budget.",2025-05-26,"Feng Xiong, Hongling Xu, Yifei Wang, Runxi Cheng, Yong Wang, Xiangxiang Chu",http://arxiv.org/pdf/2505.19866v1,cs.LG
FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields,"We introduce FruitNeRF++, a novel fruit-counting approach that combines
contrastive learning with neural radiance fields to count fruits from
unstructured input photographs of orchards. Our work is based on FruitNeRF,
which employs a neural semantic field combined with a fruit-specific clustering
approach. The requirement for adaptation for each fruit type limits the
applicability of the method, and makes it difficult to use in practice. To lift
this limitation, we design a shape-agnostic multi-fruit counting framework,
that complements the RGB and semantic data with instance masks predicted by a
vision foundation model. The masks are used to encode the identity of each
fruit as instance embeddings into a neural instance field. By volumetrically
sampling the neural fields, we extract a point cloud embedded with the instance
features, which can be clustered in a fruit-agnostic manner to obtain the fruit
count. We evaluate our approach using a synthetic dataset containing apples,
plums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark
apple dataset. Our results demonstrate that FruitNeRF++ is easier to control
and compares favorably to other state-of-the-art methods.",2025-05-26,"Lukas Meyer, Andrei-Timotei Ardelean, Tim Weyrich, Marc Stamminger",http://arxiv.org/pdf/2505.19863v1,cs.LG
REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models,"Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks
but often face the challenge of overthinking, leading to substantially high
inference costs. Existing approaches synthesize shorter reasoning responses for
LRMs to learn, but are inefficient for online usage due to the time-consuming
data generation and filtering processes. Meanwhile, online reinforcement
learning mainly adopts a length reward to encourage short reasoning responses,
but tends to lose the reflection ability and harm the performance. To address
these issues, we propose REA-RL, which introduces a small reflection model for
efficient scaling in online training, offering both parallel sampling and
sequential revision. Besides, a reflection reward is designed to further
prevent LRMs from favoring short yet non-reflective responses. Experiments show
that both methods maintain or enhance performance while significantly improving
inference efficiency. Their combination achieves a good balance between
performance and efficiency, reducing inference costs by 35% without
compromising performance. Further analysis demonstrates that our methods are
effective by maintaining reflection frequency for hard problems while
appropriately reducing it for simpler ones without losing reflection ability.
Codes are available at https://github.com/hexuandeng/REA-RL.",2025-05-26,"Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Jun Rao, Min Zhang",http://arxiv.org/pdf/2505.19862v1,cs.LG
Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?,"Large language Model (LLM) unlearning, i.e., selectively removing information
from LLMs, is vital for responsible model deployment. Differently, LLM
knowledge editing aims to modify LLM knowledge instead of removing it. Though
editing and unlearning seem to be two distinct tasks, we find there is a tight
connection between them. In this paper, we conceptualize unlearning as a
special case of editing where information is modified to a refusal or ""empty
set"" $\emptyset$ response, signifying its removal. This paper thus investigates
if knowledge editing techniques are strong baselines for LLM unlearning. We
evaluate state-of-the-art (SOTA) editing methods (e.g., ROME, MEMIT, GRACE,
WISE, and AlphaEdit) against existing unlearning approaches on pretrained and
finetuned knowledge. Results show certain editing methods, notably WISE and
AlphaEdit, are effective unlearning baselines, especially for pretrained
knowledge, and excel in generating human-aligned refusal answers. To better
adapt editing methods for unlearning applications, we propose practical recipes
including self-improvement and query merging. The former leverages the LLM's
own in-context learning ability to craft a more human-aligned unlearning
target, and the latter enables ROME and MEMIT to perform well in unlearning
longer sample sequences. We advocate for the unlearning community to adopt SOTA
editing methods as baselines and explore unlearning from an editing perspective
for more holistic LLM memory control.",2025-05-26,"Zexi Li, Xiangzhu Wang, William F. Shen, Meghdad Kurmanji, Xinchi Qiu, Dongqi Cai, Chao Wu, Nicholas D. Lane",http://arxiv.org/pdf/2505.19855v1,cs.LG
DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning,"Sparse-reward reinforcement learning (RL) can model a wide range of highly
complex tasks. Solving sparse-reward tasks is RL's core premise - requiring
efficient exploration coupled with long-horizon credit assignment - and
overcoming these challenges is key for building self-improving agents with
superhuman ability. We argue that solving complex and high-dimensional tasks
requires solving simpler tasks that are relevant to the target task. In
contrast, most prior work designs strategies for selecting exploratory tasks
with the objective of solving any task, making exploration of challenging
high-dimensional, long-horizon tasks intractable. We find that the sense of
direction, necessary for effective exploration, can be extracted from existing
RL algorithms, without needing any prior information. Based on this finding, we
propose a method for directed sparse-reward goal-conditioned very long-horizon
RL (DISCOVER), which selects exploratory goals in the direction of the target
task. We connect DISCOVER to principled exploration in bandits, formally
bounding the time until the target task becomes achievable in terms of the
agent's initial distance to the target, but independent of the volume of the
space of all tasks. Empirically, we perform a thorough evaluation in
high-dimensional environments. We find that the directed goal selection of
DISCOVER solves exploration problems that are beyond the reach of prior
state-of-the-art exploration methods in RL.",2025-05-26,"Leander Diaz-Bone, Marco Bagatella, Jonas Hübotter, Andreas Krause",http://arxiv.org/pdf/2505.19850v1,cs.LG
PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints,"Air quality forecasting (AQF) is critical for public health and environmental
management, yet remains challenging due to the complex interplay of emissions,
meteorology, and chemical transformations. Traditional numerical models, such
as CMAQ and WRF-Chem, provide physically grounded simulations but are
computationally expensive and rely on uncertain emission inventories. Deep
learning models, while computationally efficient, often struggle with
generalization due to their lack of physical constraints. To bridge this gap,
we propose PCDCNet, a surrogate model that integrates numerical modeling
principles with deep learning. PCDCNet explicitly incorporates emissions,
meteorological influences, and domain-informed constraints to model pollutant
formation, transport, and dissipation. By combining graph-based spatial
transport modeling, recurrent structures for temporal accumulation, and
representation enhancement for local interactions, PCDCNet achieves
state-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3
forecasting while significantly reducing computational costs. Furthermore, our
model is deployed in an online platform, providing free, real-time air quality
forecasts, demonstrating its scalability and societal impact. By aligning deep
learning with physical consistency, PCDCNet offers a practical and
interpretable solution for AQF, enabling informed decision-making for both
personal and regulatory applications.",2025-05-26,"Shuo Wang, Yun Cheng, Qingye Meng, Olga Saukh, Jiang Zhang, Jingfang Fan, Yuanting Zhang, Xingyuan Yuan, Lothar Thiele",http://arxiv.org/pdf/2505.19842v1,cs.LG
Efficient Deconvolution in Populational Inverse Problems,"This work is focussed on the inversion task of inferring the distribution
over parameters of interest leading to multiple sets of observations. The
potential to solve such distributional inversion problems is driven by
increasing availability of data, but a major roadblock is blind deconvolution,
arising when the observational noise distribution is unknown. However, when
data originates from collections of physical systems, a population, it is
possible to leverage this information to perform deconvolution. To this end, we
propose a methodology leveraging large data sets of observations, collected
from different instantiations of the same physical processes, to simultaneously
deconvolve the data corrupting noise distribution, and to identify the
distribution over model parameters defining the physical processes. A
parameter-dependent mathematical model of the physical process is employed. A
loss function characterizing the match between the observed data and the output
of the mathematical model is defined; it is minimized as a function of the both
the parameter inputs to the model of the physics and the parameterized
observational noise. This coupled problem is addressed with a modified gradient
descent algorithm that leverages specific structure in the noise model.
Furthermore, a new active learning scheme is proposed, based on adaptive
empirical measures, to train a surrogate model to be accurate in parameter
regions of interest; this approach accelerates computation and enables
automatic differentiation of black-box, potentially nondifferentiable, code
computing parameter-to-solution maps. The proposed methodology is demonstrated
on porous medium flow, damped elastodynamics, and simplified models of
atmospheric dynamics.",2025-05-26,"Arnaud Vadeboncoeur, Mark Girolami, Andrew M. Stuart",http://arxiv.org/pdf/2505.19841v1,cs.LG
"One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP","Deep Neural Networks (DNNs) have achieved widespread success yet remain prone
to adversarial attacks. Typically, such attacks either involve frequent queries
to the target model or rely on surrogate models closely mirroring the target
model -- often trained with subsets of the target model's training data -- to
achieve high attack success rates through transferability. However, in
realistic scenarios where training data is inaccessible and excessive queries
can raise alarms, crafting adversarial examples becomes more challenging. In
this paper, we present UnivIntruder, a novel attack framework that relies
solely on a single, publicly available CLIP model and publicly available
datasets. By using textual concepts, UnivIntruder generates universal,
transferable, and targeted adversarial perturbations that mislead DNNs into
misclassifying inputs into adversary-specified classes defined by textual
concepts.
  Our extensive experiments show that our approach achieves an Attack Success
Rate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, significantly
outperforming existing transfer-based methods. Additionally, we reveal
real-world vulnerabilities, showing that even without querying target models,
UnivIntruder compromises image search engines like Google and Baidu with ASR
rates up to 84%, and vision language models like GPT-4 and Claude-3.5 with ASR
rates up to 80%. These findings underscore the practicality of our attack in
scenarios where traditional avenues are blocked, highlighting the need to
reevaluate security paradigms in AI applications.",2025-05-26,"Binyan Xu, Xilin Dai, Di Tang, Kehuan Zhang",http://arxiv.org/pdf/2505.19840v1,cs.LG
Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications,"Multi-Agent Reinforcement Learning (MARL) has shown great potential as an
adaptive solution for addressing modern cybersecurity challenges. MARL enables
decentralized, adaptive, and collaborative defense strategies and provides an
automated mechanism to combat dynamic, coordinated, and sophisticated threats.
This survey investigates the current state of research in MARL applications for
automated cyber defense (ACD), focusing on intruder detection and lateral
movement containment. Additionally, it examines the role of Autonomous
Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and
validating MARL agents. Finally, the paper outlines existing challenges, such
as scalability and adversarial robustness, and proposes future research
directions. This also discusses how MARL integrates in AICA to provide
adaptive, scalable, and dynamic solutions to counter the increasingly
sophisticated landscape of cyber threats. It highlights the transformative
potential of MARL in areas like intrusion detection and lateral movement
containment, and underscores the value of Cyber Gyms for training and
validation of AICA.",2025-05-26,"Christoph R. Landolt, Christoph Würsch, Roland Meier, Alain Mermoud, Julian Jang-Jaccard",http://arxiv.org/pdf/2505.19837v1,cs.LG
Revisiting Glorot Initialization for Long-Range Linear Recurrences,"Proper initialization is critical for Recurrent Neural Networks (RNNs),
particularly in long-range reasoning tasks, where repeated application of the
same weight matrix can cause vanishing or exploding signals. A common baseline
for linear recurrences is Glorot initialization, designed to ensure stable
signal propagation--but derived under the infinite-width, fixed-length
regime--an unrealistic setting for RNNs processing long sequences. In this
work, we show that Glorot initialization is in fact unstable: small positive
deviations in the spectral radius are amplified through time and cause the
hidden state to explode. Our theoretical analysis demonstrates that sequences
of length $t = O(\sqrt{n})$, where $n$ is the hidden width, are sufficient to
induce instability. To address this, we propose a simple, dimension-aware
rescaling of Glorot that shifts the spectral radius slightly below one,
preventing rapid signal explosion or decay. These results suggest that standard
initialization schemes may break down in the long-sequence regime, motivating a
separate line of theory for stable recurrent initialization.",2025-05-26,"Noga Bar, Mariia Seleznova, Yotam Alexander, Gitta Kutyniok, Raja Giryes",http://arxiv.org/pdf/2505.19827v1,cs.LG
Foundation Models for Tabular Data within Systemic Contexts Need Grounding,"Current research on tabular foundation models often overlooks the
complexities of large-scale, real-world data by treating tables as isolated
entities and assuming information completeness, thereby neglecting the vital
operational context. To address this, we introduce the concept of Semantically
Linked Tables (SLT), recognizing that tables are inherently connected to both
declarative and procedural operational knowledge. We propose Foundation Models
for Semantically Linked Tables (FMSLT), which integrate these components to
ground tabular data within its true operational context. This comprehensive
representation unlocks the full potential of machine learning for complex,
interconnected tabular data across diverse domains. Realizing FMSLTs requires
access to operational knowledge that is often unavailable in public datasets,
highlighting the need for close collaboration between domain experts and
researchers. Our work exposes the limitations of current tabular foundation
models and proposes a new direction centered on FMSLTs, aiming to advance
robust, context-aware models for structured data.",2025-05-26,"Tassilo Klein, Johannes Hoffart",http://arxiv.org/pdf/2505.19825v1,cs.LG
LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning in Heterogeneous Environments,"Federated Learning (FL) is a distributed machine learning paradigm based on
protecting data privacy of devices, which however, can still be broken by
gradient leakage attack via parameter inversion techniques. Differential
privacy (DP) technology reduces the risk of private data leakage by adding
artificial noise to the gradients, but detrimental to the FL utility at the
same time, especially in the scenario where the data is Non-Independent
Identically Distributed (Non-IID). Based on the impact of heterogeneous data on
aggregation performance, this paper proposes a Lightweight Adaptive Privacy
Allocation (LAPA) strategy, which assigns personalized privacy budgets to
devices in each aggregation round without transmitting any additional
information beyond gradients, ensuring both privacy protection and aggregation
efficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG)
algorithm is employed to optimize the transmission power, in order to determine
the optimal timing at which the adaptively attenuated artificial noise aligns
with the communication noise, enabling an effective balance between DP and
system utility. Finally, a reliable aggregation strategy is designed by
integrating communication quality and data distribution characteristics, which
improves aggregation performance while preserving privacy. Experimental results
demonstrate that the personalized noise allocation and dynamic optimization
strategy based on LAPA proposed in this paper enhances convergence performance
while satisfying the privacy requirements of FL.",2025-05-26,"Pengcheng Sun, Erwu Liu, Wei Ni, Rui Wang, Yuanzhe Geng, Lijuan Lai, Abbas Jamalipour",http://arxiv.org/pdf/2505.19823v1,cs.LG
Poison in the Well: Feature Embedding Disruption in Backdoor Attacks,"Backdoor attacks embed malicious triggers into training data, enabling
attackers to manipulate neural network behavior during inference while
maintaining high accuracy on benign inputs. However, existing backdoor attacks
face limitations manifesting in excessive reliance on training data, poor
stealth, and instability, which hinder their effectiveness in real-world
applications. Therefore, this paper introduces ShadowPrint, a versatile
backdoor attack that targets feature embeddings within neural networks to
achieve high ASRs and stealthiness. Unlike traditional approaches, ShadowPrint
reduces reliance on training data access and operates effectively with
exceedingly low poison rates (as low as 0.01%). It leverages a clustering-based
optimization strategy to align feature embeddings, ensuring robust performance
across diverse scenarios while maintaining stability and stealth. Extensive
evaluations demonstrate that ShadowPrint achieves superior ASR (up to 100%),
steady CA (with decay no more than 1% in most cases), and low DDR (averaging
below 5%) across both clean-label and dirty-label settings, and with poison
rates ranging from as low as 0.01% to 0.05%, setting a new standard for
backdoor attack capabilities and emphasizing the need for advanced defense
strategies focused on feature space manipulations.",2025-05-26,"Zhou Feng, Jiahao Chen, Chunyi Zhou, Yuwen Pu, Qingming Li, Shouling Ji",http://arxiv.org/pdf/2505.19821v1,cs.LG
InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory,"Interpretability of point cloud (PC) models becomes imperative given their
deployment in safety-critical scenarios such as autonomous vehicles. We focus
on attributing PC model outputs to interpretable critical concepts, defined as
meaningful subsets of the input point cloud. To enable human-understandable
diagnostics of model failures, an ideal critical subset should be *faithful*
(preserving points that causally influence predictions) and *conceptually
coherent* (forming semantically meaningful structures that align with human
perception). We propose InfoCons, an explanation framework that applies
information-theoretic principles to decompose the point cloud into 3D concepts,
enabling the examination of their causal effect on model predictions with
learnable priors. We evaluate InfoCons on synthetic datasets for
classification, comparing it qualitatively and quantitatively with four
baselines. We further demonstrate its scalability and flexibility on two
real-world datasets and in two applications that utilize critical scores of PC.",2025-05-26,"Feifei Li, Mi Zhang, Zhaoxiang Wang, Min Yang",http://arxiv.org/pdf/2505.19820v1,cs.LG
Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees,"In many real-world applications of regression, conditional probability
estimation, and uncertainty quantification, exploiting symmetries rooted in
physics or geometry can dramatically improve generalization and sample
efficiency. While geometric deep learning has made significant empirical
advances by incorporating group-theoretic structure, less attention has been
given to statistical learning guarantees. In this paper, we introduce an
equivariant representation learning framework that simultaneously addresses
regression, conditional probability estimation, and uncertainty quantification
while providing first-of-its-kind non-asymptotic statistical learning
guarantees. Grounded in operator and group representation theory, our framework
approximates the spectral decomposition of the conditional expectation
operator, building representations that are both equivariant and disentangled
along independent symmetry subgroups. Empirical evaluations on synthetic
datasets and real-world robotics applications confirm the potential of our
approach, matching or outperforming existing equivariant baselines in
regression while additionally providing well-calibrated parametric uncertainty
estimates.",2025-05-26,"Daniel Ordoñez-Apraez, Alek Fröhlich, Vladimir Kostić, Karim Lounici, Vivien Brandt, Massimiliano Pontil",http://arxiv.org/pdf/2505.19809v1,cs.LG
Density Ratio-Free Doubly Robust Proxy Causal Learning,"We study the problem of causal function estimation in the Proxy Causal
Learning (PCL) framework, where confounders are not observed but proxies for
the confounders are available. Two main approaches have been proposed: outcome
bridge-based and treatment bridge-based methods. In this work, we propose two
kernel-based doubly robust estimators that combine the strengths of both
approaches, and naturally handle continuous and high-dimensional variables. Our
identification strategy builds on a recent density ratio-free method for
treatment bridge-based PCL; furthermore, in contrast to previous approaches, it
does not require indicator functions or kernel smoothing over the treatment
variable. These properties make it especially well-suited for continuous or
high-dimensional treatments. By using kernel mean embeddings, we have
closed-form solutions and strong consistency guarantees. Our estimators
outperform existing methods on PCL benchmarks, including a prior doubly robust
method that requires both kernel smoothing and density ratio estimation.",2025-05-26,"Bariscan Bozkurt, Houssam Zenati, Dimitri Meunier, Liyuan Xu, Arthur Gretton",http://arxiv.org/pdf/2505.19807v1,cs.LG
"Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks","Consciousness stands as one of the most profound and distinguishing features
of the human mind, fundamentally shaping our understanding of existence and
agency. As large language models (LLMs) develop at an unprecedented pace,
questions concerning intelligence and consciousness have become increasingly
significant. However, discourse on LLM consciousness remains largely unexplored
territory. In this paper, we first clarify frequently conflated terminologies
(e.g., LLM consciousness and LLM awareness). Then, we systematically organize
and synthesize existing research on LLM consciousness from both theoretical and
empirical perspectives. Furthermore, we highlight potential frontier risks that
conscious LLMs might introduce. Finally, we discuss current challenges and
outline future directions in this emerging field. The references discussed in
this paper are organized at
https://github.com/OpenCausaLab/Awesome-LLM-Consciousness.",2025-05-26,"Sirui Chen, Shuqin Ma, Shu Yu, Hanwang Zhang, Shengjie Zhao, Chaochao Lu",http://arxiv.org/pdf/2505.19806v1,cs.LG
GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation,"Understanding pain-related facial behaviors is essential for digital
healthcare in terms of effective monitoring, assisted diagnostics, and
treatment planning, particularly for patients unable to communicate verbally.
Existing data-driven methods of detecting pain from facial expressions are
limited due to interpretability and severity quantification. To this end, we
propose GraphAU-Pain, leveraging a graph-based framework to model facial Action
Units (AUs) and their interrelationships for pain intensity estimation. AUs are
represented as graph nodes, with co-occurrence relationships as edges, enabling
a more expressive depiction of pain-related facial behaviors. By utilizing a
relational graph neural network, our framework offers improved interpretability
and significant performance gains. Experiments conducted on the publicly
available UNBC dataset demonstrate the effectiveness of the GraphAU-Pain,
achieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity
estimation.",2025-05-26,"Zhiyu Wang, Yang Liu, Hatice Gunes",http://arxiv.org/pdf/2505.19802v1,cs.LG
The Missing Point in Vision Transformers for Universal Image Segmentation,"Image segmentation remains a challenging task in computer vision, demanding
robust mask generation and precise classification. Recent mask-based approaches
yield high-quality masks by capturing global context. However, accurately
classifying these masks, especially in the presence of ambiguous boundaries and
imbalanced class distributions, remains an open challenge. In this work, we
introduce ViT-P, a novel two-stage segmentation framework that decouples mask
generation from classification. The first stage employs a proposal generator to
produce class-agnostic mask proposals, while the second stage utilizes a
point-based classification model built on the Vision Transformer (ViT) to
refine predictions by focusing on mask central points. ViT-P serves as a
pre-training-free adapter, allowing the integration of various pre-trained
vision transformers without modifying their architecture, ensuring adaptability
to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding
box annotations can effectively enhance classification without requiring
additional training on fine annotation datasets, reducing annotation costs
while maintaining strong performance. Extensive experiments across COCO,
ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving
state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4
mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic
segmentation. The code and pretrained models are available at:
https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.",2025-05-26,"Sajjad Shahabodini, Mobina Mansoori, Farnoush Bayatmakou, Jamshid Abouei, Konstantinos N. Plataniotis, Arash Mohammadi",http://arxiv.org/pdf/2505.19795v1,cs.LG
What Can RL Bring to VLA Generalization? An Empirical Study,"Large Vision-Language Action (VLA) models have shown significant potential
for embodied AI. However, their predominant training via supervised fine-tuning
(SFT) limits generalization due to susceptibility to compounding errors under
distribution shifts. Reinforcement learning (RL) offers a path to overcome
these limitations by optimizing for task objectives via trial-and-error, yet a
systematic understanding of its specific generalization benefits for VLAs
compared to SFT is lacking. To address this, our study introduces a
comprehensive benchmark for evaluating VLA generalization and systematically
investigates the impact of RL fine-tuning across diverse visual, semantic, and
execution dimensions. Our extensive experiments reveal that RL fine-tuning,
particularly with PPO, significantly enhances generalization in semantic
understanding and execution robustness over SFT, while maintaining comparable
visual robustness. We identify PPO as a more effective RL algorithm for VLAs
than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for
efficient PPO training on VLAs, and demonstrate its practical utility for
improving VLA generalization. The project page is at https://rlvla.github.io",2025-05-26,"Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang",http://arxiv.org/pdf/2505.19789v1,cs.LG
MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support,"Timely and personalized treatment decisions are essential across a wide range
of healthcare settings where patient responses vary significantly and evolve
over time. Clinical data used to support these decisions are often irregularly
sampled, sparse, and noisy. Existing decision support systems commonly rely on
discretization and imputation, which can distort critical temporal dynamics and
degrade decision quality. Moreover, they often overlook the clinical
significance of irregular recording frequencies, filtering out patterns in how
and when data is collected. Reinforcement Learning (RL) is a natural fit for
clinical decision-making, enabling sequential, long-term optimization in
dynamic, uncertain environments. However, most existing treatment
recommendation systems are model-free and trained solely on offline data,
making them sample-inefficient, sensitive to data quality, and poorly
generalizable across tasks or cohorts. To address these limitations, we propose
MedDreamer, a two-phase model-based RL framework for personalized treatment
recommendation. MedDreamer uses a world model with an Adaptive Feature
Integration (AFI) module to effectively model irregular, sparse clinical data.
Through latent imagination, it simulates plausible patient trajectories to
enhance learning, refining its policy using a mix of real and imagined
experiences. This enables learning policies that go beyond suboptimal
historical decisions while remaining grounded in clinical data. To our
knowledge, this is the first application of latent imagination to irregular
healthcare data. Evaluations on sepsis and mechanical ventilation (MV)
treatment using two large-scale EHR datasets show that MedDreamer outperforms
both model-free and model-based baselines in clinical outcomes and off-policy
metrics.",2025-05-26,"Qianyi Xu, Gousia Habib, Dilruk Perera, Mengling Feng",http://arxiv.org/pdf/2505.19785v1,cs.LG
Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models,"Using massive datasets, foundation models are large-scale, pre-trained models
that perform a wide range of tasks. These models have shown consistently
improved results with the introduction of new methods. It is crucial to analyze
how these trends impact the medical field and determine whether these
advancements can drive meaningful change. This study investigates the
application of recent state-of-the-art foundation models, DINOv2, MAE, VMamba,
CoCa, SAM2, and AIMv2, for medical image classification. We explore their
effectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for
skin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest
radiographs. By fine-tuning these models and evaluating their configurations,
we aim to understand the potential of these advancements in medical image
classification. The results indicate that these advanced models significantly
enhance classification outcomes, demonstrating robust performance despite
limited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models
outperformed others, demonstrating that progress in natural domain training has
positively impacted the medical domain and improved classification outcomes.
Our code is publicly available at:
https://github.com/sajjad-sh33/Medical-Transfer-Learning.",2025-05-26,"Mobina Mansoori, Sajjad Shahabodini, Farnoush Bayatmakou, Jamshid Abouei, Konstantinos N. Plataniotis, Arash Mohammadi",http://arxiv.org/pdf/2505.19779v1,cs.LG
Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO,"We present a fine-grained theoretical analysis of the performance gap between
reinforcement learning from human feedback (RLHF) and direct preference
optimization (DPO) under a representation gap. Our study decomposes this gap
into two sources: an explicit representation gap under exact optimization and
an implicit representation gap under finite samples. In the exact optimization
setting, we characterize how the relative capacities of the reward and policy
model classes influence the final policy qualities. We show that RLHF, DPO, or
online DPO can outperform one another depending on the type of model
mis-specifications. Notably, online DPO can outperform both RLHF and standard
DPO when the reward and policy model classes are isomorphic and both
mis-specified. In the approximate optimization setting, we provide a concrete
construction where the ground-truth reward is implicitly sparse and show that
RLHF requires significantly fewer samples than DPO to recover an effective
reward model -- highlighting a statistical advantage of two-stage learning.
Together, these results provide a comprehensive understanding of the
performance gap between RLHF and DPO under various settings, and offer
practical insights into when each method is preferred.",2025-05-26,"Ruizhe Shi, Minhak Song, Runlong Zhou, Zihan Zhang, Maryam Fazel, Simon S. Du",http://arxiv.org/pdf/2505.19770v1,cs.LG
Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding,"Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but optimizing LLM-based agentic systems remains challenging due
to the vast search space of agent configurations, prompting strategies, and
communication patterns. Existing approaches often rely on heuristic-based
tuning or exhaustive evaluation, which can be computationally expensive and
suboptimal. This paper proposes Agentic Predictor, a lightweight predictor for
efficient agentic workflow evaluation. Agentic Predictor is equipped with a
multi-view workflow encoding technique that leverages multi-view representation
learning of agentic systems by incorporating code architecture, textual
prompts, and interaction graph features. To achieve high predictive accuracy
while significantly reducing the number of required workflow evaluations for
training a predictor, Agentic Predictor employs cross-domain unsupervised
pretraining. By learning to approximate task success rates, Agentic Predictor
enables fast and accurate selection of optimal agentic workflow configurations
for a given task, significantly reducing the need for expensive trial-and-error
evaluations. Experiments on a carefully curated benchmark spanning three
domains show that our predictor outperforms state-of-the-art methods in both
predictive accuracy and workflow utility, highlighting the potential of
performance predictors in streamlining the design of LLM-based agentic
workflows.",2025-05-26,"Patara Trirat, Wonyong Jeong, Sung Ju Hwang",http://arxiv.org/pdf/2505.19764v1,cs.LG
Unfolding AlphaFold's Bayesian Roots in Probability Kinematics,"We present a novel theoretical interpretation of AlphaFold1. The seminal
breakthrough of AlphaFold1 in protein structure prediction by deep learning
relied on a learned potential energy function, in contrast to the later
end-to-end architectures of AlphaFold2 and AlphaFold3. While this potential was
originally justified by referring to physical potentials of mean force (PMFs),
we reinterpret AlphaFold1's potential as an instance of probability kinematics
- also known as Jeffrey conditioning - a principled but underrecognised
generalization of conventional Bayesian updating. Probability kinematics
accommodates uncertain or soft evidence in the form of updated probabilities
over a partition. This perspective reveals AlphaFold1's potential as a form of
generalized Bayesian updating, rather than a thermodynamic potential. To
confirm our probabilistic framework's scope and precision, we analyze a
synthetic 2D model in which an angular random walk prior is updated with
evidence on distances via probability kinematics, mirroring AlphaFold1's
approach. This theoretical contribution connects AlphaFold1 to a broader class
of well-justified Bayesian methods, allowing precise quantification, surpassing
merely qualitative heuristics based on PMFs. More broadly, given the
achievements of AlphaFold1, probability kinematics holds considerable promise
for probabilistic deep learning, as it allows for the formulation of complex
models from a few simpler components.",2025-05-26,"Thomas Hamelryck, Kanti V. Mardia",http://arxiv.org/pdf/2505.19763v1,cs.LG
CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement,"Effective generation of structured code comments requires robust quality
metrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)
suffer from limited code-comment analysis. We propose CIDRe, a
language-agnostic reference-free quality criterion combining four synergistic
aspects: (1) relevance (code-comment semantic alignment), (2) informativeness
(functional coverage), (3) completeness (presence of all structure sections),
and (4) description length (detail sufficiency). We validate our criterion on a
manually annotated dataset. Experiments demonstrate CIDRe's superiority over
existing metrics, achieving improvement in cross-entropy evaluation. When
applied to filter comments, the models finetuned on CIDRe-filtered data show
statistically significant quality gains in GPT-4o-mini assessments.",2025-05-26,"Maria Dziuba, Valentin Malykh",http://arxiv.org/pdf/2505.19757v1,cs.LG
Discrete Markov Bridge,"Discrete diffusion has recently emerged as a promising paradigm in discrete
data modeling. However, existing methods typically rely on a fixed rate
transition matrix during training, which not only limits the expressiveness of
latent representations, a fundamental strength of variational methods, but also
constrains the overall design space. To address these limitations, we propose
Discrete Markov Bridge, a novel framework specifically designed for discrete
representation learning. Our approach is built upon two key components: Matrix
Learning and Score Learning. We conduct a rigorous theoretical analysis,
establishing formal performance guarantees for Matrix Learning and proving the
convergence of the overall framework. Furthermore, we analyze the space
complexity of our method, addressing practical constraints identified in prior
studies. Extensive empirical evaluations validate the effectiveness of the
proposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)
of 1.38 on the Text8 dataset, outperforming established baselines. Moreover,
the proposed model demonstrates competitive performance on the CIFAR-10
dataset, achieving results comparable to those obtained by image-specific
generation approaches.",2025-05-26,"Hengli Li, Yuxuan Wang, Song-Chun Zhu, Ying Nian Wu, Zilong Zheng",http://arxiv.org/pdf/2505.19752v1,cs.LG
Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models,"With the rapid development of Large Language Models (LLMs), aligning these
models with human preferences and values is critical to ensuring ethical and
safe applications. However, existing alignment techniques such as RLHF or DPO
often require direct fine-tuning on LLMs with billions of parameters, resulting
in substantial computational costs and inefficiencies. To address this, we
propose Micro token-level Accept-Reject Aligning (MARA) approach designed to
operate independently of the language models. MARA simplifies the alignment
process by decomposing sentence-level preference learning into token-level
binary classification, where a compact three-layer fully-connected network
determines whether candidate tokens are ""Accepted"" or ""Rejected"" as part of the
response. Extensive experiments across seven different LLMs and three
open-source datasets show that MARA achieves significant improvements in
alignment performance while reducing computational costs.",2025-05-26,"Yang Zhang, Yu Yu, Bo Tang, Yu Zhu, Chuxiong Sun, Wenqiang Wei, Jie Hu, Zipeng Xie, Zhiyu Li, Feiyu Xiong, Edward Chung",http://arxiv.org/pdf/2505.19743v1,cs.LG
Machine Learning Algorithm for Noise Reduction and Disease-Causing Gene Feature Extraction in Gene Sequencing Data,"In this study, we propose a machine learning-based method for noise reduction
and disease-causing gene feature extraction in gene sequencing DeepSeqDenoise
algorithm combines CNN and RNN to effectively remove the sequencing noise, and
improves the signal-to-noise ratio by 9.4 dB. We screened 17 key features by
feature engineering, and constructed an integrated learning model to predict
disease-causing genes with 94.3% accuracy. We successfully identified 57 new
candidate disease-causing genes in a cardiovascular disease cohort validation,
and detected 3 missed variants in clinical applications. The method
significantly outperforms existing tools and provides strong support for
accurate diagnosis of genetic diseases.",2025-05-26,"Weichen Si, Yihao Ou, Zhen Tian",http://arxiv.org/pdf/2505.19740v1,cs.LG
Weighted Leave-One-Out Cross Validation,"We present a weighted version of Leave-One-Out (LOO) cross-validation for
estimating the Integrated Squared Error (ISE) when approximating an unknown
function by a predictor that depends linearly on evaluations of the function
over a finite collection of sites. The method relies on the construction of the
best linear estimator of the squared prediction error at an arbitrary unsampled
site based on squared LOO residuals, assuming that the function is a
realization of a Gaussian Process (GP). A theoretical analysis of performance
of the ISE estimator is presented, and robustness with respect to the choice of
the GP kernel is investigated first analytically, then through numerical
examples. Overall, the estimation of ISE is significantly more precise than
with classical, unweighted, LOO cross validation. Application to model
selection is briefly considered through examples.",2025-05-26,"Luc Pronzato, Maria-João Rendas",http://arxiv.org/pdf/2505.19737v1,cs.LG
Accelerating Nash Learning from Human Feedback via Mirror Prox,"Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on
reward models, frequently assuming preference structures like the Bradley-Terry
model, which may not accurately capture the complexities of real human
preferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF)
offers a more direct alternative by framing the problem as finding a Nash
equilibrium of a game defined by these preferences. In this work, we introduce
Nash Mirror Prox ($\mathtt{Nash-MP}$), an online NLHF algorithm that leverages
the Mirror Prox optimization scheme to achieve fast and stable convergence to
the Nash equilibrium. Our theoretical analysis establishes that Nash-MP
exhibits last-iterate linear convergence towards the $\beta$-regularized Nash
equilibrium. Specifically, we prove that the KL-divergence to the optimal
policy decreases at a rate of order $(1+2\beta)^{-N/2}$, where $N$ is a number
of preference queries. We further demonstrate last-iterate linear convergence
for the exploitability gap and uniformly for the span semi-norm of
log-probabilities, with all these rates being independent of the size of the
action space. Furthermore, we propose and analyze an approximate version of
Nash-MP where proximal steps are estimated using stochastic policy gradients,
making the algorithm closer to applications. Finally, we detail a practical
implementation strategy for fine-tuning large language models and present
experiments that demonstrate its competitive performance and compatibility with
existing methods.",2025-05-26,"Daniil Tiapkin, Daniele Calandriello, Denis Belomestny, Eric Moulines, Alexey Naumov, Kashif Rasul, Michal Valko, Pierre Menard",http://arxiv.org/pdf/2505.19731v1,cs.LG
A Structured Tour of Optimization with Finite Differences,"Finite-difference methods are widely used for zeroth-order optimization in
settings where gradient information is unavailable or expensive to compute.
These procedures mimic first-order strategies by approximating gradients
through function evaluations along a set of random directions. From a
theoretical perspective, recent studies indicate that imposing structure (such
as orthogonality) on the chosen directions allows for the derivation of
convergence rates comparable to those achieved with unstructured random
directions (i.e., directions sampled independently from a distribution).
Empirically, although structured directions are expected to enhance
performance, they often introduce additional computational costs, which can
limit their applicability in high-dimensional settings. In this work, we
examine the impact of structured direction selection in finite-difference
methods. We review and extend several strategies for constructing structured
direction matrices and compare them with unstructured approaches in terms of
computational cost, gradient approximation quality, and convergence behavior.
Our evaluation spans both synthetic tasks and real-world applications such as
adversarial perturbation. The results demonstrate that structured directions
can be generated with computational costs comparable to unstructured ones while
significantly improving gradient estimation accuracy and optimization
performance.",2025-05-26,"Marco Rando, Cesare Molinari, Lorenzo Rosasco, Silvia Villa",http://arxiv.org/pdf/2505.19720v1,cs.LG
OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction,"Common Neighbors (CNs) and their higher-order variants are important pairwise
features widely used in state-of-the-art link prediction methods. However,
existing methods often struggle with the repetition across different orders of
CNs and fail to fully leverage their potential. We identify that these
limitations stem from two key issues: redundancy and over-smoothing in
high-order common neighbors. To address these challenges, we design
orthogonalization to eliminate redundancy between different-order CNs and
normalization to mitigate over-smoothing. By combining these two techniques, we
propose Orthogonal Common Neighbor (OCN), a novel approach that significantly
outperforms the strongest baselines by an average of 7.7% on popular link
prediction benchmarks. A thorough theoretical analysis is provided to support
our method. Ablation studies also verify the effectiveness of our
orthogonalization and normalization techniques.",2025-05-26,"Juntong Wang, Xiyuan Wang, Muhan Zhang",http://arxiv.org/pdf/2505.19719v1,cs.LG
Graceful Forgetting in Generative Language Models,"Recently, the pretrain-finetune paradigm has become a cornerstone in various
deep learning areas. While in general the pre-trained model would promote both
effectiveness and efficiency of downstream tasks fine-tuning, studies have
shown that not all knowledge acquired during pre-training is beneficial. Some
of the knowledge may actually bring detrimental effects to the fine-tuning
tasks, which is also known as negative transfer. To address this problem,
graceful forgetting has emerged as a promising approach. The core principle of
graceful forgetting is to enhance the learning plasticity of the target task by
selectively discarding irrelevant knowledge. However, this approach remains
underexplored in the context of generative language models, and it is often
challenging to migrate existing forgetting algorithms to these models due to
architecture incompatibility. To bridge this gap, in this paper we propose a
novel framework, Learning With Forgetting (LWF), to achieve graceful forgetting
in generative language models. With Fisher Information Matrix weighting the
intended parameter updates, LWF computes forgetting confidence to evaluate
self-generated knowledge regarding the forgetting task, and consequently,
knowledge with high confidence is periodically unlearned during fine-tuning.
Our experiments demonstrate that, although thoroughly uncovering the mechanisms
of knowledge interaction remains challenging in pre-trained language models,
applying graceful forgetting can contribute to enhanced fine-tuning
performance.",2025-05-26,"Chunyang Jiang, Chi-min Chan, Yiyang Cai, Yulong Liu, Wei Xue, Yike Guo",http://arxiv.org/pdf/2505.19715v1,cs.LG
MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning,"Text Image Machine Translation (TIMT)-the task of translating textual content
embedded in images-is critical for applications in accessibility, cross-lingual
information access, and real-world document understanding. However, TIMT
remains a complex challenge due to the need for accurate optical character
recognition (OCR), robust visual-text reasoning, and high-quality translation,
often requiring cascading multi-stage pipelines. Recent advances in large-scale
Reinforcement Learning (RL) have improved reasoning in Large Language Models
(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is
still underexplored. To bridge this gap, we introduce MT$^{3}$, the first
framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts
a multi-task optimization paradigm targeting three key sub-skills: text
recognition, context-aware reasoning, and translation. It is trained using a
novel multi-mixed reward mechanism that adapts rule-based RL strategies to
TIMT's intricacies, offering fine-grained, non-binary feedback across tasks.
Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural
and real-world social media contexts, we introduced XHSPost, the first social
media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on
the latest in-domain MIT-10M benchmark, outperforming strong baselines such as
Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.
Additionally, the model shows strong generalization to out-of-distribution
language pairs and datasets. In-depth analyses reveal how multi-task synergy,
reinforcement learning initialization, curriculum design, and reward
formulation contribute to advancing MLLM-driven TIMT.",2025-05-26,"Zhaopeng Feng, Yupu Liang, Shaosheng Cao, Jiayuan Su, Jiahan Ren, Zhe Xu, Yao Hu, Wenxuan Huang, Jian Wu, Zuozhu Liu",http://arxiv.org/pdf/2505.19714v1,cs.LG
On the Relation between Rectified Flows and Optimal Transport,"This paper investigates the connections between rectified flows, flow
matching, and optimal transport. Flow matching is a recent approach to learning
generative models by estimating velocity fields that guide transformations from
a source to a target distribution. Rectified flow matching aims to straighten
the learned transport paths, yielding more direct flows between distributions.
Our first contribution is a set of invariance properties of rectified flows and
explicit velocity fields. In addition, we also provide explicit constructions
and analysis in the Gaussian (not necessarily independent) and Gaussian mixture
settings and study the relation to optimal transport. Our second contribution
addresses recent claims suggesting that rectified flows, when constrained such
that the learned velocity field is a gradient, can yield (asymptotically)
solutions to optimal transport problems. We study the existence of solutions
for this problem and demonstrate that they only relate to optimal transport
under assumptions that are significantly stronger than those previously
acknowledged. In particular, we present several counter-examples that
invalidate earlier equivalence results in the literature, and we argue that
enforcing a gradient constraint on rectified flows is, in general, not a
reliable method for computing optimal transport maps.",2025-05-26,"Johannes Hertrich, Antonin Chambolle, Julie Delon",http://arxiv.org/pdf/2505.19712v1,cs.LG
Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments,"Federated Learning (FL) is a decentralized machine learning paradigm that
enables clients to collaboratively train models while preserving data privacy.
However, the coexistence of model and data heterogeneity gives rise to
inconsistent representations and divergent optimization dynamics across
clients, ultimately hindering robust global performance. To transcend these
challenges, we propose Mosaic, a novel data-free knowledge distillation
framework tailored for heterogeneous distributed environments. Mosaic first
trains local generative models to approximate each client's personalized
distribution, enabling synthetic data generation that safeguards privacy
through strict separation from real data. Subsequently, Mosaic forms a
Mixture-of-Experts (MoE) from client models based on their specialized
knowledge, and distills it into a global model using the generated data. To
further enhance the MoE architecture, Mosaic integrates expert predictions via
a lightweight meta model trained on a few representative prototypes. Extensive
experiments on standard image classification benchmarks demonstrate that Mosaic
consistently outperforms state-of-the-art approaches under both model and data
heterogeneity. The source code has been published at
https://github.com/Wings-Of-Disaster/Mosaic.",2025-05-26,"Junming Liu, Yanting Gao, Siyuan Meng, Yifei Sun, Aoqi Wu, Yufei Jin, Yirong Chen, Ding Wang, Guosun Zeng",http://arxiv.org/pdf/2505.19699v1,cs.LG
JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning,"Recent advances in model-based reinforcement learning (MBRL) have achieved
super-human level performance on the Atari100k benchmark, driven by
reinforcement learning agents trained on powerful diffusion world models.
However, we identify that the current aggregates mask a major performance
asymmetry: MBRL agents dramatically outperform humans in some tasks despite
drastically underperforming in others, with the former inflating the aggregate
metrics. This is especially pronounced in pixel-based agents trained with
diffusion world models. In this work, we address the pronounced asymmetry
observed in pixel-based agents as an initial attempt to reverse the worrying
upward trend observed in them. We address the problematic aggregates by
delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal
importance on metrics from both sets. Next, we hypothesize this pronounced
asymmetry is due to the lack of temporally-structured latent space trained with
the World Model objective in pixel-based methods. Lastly, to address this
issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion
world model trained end-to-end with the self-consistency objective. JEDI
outperforms SOTA models in human-optimal tasks while staying competitive across
the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the
latest pixel-based diffusion baseline. Overall, our work rethinks what it truly
means to cross human-level performance in Atari100k.",2025-05-26,"Jing Yu Lim, Zarif Ikram, Samson Yu, Haozhe Ma, Tze-Yun Leong, Dianbo Liu",http://arxiv.org/pdf/2505.19698v1,cs.LG
Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation,"Diffusion models have emerged as powerful generative models for graph
generation, yet their use for conditional graph generation remains a
fundamental challenge. In particular, guiding diffusion models on graphs under
arbitrary reward signals is difficult: gradient-based methods, while powerful,
are often unsuitable due to the discrete and combinatorial nature of graphs,
and non-differentiable rewards further complicate gradient-based guidance. We
propose Graph Guided Diffusion (GGDiff), a novel guidance framework that
interprets conditional diffusion on graphs as a stochastic control problem to
address this challenge. GGDiff unifies multiple guidance strategies, including
gradient-based guidance (for differentiable rewards), control-based guidance
(using control signals from forward reward evaluations), and zero-order
approximations (bridging gradient-based and gradient-free optimization). This
comprehensive, plug-and-play framework enables zero-shot guidance of
pre-trained diffusion models under both differentiable and non-differentiable
reward functions, adapting well-established guidance techniques to graph
generation--a direction largely unexplored. Our formulation balances
computational efficiency, reward alignment, and sample quality, enabling
practical conditional generation across diverse reward types. We demonstrate
the efficacy of GGDiff in various tasks, including constraints on graph motifs,
fairness, and link prediction, achieving superior alignment with target rewards
while maintaining diversity and fidelity.",2025-05-26,"Victor M. Tenorio, Nicolas Zilberstein, Santiago Segarra, Antonio G. Marques",http://arxiv.org/pdf/2505.19685v1,cs.LG
Deep Actor-Critics with Tight Risk Certificates,"After a period of research, deep actor-critic algorithms have reached a level
where they influence our everyday lives. They serve as the driving force behind
the continual improvement of large language models through user-collected
feedback. However, their deployment in physical systems is not yet widely
adopted, mainly because no validation scheme that quantifies their risk of
malfunction. We demonstrate that it is possible to develop tight risk
certificates for deep actor-critic algorithms that predict generalization
performance from validation-time observations. Our key insight centers on the
effectiveness of minimal evaluation data. Surprisingly, a small feasible of
evaluation roll-outs collected from a pretrained policy suffices to produce
accurate risk certificates when combined with a simple adaptation of PAC-Bayes
theory. Specifically, we adopt a recently introduced recursive PAC-Bayes
approach, which splits validation data into portions and recursively builds
PAC-Bayes bounds on the excess loss of each portion's predictor, using the
predictor from the previous portion as a data-informed prior. Our empirical
results across multiple locomotion tasks and policy expertise levels
demonstrate risk certificates that are tight enough to be considered for
practical use.",2025-05-26,"Bahareh Tasdighi, Manuel Haussmann, Yi-Shan Wu, Andres R. Masegosa, Melih Kandemir",http://arxiv.org/pdf/2505.19682v1,cs.LG
Cut out and Replay: A Simple yet Versatile Strategy for Multi-Label Online Continual Learning,"Multi-Label Online Continual Learning (MOCL) requires models to learn
continuously from endless multi-label data streams, facing complex challenges
including persistent catastrophic forgetting, potential missing labels, and
uncontrollable imbalanced class distributions. While existing MOCL methods
attempt to address these challenges through various techniques, \textit{they
all overlook label-specific region identifying and feature learning} - a
fundamental solution rooted in multi-label learning but challenging to achieve
in the online setting with incremental and partial supervision. To this end, we
first leverage the inherent structural information of input data to evaluate
and verify the innate localization capability of different pre-trained models.
Then, we propose CUTER (CUT-out-and-Experience-Replay), a simple yet versatile
strategy that provides fine-grained supervision signals by further identifying,
strengthening and cutting out label-specific regions for efficient experience
replay. It not only enables models to simultaneously address catastrophic
forgetting, missing labels, and class imbalance challenges, but also serves as
an orthogonal solution that seamlessly integrates with existing approaches.
Extensive experiments on multiple multi-label image benchmarks demonstrate the
superiority of our proposed method. The code is available at
\href{https://github.com/wxr99/Cut-Replay}{https://github.com/wxr99/Cut-Replay}",2025-05-26,"Xinrui Wang, Shao-yuan Li, Jiaqiang Zhang, Songcan Chen",http://arxiv.org/pdf/2505.19680v1,cs.LG
Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling,"Zero-shot streaming text-to-speech is an important research topic in
human-computer interaction. Existing methods primarily use a lookahead
mechanism, relying on future text to achieve natural streaming speech
synthesis, which introduces high processing latency. To address this issue, we
propose SMLLE, a streaming framework for generating high-quality speech
frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens
in real time while simultaneously obtaining duration alignment information. The
combined outputs are then fed into a fully autoregressive (AR) streaming model
to reconstruct mel-spectrograms. To further stabilize the generation process,
we design a Delete < Bos > Mechanism that allows the AR model to access future
text introducing as minimal delay as possible. Experimental results suggest
that the SMLLE outperforms current streaming TTS methods and achieves
comparable performance over sentence-level TTS systems. Samples are available
on https://anonymous.4open.science/w/demo_page-48B7/.",2025-05-26,"Haiyang Sun, Shujie Hu, Shujie Liu, Lingwei Meng, Hui Wang, Bing Han, Yifan Yang, Yanqing Liu, Sheng Zhao, Yan Lu, Yanmin Qian",http://arxiv.org/pdf/2505.19669v1,cs.LG
A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?,"We present a framework to foster the evaluation of deep learning-based audio
watermarking algorithms, establishing a standardized benchmark and allowing
systematic comparisons. To simulate real-world usage, we introduce a
comprehensive audio attack pipeline, featuring various distortions such as
compression, background noise, and reverberation, and propose a diverse test
dataset, including speech, environmental sounds, and music recordings. By
assessing the performance of four existing watermarking algorithms on our
framework, two main insights stand out: (i) neural compression techniques pose
the most significant challenge, even when algorithms are trained with such
compressions; and (ii) training with audio attacks generally improves
robustness, although it is insufficient in some cases. Furthermore, we find
that specific distortions, such as polarity inversion, time stretching, or
reverb, seriously affect certain algorithms. Our contributions strengthen the
robustness and perceptual assessment of audio watermarking algorithms across a
wide range of applications, while ensuring a fair and consistent evaluation
approach. The evaluation framework, including the attack pipeline, is
accessible at github.com/SonyResearch/wm_robustness_eval.",2025-05-26,"Yigitcan Özer, Woosung Choi, Joan Serrà, Mayank Kumar Singh, Wei-Hsiang Liao, Yuki Mitsufuji",http://arxiv.org/pdf/2505.19663v1,cs.LG
Energy-based generator matching: A neural sampler for general state space,"We propose Energy-based generator matching (EGM), a modality-agnostic
approach to train generative models from energy functions in the absence of
data. Extending the recently proposed generator matching, EGM enables training
of arbitrary continuous-time Markov processes, e.g., diffusion, flow, and jump,
and can generate data from continuous, discrete, and a mixture of two
modalities. To this end, we propose estimating the generator matching loss
using self-normalized importance sampling with an additional bootstrapping
trick to reduce variance in the importance weight. We validate EGM on both
discrete and multimodal tasks up to 100 and 20 dimensions, respectively.",2025-05-26,"Dongyeop Woo, Minsu Kim, Minkyu Kim, Kiyoung Seong, Sungsoo Ahn",http://arxiv.org/pdf/2505.19646v1,cs.LG
MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE,"Large Language Models (LLMs) have achieved remarkable success across many
applications, with Mixture of Experts (MoE) models demonstrating great
potential. Compared to traditional dense models, MoEs achieve better
performance with less computation. Speculative decoding (SD) is a widely used
technique to accelerate LLM inference without accuracy loss, but it has been
considered efficient only for dense models. In this work, we first demonstrate
that, under medium batch sizes, MoE surprisingly benefits more from SD than
dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in
MoE designs -- the batch size range where SD acceleration is expected to be
effective becomes broader. To quantitatively understand tradeoffs involved in
SD, we develop a reliable modeling based on theoretical analyses. While current
SD research primarily focuses on improving acceptance rates of algorithms,
changes in workload and model architecture can still lead to degraded SD
acceleration even with high acceptance rates. To address this limitation, we
introduce a new metric 'target efficiency' that characterizes these effects,
thus helping researchers identify system bottlenecks and understand SD
acceleration more comprehensively. For scenarios like private serving, this
work unveils a new perspective to speed up MoE inference, where existing
solutions struggle. Experiments on different GPUs show up to 2.29x speedup for
Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.",2025-05-26,"Zongle Huang, Lei Zhu, Zongyuan Zhan, Ting Hu, Weikai Mao, Xianzhi Yu, Yongpan Liu, Tianyu Zhang",http://arxiv.org/pdf/2505.19645v1,cs.LG
When fractional quasi p-norms concentrate,"Concentration of distances in high dimension is an important factor for the
development and design of stable and reliable data analysis algorithms. In this
paper, we address the fundamental long-standing question about the
concentration of distances in high dimension for fractional quasi $p$-norms,
$p\in(0,1)$. The topic has been at the centre of various theoretical and
empirical controversies. Here we, for the first time, identify conditions when
fractional quasi $p$-norms concentrate and when they don't. We show that
contrary to some earlier suggestions, for broad classes of distributions,
fractional quasi $p$-norms admit exponential and uniform in $p$ concentration
bounds. For these distributions, the results effectively rule out previously
proposed approaches to alleviate concentration by ""optimal"" setting the values
of $p$ in $(0,1)$. At the same time, we specify conditions and the
corresponding families of distributions for which one can still control
concentration rates by appropriate choices of $p$. We also show that in an
arbitrarily small vicinity of a distribution from a large class of
distributions for which uniform concentration occurs, there are uncountably
many other distributions featuring anti-concentration properties. Importantly,
this behavior enables devising relevant data encoding or representation schemes
favouring or discouraging distance concentration. The results shed new light on
this long-standing problem and resolve the tension around the topic in both
theory and empirical evidence reported in the literature.",2025-05-26,"Ivan Y. Tyukin, Bogdan Grechuk, Evgeny M. Mirkes, Alexander N. Gorban",http://arxiv.org/pdf/2505.19635v1,cs.LG
Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs,"Spatio-temporal prediction is a pivotal task with broad applications in
traffic management, climate monitoring, energy scheduling, etc. However,
existing methodologies often struggle to balance model expressiveness and
computational efficiency, especially when scaling to large real-world datasets.
To tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph
Separation Networks), a novel framework that decouples temporal and spatial
modeling to enhance both efficiency and precision. Therein, the temporal
dimension is modeled using lightweight large language models, which effectively
capture low-rank temporal dynamics. Concurrently, the spatial dimension is
addressed through an adaptive hypergraph neural network, which dynamically
constructs hyperedges to model intricate, higher-order interactions. A
carefully designed gating mechanism is integrated to seamlessly fuse temporal
and spatial representations. By leveraging the fundamental principles of
low-rank temporal dynamics and spatial interactions, STH-SepNet offers a
pragmatic and scalable solution for spatio-temporal prediction in real-world
applications. Extensive experiments on large-scale real-world datasets across
multiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting
predictive performance while maintaining computational efficiency. This work
may provide a promising lightweight framework for spatio-temporal prediction,
aiming to reduce computational demands and while enhancing predictive
performance. Our code is avaliable at
https://github.com/SEU-WENJIA/ST-SepNet-Lightweight-LLMs-Meet-Adaptive-Hypergraphs.",2025-05-26,"Jiawen Chen, Qi Shao, Duxin Chen, Wenwu Yu",http://arxiv.org/pdf/2505.19620v1,cs.LG
SESaMo: Symmetry-Enforcing Stochastic Modulation for Normalizing Flows,"Deep generative models have recently garnered significant attention across
various fields, from physics to chemistry, where sampling from unnormalized
Boltzmann-like distributions represents a fundamental challenge. In particular,
autoregressive models and normalizing flows have become prominent due to their
appealing ability to yield closed-form probability densities. Moreover, it is
well-established that incorporating prior knowledge - such as symmetries - into
deep neural networks can substantially improve training performances. In this
context, recent advances have focused on developing symmetry-equivariant
generative models, achieving remarkable results. Building upon these
foundations, this paper introduces Symmetry-Enforcing Stochastic Modulation
(SESaMo). Similar to equivariant normalizing flows, SESaMo enables the
incorporation of inductive biases (e.g., symmetries) into normalizing flows
through a novel technique called stochastic modulation. This approach enhances
the flexibility of the generative model, allowing to effectively learn a
variety of exact and broken symmetries. Our numerical experiments benchmark
SESaMo in different scenarios, including an 8-Gaussian mixture model and
physically relevant field theories, such as the $\phi^4$ theory and the Hubbard
model.",2025-05-26,"Janik Kreit, Dominic Schuh, Kim A. Nicoli, Lena Funcke",http://arxiv.org/pdf/2505.19619v1,cs.LG
Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models,"Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across tasks, yet they often exhibit difficulty in distinguishing
task-relevant from irrelevant signals, particularly in tasks like Visual
Question Answering (VQA), which can lead to susceptibility to misleading or
spurious inputs. We refer to this broader limitation as the Cross-Modality
Competency Problem: the model's inability to fairly evaluate all modalities.
This vulnerability becomes more evident in modality-specific tasks such as
image classification or pure text question answering, where models are expected
to rely solely on one modality. In such tasks, spurious information from
irrelevant modalities often leads to significant performance degradation. We
refer to this failure as Modality Interference, which serves as a concrete and
measurable instance of the cross-modality competency problem. We further design
a perturbation-based causal diagnostic experiment to verify and quantify this
problem. To mitigate modality interference, we propose a novel framework to
fine-tune MLLMs, including perturbation-based data augmentations with both
heuristic perturbations and adversarial perturbations via Projected Gradient
Descent (PGD), and a consistency regularization strategy applied to model
outputs with original and perturbed inputs. Experiments on multiple benchmark
datasets (image-heavy, text-heavy, and VQA tasks) and multiple model families
with different scales demonstrate significant improvements in robustness and
cross-modality competency, indicating our method's effectiveness in boosting
unimodal reasoning ability while enhancing performance on multimodal tasks.",2025-05-26,"Rui Cai, Bangzheng Li, Xiaofei Wen, Muhao Chen, Zhe Zhao",http://arxiv.org/pdf/2505.19616v1,cs.LG
Multiplicity is an Inevitable and Inherent Challenge in Multimodal Learning,"Multimodal learning has seen remarkable progress, particularly with the
emergence of large-scale pre-training across various modalities. However, most
current approaches are built on the assumption of a deterministic, one-to-one
alignment between modalities. This oversimplifies real-world multimodal
relationships, where their nature is inherently many-to-many. This phenomenon,
named multiplicity, is not a side-effect of noise or annotation error, but an
inevitable outcome of semantic abstraction, representational asymmetry, and
task-dependent ambiguity in multimodal tasks. This position paper argues that
multiplicity is a fundamental bottleneck that manifests across all stages of
the multimodal learning pipeline: from data construction to training and
evaluation. This paper examines the causes and consequences of multiplicity,
and highlights how multiplicity introduces training uncertainty, unreliable
evaluation, and low dataset quality. This position calls for new research
directions on multimodal learning: novel multiplicity-aware learning frameworks
and dataset construction protocols considering multiplicity.",2025-05-26,Sanghyuk Chun,http://arxiv.org/pdf/2505.19614v1,cs.LG
Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling,"Long-context supervised fine-tuning (Long-SFT) plays a vital role in
enhancing the performance of large language models (LLMs) on long-context
tasks. To smoothly adapt LLMs to long-context scenarios, this process typically
entails training on mixed datasets containing both long and short sequences.
However, this heterogeneous sequence length distribution poses significant
challenges for existing training systems, as they fail to simultaneously
achieve high training efficiency for both long and short sequences, resulting
in sub-optimal end-to-end system performance in Long-SFT. In this paper, we
present a novel perspective on data scheduling to address the challenges posed
by the heterogeneous data distributions in Long-SFT. We propose Skrull, a
dynamic data scheduler specifically designed for efficient long-SFT. Through
dynamic data scheduling, Skrull balances the computation requirements of long
and short sequences, improving overall training efficiency. Furthermore, we
formulate the scheduling process as a joint optimization problem and thoroughly
analyze the trade-offs involved. Based on those analysis, Skrull employs a
lightweight scheduling algorithm to achieve near-zero cost online scheduling in
Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art
distributed training system for LLMs. Experimental results demonstrate that
Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world
long-SFT scenarios.",2025-05-26,"Hongtao Xu, Wenting Shen, Yuanxin Wei, Ang Wang, Guo Runfan, Tianxing Wang, Yong Li, Mingzhen Li, Weile Jia",http://arxiv.org/pdf/2505.19609v1,cs.LG
Energy-based Preference Optimization for Test-time Adaptation,"Test-Time Adaptation (TTA) enhances model robustness by enabling adaptation
to target distributions that differ from training distributions, improving
real-world generalizability. Existing TTA approaches focus on adjusting the
conditional distribution; however these methods often depend on uncertain
predictions in the absence of label information, leading to unreliable
performance. Energy-based frameworks suggest a promising alternative to address
distribution shifts without relying on uncertain predictions, instead computing
the marginal distribution of target data. However, they involve the critical
challenge of requiring extensive SGLD sampling, which is impractical for
test-time scenarios requiring immediate adaptation. In this work, we propose
Energy-based Preference Optimization for Test-time Adaptation (EPOTTA), which
is based on a sampling free strategy. We first parameterize the target model
using a pretrained model and residual energy function, enabling marginal
likelihood maximization of target data without sampling. Building on the
observation that the parameterization is mathematically equivalent to DPO
objective, we then directly adapt the model to a target distribution without
explicitly training the residual. Our experiments verify that EPOTTA is
well-calibrated and performant while achieving computational efficiency.",2025-05-26,"Yewon Han, Seoyun Yang, Taesup Kim",http://arxiv.org/pdf/2505.19607v1,cs.LG
Kuramoto-FedAvg: Using Synchronization Dynamics to Improve Federated Learning Optimization under Statistical Heterogeneity,"Federated learning on heterogeneous (non-IID) client data experiences slow
convergence due to client drift. To address this challenge, we propose
Kuramoto-FedAvg, a federated optimization algorithm that reframes the weight
aggregation step as a synchronization problem inspired by the Kuramoto model of
coupled oscillators. The server dynamically weighs each client's update based
on its phase alignment with the global update, amplifying contributions that
align with the global gradient direction while minimizing the impact of updates
that are out of phase. We theoretically prove that this synchronization
mechanism reduces client drift, providing a tighter convergence bound compared
to the standard FedAvg under heterogeneous data distributions. Empirical
validation supports our theoretical findings, showing that Kuramoto-FedAvg
significantly accelerates convergence and improves accuracy across multiple
benchmark datasets. Our work highlights the potential of coordination and
synchronization-based strategies for managing gradient diversity and
accelerating federated optimization in realistic non-IID settings.",2025-05-26,"Aggrey Muhebwa, Khotso Selialia, Fatima Anwar, Khalid K. Osman",http://arxiv.org/pdf/2505.19605v1,cs.LG
Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis,"Machine translation has become a critical tool in bridging linguistic gaps,
especially between languages as diverse as English and Hindi. This paper
comprehensively evaluates various machine translation models for translating
between English and Hindi. We assess the performance of these models using a
diverse set of automatic evaluation metrics, both lexical and machine
learning-based metrics. Our evaluation leverages an 18000+ corpus of English
Hindi parallel dataset and a custom FAQ dataset comprising questions from
government websites. The study aims to provide insights into the effectiveness
of different machine translation approaches in handling both general and
specialized language domains. Results indicate varying performance levels
across different metrics, highlighting strengths and areas for improvement in
current translation systems.",2025-05-26,Ahan Prasannakumar Shetty,http://arxiv.org/pdf/2505.19604v1,cs.LG
Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling for Medical Imaging,"In contrast to vision transformers, which model long-range dependencies
through global self-attention, large kernel convolutions provide a more
efficient and scalable alternative, particularly in high-resolution 3D
volumetric settings. However, naively increasing kernel size often leads to
optimization instability and degradation in performance. Motivated by the
spatial bias observed in effective receptive fields (ERFs), we hypothesize that
different kernel elements converge at variable rates during training. To
support this, we derive a theoretical connection between element-wise gradients
and first-order optimization, showing that structurally re-parameterized
convolution blocks inherently induce spatially varying learning rates. Building
on this insight, we introduce Rep3D, a 3D convolutional framework that
incorporates a learnable spatial prior into large kernel training. A
lightweight two-stage modulation network generates a receptive-biased scaling
mask, adaptively re-weighting kernel updates and enabling local-to-global
convergence behavior. Rep3D adopts a plain encoder design with large depthwise
convolutions, avoiding the architectural complexity of multi-branch
compositions. We evaluate Rep3D on five challenging 3D segmentation benchmarks
and demonstrate consistent improvements over state-of-the-art baselines,
including transformer-based and fixed-prior re-parameterization methods. By
unifying spatial inductive bias with optimization-aware learning, Rep3D offers
an interpretable, and scalable solution for 3D medical image analysis. The
source code is publicly available at https://github.com/leeh43/Rep3D.",2025-05-26,"Ho Hin Lee, Quan Liu, Shunxing Bao, Yuankai Huo, Bennett A. Landman",http://arxiv.org/pdf/2505.19603v1,cs.LG
Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression,"Visual Autoregressive (VAR) modeling has garnered significant attention for
its innovative next-scale prediction approach, which yields substantial
improvements in efficiency, scalability, and zero-shot generalization.
Nevertheless, the coarse-to-fine methodology inherent in VAR results in
exponential growth of the KV cache during inference, causing considerable
memory consumption and computational redundancy. To address these bottlenecks,
we introduce ScaleKV, a novel KV cache compression framework tailored for VAR
architectures. ScaleKV leverages two critical observations: varying cache
demands across transformer layers and distinct attention patterns at different
scales. Based on these insights, ScaleKV categorizes transformer layers into
two functional groups: drafters and refiners. Drafters exhibit dispersed
attention across multiple scales, thereby requiring greater cache capacity.
Conversely, refiners focus attention on the current token map to process local
details, consequently necessitating substantially reduced cache capacity.
ScaleKV optimizes the multi-scale inference pipeline by identifying
scale-specific drafters and refiners, facilitating differentiated cache
management tailored to each scale. Evaluation on the state-of-the-art
text-to-image VAR model family, Infinity, demonstrates that our approach
effectively reduces the required KV cache memory to 10% while preserving
pixel-level fidelity.",2025-05-26,"Kunjun Li, Zigeng Chen, Cheng-Yen Yang, Jenq-Neng Hwang",http://arxiv.org/pdf/2505.19602v1,cs.LG
Preference Optimization by Estimating the Ratio of the Data Distribution,"Direct preference optimization (DPO) is widely used as a simple and stable
method for aligning large language models (LLMs) with human preferences. This
paper investigates a generalized DPO loss that enables a policy model to match
the target policy from a likelihood ratio estimation perspective. The ratio of
the target policy provides a unique identification of the policy distribution
without relying on reward models or partition functions. This allows the
generalized loss to retain both simplicity and theoretical guarantees, which
prior work such as $f$-PO fails to achieve simultaneously. We propose Bregman
preference optimization (BPO), a generalized framework for ratio matching that
provides a family of objective functions achieving target policy optimality.
BPO subsumes DPO as a special case and offers tractable forms for all
instances, allowing implementation with a few lines of code. We further develop
scaled Basu's power divergence (SBA), a gradient scaling method that can be
used for BPO instances. The BPO framework complements other DPO variants and is
applicable to target policies defined by these variants. In experiments, unlike
other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a
trade-off between generation fidelity and diversity, instances of BPO improve
both win rate and entropy compared with DPO. When applied to
Llama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B
backbones, with a 55.9\% length-controlled win rate on AlpacaEval2.",2025-05-26,"Yeongmin Kim, Heesun Bae, Byeonghu Na, Il-Chul Moon",http://arxiv.org/pdf/2505.19601v1,cs.LG
Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar,"Typical methods for evaluating the performance of language models evaluate
their ability to answer questions accurately. These evaluation metrics are
acceptable for determining the extent to which language models can understand
and reason about text in a general sense, but fail to capture nuanced
capabilities, such as the ability of language models to recognize and obey rare
grammar points, particularly in languages other than English. We measure the
perplexity of language models when confronted with the ""first person psych
predicate restriction"" grammar point in Japanese. Weblab is the only tested
open source model in the 7-10B parameter range which consistently assigns
higher perplexity to ungrammatical psych predicate sentences than grammatical
ones. We give evidence that Weblab's uniformly bad tokenization is a possible
root cause for its good performance, and show that Llama 3's perplexity on
grammatical psych predicate sentences can be reduced by orders of magnitude
(28x difference) by restricting test sentences to those with uniformly
well-behaved tokenizations. We show in further experiments on machine
translation tasks that language models will use alternative grammar patterns in
order to produce grammatical sentences when tokenization issues prevent the
most natural sentence from being output.",2025-05-26,"Andrew Gambardella, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo",http://arxiv.org/pdf/2505.19599v1,cs.LG
Learning to Reason without External Rewards,"Training large language models (LLMs) for complex reasoning via Reinforcement
Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on
costly, domain-specific supervision. We explore Reinforcement Learning from
Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic
signals without external rewards or labeled data. We propose Intuitor, an RLIF
method that uses a model's own confidence, termed self-certainty, as its sole
reward signal. Intuitor replaces external rewards in Group Relative Policy
Optimization (GRPO) with self-certainty scores, enabling fully unsupervised
learning. Experiments demonstrate that Intuitor matches GRPO's performance on
mathematical benchmarks while achieving superior generalization to
out-of-domain tasks like code generation, without requiring gold solutions or
test cases. Our findings show that intrinsic model signals can drive effective
learning across domains, offering a scalable alternative to RLVR for autonomous
AI systems where verifiable rewards are unavailable. Code is available at
https://github.com/sunblaze-ucb/Intuitor",2025-05-26,"Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, Dawn Song",http://arxiv.org/pdf/2505.19590v1,cs.LG
Model Agnostic Differentially Private Causal Inference,"Estimating causal effects from observational data is essential in fields such
as medicine, economics and social sciences, where privacy concerns are
paramount. We propose a general, model-agnostic framework for differentially
private estimation of average treatment effects (ATE) that avoids strong
structural assumptions on the data-generating process or the models used to
estimate propensity scores and conditional outcomes. In contrast to prior work,
which enforces differential privacy by directly privatizing these nuisance
components and results in a privacy cost that scales with model complexity, our
approach decouples nuisance estimation from privacy protection. This separation
allows the use of flexible, state-of-the-art black-box models, while
differential privacy is achieved by perturbing only predictions and aggregation
steps within a fold-splitting scheme with ensemble techniques. We instantiate
the framework for three classical estimators -- the G-formula, inverse
propensity weighting (IPW), and augmented IPW (AIPW) -- and provide formal
utility and privacy guarantees. Empirical results show that our methods
maintain competitive performance under realistic privacy budgets. We further
extend our framework to support meta-analysis of multiple private ATE
estimates. Our results bridge a critical gap between causal inference and
privacy-preserving data analysis.",2025-05-26,"Christiant Lebeda, Mathieu Even, Aurélien Bellet, Julie Josse",http://arxiv.org/pdf/2505.19589v1,cs.LG
WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts,"Conformal prediction (CP) provides a framework for constructing prediction
sets with guaranteed coverage, assuming exchangeable data. However, real-world
scenarios often involve distribution shifts that violate exchangeability,
leading to unreliable coverage and inflated prediction sets. To address this
challenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction
(RLSCP), which utilizes reconstruction losses derived from a Variational
Autoencoder (VAE) as an uncertainty metric to scale score functions. While
RLSCP demonstrates performance improvements, mainly resulting in better
coverage, it quantifies quantiles based on a fixed calibration dataset without
considering the discrepancies between test and train datasets in an
unexchangeable setting. In the next step, we propose Weighted Quantile
Loss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating
a weighted notion of exchangeability, adjusting the calibration quantile
threshold based on weights with respect to the ratio of calibration and test
loss values. This approach improves the CP-generated prediction set outputs in
the presence of distribution shifts. Experiments on large-scale datasets,
including ImageNet variants, demonstrate that WQLCP outperforms existing
baselines by consistently maintaining coverage while reducing prediction set
sizes, providing a robust solution for CP under distribution shifts.",2025-05-26,"Shadi Alijani, Homayoun Najjaran",http://arxiv.org/pdf/2505.19587v1,cs.LG
Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing,"Sparse attention methods exploit the inherent sparsity in attention to speed
up the prefilling phase of long-context inference, mitigating the quadratic
complexity of full attention computation. While existing sparse attention
methods rely on predefined patterns or inaccurate estimations to approximate
attention behavior, they often fail to fully capture the true dynamics of
attention, resulting in reduced efficiency and compromised accuracy. Instead,
we propose a highly accurate sparse attention mechanism that shares similar yet
precise attention patterns across heads, enabling a more realistic capture of
the dynamic behavior of attention. Our approach is grounded in two key
observations: (1) attention patterns demonstrate strong inter-head similarity,
and (2) this similarity remains remarkably consistent across diverse inputs. By
strategically sharing computed accurate patterns across attention heads, our
method effectively captures actual patterns while requiring full attention
computation for only a small subset of heads. Comprehensive evaluations
demonstrate that our approach achieves superior or comparable speedup relative
to state-of-the-art methods while delivering the best overall accuracy.",2025-05-26,"Dan Peng, Zhihui Fu, Zewen Ye, Zhuoran Song, Jun Wang",http://arxiv.org/pdf/2505.19578v1,cs.LG
Situationally-Aware Dynamics Learning,"Autonomous robots operating in complex, unstructured environments face
significant challenges due to latent, unobserved factors that obscure their
understanding of both their internal state and the external world. Addressing
this challenge would enable robots to develop a more profound grasp of their
operational context. To tackle this, we propose a novel framework for online
learning of hidden state representations, with which the robots can adapt in
real-time to uncertain and dynamic conditions that would otherwise be ambiguous
and result in suboptimal or erroneous behaviors. Our approach is formalized as
a Generalized Hidden Parameter Markov Decision Process, which explicitly models
the influence of unobserved parameters on both transition dynamics and reward
structures. Our core innovation lies in learning online the joint distribution
of state transitions, which serves as an expressive representation of latent
ego- and environmental-factors. This probabilistic approach supports the
identification and adaptation to different operational situations, improving
robustness and safety. Through a multivariate extension of Bayesian Online
Changepoint Detection, our method segments changes in the underlying data
generating process governing the robot's dynamics. The robot's transition model
is then informed with a symbolic representation of the current situation
derived from the joint distribution of latest state transitions, enabling
adaptive and context-aware decision-making. To showcase the real-world
effectiveness, we validate our approach in the challenging task of unstructured
terrain navigation, where unmodeled and unmeasured terrain characteristics can
significantly impact the robot's motion. Extensive experiments in both
simulation and real world reveal significant improvements in data efficiency,
policy performance, and the emergence of safer, adaptive navigation strategies.",2025-05-26,"Alejandro Murillo-Gonzalez, Lantao Liu",http://arxiv.org/pdf/2505.19574v1,cs.LG
MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model,"Maritime transportation is the backbone of global trade, making ship
inspection essential for ensuring maritime safety and environmental protection.
Port State Control (PSC), conducted by national ports, enforces compliance with
safety regulations, with ship detention being the most severe consequence,
impacting both ship schedules and company reputations. Traditional machine
learning methods for ship detention prediction are limited by the capacity of
representation learning and thus suffer from low accuracy. Meanwhile,
autoencoder-based deep learning approaches face challenges due to the severe
data imbalance in learning historical PSC detention records. To address these
limitations, we propose Maritime Ship Detention with Large Language Models
(MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based
autoencoder with a progressive learning pipeline to handle imbalanced data and
extract meaningful PSC representations. Then, a large language model groups and
ranks features to identify likely detention cases, enabling dynamic
thresholding for flexible detention predictions. Extensive evaluations on
31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM
outperforms state-of-the-art methods more than 12\% on Area Under the Curve
(AUC) for Singapore ports. Additionally, it demonstrates robustness to
real-world challenges, making it adaptable to diverse maritime risk assessment
scenarios.",2025-05-26,"Jiongchao Jin, Xiuju Fu, Xiaowei Gao, Tao Cheng, Ran Yan",http://arxiv.org/pdf/2505.19568v1,cs.LG
Lego Sketch: A Scalable Memory-augmented Neural Network for Sketching Data Streams,"Sketches, probabilistic structures for estimating item frequencies in
infinite data streams with limited space, are widely used across various
domains. Recent studies have shifted the focus from handcrafted sketches to
neural sketches, leveraging memory-augmented neural networks (MANNs) to enhance
the streaming compression capabilities and achieve better space-accuracy
trade-offs.However, existing neural sketches struggle to scale across different
data domains and space budgets due to inflexible MANN configurations. In this
paper, we introduce a scalable MANN architecture that brings to life the {\it
Lego sketch}, a novel sketch with superior scalability and accuracy. Much like
assembling creations with modular Lego bricks, the Lego sketch dynamically
coordinates multiple memory bricks to adapt to various space budgets and
diverse data domains. Our theoretical analysis guarantees its high scalability
and provides the first error bound for neural sketch. Furthermore, extensive
experimental evaluations demonstrate that the Lego sketch exhibits superior
space-accuracy trade-offs, outperforming existing handcrafted and neural
sketches. Our code is available at https://github.com/FFY0/LegoSketch_ICML.",2025-05-26,"Yuan Feng, Yukun Cao, Hairu Wang, Xike Xie, S Kevin Zhou",http://arxiv.org/pdf/2505.19561v1,cs.LG
EuroCon: Benchmarking Parliament Deliberation for Political Consensus Finding,"Achieving political consensus is crucial yet challenging for the effective
functioning of social governance. However, although frontier AI systems
represented by large language models (LLMs) have developed rapidly in recent
years, their capabilities on this scope are still understudied. In this paper,
we introduce EuroCon, a novel benchmark constructed from 2,225 high-quality
deliberation records of the European Parliament over 13 years, ranging from
2009 to 2022, to evaluate the ability of LLMs to reach political consensus
among divergent party positions across diverse parliament settings.
Specifically, EuroCon incorporates four factors to build each simulated
parliament setting: specific political issues, political goals, participating
parties, and power structures based on seat distribution. We also develop an
evaluation framework for EuroCon to simulate real voting outcomes in different
parliament settings, assessing whether LLM-generated resolutions meet
predefined political goals. Our experimental results demonstrate that even
state-of-the-art models remain undersatisfied with complex tasks like passing
resolutions by a two-thirds majority and addressing security issues, while
revealing some common strategies LLMs use to find consensus under different
power structures, such as prioritizing the stance of the dominant party,
highlighting EuroCon's promise as an effective platform for studying LLMs'
ability to find political consensus.",2025-05-26,"Zhaowei Zhang, Minghua Yi, Mengmeng Wang, Fengshuo Bai, Zilong Zheng, Yipeng Kang, Yaodong Yang",http://arxiv.org/pdf/2505.19558v1,cs.LG
On scalable and efficient training of diffusion samplers,"We address the challenge of training diffusion models to sample from
unnormalized energy distributions in the absence of data, the so-called
diffusion samplers. Although these approaches have shown promise, they struggle
to scale in more demanding scenarios where energy evaluations are expensive and
the sampling space is high-dimensional. To address this limitation, we propose
a scalable and sample-efficient framework that properly harmonizes the powerful
classical sampling method and the diffusion sampler. Specifically, we utilize
Monte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy
as a Searcher to collect off-policy samples, using an auxiliary energy function
to compensate for exploring modes the diffusion sampler rarely visits. These
off-policy samples are then combined with on-policy data to train the diffusion
sampler, thereby expanding its coverage of the energy landscape. Furthermore,
we identify primacy bias, i.e., the preference of samplers for early experience
during training, as the main cause of mode collapse during training, and
introduce a periodic re-initialization trick to resolve this issue. Our method
significantly improves sample efficiency on standard benchmarks for diffusion
samplers and also excels at higher-dimensional problems and real-world
molecular conformer generation.",2025-05-26,"Minkyu Kim, Kiyoung Seong, Dongyeop Woo, Sungsoo Ahn, Minsu Kim",http://arxiv.org/pdf/2505.19552v1,cs.LG
STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization,"Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful
tool for modeling dynamic graph-structured data across diverse domains.
However, they often fail to generalize in Spatio-Temporal Out-of-Distribution
(STOOD) scenarios, where both temporal dynamics and spatial structures evolve
beyond the training distribution. To address this problem, we propose an
innovative Spatio-Temporal Retrieval-Augmented Pattern Learning
framework,STRAP, which enhances model generalization by integrating
retrieval-augmented learning into the STGNN continue learning pipeline. The
core of STRAP is a compact and expressive pattern library that stores
representative spatio-temporal patterns enriched with historical, structural,
and semantic information, which is obtained and optimized during the training
phase. During inference, STRAP retrieves relevant patterns from this library
based on similarity to the current input and injects them into the model via a
plug-and-play prompting mechanism. This not only strengthens spatio-temporal
representations but also mitigates catastrophic forgetting. Moreover, STRAP
introduces a knowledge-balancing objective to harmonize new information with
retrieved knowledge. Extensive experiments across multiple real-world streaming
graph datasets show that STRAP consistently outperforms state-of-the-art STGNN
baselines on STOOD tasks, demonstrating its robustness, adaptability, and
strong generalization capability without task-specific fine-tuning.",2025-05-26,"Haoyu Zhang, Wentao Zhang, Hao Miao, Xinke Jiang, Yuchen Fang, Yifan Zhang",http://arxiv.org/pdf/2505.19547v1,cs.LG
Unlocking the Power of Diffusion Models in Sequential Recommendation: A Simple and Effective Approach,"In this paper, we focus on the often-overlooked issue of embedding collapse
in existing diffusion-based sequential recommendation models and propose ADRec,
an innovative framework designed to mitigate this problem. Diverging from
previous diffusion-based methods, ADRec applies an independent noise process to
each token and performs diffusion across the entire target sequence during
training. ADRec captures token interdependency through auto-regression while
modeling per-token distributions through token-level diffusion. This dual
approach enables the model to effectively capture both sequence dynamics and
item representations, overcoming the limitations of existing methods. To
further mitigate embedding collapse, we propose a three-stage training
strategy: (1) pre-training the embedding weights, (2) aligning these weights
with the ADRec backbone, and (3) fine-tuning the model. During inference, ADRec
applies the denoising process only to the last token, ensuring that the
meaningful patterns in historical interactions are preserved. Our comprehensive
empirical evaluation across six datasets underscores the effectiveness of ADRec
in enhancing both the accuracy and efficiency of diffusion-based sequential
recommendation systems.",2025-05-26,"Jialei Chen, Yuanbo Xu, Yiheng Jiang",http://arxiv.org/pdf/2505.19544v1,cs.LG
Cuff-KT: Tackling Learners' Real-time Learning Pattern Adjustment via Tuning-Free Knowledge State Guided Model Updating,"Knowledge Tracing (KT) is a core component of Intelligent Tutoring Systems,
modeling learners' knowledge state to predict future performance and provide
personalized learning support. Traditional KT models assume that learners'
learning abilities remain relatively stable over short periods or change in
predictable ways based on prior performance. However, in reality, learners'
abilities change irregularly due to factors like cognitive fatigue, motivation,
and external stress -- a task introduced, which we refer to as Real-time
Learning Pattern Adjustment (RLPA). Existing KT models, when faced with RLPA,
lack sufficient adaptability, because they fail to timely account for the
dynamic nature of different learners' evolving learning patterns. Current
strategies for enhancing adaptability rely on retraining, which leads to
significant overfitting and high time overhead issues. To address this, we
propose Cuff-KT, comprising a controller and a generator. The controller
assigns value scores to learners, while the generator generates personalized
parameters for selected learners. Cuff-KT controllably adapts to data changes
fast and flexibly without fine-tuning. Experiments on five datasets from
different subjects demonstrate that Cuff-KT significantly improves the
performance of five KT models with different structures under intra- and
inter-learner shifts, with an average relative increase in AUC of 10% and 4%,
respectively, at a negligible time cost, effectively tackling RLPA task. Our
code and datasets are fully available at https://github.com/zyy-2001/Cuff-KT.",2025-05-26,"Yiyun Zhou, Zheqi Lv, Shengyu Zhang, Jingyuan Chen",http://arxiv.org/pdf/2505.19543v1,cs.LG
Continuous-Time Analysis of Heavy Ball Momentum in Min-Max Games,"Since Polyak's pioneering work, heavy ball (HB) momentum has been widely
studied in minimization. However, its role in min-max games remains largely
unexplored. As a key component of practical min-max algorithms like Adam, this
gap limits their effectiveness. In this paper, we present a continuous-time
analysis for HB with simultaneous and alternating update schemes in min-max
games. Locally, we prove smaller momentum enhances algorithmic stability by
enabling local convergence across a wider range of step sizes, with alternating
updates generally converging faster. Globally, we study the implicit
regularization of HB, and find smaller momentum guides algorithms trajectories
towards shallower slope regions of the loss landscapes, with alternating
updates amplifying this effect. Surprisingly, all these phenomena differ from
those observed in minimization, where larger momentum yields similar effects.
Our results reveal fundamental differences between HB in min-max games and
minimization, and numerical experiments further validate our theoretical
results.",2025-05-26,"Yi Feng, Kaito Fujii, Stratis Skoulakis, Xiao Wang, Volkan Cevher",http://arxiv.org/pdf/2505.19537v1,cs.LG
Training-Free Multi-Step Audio Source Separation,"Audio source separation aims to separate a mixture into target sources.
Previous audio source separation systems usually conduct one-step inference,
which does not fully explore the separation ability of models. In this work, we
reveal that pretrained one-step audio source separation models can be leveraged
for multi-step separation without additional training. We propose a simple yet
effective inference method that iteratively applies separation by optimally
blending the input mixture with the previous step's separation result. At each
step, we determine the optimal blending ratio by maximizing a metric. We prove
that our method always yield improvement over one-step inference, provide error
bounds based on model smoothness and metric robustness, and provide theoretical
analysis connecting our method to denoising along linear interpolation paths
between noise and clean distributions, a property we link to denoising
diffusion bridge models. Our approach effectively delivers improved separation
performance as a ""free lunch"" from existing models. Our empirical results
demonstrate that our multi-step separation approach consistently outperforms
one-step inference across both speech enhancement and music source separation
tasks, and can achieve scaling performance similar to training a larger model,
using more data, or in some cases employing a multi-step training objective.
These improvements appear not only on the optimization metric during multi-step
inference, but also extend to nearly all non-optimized metrics (with one
exception). We also discuss limitations of our approach and directions for
future research.",2025-05-26,"Yongyi Zang, Jingyi Li, Qiuqiang Kong",http://arxiv.org/pdf/2505.19534v1,cs.LG
ExAnte: A Benchmark for Ex-Ante Inference in Large Language Models,"Large language models (LLMs) face significant challenges in ex-ante
reasoning, where analysis, inference, or predictions must be made without
access to information from future events. Even with explicit prompts enforcing
temporal cutoffs, LLMs often generate outputs influenced by internalized
knowledge of events beyond the specified cutoff. This paper introduces a novel
task and benchmark designed to evaluate the ability of LLMs to reason while
adhering to such temporal constraints. The benchmark includes a variety of
tasks: stock prediction, Wikipedia event prediction, scientific publication
prediction, and Question Answering (QA), designed to assess factual knowledge
under temporal cutoff constraints. We use leakage rate to quantify models'
reliance on future information beyond cutoff timestamps. Experimental results
reveal that LLMs struggle to consistently adhere to temporal cutoffs across
common prompting strategies and tasks, demonstrating persistent challenges in
ex-ante reasoning. This benchmark provides a potential evaluation framework to
advance the development of LLMs' temporal reasoning ability for time-sensitive
applications.",2025-05-26,"Yachuan Liu, Xiaochun Wei, Lin Shi, Xinnuo Li, Bohan Zhang, Paramveer Dhillon, Qiaozhu Mei",http://arxiv.org/pdf/2505.19533v1,cs.LG
Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning,"The current state-of-the-art backdoor attacks against Reinforcement Learning
(RL) rely upon unrealistically permissive access models, that assume the
attacker can read (or even write) the victim's policy parameters, observations,
or rewards. In this work, we question whether such a strong assumption is
required to launch backdoor attacks against RL. To answer this question, we
propose the \underline{S}upply-\underline{C}h\underline{a}in
\underline{B}ackdoor (SCAB) attack, which targets a common RL workflow:
training agents using external agents that are provided separately or embedded
within the environment. In contrast to prior works, our attack only relies on
legitimate interactions of the RL agent with the supplied agents. Despite this
limited access model, by poisoning a mere $3\%$ of training experiences, our
attack can successfully activate over $90\%$ of triggered actions, reducing the
average episodic return by $80\%$ for the victim. Our novel attack demonstrates
that RL attacks are likely to become a reality under untrusted RL training
supply-chains.",2025-05-26,"Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah Erfani, Benjamin I. P. Rubinstein",http://arxiv.org/pdf/2505.19532v1,cs.LG
Minimalist Softmax Attention Provably Learns Constrained Boolean Functions,"We study the computational limits of learning $k$-bit Boolean functions
(specifically, $\mathrm{AND}$, $\mathrm{OR}$, and their noisy variants), using
a minimalist single-head softmax-attention mechanism, where $k=\Theta(d)$
relevant bits are selected from $d$ inputs. We show that these simple
$\mathrm{AND}$ and $\mathrm{OR}$ functions are unsolvable with a single-head
softmax-attention mechanism alone. However, with teacher forcing, the same
minimalist attention is capable of solving them. These findings offer two key
insights: Architecturally, solving these Boolean tasks requires only minimalist
attention, without deep Transformer blocks or FFNs. Methodologically, one
gradient descent update with supervision suffices and replaces the multi-step
Chain-of-Thought (CoT) reasoning scheme of [Kim and Suzuki, ICLR 2025] for
solving Boolean problems. Together, the bounds expose a fundamental gap between
what this minimal architecture achieves under ideal supervision and what is
provably impossible under standard training.",2025-05-26,"Jerry Yao-Chieh Hu, Xiwen Zhang, Maojiang Su, Zhao Song, Han Liu",http://arxiv.org/pdf/2505.19531v1,cs.LG
Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation,"Training large neural networks through gradient-based optimization requires
navigating high-dimensional loss landscapes, which often exhibit pathological
geometry, leading to undesirable training dynamics. In particular, poor
generalization frequently results from convergence to sharp minima that are
highly sensitive to input perturbations, causing the model to overfit the
training data while failing to generalize to unseen examples. Furthermore,
these optimization procedures typically display strong dependence on the fine
structure of the loss landscape, leading to unstable training dynamics, due to
the fractal-like nature of the loss surface. In this work, we propose an
alternative optimizer that simultaneously reduces this dependence, and avoids
sharp minima, thereby improving generalization. This is achieved by simulating
the motion of the center of a ball rolling on the loss landscape. The degree to
which our optimizer departs from the standard gradient descent is controlled by
a hyperparameter, representing the radius of the ball. Changing this
hyperparameter allows for probing the loss landscape at different scales,
making it a valuable tool for understanding its geometry.",2025-05-26,"Mohammed D. Belgoumri, Mohamed Reda Bouadjenek, Hakim Hacid, Imran Razzak, Sunil Aryal",http://arxiv.org/pdf/2505.19527v1,cs.LG
Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate,"Effectively managing missing modalities is a fundamental challenge in
real-world multimodal learning scenarios, where data incompleteness often
results from systematic collection errors or sensor failures. Sparse
Mixture-of-Experts (SMoE) architectures have the potential to naturally handle
multimodal data, with individual experts specializing in different modalities.
However, existing SMoE approach often lacks proper ability to handle missing
modality, leading to performance degradation and poor generalization in
real-world applications. We propose Conf-SMoE to introduce a two-stage
imputation module to handle the missing modality problem for the SMoE
architecture and reveal the insight of expert collapse from theoretical
analysis with strong empirical evidence. Inspired by our theoretical analysis,
Conf-SMoE propose a novel expert gating mechanism by detaching the softmax
routing score to task confidence score w.r.t ground truth. This naturally
relieves expert collapse without introducing additional load balance loss
function. We show that the insights of expert collapse aligns with other gating
mechanism such as Gaussian and Laplacian gate. We also evaluate the proposed
method on four different real world dataset with three different experiment
settings to conduct comprehensive the analysis of Conf-SMoE on modality fusion
and resistance to missing modality.",2025-05-26,"Liangwei Nathan Zheng, Wei Emma Zhang, Mingyu Guo, Miao Xu, Olaf Maennel, Weitong Chen",http://arxiv.org/pdf/2505.19525v1,cs.LG
Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning,"In recent years, image classification, as a core task in computer vision,
relies on high-quality labelled data, which restricts the wide application of
deep learning models in practical scenarios. To alleviate the problem of
insufficient labelled samples, semi-supervised learning has gradually become a
research hotspot. In this paper, we construct a semi-supervised image
classification model based on Generative Adversarial Networks (GANs), and
through the introduction of the collaborative training mechanism of generators,
discriminators and classifiers, we achieve the effective use of limited
labelled data and a large amount of unlabelled data, improve the quality of
image generation and classification accuracy, and provide an effective solution
for the task of image recognition in complex environments.",2025-05-26,"Jiyu Hu, Haijiang Zeng, Zhen Tian",http://arxiv.org/pdf/2505.19522v1,cs.LG
Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures,"Learning unknown dynamics under environmental (or external) constraints is
fundamental to many fields (e.g., modern robotics), particularly challenging
when constraint information is only locally available and uncertain. Existing
approaches requiring global constraints or using probabilistic filtering fail
to fully exploit the geometric structure inherent in local measurements (by
using, e.g., sensors) and constraints. This paper presents a geometric
framework unifying measurements, constraints, and dynamics learning through a
fiber bundle structure over the state space. This naturally induced geometric
structure enables measurement-aware Control Barrier Functions that adapt to
local sensing (or measurement) conditions. By integrating Neural ODEs, our
framework learns continuous-time dynamics while preserving geometric
constraints, with theoretical guarantees of learning convergence and constraint
satisfaction dependent on sensing quality. The geometric framework not only
enables efficient dynamics learning but also suggests promising directions for
integration with reinforcement learning approaches. Extensive simulations
demonstrate significant improvements in both learning efficiency and constraint
satisfaction over traditional methods, especially under limited and uncertain
sensing conditions.",2025-05-26,"Dongzhe Zheng, Wenjie Mei",http://arxiv.org/pdf/2505.19521v1,cs.LG
SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback,"Prompt quality plays a critical role in the performance of large language
models (LLMs), motivating a growing body of work on prompt optimization. Most
existing methods optimize prompts over a fixed dataset, assuming static input
distributions and offering limited support for iterative improvement. We
introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a
closed-loop framework for prompt learning that integrates synthetic data
generation into the optimization process. SIPDO couples a synthetic data
generator with a prompt optimizer, where the generator produces new examples
that reveal current prompt weaknesses and the optimizer incrementally refines
the prompt in response. This feedback-driven loop enables systematic
improvement of prompt performance without assuming access to external
supervision or new tasks. Experiments across question answering and reasoning
benchmarks show that SIPDO outperforms standard prompt tuning methods,
highlighting the value of integrating data synthesis into prompt learning
workflows.",2025-05-26,"Yaoning Yu, Ye Yu, Kai Wei, Haojing Luo, Haohan Wang",http://arxiv.org/pdf/2505.19514v1,cs.LG
Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models,"Large Multimodal Models(LMMs) face notable challenges when encountering
multimodal knowledge conflicts, particularly under retrieval-augmented
generation(RAG) frameworks where the contextual information from external
sources may contradict the model's internal parametric knowledge, leading to
unreliable outputs. However, existing benchmarks fail to reflect such realistic
conflict scenarios. Most focus solely on intra-memory conflicts, while
context-memory and inter-context conflicts remain largely investigated.
Furthermore, commonly used factual knowledge-based evaluations are often
overlooked, and existing datasets lack a thorough investigation into conflict
detection capabilities. To bridge this gap, we propose MMKC-Bench, a benchmark
designed to evaluate factual knowledge conflicts in both context-memory and
inter-context scenarios. MMKC-Bench encompasses three types of multimodal
knowledge conflicts and includes 1,573 knowledge instances and 3,381 images
across 23 broad types, collected through automated pipelines with human
verification. We evaluate three representative series of LMMs on both model
behavior analysis and conflict detection tasks. Our findings show that while
current LMMs are capable of recognizing knowledge conflicts, they tend to favor
internal parametric knowledge over external evidence. We hope MMKC-Bench will
foster further research in multimodal knowledge conflict and enhance the
development of multimodal RAG systems. The source code is available at
https://github.com/MLLMKCBENCH/MLLMKC.",2025-05-26,"Yifan Jia, Kailin Jiang, Yuyang Liang, Qihan Ren, Yi Xin, Rui Yang, Fenze Feng, Mingcai Chen, Hengyang Lu, Haozhe Wang, Xiaoye Qu, Dongrui Liu, Lizhen Cui, Yuntao Du",http://arxiv.org/pdf/2505.19509v1,cs.LG
Multimodal Machine Translation with Visual Scene Graph Pruning,"Multimodal machine translation (MMT) seeks to address the challenges posed by
linguistic polysemy and ambiguity in translation tasks by incorporating visual
information. A key bottleneck in current MMT research is the effective
utilization of visual data. Previous approaches have focused on extracting
global or region-level image features and using attention or gating mechanisms
for multimodal information fusion. However, these methods have not adequately
tackled the issue of visual information redundancy in MMT, nor have they
proposed effective solutions. In this paper, we introduce a novel
approach--multimodal machine translation with visual Scene Graph Pruning (PSG),
which leverages language scene graph information to guide the pruning of
redundant nodes in visual scene graphs, thereby reducing noise in downstream
translation tasks. Through extensive comparative experiments with
state-of-the-art methods and ablation studies, we demonstrate the effectiveness
of the PSG model. Our results also highlight the promising potential of visual
information pruning in advancing the field of MMT.",2025-05-26,"Chenyu Lu, Shiliang Sun, Jing Zhao, Nan Zhang, Tengfei Song, Hao Yang",http://arxiv.org/pdf/2505.19507v1,cs.LG
DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation,"Large Language Models (LLMs) represent substantial intellectual and economic
investments, yet their effectiveness can inadvertently facilitate model
imitation via knowledge distillation (KD).In practical scenarios, competitors
can distill proprietary LLM capabilities by simply observing publicly
accessible outputs, akin to reverse-engineering a complex performance by
observation alone. Existing protective methods like watermarking only identify
imitation post-hoc, while other defenses assume the student model mimics the
teacher's internal logits, rendering them ineffective against distillation
purely from observed output text. This paper confronts the challenge of
actively protecting LLMs within the realistic constraints of API-based access.
We introduce an effective and efficient Defensive Output Generation (DOGe)
strategy that subtly modifies the output behavior of an LLM. Its outputs remain
accurate and useful for legitimate users, yet are designed to be misleading for
distillation, significantly undermining imitation attempts. We achieve this by
fine-tuning only the final linear layer of the teacher LLM with an adversarial
loss. This targeted training approach anticipates and disrupts distillation
attempts during inference time. Our experiments show that, while preserving or
even improving the original performance of the teacher model, student models
distilled from the defensively generated teacher outputs demonstrate
catastrophically reduced performance, demonstrating our method's effectiveness
as a practical safeguard against KD-based model imitation.",2025-05-26,"Pingzhi Li, Zhen Tan, Huaizhi Qu, Huan Liu, Tianlong Chen",http://arxiv.org/pdf/2505.19504v1,cs.LG
Learning for Dynamic Combinatorial Optimization without Training Data,"We introduce DyCO-GNN, a novel unsupervised learning framework for Dynamic
Combinatorial Optimization that requires no training data beyond the problem
instance itself. DyCO-GNN leverages structural similarities across
time-evolving graph snapshots to accelerate optimization while maintaining
solution quality. We evaluate DyCO-GNN on dynamic maximum cut, maximum
independent set, and the traveling salesman problem across diverse datasets of
varying sizes, demonstrating its superior performance under tight and moderate
time budgets. DyCO-GNN consistently outperforms the baseline methods, achieving
high-quality solutions up to 3-60x faster, highlighting its practical
effectiveness in rapidly evolving resource-constrained settings.",2025-05-26,"Yiqiao Liao, Farinaz Koushanfar, Parinaz Naghizadeh",http://arxiv.org/pdf/2505.19497v1,cs.LG
Discounted Online Convex Optimization: Uniform Regret Across a Continuous Interval,"Reflecting the greater significance of recent history over the distant past
in non-stationary environments, $\lambda$-discounted regret has been introduced
in online convex optimization (OCO) to gracefully forget past data as new
information arrives. When the discount factor $\lambda$ is given, online
gradient descent with an appropriate step size achieves an
$O(1/\sqrt{1-\lambda})$ discounted regret. However, the value of $\lambda$ is
often not predetermined in real-world scenarios. This gives rise to a
significant open question: is it possible to develop a discounted algorithm
that adapts to an unknown discount factor. In this paper, we affirmatively
answer this question by providing a novel analysis to demonstrate that smoothed
OGD (SOGD) achieves a uniform $O(\sqrt{\log T/1-\lambda})$ discounted regret,
holding for all values of $\lambda$ across a continuous interval
simultaneously. The basic idea is to maintain multiple OGD instances to handle
different discount factors, and aggregate their outputs sequentially by an
online prediction algorithm named as Discounted-Normal-Predictor (DNP)
(Kapralov and Panigrahy,2010). Our analysis reveals that DNP can combine the
decisions of two experts, even when they operate on discounted regret with
different discount factors.",2025-05-26,"Wenhao Yang, Sifan Yang, Lijun Zhang",http://arxiv.org/pdf/2505.19491v1,cs.LG
Understanding Transformer from the Perspective of Associative Memory,"In this paper, we share our reflections and insights on understanding
Transformer architectures through the lens of associative memory--a classic
psychological concept inspired by human cognition. We start with the basics of
associative memory (think simple linear attention) and then dive into two
dimensions:
  Memory Capacity: How much can a Transformer really remember, and how well? We
introduce retrieval SNR to measure this and use a kernel perspective to
mathematically reveal why Softmax Attention is so effective. We also show how
FFNs can be seen as a type of associative memory, leading to insights on their
design and potential improvements.
  Memory Update: How do these memories learn and evolve? We present a unified
framework for understanding how different Transformer variants (like DeltaNet
and Softmax Attention) update their ""knowledge base"". This leads us to tackle
two provocative questions: 1. Are Transformers fundamentally limited in what
they can express, and can we break these barriers? 2. If a Transformer had
infinite context, would it become infinitely intelligent?
  We want to demystify Transformer architecture, offering a clearer
understanding of existing designs. This exploration aims to provide fresh
insights and spark new avenues for Transformer innovation.",2025-05-26,"Shu Zhong, Mingyu Xu, Tenglong Ao, Guang Shi",http://arxiv.org/pdf/2505.19488v1,cs.LG
VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning,"Traffic signal control (TSC) is a core challenge in urban mobility, where
real-time decisions must balance efficiency and safety. Existing methods -
ranging from rule-based heuristics to reinforcement learning (RL) - often
struggle to generalize to complex, dynamic, and safety-critical scenarios. We
introduce VLMLight, a novel TSC framework that integrates vision-language
meta-control with dual-branch reasoning. At the core of VLMLight is the first
image-based traffic simulator that enables multi-view visual perception at
intersections, allowing policies to reason over rich cues such as vehicle type,
motion, and spatial density. A large language model (LLM) serves as a
safety-prioritized meta-controller, selecting between a fast RL policy for
routine traffic and a structured reasoning branch for critical cases. In the
latter, multiple LLM agents collaborate to assess traffic phases, prioritize
emergency vehicles, and verify rule compliance. Experiments show that VLMLight
reduces waiting times for emergency vehicles by up to 65% over RL-only systems,
while preserving real-time performance in standard conditions with less than 1%
degradation. VLMLight offers a scalable, interpretable, and safety-aware
solution for next-generation traffic signal control.",2025-05-26,"Maonan Wang, Yirong Chen, Aoyu Pang, Yuxin Cai, Chung Shue Chen, Yuheng Kan, Man-On Pun",http://arxiv.org/pdf/2505.19486v1,cs.LG
Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs,"Large language models (LLMs) have shown remarkable performance across diverse
reasoning and generation tasks, and are increasingly deployed as agents in
dynamic environments such as code generation and recommendation systems.
However, many real-world applications, such as high-frequency trading and
real-time competitive gaming, require decisions under strict latency
constraints, where faster responses directly translate into higher rewards.
Despite the importance of this latency quality trade off, it remains
underexplored in the context of LLM based agents. In this work, we present the
first systematic study of this trade off in real time decision making tasks. To
support our investigation, we introduce two new benchmarks: HFTBench, a high
frequency trading simulation, and StreetFighter, a competitive gaming platform.
Our analysis reveals that optimal latency quality balance varies by task, and
that sacrificing quality for lower latency can significantly enhance downstream
performance. To address this, we propose FPX, an adaptive framework that
dynamically selects model size and quantization level based on real time
demands. Our method achieves the best performance on both benchmarks, improving
win rate by up to 80% in Street Fighter and boosting daily yield by up to
26.52% in trading, underscoring the need for latency aware evaluation and
deployment strategies for LLM based agents. These results demonstrate the
critical importance of latency aware evaluation and deployment strategies for
real world LLM based agents. Our benchmarks are available at Latency Sensitive
Benchmarks.",2025-05-26,"Hao Kang, Qingru Zhang, Han Cai, Weiyuan Xu, Tushar Krishna, Yilun Du, Tsachy Weissman",http://arxiv.org/pdf/2505.19481v1,cs.LG
Revolutionizing Wildfire Detection with Convolutional Neural Networks: A VGG16 Model Approach,"Over 8,024 wildfire incidents have been documented in 2024 alone, affecting
thousands of fatalities and significant damage to infrastructure and
ecosystems. Wildfires in the United States have inflicted devastating losses.
Wildfires are becoming more frequent and intense, which highlights how urgently
efficient warning systems are needed to avoid disastrous outcomes. The goal of
this study is to enhance the accuracy of wildfire detection by using
Convolutional Neural Network (CNN) built on the VGG16 architecture. The D-FIRE
dataset, which includes several kinds of wildfire and non-wildfire images, was
employed in the study. Low-resolution images, dataset imbalance, and the
necessity for real-time applicability are some of the main challenges. These
problems were resolved by enriching the dataset using data augmentation
techniques and optimizing the VGG16 model for binary classification. The model
produced a low false negative rate, which is essential for reducing unexplored
fires, despite dataset boundaries. In order to help authorities execute fast
responses, this work shows that deep learning models such as VGG16 can offer a
reliable, automated approach for early wildfire recognition. For the purpose of
reducing the impact of wildfires, our future work will concentrate on
connecting to systems with real-time surveillance networks and enlarging the
dataset to cover more varied fire situations.",2025-05-26,"Lakshmi Aishwarya Malladi, Navarun Gupta, Ahmed El-Sayed, Xingguo Xiong",http://arxiv.org/pdf/2505.19479v1,cs.LG
Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables,"Latent variables (LVs) play a crucial role in encoder-decoder models by
enabling effective data compression, prediction, and generation. Although their
theoretical properties, such as generalization, have been extensively studied
in supervised learning, similar analyses for unsupervised models such as
variational autoencoders (VAEs) remain insufficiently underexplored. In this
work, we extend information-theoretic generalization analysis to
vector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel
data-dependent prior to rigorously analyze the relationship among LVs,
generalization, and data generation. We derive a novel generalization error
bound of the reconstruction loss of VQ-VAEs, which depends solely on the
complexity of LVs and the encoder, independent of the decoder. Additionally, we
provide the upper bound of the 2-Wasserstein distance between the distributions
of the true data and the generated data, explaining how the regularization of
the LVs contributes to the data generation performance.",2025-05-26,"Futoshi Futami, Masahiro Fujisawa",http://arxiv.org/pdf/2505.19470v1,cs.LG
Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory,"Dataset distillation enables the training of deep neural networks with
comparable performance in significantly reduced time by compressing large
datasets into small and representative ones. Although the introduction of
generative models has made great achievements in this field, the distributions
of their distilled datasets are not diverse enough to represent the original
ones, leading to a decrease in downstream validation accuracy. In this paper,
we present a diversity-driven generative dataset distillation method based on a
diffusion model to solve this problem. We introduce self-adaptive memory to
align the distribution between distilled and real datasets, assessing the
representativeness. The degree of alignment leads the diffusion model to
generate more diverse datasets during the distillation process. Extensive
experiments show that our method outperforms existing state-of-the-art methods
in most situations, proving its ability to tackle dataset distillation tasks.",2025-05-26,"Mingzhuo Li, Guang Li, Jiafeng Mao, Takahiro Ogawa, Miki Haseyama",http://arxiv.org/pdf/2505.19469v1,cs.LG
Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs,"As large language models (LLMs) continue to advance, their deployment often
involves fine-tuning to enhance performance on specific downstream tasks.
However, this customization is sometimes accompanied by misleading claims about
the origins, raising significant concerns about transparency and trust within
the open-source community. Existing model verification techniques typically
assess functional, representational, and weight similarities. However, these
approaches often struggle against obfuscation techniques, such as permutations
and scaling transformations. To address this limitation, we propose a novel
detection method Origin-Tracer that rigorously determines whether a model has
been fine-tuned from a specified base model. This method includes the ability
to extract the LoRA rank utilized during the fine-tuning process, providing a
more robust verification framework. This framework is the first to provide a
formalized approach specifically aimed at pinpointing the sources of model
fine-tuning. We empirically validated our method on thirty-one diverse
open-source models under conditions that simulate real-world obfuscation
scenarios. We empirically analyze the effectiveness of our framework and
finally, discuss its limitations. The results demonstrate the effectiveness of
our approach and indicate its potential to establish new benchmarks for model
verification.",2025-05-26,"Hongyu Liang, Yuting Zheng, Yihan Li, Yiran Zhang, Shiyu Liang",http://arxiv.org/pdf/2505.19466v1,cs.LG
Residual Cross-Attention Transformer-Based Multi-User CSI Feedback with Deep Joint Source-Channel Coding,"This letter proposes a deep-learning (DL)-based multi-user channel state
information (CSI) feedback framework for massive multiple-input multiple-output
systems, where the deep joint source-channel coding (DJSCC) is utilized to
improve the CSI reconstruction accuracy. Specifically, we design a multi-user
joint CSI feedback framework, whereby the CSI correlation of nearby users is
utilized to reduce the feedback overhead. Under the framework, we propose a new
residual cross-attention transformer architecture, which is deployed at the
base station to further improve the CSI feedback performance. Moreover, to
tackle the ""cliff-effect"" of conventional bit-level CSI feedback approaches, we
integrated DJSCC into the multi-user CSI feedback, together with utilizing a
two-stage training scheme to adapt to varying uplink noise levels. Experimental
results demonstrate the superiority of our methods in CSI feedback performance,
with low network complexity and better scalability.",2025-05-26,"Hengwei Zhang, Minghui Wu, Li Qiao, Ling Liu, Ziqi Han, Zhen Gao",http://arxiv.org/pdf/2505.19465v1,cs.LG
"Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation","Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative
models, are well known for their ability to achieve both high classification
accuracy and generative capability within a single model. However, their
robustness still lags significantly behind the classifiers based adversarial
training (AT). Conversely, while AT is currently the most effective approach to
improving the classifier's robustness, it typically sacrifices accuracy on
clean data and lacks generative capability. The triple trade-off between
classification accuracy, generative capability and robustness, raises a natural
question: Can a single model simultaneously achieve high classification
accuracy, adversarial robustness, and generative performance? -- a goal that
has been rarely explored. To address this question, we systematically analyze
the energy distribution differences of clean, adversarial, and generated
samples across various JEM variants and adversarially trained models. We
observe that AT tends to reduce the energy gap between clean and adversarial
samples, while JEMs reduce the gap between clean and synthetic ones. This
observation suggests a key insight: if the energy distributions of all three
data types can be aligned, we might unify the strengths of AT and JEMs,
resolving their inherent trade-offs. Building on this idea, we propose
Energy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly
model the clean data distribution, the adversarial distribution, and the
classifier by maximizing their joint probability. EB-JDAT is a general and
flexible optimization method, compatible with various JEM variants. Extensive
experimental results demonstrate that EB-JDAT not only maintains near original
accuracy and generative capability of JEMs, but also significantly enhances
robustness, even surpassing state-of-the-art ATs.",2025-05-26,"Kaichao Jiang, He Wang, Xiaoshuai Hao, Xiulong Yang, Ajian Liu, Qi Chu, Yunfeng Diao",http://arxiv.org/pdf/2505.19459v1,cs.LG
Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians,"The theoretical understanding of self-attention (SA) has been steadily
progressing. A prominent line of work studies a class of SA layers that admit
an energy function decreased by state updates. While it provides valuable
insights into inherent biases in signal propagation, it often relies on
idealized assumptions or additional constraints not necessarily present in
standard SA. Thus, to broaden our understanding, this work aims to relax these
energy constraints and provide an energy-agnostic characterization of inference
dynamics by dynamical systems analysis. In more detail, we first consider
relaxing the symmetry and single-head constraints traditionally required in
energy-based formulations. Next, to investigate more general SA architectures
capable of oscillatory dynamics without necessarily admitting an energy
function, we analyze the Jacobian matrix of the state. We reveal that
normalization layers effectively normalize the Jacobian's complex eigenvalues,
forcing the dynamics close to a critical state. This significantly enhances
inference performance. Furthermore, we utilize the Jacobian perspective to
develop regularization methods for training and a pseudo-energy for monitoring
inference dynamics.",2025-05-26,"Akiyoshi Tomihari, Ryo Karakida",http://arxiv.org/pdf/2505.19458v1,cs.LG
MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering,"Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs)
has achieved promising progress by leveraging prompt tuning to enable continual
multi-modal learning. However, most existing methods adopt cross-modal prompt
isolation, constructing visual and textual prompts separately, which
exacerbates modality imbalance and leads to degraded performance over time. To
tackle this issue, we propose MM-Prompt, a novel framework incorporating
cross-modal prompt query and cross-modal prompt recovery. The former enables
balanced prompt selection by incorporating cross-modal signals during query
formation, while the latter promotes joint prompt reconstruction through
iterative cross-modal interactions, guided by an alignment loss to prevent
representational drift. Extensive experiments show that MM-Prompt surpasses
prior approaches in accuracy and knowledge retention, while maintaining
balanced modality engagement throughout continual learning.",2025-05-26,"Xu Li, Fan Lyu",http://arxiv.org/pdf/2505.19455v1,cs.LG
MetaGMT: Improving Actionable Interpretability of Graph Multilinear Networks via Meta-Learning Filtration,"The growing adoption of Graph Neural Networks (GNNs) in high-stakes domains
like healthcare and finance demands reliable explanations of their
decision-making processes. While inherently interpretable GNN architectures
like Graph Multi-linear Networks (GMT) have emerged, they remain vulnerable to
generating explanations based on spurious correlations, potentially undermining
trust in critical applications. We present MetaGMT, a meta-learning framework
that enhances explanation fidelity through a novel bi-level optimization
approach. We demonstrate that MetaGMT significantly improves both explanation
quality (AUC-ROC, Precision@K) and robustness to spurious patterns, across
BA-2Motifs, MUTAG, and SP-Motif benchmarks. Our approach maintains competitive
classification accuracy while producing more faithful explanations (with an
increase up to 8% of Explanation ROC on SP-Motif 0.5) compared to baseline
methods. These advancements in interpretability could enable safer deployment
of GNNs in sensitive domains by (1) facilitating model debugging through more
reliable explanations, (2) supporting targeted retraining when biases are
identified, and (3) enabling meaningful human oversight. By addressing the
critical challenge of explanation reliability, our work contributes to building
more trustworthy and actionable GNN systems for real-world applications.",2025-05-26,"Rishabh Bhattacharya, Hari Shankar, Vaishnavi Shivkumar, Ponnurangam Kumaraguru",http://arxiv.org/pdf/2505.19445v1,cs.LG
Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems,"The rapid proliferation of recommender systems necessitates robust fairness
practices to address inherent biases. Assessing fairness, though, is
challenging due to constantly evolving metrics and best practices. This paper
analyzes how industry practitioners perceive and incorporate these changing
fairness standards in their workflows. Through semi-structured interviews with
11 practitioners from technical teams across a range of large technology
companies, we investigate industry implementations of fairness in
recommendation system products. We focus on current debiasing practices,
applied metrics, collaborative strategies, and integrating academic research
into practice. Findings show a preference for multi-dimensional debiasing over
traditional demographic methods, and a reliance on intuitive rather than
academic metrics. This study also highlights the difficulties in balancing
fairness with both the practitioner's individual (bottom-up) roles and
organizational (top-down) workplace constraints, including the interplay with
legal and compliance experts. Finally, we offer actionable recommendations for
the recommender system community and algorithmic fairness practitioners,
underlining the need to refine fairness practices continually.",2025-05-26,"Jing Nathan Yan, Junxiong Wang, Jeffrey M. Rzeszotarski, Allison Koenecke",http://arxiv.org/pdf/2505.19441v1,cs.LG
"The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models","This paper studies the emergence of interpretable categorical features within
large language models (LLMs), analyzing their behavior across training
checkpoints (time), transformer layers (space), and varying model sizes
(scale). Using sparse autoencoders for mechanistic interpretability, we
identify when and where specific semantic concepts emerge within neural
activations. Results indicate clear temporal and scale-specific thresholds for
feature emergence across multiple domains. Notably, spatial analysis reveals
unexpected semantic reactivation, with early-layer features re-emerging at
later layers, challenging standard assumptions about representational dynamics
in transformer models.",2025-05-26,"Shashata Sawmya, Micah Adler, Nir Shavit",http://arxiv.org/pdf/2505.19440v1,cs.LG
Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression,"Post-training compression reduces the computational and memory costs of large
language models (LLMs), enabling resource-efficient deployment. However,
existing compression benchmarks only focus on language modeling (e.g.,
perplexity) and natural language understanding tasks (e.g., GLUE accuracy),
ignoring the agentic capabilities - workflow, tool use/function call,
long-context understanding and real-world application. We introduce the Agent
Compression Benchmark (ACBench), the first comprehensive benchmark for
evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1)
12 tasks across 4 capabilities (e.g., WorfBench for workflow generation,
Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ)
and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B),
standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill).
Our experiments reveal compression tradeoffs: 4-bit quantization preserves
workflow generation and tool use (1%-3% drop) but degrades real-world
application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation
and Energy to systematize analysis. ACBench provides actionable insights for
optimizing LLM compression in agentic scenarios. The code can be found in
https://github.com/pprp/ACBench.",2025-05-26,"Peijie Dong, Zhenheng Tang, Xiang Liu, Lujun Li, Xiaowen Chu, Bo Li",http://arxiv.org/pdf/2505.19433v1,cs.LG
Advanced long-term earth system forecasting by learning the small-scale nature,"Reliable long-term forecast of Earth system dynamics is heavily hampered by
instabilities in current AI models during extended autoregressive simulations.
These failures often originate from inherent spectral bias, leading to
inadequate representation of critical high-frequency, small-scale processes and
subsequent uncontrolled error amplification. We present Triton, an AI framework
designed to address this fundamental challenge. Inspired by increasing grids to
explicitly resolve small scales in numerical models, Triton employs a
hierarchical architecture processing information across multiple resolutions to
mitigate spectral bias and explicitly model cross-scale dynamics. We
demonstrate Triton's superior performance on challenging forecast tasks,
achieving stable year-long global temperature forecasts, skillful Kuroshio eddy
predictions till 120 days, and high-fidelity turbulence simulations preserving
fine-scale structures all without external forcing, with significantly
surpassing baseline AI models in long-term stability and accuracy. By
effectively suppressing high-frequency error accumulation, Triton offers a
promising pathway towards trustworthy AI-driven simulation for climate and
earth system science.",2025-05-26,"Hao Wu, Yuan Gao, Ruiqi Shu, Kun Wang, Ruijian Gou, Chuhan Wu, Xinliang Liu, Juncai He, Shuhao Cao, Junfeng Fang, Xingjian Shi, Feng Tao, Qi Song, Shengxuan Ji, Yanfei Xiang, Yuze Sun, Jiahao Li, Fan Xu, Huanshuo Dong, Haixin Wang, Fan Zhang, Penghao Zhao, Xian Wu, Qingsong Wen, Deliang Chen, Xiaomeng Huang",http://arxiv.org/pdf/2505.19432v1,cs.LG
Importance Weighted Score Matching for Diffusion Samplers with Enhanced Mode Coverage,"Training neural samplers directly from unnormalized densities without access
to target distribution samples presents a significant challenge. A critical
desideratum in these settings is achieving comprehensive mode coverage,
ensuring the sampler captures the full diversity of the target distribution.
However, prevailing methods often circumvent the lack of target data by
optimizing reverse KL-based objectives. Such objectives inherently exhibit
mode-seeking behavior, potentially leading to incomplete representation of the
underlying distribution. While alternative approaches strive for better mode
coverage, they typically rely on implicit mechanisms like heuristics or
iterative refinement. In this work, we propose a principled approach for
training diffusion-based samplers by directly targeting an objective analogous
to the forward KL divergence, which is conceptually known to encourage mode
coverage. We introduce \textit{Importance Weighted Score Matching}, a method
that optimizes this desired mode-covering objective by re-weighting the score
matching loss using tractable importance sampling estimates, thereby overcoming
the absence of target distribution data. We also provide theoretical analysis
of the bias and variance for our proposed Monte Carlo estimator and the
practical loss function used in our method. Experiments on increasingly complex
multi-modal distributions, including 2D Gaussian Mixture Models with up to 120
modes and challenging particle systems with inherent symmetries -- demonstrate
that our approach consistently outperforms existing neural samplers across all
distributional distance metrics, achieving state-of-the-art results on all
benchmarks.",2025-05-26,"Chenguang Wang, Xiaoyu Zhang, Kaiyuan Cui, Weichen Zhao, Yongtao Guan, Tianshu Yu",http://arxiv.org/pdf/2505.19431v1,cs.LG
WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference,"The growing computational demands of large language models (LLMs) make
efficient inference and activation strategies increasingly critical. While
recent approaches, such as Mixture-of-Experts (MoE), leverage selective
activation but require specialized training, training-free sparse activation
methods offer broader applicability and superior resource efficiency through
their plug-and-play design. However, many existing methods rely solely on
hidden state magnitudes to determine activation, resulting in high
approximation errors and suboptimal inference accuracy. To address these
limitations, we propose WINA (Weight Informed Neuron Activation), a novel,
simple, and training-free sparse activation framework that jointly considers
hidden state magnitudes and the column-wise $\ell_2$-norms of weight matrices.
We show that this leads to a sparsification strategy that obtains optimal
approximation error bounds with theoretical guarantees tighter than existing
techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,
TEAL) by up to $2.94\%$ in average performance at the same sparsity levels,
across a diverse set of LLM architectures and datasets. These results position
WINA as a new performance frontier for training-free sparse activation in LLM
inference, advancing training-free sparse activation methods and setting a
robust baseline for efficient inference. The source code is available at
https://github.com/microsoft/wina.",2025-05-26,"Sihan Chen, Dan Zhao, Jongwoo Ko, Colby Banbury, Huiping Zhuang, Luming Liang, Tianyi Chen",http://arxiv.org/pdf/2505.19427v1,cs.LG
The Role of Diversity in In-Context Learning for Large Language Models,"In-context learning (ICL) is a crucial capability of current large language
models (LLMs), where the selection of examples plays a key role in performance.
While most existing approaches focus on selecting the most similar examples to
the query, the impact of diversity in example selection remains underexplored.
We systematically investigate the role of diversity in in-context example
selection through experiments across a range of tasks, from sentiment
classification to more challenging math and code problems. Experiments on
Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that
diversity-aware selection methods improve performance, particularly on complex
tasks like math and code, and enhance robustness to out-of-distribution
queries. To support these findings, we introduce a theoretical framework that
explains the benefits of incorporating diversity in in-context example
selection.",2025-05-26,"Wenyang Xiao, Haoyu Zhao, Lingxiao Huang",http://arxiv.org/pdf/2505.19426v1,cs.LG
Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation,"The rapid advancement of diffusion models has enhanced their image inpainting
and editing capabilities but also introduced significant societal risks.
Adversaries can exploit user images from social media to generate misleading or
harmful content. While adversarial perturbations can disrupt inpainting, global
perturbation-based methods fail in mask-guided editing tasks due to spatial
constraints. To address these challenges, we propose Structure Disruption
Attack (SDA), a powerful protection framework for safeguarding sensitive image
regions against inpainting-based editing. Building upon the contour-focused
nature of self-attention mechanisms of diffusion models, SDA optimizes
perturbations by disrupting queries in self-attention during the initial
denoising step to destroy the contour generation process. This targeted
interference directly disrupts the structural generation capability of
diffusion models, effectively preventing them from producing coherent images.
We validate our motivation through visualization techniques and extensive
experiments on public datasets, demonstrating that SDA achieves
state-of-the-art (SOTA) protection performance while maintaining strong
robustness.",2025-05-26,"Yuhao He, Jinyu Tian, Haiwei Wu, Jianqing Li",http://arxiv.org/pdf/2505.19425v1,cs.LG
Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network,"Evolutionary Reinforcement Learning (ERL), training the Reinforcement
Learning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated
enhanced exploration capabilities and greater robustness than using traditional
policy gradient. However, ERL suffers from the high computational costs and low
search efficiency, as EAs require evaluating numerous candidate policies with
expensive simulations, many of which are ineffective and do not contribute
meaningfully to the training. One intuitive way to reduce the ineffective
evaluations is to adopt the surrogates. Unfortunately, existing ERL policies
are often modeled as deep neural networks (DNNs) and thus naturally represented
as high-dimensional vectors containing millions of weights, which makes the
building of effective surrogates for ERL policies extremely challenging. This
paper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE)
and Hyperbolic Neural Networks (HNN). Specifically, AE compresses
high-dimensional policies into low-dimensional representations while extracting
key features as the inputs for the surrogate. HNN, functioning as a
classification-based surrogate model, can learn complex nonlinear relationships
from sampled data and enable more accurate pre-selection of the sampled
policies without real evaluations. The experiments on 10 Atari and 4 Mujoco
games have verified that the proposed method outperforms previous approaches
significantly. The search trajectories guided by AE and HNN are also visually
demonstrated to be more effective, in terms of both exploration and
convergence. This paper not only presents the first learnable policy embedding
and surrogate-modeling modules for high-dimensional ERL policies, but also
empirically reveals when and why they can be successful.",2025-05-26,"Bingdong Li, Mei Jiang, Hong Qian, Peng Yang, Wenjing Hong, Hong Qian, Ke Tang",http://arxiv.org/pdf/2505.19423v1,cs.LG
Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study,"Data centers are the backbone of computing capacity. Operating data centers
in the tropical regions faces unique challenges due to consistently high
ambient temperature and elevated relative humidity throughout the year. These
conditions result in increased cooling costs to maintain the reliability of the
computing systems. While existing machine learning-based approaches have
demonstrated potential to elevate operations to a more proactive and
intelligent level, their deployment remains dubious due to concerns about model
extrapolation capabilities and associated system safety issues. To address
these concerns, this article proposes incorporating the physical
characteristics of data centers into traditional data-driven machine learning
solutions. We begin by introducing the data center system, including the
relevant multiphysics processes and the data-physics availability. Next, we
outline the associated modeling and optimization problems and propose an
integrated, physics-informed machine learning system to address them. Using the
proposed system, we present relevant applications across varying levels of
operational intelligence. A case study on an industry-grade tropical data
center is provided to demonstrate the effectiveness of our approach. Finally,
we discuss key challenges and highlight potential future directions.",2025-05-26,"Ruihang Wang, Zhiwei Cao, Qingang Zhang, Rui Tan, Yonggang Wen, Tommy Leung, Stuart Kennedy, Justin Teoh",http://arxiv.org/pdf/2505.19414v1,cs.LG
Future Link Prediction Without Memory or Aggregation,"Future link prediction on temporal graphs is a fundamental task with wide
applicability in real-world dynamic systems. These scenarios often involve both
recurring (seen) and novel (unseen) interactions, requiring models to
generalize effectively across both types of edges. However, existing methods
typically rely on complex memory and aggregation modules, yet struggle to
handle unseen edges. In this paper, we revisit the architecture of existing
temporal graph models and identify two essential but overlooked modeling
requirements for future link prediction: representing nodes with unique
identifiers and performing target-aware matching between source and destination
nodes. To this end, we propose Cross-Attention based Future Link Predictor on
Temporal Graphs (CRAFT), a simple yet effective architecture that discards
memory and aggregation modules and instead builds on two components: learnable
node embeddings and cross-attention between the destination and the source's
recent interactions. This design provides strong expressive power and enables
target-aware modeling of the compatibility between candidate destinations and
the source's interaction patterns. Extensive experiments on diverse datasets
demonstrate that CRAFT consistently achieves superior performance with high
efficiency, making it well-suited for large-scale real-world applications.",2025-05-26,"Lu Yi, Runlin Lei, Fengran Mo, Yanping Zheng, Zhewei Wei, Yuhang Ye",http://arxiv.org/pdf/2505.19408v1,cs.LG
Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning,"Federated Active Learning (FAL) seeks to reduce the burden of annotation
under the realistic constraints of federated learning by leveraging Active
Learning (AL). As FAL settings make it more expensive to obtain ground truth
labels, FAL strategies that work well in low-budget regimes, where the amount
of annotation is very limited, are needed. In this work, we investigate the
effectiveness of TypiClust, a successful low-budget AL strategy, in low-budget
FAL settings. Our empirical results show that TypiClust works well even in
low-budget FAL settings contrasted with relatively low performances of other
methods, although these settings present additional challenges, such as data
heterogeneity, compared to AL. In addition, we show that FAL settings cause
distribution shifts in terms of typicality, but TypiClust is not very
vulnerable to the shifts. We also analyze the sensitivity of TypiClust to
feature extraction methods, and it suggests a way to perform FAL even in
limited data situations.",2025-05-26,"Yuta Ono, Hiroshi Nakamura, Hideki Takase",http://arxiv.org/pdf/2505.19404v1,cs.LG
Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains,"Time Series Foundation Models (TSFMs), which are pretrained on large-scale,
cross-domain data and capable of zero-shot forecasting in new scenarios without
further training, are increasingly adopted in real-world applications. However,
as the zero-shot forecasting paradigm gets popular, a critical yet overlooked
question emerges: Are TSFMs robust to adversarial input perturbations? Such
perturbations could be exploited in man-in-the-middle attacks or data
poisoning. To address this gap, we conduct a systematic investigation into the
adversarial robustness of TSFMs. Our results show that even minimal
perturbations can induce significant and controllable changes in forecast
behaviors, including trend reversal, temporal drift, and amplitude shift,
posing serious risks to TSFM-based services. Through experiments on
representative TSFMs and multiple datasets, we reveal their consistent
vulnerabilities and identify potential architectural designs, such as
structural sparsity and multi-task pretraining, that may improve robustness.
Our findings offer actionable guidance for designing more resilient forecasting
systems and provide a critical assessment of the adversarial robustness of
TSFMs.",2025-05-26,"Jiawen Zhang, Zhenwei Zhang, Shun Zheng, Xumeng Wen, Jia Li, Jiang Bian",http://arxiv.org/pdf/2505.19397v1,cs.LG
Uniform convergence of the smooth calibration error and its relationship with functional gradient,"Calibration is a critical requirement for reliable probabilistic prediction,
especially in high-risk applications. However, the theoretical understanding of
which learning algorithms can simultaneously achieve high accuracy and good
calibration remains limited, and many existing studies provide empirical
validation or a theoretical guarantee in restrictive settings. To address this
issue, in this work, we focus on the smooth calibration error (CE) and provide
a uniform convergence bound, showing that the smooth CE is bounded by the sum
of the smooth CE over the training dataset and a generalization gap. We further
prove that the functional gradient of the loss function can effectively control
the training smooth CE. Based on this framework, we analyze three
representative algorithms: gradient boosting trees, kernel boosting, and
two-layer neural networks. For each, we derive conditions under which both
classification and calibration performances are simultaneously guaranteed. Our
results offer new theoretical insights and practical guidance for designing
reliable probabilistic models with provable calibration guarantees.",2025-05-26,"Futoshi Futami, Atsushi Nitanda",http://arxiv.org/pdf/2505.19396v1,cs.LG
Alignment of large language models with constrained learning,"We study the problem of computing an optimal large language model (LLM)
policy for a constrained alignment problem, where the goal is to maximize a
primary reward objective while satisfying constraints on secondary utilities.
Despite the popularity of Lagrangian-based LLM policy search in constrained
alignment, iterative primal-dual methods often fail to converge, and
non-iterative dual-based methods do not achieve optimality in the LLM parameter
space. To address these challenges, we employ Lagrangian duality to develop an
iterative dual-based alignment method that alternates between updating the LLM
policy via Lagrangian maximization and updating the dual variable via dual
descent. In theory, we characterize the primal-dual gap between the primal
value in the distribution space and the dual value in the LLM parameter space.
We further quantify the optimality gap of the learned LLM policies at
near-optimal dual variables with respect to both the objective and the
constraint functions. These results prove that dual-based alignment methods can
find an optimal constrained LLM policy, up to an LLM parametrization gap. We
demonstrate the effectiveness and merits of our approach through extensive
experiments conducted on the PKU-SafeRLHF dataset.",2025-05-26,"Botong Zhang, Shuo Li, Ignacio Hounie, Osbert Bastani, Dongsheng Ding, Alejandro Ribeiro",http://arxiv.org/pdf/2505.19387v1,cs.LG
Foundations of Top-$k$ Decoding For Language Models,"Top-$k$ decoding is a widely used method for sampling from LLMs: at each
token, only the largest $k$ next-token-probabilities are kept, and the next
token is sampled after re-normalizing them to sum to unity. Top-$k$ and other
sampling methods are motivated by the intuition that true next-token
distributions are sparse, and the noisy LLM probabilities need to be truncated.
However, to our knowledge, a precise theoretical motivation for the use of
top-$k$ decoding is missing. In this work, we develop a theoretical framework
that both explains and generalizes top-$k$ decoding. We view decoding at a
fixed token as the recovery of a sparse probability distribution. We consider
\emph{Bregman decoders} obtained by minimizing a separable Bregman divergence
(for both the \emph{primal} and \emph{dual} cases) with a sparsity-inducing
$\ell_0$ regularization. Despite the combinatorial nature of the objective, we
show how to optimize it efficiently for a large class of divergences. We show
that the optimal decoding strategies are greedy, and further that the loss
function is discretely convex in $k$, so that binary search provably and
efficiently finds the optimal $k$. We show that top-$k$ decoding arises as a
special case for the KL divergence, and identify new decoding strategies that
have distinct behaviors (e.g., non-linearly up-weighting larger probabilities
after re-normalization).",2025-05-25,"Georgy Noarov, Soham Mallick, Tao Wang, Sunay Joshi, Yan Sun, Yangxinyu Xie, Mengxin Yu, Edgar Dobriban",http://arxiv.org/pdf/2505.19371v1,cs.LG
SETransformer: A Hybrid Attention-Based Architecture for Robust Human Activity Recognition,"Human Activity Recognition (HAR) using wearable sensor data has become a
central task in mobile computing, healthcare, and human-computer interaction.
Despite the success of traditional deep learning models such as CNNs and RNNs,
they often struggle to capture long-range temporal dependencies and contextual
relevance across multiple sensor channels. To address these limitations, we
propose SETransformer, a hybrid deep neural architecture that combines
Transformer-based temporal modeling with channel-wise squeeze-and-excitation
(SE) attention and a learnable temporal attention pooling mechanism. The model
takes raw triaxial accelerometer data as input and leverages global
self-attention to capture activity-specific motion dynamics over extended time
windows, while adaptively emphasizing informative sensor channels and critical
time steps.
  We evaluate SETransformer on the WISDM dataset and demonstrate that it
significantly outperforms conventional models including LSTM, GRU, BiLSTM, and
CNN baselines. The proposed model achieves a validation accuracy of 84.68\% and
a macro F1-score of 84.64\%, surpassing all baseline architectures by a notable
margin. Our results show that SETransformer is a competitive and interpretable
solution for real-world HAR tasks, with strong potential for deployment in
mobile and ubiquitous sensing applications.",2025-05-25,"Yunbo Liu, Xukui Qin, Yifan Gao, Xiang Li, Chengwei Feng",http://arxiv.org/pdf/2505.19369v1,cs.LG
Adaptive Diffusion Guidance via Stochastic Optimal Control,"Guidance is a cornerstone of modern diffusion models, playing a pivotal role
in conditional generation and enhancing the quality of unconditional samples.
However, current approaches to guidance scheduling--determining the appropriate
guidance weight--are largely heuristic and lack a solid theoretical foundation.
This work addresses these limitations on two fronts. First, we provide a
theoretical formalization that precisely characterizes the relationship between
guidance strength and classifier confidence. Second, building on this insight,
we introduce a stochastic optimal control framework that casts guidance
scheduling as an adaptive optimization problem. In this formulation, guidance
strength is not fixed but dynamically selected based on time, the current
sample, and the conditioning class, either independently or in combination. By
solving the resulting control problem, we establish a principled foundation for
more effective guidance in diffusion models.",2025-05-25,"Iskander Azangulov, Peter Potaptchik, Qinyu Li, Eddie Aamari, George Deligiannidis, Judith Rousseau",http://arxiv.org/pdf/2505.19367v1,cs.LG
Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments,"The deployment of pre-trained perception models in novel environments often
leads to performance degradation due to distributional shifts. Although recent
artificial intelligence approaches for metacognition use logical rules to
characterize and filter model errors, improving precision often comes at the
cost of reduced recall. This paper addresses the hypothesis that leveraging
multiple pre-trained models can mitigate this recall reduction. We formulate
the challenge of identifying and managing conflicting predictions from various
models as a consistency-based abduction problem. The input predictions and the
learned error detection rules derived from each model are encoded in a logic
program. We then seek an abductive explanation--a subset of model
predictions--that maximizes prediction coverage while ensuring the rate of
logical inconsistencies (derived from domain constraints) remains below a
specified threshold. We propose two algorithms for this knowledge
representation task: an exact method based on Integer Programming (IP) and an
efficient Heuristic Search (HS). Through extensive experiments on a simulated
aerial imagery dataset featuring controlled, complex distributional shifts, we
demonstrate that our abduction-based framework outperforms individual models
and standard ensemble baselines, achieving, for instance, average relative
improvements of approximately 13.6% in F1-score and 16.6% in accuracy across 15
diverse test datasets when compared to the best individual model. Our results
validate the use of consistency-based abduction as an effective mechanism to
robustly integrate knowledge from multiple imperfect reasoners in challenging,
novel scenarios.",2025-05-25,"Mario Leiva, Noel Ngu, Joshua Shay Kricheli, Aditya Taparia, Ransalu Senanayake, Paulo Shakarian, Nathaniel Bastian, John Corcoran, Gerardo Simari",http://arxiv.org/pdf/2505.19361v1,cs.LG
Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval,"Neural retrieval methods using transformer-based pre-trained language models
have advanced multilingual and cross-lingual retrieval. However, their
effectiveness for low-resource, morphologically rich languages such as Amharic
remains underexplored due to data scarcity and suboptimal tokenization. We
address this gap by introducing Amharic-specific dense retrieval models based
on pre-trained Amharic BERT and RoBERTa backbones. Our proposed
RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative
improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest
multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact
variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while
being over 13x smaller. Additionally, we train a ColBERT-based late interaction
retrieval model that achieves the highest MRR@10 score (0.843) among all
evaluated models. We benchmark our proposed models against both sparse and
dense retrieval baselines to systematically assess retrieval effectiveness in
Amharic. Our analysis highlights key challenges in low-resource settings and
underscores the importance of language-specific adaptation. To foster future
research in low-resource IR, we publicly release our dataset, codebase, and
trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.",2025-05-25,"Kidist Amde Mekonnen, Yosef Worku Alemneh, Maarten de Rijke",http://arxiv.org/pdf/2505.19356v1,cs.LG
"FlashMD: long-stride, universal prediction of molecular dynamics","Molecular dynamics (MD) provides insights into atomic-scale processes by
integrating over time the equations that describe the motion of atoms under the
action of interatomic forces. Machine learning models have substantially
accelerated MD by providing inexpensive predictions of the forces, but they
remain constrained to minuscule time integration steps, which are required by
the fast time scale of atomic motion. In this work, we propose FlashMD, a
method to predict the evolution of positions and momenta over strides that are
between one and two orders of magnitude longer than typical MD time steps. We
incorporate considerations on the mathematical and physical properties of
Hamiltonian dynamics in the architecture, generalize the approach to allow the
simulation of any thermodynamic ensemble, and carefully assess the possible
failure modes of such a long-stride MD approach. We validate FlashMD's accuracy
in reproducing equilibrium and time-dependent properties, using both
system-specific and general-purpose models, extending the ability of MD
simulation to reach the long time scales needed to model microscopic processes
of high scientific and technological relevance.",2025-05-25,"Filippo Bigi, Sanggyu Chong, Agustinus Kristiadi, Michele Ceriotti",http://arxiv.org/pdf/2505.19350v1,cs.LG
Communication-Efficient Multi-Device Inference Acceleration for Transformer Models,"Transformer models power many AI applications but suffer from high inference
latency, limiting their use in real-time settings. Multi-device inference can
reduce latency by parallelizing computation. Yet, existing methods require high
inter-device bandwidth, making them impractical for bandwidth-constrained
environments. We propose ASTRA, a communication-efficient framework that
accelerates Transformer inference through a novel integration of sequence
parallelism and a Mixed-Precision Attention mechanism designed to minimize
inter-device communication. ASTRA compresses non-local token embeddings via
vector quantization and preserves task accuracy through two optimizations,
Noise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT
and GPT2 across vision and NLP tasks show that ASTRA achieves up to 2.64X
speedups over single-device inference and up to 15.25X speedups over
state-of-the-art multi-device inferences, while operating under bandwidths as
low as 10 Mbps. ASTRA is open-sourced at https://github.com/xl1990/Astra.",2025-05-25,"Xiao Liu, Lijun Zhang, Deepak Ganesan, Hui Guan",http://arxiv.org/pdf/2505.19342v1,cs.LG
Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies,"Offline goal-conditioned reinforcement learning methods have shown promise
for reach-avoid tasks, where an agent must reach a target state while avoiding
undesirable regions of the state space. Existing approaches typically encode
avoid-region information into an augmented state space and cost function, which
prevents flexible, dynamic specification of novel avoid-region information at
evaluation time. They also rely heavily on well-designed reward and cost
functions, limiting scalability to complex or poorly structured environments.
We introduce RADT, a decision transformer model for offline, reward-free,
goal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid
regions directly as prompt tokens, allowing any number of avoid regions of
arbitrary size to be specified at evaluation time. Using only suboptimal
offline trajectories from a random policy, RADT learns reach-avoid behavior
through a novel combination of goal and avoid-region hindsight relabeling. We
benchmark RADT against 3 existing offline goal-conditioned RL models across 11
tasks, environments, and experimental settings. RADT generalizes in a zero-shot
manner to out-of-distribution avoid region sizes and counts, outperforming
baselines that require retraining. In one such zero-shot setting, RADT achieves
35.7% improvement in normalized cost over the best retrained baseline while
maintaining high goal-reaching success. We apply RADT to cell reprogramming in
biology, where it reduces visits to undesirable intermediate gene expression
states during trajectories to desired target states, despite stochastic
transitions and discrete, structured state dynamics.",2025-05-25,"Kevin Li, Marinka Zitnik",http://arxiv.org/pdf/2505.19337v1,cs.LG
Likert or Not: LLM Absolute Relevance Judgments on Fine-Grained Ordinal Scales,"Large language models (LLMs) obtain state of the art zero shot relevance
ranking performance on a variety of information retrieval tasks. The two most
common prompts to elicit LLM relevance judgments are pointwise scoring (a.k.a.
relevance generation), where the LLM sees a single query-document pair and
outputs a single relevance score, and listwise ranking (a.k.a. permutation
generation), where the LLM sees a query and a list of documents and outputs a
permutation, sorting the documents in decreasing order of relevance. The
current research community consensus is that listwise ranking yields superior
performance, and significant research effort has been devoted to crafting LLM
listwise ranking algorithms. The underlying hypothesis is that LLMs are better
at making relative relevance judgments than absolute ones. In tension with this
hypothesis, we find that the gap between pointwise scoring and listwise ranking
shrinks when pointwise scoring is implemented using a sufficiently large
ordinal relevance label space, becoming statistically insignificant for many
LLM-benchmark dataset combinations (where ``significant'' means ``95\%
confidence that listwise ranking improves NDCG@10''). Our evaluations span four
LLMs, eight benchmark datasets from the BEIR and TREC-DL suites, and two
proprietary datasets with relevance labels collected after the training cut-off
of all LLMs evaluated.",2025-05-25,"Charles Godfrey, Ping Nie, Natalia Ostapuk, David Ken, Shang Gao, Souheil Inati",http://arxiv.org/pdf/2505.19334v1,cs.LG
BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change,"Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can
play a critical role in the personalization and effectiveness of digital
behaviour change interventions. These subtle and conflicting emotions are
manifested by a discord between multiple modalities, such as facial and vocal
expressions, and body language. Although experts can be trained to identify
A/H, integrating them into digital interventions is costly and less effective.
Automatic learning systems provide a cost-effective alternative that can adapt
to individual users, and operate seamlessly within real-time, and
resource-limited environments. However, there are currently no datasets
available for the design of ML models to recognize A/H. This paper introduces a
first Behavioural Ambivalence/Hesitancy (BAH) dataset collected for
subject-based multimodal recognition of A/H in videos. It contains videos from
224 participants captured across 9 provinces in Canada, with different age, and
ethnicity. Through our web platform, we recruited participants to answer 7
questions, some of which were designed to elicit A/H while recording themselves
via webcam with microphone. BAH amounts to 1,118 videos for a total duration of
8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp
segments to indicate where A/H occurs, and provide frame- and video-level
annotations with the A/H cues. Video transcripts and their timestamps are also
included, along with cropped and aligned faces in each frame, and a variety of
participants meta-data. We include results baselines for BAH at frame- and
video-level recognition in multi-modal setups, in addition to zero-shot
prediction, and for personalization using unsupervised domain adaptation. The
limited performance of baseline models highlights the challenges of recognizing
A/H in real-world videos. The data, code, and pretrained weights are available.",2025-05-25,"Manuela González-González, Soufiane Belharbi, Muhammad Osama Zeeshan, Masoumeh Sharafi, Muhammad Haseeb Aslam, Marco Pedersoli, Alessandro Lameiras Koerich, Simon L Bacon, Eric Granger",http://arxiv.org/pdf/2505.19328v1,cs.LG
Paying Alignment Tax with Contrastive Learning,"Current debiasing approaches often result a degradation in model capabilities
such as factual accuracy and knowledge retention. Through systematic evaluation
across multiple benchmarks, we demonstrate that existing debiasing methods face
fundamental trade-offs, particularly in smaller models, leading to reduced
truthfulness, knowledge loss, or unintelligible outputs. To address these
limitations, we propose a contrastive learning framework that learns through
carefully constructed positive and negative examples. Our approach introduces
contrast computation and dynamic loss scaling to balance bias mitigation with
faithfulness preservation. Experimental results across multiple model scales
demonstrate that our method achieves substantial improvements in both toxicity
reduction and faithfulness preservation. Most importantly, we show that our
framework is the first to consistently improve both metrics simultaneously,
avoiding the capability degradation characteristic of existing approaches.
These results suggest that explicit modeling of both positive and negative
examples through contrastive learning could be a promising direction for
reducing the alignment tax in language model debiasing.",2025-05-25,"Buse Sibel Korkmaz, Rahul Nair, Elizabeth M. Daly, Antonio del Rio Chanona",http://arxiv.org/pdf/2505.19327v1,cs.LG
PIGPVAE: Physics-Informed Gaussian Process Variational Autoencoders,"Recent advances in generative AI offer promising solutions for synthetic data
generation but often rely on large datasets for effective training. To address
this limitation, we propose a novel generative model that learns from limited
data by incorporating physical constraints to enhance performance.
Specifically, we extend the VAE architecture by incorporating physical models
in the generative process, enabling it to capture underlying dynamics more
effectively. While physical models provide valuable insights, they struggle to
capture complex temporal dependencies present in real-world data. To bridge
this gap, we introduce a discrepancy term to account for unmodeled dynamics,
represented within a latent Gaussian Process VAE (GPVAE). Furthermore, we apply
regularization to ensure the generated data aligns closely with observed data,
enhancing both the diversity and accuracy of the synthetic samples. The
proposed method is applied to indoor temperature data, achieving
state-of-the-art performance. Additionally, we demonstrate that PIGPVAE can
produce realistic samples beyond the observed distribution, highlighting its
robustness and usefulness under distribution shifts.",2025-05-25,"Michail Spitieris, Massimiliano Ruocco, Abdulmajid Murad, Alessandro Nocente",http://arxiv.org/pdf/2505.19320v1,cs.LG
"Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics","Although popularized AI fairness metrics, e.g., demographic parity, have
uncovered bias in AI-assisted decision-making outcomes, they do not consider
how much effort one has spent to get to where one is today in the input feature
space. However, the notion of effort is important in how Philosophy and humans
understand fairness. We propose a philosophy-informed way to conceptualize and
evaluate Effort-aware Fairness (EaF) based on the concept of Force, or temporal
trajectory of predictive features coupled with inertia. In addition to our
theoretical formulation of EaF metrics, our empirical contributions include: 1/
a pre-registered human subjects experiment, which demonstrates that for both
stages of the (individual) fairness evaluation process, people consider the
temporal trajectory of a predictive feature more than its aggregate value; 2/
pipelines to compute Effort-aware Individual/Group Fairness in the criminal
justice and personal finance contexts. Our work may enable AI model auditors to
uncover and potentially correct unfair decisions against individuals who spent
significant efforts to improve but are still stuck with systemic/early-life
disadvantages outside their control.",2025-05-25,"Tin Nguyen, Jiannan Xu, Zora Che, Phuong-Anh Nguyen-Le, Rushil Dandamudi, Donald Braman, Furong Huang, Hal Daumé III, Zubin Jelveh",http://arxiv.org/pdf/2505.19317v1,cs.LG
Demand Selection for VRP with Emission Quota,"Combinatorial optimization (CO) problems are traditionally addressed using
Operations Research (OR) methods, including metaheuristics. In this study, we
introduce a demand selection problem for the Vehicle Routing Problem (VRP) with
an emission quota, referred to as QVRP. The objective is to minimize the number
of omitted deliveries while respecting the pollution quota. We focus on the
demand selection part, called Maximum Feasible Vehicle Assignment (MFVA), while
the construction of a routing for the VRP instance is solved using classical OR
methods. We propose several methods for selecting the packages to omit, both
from machine learning (ML) and OR. Our results show that, in this static
problem setting, classical OR-based methods consistently outperform ML-based
approaches.",2025-05-25,"Farid Najar, Dominique Barth, Yann Strozecki",http://arxiv.org/pdf/2505.19315v1,cs.LG
Concept Reachability in Diffusion Models: Beyond Dataset Constraints,"Despite significant advances in quality and complexity of the generations in
text-to-image models, prompting does not always lead to the desired outputs.
Controlling model behaviour by directly steering intermediate model activations
has emerged as a viable alternative allowing to reach concepts in latent space
that may otherwise remain inaccessible by prompt. In this work, we introduce a
set of experiments to deepen our understanding of concept reachability. We
design a training data setup with three key obstacles: scarcity of concepts,
underspecification of concepts in the captions, and data biases with tied
concepts. Our results show: (i) concept reachability in latent space exhibits a
distinct phase transition, with only a small number of samples being sufficient
to enable reachability, (ii) where in the latent space the intervention is
performed critically impacts reachability, showing that certain concepts are
reachable only at certain stages of transformation, and (iii) while prompting
ability rapidly diminishes with a decrease in quality of the dataset, concepts
often remain reliably reachable through steering. Model providers can leverage
this to bypass costly retraining and dataset curation and instead innovate with
user-facing control mechanisms.",2025-05-25,"Marta Aparicio Rodriguez, Xenia Miscouridou, Anastasia Borovykh",http://arxiv.org/pdf/2505.19313v1,cs.LG
Fractional-Boundary-Regularized Deep Galerkin Method for Variational Inequalities in Mixed Optimal Stopping and Control,"Mixed optimal stopping and stochastic control problems define variational
inequalities with non-linear Hamilton-Jacobi-Bellman (HJB) operators, whose
numerical solution is notoriously difficult and lack of reliable benchmarks. We
first use the dual approach to transform it into a linear operator, and then
introduce a Fractional-Boundary-Regularized Deep Galerkin Method (FBR-DGM) that
augments the classical $L^2$ loss with Sobolev-Slobodeckij norms on the
parabolic boundary, enforcing regularity and yielding consistent improvements
in the network approximation and its derivatives. The improved accuracy allows
the network to be converted back to the original solution using the dual
transform. The self-consistency and stability of the network can be tested by
checking the primal-dual relationship among optimal value, optimal wealth, and
optimal control, offering innovative benchmarks in the absence of analytical
solutions.",2025-05-25,"Yun Zhao, Harry Zheng",http://arxiv.org/pdf/2505.19309v1,cs.LG
From Single Images to Motion Policies via Video-Generation Environment Representations,"Autonomous robots typically need to construct representations of their
surroundings and adapt their motions to the geometry of their environment.
Here, we tackle the problem of constructing a policy model for collision-free
motion generation, consistent with the environment, from a single input RGB
image. Extracting 3D structures from a single image often involves monocular
depth estimation. Developments in depth estimation have given rise to large
pre-trained models such as DepthAnything. However, using outputs of these
models for downstream motion generation is challenging due to frustum-shaped
errors that arise. Instead, we propose a framework known as Video-Generation
Environment Representation (VGER), which leverages the advances of large-scale
video generation models to generate a moving camera video conditioned on the
input image. Frames of this video, which form a multiview dataset, are then
input into a pre-trained 3D foundation model to produce a dense point cloud. We
then introduce a multi-scale noise approach to train an implicit representation
of the environment structure and build a motion generation model that complies
with the geometry of the representation. We extensively evaluate VGER over a
diverse set of indoor and outdoor environments. We demonstrate its ability to
produce smooth motions that account for the captured geometry of a scene, all
from a single RGB input image.",2025-05-25,"Weiming Zhi, Ziyong Ma, Tianyi Zhang, Matthew Johnson-Roberson",http://arxiv.org/pdf/2505.19306v1,cs.LG
100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?,"Long-context capability is considered one of the most important abilities of
LLMs, as a truly long context-capable LLM enables users to effortlessly process
many originally exhausting tasks -- e.g., digesting a long-form document to
find answers vs. directly asking an LLM about it. However, existing
real-task-based long-context evaluation benchmarks have two major shortcomings.
First, benchmarks like LongBench often do not provide proper metrics to
separate long-context performance from the model's baseline ability, making
cross-model comparison unclear. Second, such benchmarks are usually constructed
with fixed input lengths, which limits their applicability across different
models and fails to reveal when a model begins to break down. To address these
issues, we introduce a length-controllable long-context benchmark and a novel
metric that disentangles baseline knowledge from true long-context
capabilities. Experiments demonstrate the superiority of our approach in
effectively evaluating LLMs.",2025-05-25,"Wang Yang, Hongye Jin, Shaochen Zhong, Song Jiang, Qifan Wang, Vipin Chaudhary, Xiaotian Han",http://arxiv.org/pdf/2505.19293v1,cs.LG
Hypercube-RAG: Hypercube-Based Retrieval-Augmented Generation for In-domain Scientific Question-Answering,"Large language models (LLMs) often need to incorporate external knowledge to
solve theme-specific problems. Retrieval-augmented generation (RAG), which
empowers LLMs to generate more qualified responses with retrieved external data
and knowledge, has shown its high promise. However, traditional semantic
similarity-based RAGs struggle to return concise yet highly relevant
information for domain knowledge-intensive tasks, such as scientific
question-answering (QA). Built on a multi-dimensional (cube) structure called
Hypercube, which can index documents in an application-driven, human-defined,
multi-dimensional space, we introduce the Hypercube-RAG, a novel RAG framework
for precise and efficient retrieval. Given a query, Hypercube-RAG first
decomposes it based on its entities and topics and then retrieves relevant
documents from cubes by aligning these decomposed components with hypercube
dimensions. Experiments on three in-domain scientific QA datasets demonstrate
that our method improves accuracy by 3.7% and boosts retrieval efficiency by
81.2%, measured as relative gains over the strongest RAG baseline. More
importantly, our Hypercube-RAG inherently offers explainability by revealing
the underlying predefined hypercube dimensions used for retrieval. The code and
data sets are available at https://github.com/JimengShi/Hypercube-RAG.",2025-05-25,"Jimeng Shi, Sizhe Zhou, Bowen Jin, Wei Hu, Shaowen Wang, Giri Narasimhan, Jiawei Han",http://arxiv.org/pdf/2505.19288v1,cs.LG
A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models,"Large language models have been extensively studied as neural knowledge bases
for their knowledge access, editability, reasoning, and explainability.
However, few works focus on the structural patterns of their knowledge.
Motivated by this gap, we investigate these structural patterns from a graph
perspective. We quantify the knowledge of LLMs at both the triplet and entity
levels, and analyze how it relates to graph structural properties such as node
degree. Furthermore, we uncover the knowledge homophily, where topologically
close entities exhibit similar levels of knowledgeability, which further
motivates us to develop graph machine learning models to estimate entity
knowledge based on its local neighbors. This model further enables valuable
knowledge checking by selecting triplets less known to LLMs. Empirical results
show that using selected triplets for fine-tuning leads to superior
performance.",2025-05-25,"Utkarsh Sahu, Zhisheng Qi, Yongjia Lei, Ryan A. Rossi, Franck Dernoncourt, Nesreen K. Ahmed, Mahantesh M Halappanavar, Yao Ma, Yu Wang",http://arxiv.org/pdf/2505.19286v1,cs.LG
A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning,"Online reinforcement learning (RL) excels in complex, safety-critical
domains, yet it faces challenges such as sample inefficiency, training
instability, and a lack of interpretability. Data attribution offers a
principled way to trace model behavior back to individual training samples.
However, in online RL, each training sample not only drives policy updates but
also influences future data collection, violating the fixed dataset assumption
in existing attribution methods. In this paper, we initiate the study of data
attribution for online RL, focusing on the widely used Proximal Policy
Optimization (PPO) algorithm. We start by establishing a local attribution
framework, interpreting model checkpoints with respect to the records in the
recent training buffer. We design two target functions, capturing agent action
and cumulative return respectively, and measure each record's contribution
through gradient similarity between its training loss and these targets. We
demonstrate the power of this framework through three concrete applications:
diagnosis of learning, temporal analysis of behavior formation, and targeted
intervention during training. Leveraging this framework, we further propose an
algorithm, iterative influence-based filtering (IIF), for online RL training
that iteratively performs experience filtering to refine policy updates. Across
standard RL benchmarks (classic control, navigation, locomotion) to RLHF for
large language models, IIF reduces sample complexity, speeds up training, and
achieves higher returns. Overall, these results advance interpretability,
efficiency, and effectiveness of online RL.",2025-05-25,"Yuzheng Hu, Fan Wu, Haotian Ye, David Forsyth, James Zou, Nan Jiang, Jiaqi W. Ma, Han Zhao",http://arxiv.org/pdf/2505.19281v1,cs.LG
Cellular Traffic Prediction via Byzantine-robust Asynchronous Federated Learning,"Network traffic prediction plays a crucial role in intelligent network
operation. Traditional prediction methods often rely on centralized training,
necessitating the transfer of vast amounts of traffic data to a central server.
This approach can lead to latency and privacy concerns. To address these
issues, federated learning integrated with differential privacy has emerged as
a solution to improve data privacy and model robustness in distributed
settings. Nonetheless, existing federated learning protocols are vulnerable to
Byzantine attacks, which may significantly compromise model robustness.
Developing a robust and privacy-preserving prediction model in the presence of
Byzantine clients remains a significant challenge. To this end, we propose an
asynchronous differential federated learning framework based on
distributionally robust optimization. The proposed framework utilizes multiple
clients to train the prediction model collaboratively with local differential
privacy. In addition, regularization techniques have been employed to further
improve the Byzantine robustness of the models. We have conducted extensive
experiments on three real-world datasets, and the results elucidate that our
proposed distributed algorithm can achieve superior performance over existing
methods.",2025-05-25,"Hui Ma, Kai Yang, Yang Jiao",http://arxiv.org/pdf/2505.19263v1,cs.LG
Towards Large Reasoning Models for Agriculture,"Agricultural decision-making involves complex, context-specific reasoning,
where choices about crops, practices, and interventions depend heavily on
geographic, climatic, and economic conditions. Traditional large language
models (LLMs) often fall short in navigating this nuanced problem due to
limited reasoning capacity. We hypothesize that recent advances in large
reasoning models (LRMs) can better handle such structured, domain-specific
inference. To investigate this, we introduce AgReason, the first expert-curated
open-ended science benchmark with 100 questions for agricultural reasoning.
Evaluations across thirteen open-source and proprietary models reveal that LRMs
outperform conventional ones, though notable challenges persist, with the
strongest Gemini-based baseline achieving 36% accuracy. We also present
AgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with
human oversight and equipped with synthetically generated reasoning traces.
Using AgThoughts, we develop AgThinker, a suite of small reasoning models that
can be run on consumer-grade GPUs, and show that our dataset can be effective
in unlocking agricultural reasoning abilities in LLMs. Our project page is
here: https://baskargroup.github.io/Ag_reasoning/",2025-05-25,"Hossein Zaremehrjerdi, Shreyan Ganguly, Ashlyn Rairdin, Elizabeth Tranel, Benjamin Feuer, Juan Ignacio Di Salvo, Srikanth Panthulugiri, Victoria Moser, Sarah Jones, Joscif G Raigne, Yanben Shen, Heidi M. Dornath, Aditya Balu, Adarsh Krishnamurthy, Asheesh K Singh, Arti Singh, Baskar Ganapathysubramanian, Chinmay Hegde, Soumik Sarkar",http://arxiv.org/pdf/2505.19259v1,cs.LG
Towards a Spatiotemporal Fusion Approach to Precipitation Nowcasting,"With the increasing availability of meteorological data from various sensors,
numerical models and reanalysis products, the need for efficient data
integration methods has become paramount for improving weather forecasts and
hydrometeorological studies. In this work, we propose a data fusion approach
for precipitation nowcasting by integrating data from meteorological and rain
gauge stations in Rio de Janeiro metropolitan area with ERA5 reanalysis data
and GFS numerical weather prediction. We employ the spatiotemporal deep
learning architecture called STConvS2S, leveraging a structured dataset
covering a 9 x 11 grid. The study spans from January 2011 to October 2024, and
we evaluate the impact of integrating three surface station systems. Among the
tested configurations, the fusion-based model achieves an F1-score of 0.2033
for forecasting heavy precipitation events (greater than 25 mm/h) at a one-hour
lead time. Additionally, we present an ablation study to assess the
contribution of each station network and propose a refined inference strategy
for precipitation nowcasting, integrating the GFS numerical weather prediction
(NWP) data with in-situ observations.",2025-05-25,"Felipe Curcio, Pedro Castro, Augusto Fonseca, Rafaela Castro, Raquel Franco, Eduardo Ogasawara, Victor Stepanenko, Fabio Porto, Mariza Ferro, Eduardo Bezerra",http://arxiv.org/pdf/2505.19258v1,cs.LG
VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use,"Reinforcement Learning Finetuning (RFT) has significantly advanced the
reasoning capabilities of large language models (LLMs) by enabling long chains
of thought, self-correction, and effective tool use. While recent works attempt
to extend RFT to vision-language models (VLMs), these efforts largely produce
text-only reasoning conditioned on static image inputs, falling short of true
multimodal reasoning in the response. In contrast, test-time methods like
Visual Sketchpad incorporate visual steps but lack training mechanisms.
  We introduce VTool-R1, the first framework that trains VLMs to generate
multimodal chains of thought by interleaving text and intermediate visual
reasoning steps. VTool-R1 integrates Python-based visual editing tools into the
RFT process, enabling VLMs to learn when and how to generate visual reasoning
steps that benefit final reasoning. Trained with outcome-based rewards tied to
task accuracy, our approach elicits strategic visual tool use for reasoning
without relying on process-based supervision. Experiments on structured visual
question answering over charts and tables show that VTool-R1 enhances reasoning
performance by teaching VLMs to ""think with images"" and generate multimodal
chain of thoughts with tools.",2025-05-25,"Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, Klara Nahrstedt",http://arxiv.org/pdf/2505.19255v1,cs.LG
Learning-Augmented Online Bipartite Fractional Matching,"Online bipartite matching is a fundamental problem in online optimization,
extensively studied both in its integral and fractional forms due to its
theoretical significance and practical applications, such as online advertising
and resource allocation. Motivated by recent progress in learning-augmented
algorithms, we study online bipartite fractional matching when the algorithm is
given advice in the form of a suggested matching in each iteration. We develop
algorithms for both the vertex-weighted and unweighted variants that provably
dominate the naive ""coin flip"" strategy of randomly choosing between the
advice-following and advice-free algorithms. Moreover, our algorithm for the
vertex-weighted setting extends to the AdWords problem under the small bids
assumption, yielding a significant improvement over the seminal work of
Mahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our
positive results, we establish a hardness bound on the robustness-consistency
tradeoff that is attainable by any algorithm. We empirically validate our
algorithms through experiments on synthetic and real-world data.",2025-05-25,"Davin Choo, Billy Jin, Yongho Shin",http://arxiv.org/pdf/2505.19252v1,cs.LG
Improving Value Estimation Critically Enhances Vanilla Policy Gradient,"Modern policy gradient algorithms, such as TRPO and PPO, outperform vanilla
policy gradient in many RL tasks. Questioning the common belief that enforcing
approximate trust regions leads to steady policy improvement in practice, we
show that the more critical factor is the enhanced value estimation accuracy
from more value update steps in each iteration. To demonstrate, we show that by
simply increasing the number of value update steps per iteration, vanilla
policy gradient itself can achieve performance comparable to or better than PPO
in all the standard continuous control benchmark environments. Importantly,
this simple change to vanilla policy gradient is significantly more robust to
hyperparameter choices, opening up the possibility that RL algorithms may still
become more effective and easier to use.",2025-05-25,"Tao Wang, Ruipeng Zhang, Sicun Gao",http://arxiv.org/pdf/2505.19247v1,cs.LG
To CoT or To Loop? A Formal Comparison Between Chain-of-Thought and Looped Transformers,"Chain-of-Thought (CoT) and Looped Transformers have been shown to empirically
improve performance on reasoning tasks and to theoretically enhance
expressivity by recursively increasing the number of computational steps.
However, their comparative capabilities are still not well understood. In this
paper, we provide a formal analysis of their respective strengths and
limitations. We show that Looped Transformers can efficiently simulate parallel
computations for deterministic tasks, which we formalize as evaluation over
directed acyclic graphs. In contrast, CoT with stochastic decoding excels at
approximate inference for compositional structures, namely self-reducible
problems. These separations suggest the tasks for which depth-driven recursion
is more suitable, thereby offering practical cues for choosing between
reasoning paradigms.",2025-05-25,"Kevin Xu, Issei Sato",http://arxiv.org/pdf/2505.19245v1,cs.LG
ActiveDPO: Active Direct Preference Optimization for Sample-Efficient Alignment,"The recent success of using human preferences to align large language models
(LLMs) has significantly improved their performance in various downstream tasks
like question answering, mathematical reasoning, and code generation. However,3
achieving effective LLM alignment depends on high-quality human preference
datasets. Collecting these datasets requires human preference annotation, which
is costly and resource-intensive, necessitating efficient active data selection
methods. Existing methods either lack a strong theoretical foundation or depend
on restrictive reward function assumptions (e.g., linearity). To this end, we
propose an algorithm, ActiveDPO, that uses a theoretically grounded data
selection criterion for non-linear reward functions while directly leveraging
the LLM itself to parameterize the reward model that is used for active data
selection. As a result, ActiveDPO explicitly accounts for the influence of LLM
on data selection, unlike methods that select the data without considering the
LLM that is being aligned, thereby leading to more effective and efficient data
collection. Extensive experiments show that ActiveDPO outperforms existing
methods across various models and datasets.",2025-05-25,"Xiaoqiang Lin, Arun Verma, Zhongxiang Dai, Daniela Rus, See-Kiong Ng, Bryan Kian Hsiang Low",http://arxiv.org/pdf/2505.19241v1,cs.LG
LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models,"Large language model (LLM) research has grown rapidly, along with increasing
concern about their limitations such as failures in reasoning, hallucinations,
and limited multilingual capability. In this survey, we conduct a data-driven,
semi-automated review of research on limitations of LLM (LLLMs) from 2022 to
2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,
we identify 14,648 relevant papers using keyword filtering, LLM-based
classification, validated against expert labels, and topic clustering (via two
approaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research
increases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs
research grows even faster, reaching over 30% of LLM papers by late 2024.
Reasoning remains the most studied limitation, followed by generalization,
hallucination, bias, and security. The distribution of topics in the ACL
dataset stays relatively stable over time, while arXiv shifts toward safety and
controllability (with topics like security risks, alignment, hallucinations,
knowledge editing), and multimodality between 2022 and 2024. We release a
dataset of annotated abstracts and a validated methodology, and offer a
quantitative view of trends in LLM limitations research.",2025-05-25,"Aida Kostikova, Zhipin Wang, Deidamea Bajri, Ole Pütz, Benjamin Paaßen, Steffen Eger",http://arxiv.org/pdf/2505.19240v1,cs.LG
Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees,"Constrained decision-making is essential for designing safe policies in
real-world control systems, yet simulated environments often fail to capture
real-world adversities. We consider the problem of learning a policy that will
maximize the cumulative reward while satisfying a constraint, even when there
is a mismatch between the real model and an accessible simulator/nominal model.
In particular, we consider the robust constrained Markov decision problem
(RCMDP) where an agent needs to maximize the reward and satisfy the constraint
against the worst possible stochastic model under the uncertainty set centered
around an unknown nominal model. Primal-dual methods, effective for standard
constrained MDP (CMDP), are not applicable here because of the lack of the
strong duality property. Further, one cannot apply the standard robust
value-iteration based approach on the composite value function either as the
worst case models may be different for the reward value function and the
constraint value function. We propose a novel technique that effectively
minimizes the constraint value function--to satisfy the constraints; on the
other hand, when all the constraints are satisfied, it can simply maximize the
robust reward value function. We prove that such an algorithm finds a policy
with at most $\epsilon$ sub-optimality and feasible policy after
$O(\epsilon^{-2})$ iterations. In contrast to the state-of-the-art method, we
do not need to employ a binary search, thus, we reduce the computation time by
at least 4x for smaller value of discount factor ($\gamma$) and by at least 6x
for larger value of $\gamma$.",2025-05-25,"Sourav Ganguly, Arnob Ghosh, Kishan Panaganti, Adam Wierman",http://arxiv.org/pdf/2505.19238v1,cs.LG
CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models,"Vision-Language Models (VLMs) excel across diverse tasks but suffer from high
inference costs in time and memory. Token sparsity mitigates inefficiencies in
token usage, while neuron sparsity reduces high-dimensional computations, both
offering promising solutions to enhance efficiency. Recently, these two
sparsity paradigms have evolved largely in parallel, fostering the prevailing
assumption that they function independently. However, a fundamental yet
underexplored question remains: Do they truly operate in isolation, or is there
a deeper underlying interplay that has yet to be uncovered? In this paper, we
conduct the first comprehensive investigation into this question. By
introducing and analyzing the matching mechanism between Core Neurons and Core
Tokens, we found that key neurons and tokens for inference mutually influence
and reinforce each other. Building on this insight, we propose CoreMatching, a
co-adaptive sparse inference framework, which leverages the synergy between
token and neuron sparsity to enhance inference efficiency. Through theoretical
analysis and efficiency evaluations, we demonstrate that the proposed method
surpasses state-of-the-art baselines on ten image understanding tasks and three
hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs
reduction and a 10x overall speedup. Code is released at
https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main.",2025-05-25,"Qinsi Wang, Hancheng Ye, Ming-Yu Chung, Yudong Liu, Yueqian Lin, Martin Kuo, Mingyuan Ma, Jianyi Zhang, Yiran Chen",http://arxiv.org/pdf/2505.19235v1,cs.LG
Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law,"Recent works have highlighted optimization difficulties faced by gradient
descent in training the first and last layers of transformer-based language
models, which are overcome by optimizers such as Adam. These works suggest that
the difficulty is linked to the heavy-tailed distribution of words in text
data, where the frequency of the $k$th most frequent word $\pi_k$ is
proportional to $1/k$, following Zipf's law. To better understand the impact of
the data distribution on training performance, we study a linear bigram model
for next-token prediction when the tokens follow a power law $\pi_k \propto
1/k^\alpha$ parameterized by the exponent $\alpha > 0$. We derive optimization
scaling laws for deterministic gradient descent and sign descent as a proxy for
Adam as a function of the exponent $\alpha$. Existing theoretical
investigations in scaling laws assume that the eigenvalues of the data decay as
a power law with exponent $\alpha > 1$. This assumption effectively makes the
problem ``finite dimensional'' as most of the loss comes from a few of the
largest eigencomponents. In comparison, we show that the problem is more
difficult when the data have heavier tails. The case $\alpha = 1$ as found in
text data is ``worst-case'' for gradient descent, in that the number of
iterations required to reach a small relative error scales almost linearly with
dimension. While the performance of sign descent also depends on the dimension,
for Zipf-distributed data the number of iterations scales only with the
square-root of the dimension, leading to a large improvement for large
vocabularies.",2025-05-25,"Frederik Kunstner, Francis Bach",http://arxiv.org/pdf/2505.19227v1,cs.LG
LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models,"While Masked Diffusion Models (MDMs), such as LLaDA, present a promising
paradigm for language modeling, there has been relatively little effort in
aligning these models with human preferences via reinforcement learning. The
challenge primarily arises from the high variance in Evidence Lower Bound
(ELBO)-based likelihood estimates required for preference optimization. To
address this issue, we propose Variance-Reduced Preference Optimization (VRPO),
a framework that formally analyzes the variance of ELBO estimators and derives
bounds on both the bias and variance of preference optimization gradients.
Building on this theoretical foundation, we introduce unbiased variance
reduction strategies, including optimal Monte Carlo budget allocation and
antithetic sampling, that significantly improve the performance of MDM
alignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA,
and the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor
consistently and significantly across mathematical (GSM8K +4.7), code
(HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard
+4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical
performance compared to strong language MDMs and ARMs. Project page:
https://ml-gsai.github.io/LLaDA-1.5-Demo/.",2025-05-25,"Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, Chongxuan Li",http://arxiv.org/pdf/2505.19223v1,cs.LG
Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding,"Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial
intelligence and robotics, requiring the computation of collision-free paths
for multiple agents navigating from their start locations to designated goals.
As autonomous systems become increasingly prevalent in warehouses, urban
transportation, and other complex environments, MAPF has evolved from a
theoretical challenge to a critical enabler of real-world multi-robot
coordination. This comprehensive survey bridges the long-standing divide
between classical algorithmic approaches and emerging learning-based methods in
MAPF research. We present a unified framework that encompasses search-based
methods (including Conflict-Based Search, Priority-Based Search, and Large
Neighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP
formulations), and data-driven techniques (reinforcement learning, supervised
learning, and hybrid strategies). Through systematic analysis of experimental
practices across 200+ papers, we uncover significant disparities in evaluation
methodologies, with classical methods typically tested on larger-scale
instances (up to 200 by 200 grids with 1000+ agents) compared to learning-based
approaches (predominantly 10-100 agents). We provide a comprehensive taxonomy
of evaluation metrics, environment types, and baseline selections, highlighting
the need for standardized benchmarking protocols. Finally, we outline promising
future directions including mixed-motive MAPF with game-theoretic
considerations, language-grounded planning with large language models, and
neural solver architectures that combine the rigor of classical methods with
the flexibility of deep learning. This survey serves as both a comprehensive
reference for researchers and a practical guide for deploying MAPF solutions in
increasingly complex real-world applications.",2025-05-25,"Shiyue Wang, Haozheng Xu, Yuhan Zhang, Jingran Lin, Changhong Lu, Xiangfeng Wang, Wenhao Li",http://arxiv.org/pdf/2505.19219v1,cs.LG
SpeakStream: Streaming Text-to-Speech with Interleaved Data,"The latency bottleneck of traditional text-to-speech (TTS) systems
fundamentally hinders the potential of streaming large language models (LLMs)
in conversational AI. These TTS systems, typically trained and inferenced on
complete utterances, introduce unacceptable delays, even with optimized
inference speeds, when coupled with streaming LLM outputs. This is particularly
problematic for creating responsive conversational agents where low first-token
latency is critical. In this paper, we present SpeakStream, a streaming TTS
system that generates audio incrementally from streaming text using a
decoder-only architecture. SpeakStream is trained using a next-step prediction
loss on interleaved text-speech data. During inference, it generates speech
incrementally while absorbing streaming input text, making it particularly
suitable for cascaded conversational AI agents where an LLM streams text to a
TTS system. Our experiments demonstrate that SpeakStream achieves
state-of-the-art latency results in terms of first-token latency while
maintaining the quality of non-streaming TTS systems.",2025-05-25,"Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly",http://arxiv.org/pdf/2505.19206v1,cs.LG
OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization,"Hyperparameter optimization (HPO) is a critical yet challenging aspect of
machine learning model development, significantly impacting model performance
and generalization. Traditional HPO methods often struggle with high
dimensionality, complex interdependencies, and computational expense. This
paper introduces OptiMindTune, a novel multi-agent framework designed to
intelligently and efficiently optimize hyperparameters. OptiMindTune leverages
the collaborative intelligence of three specialized AI agents -- a Recommender
Agent, an Evaluator Agent, and a Decision Agent -- each powered by Google's
Gemini models. These agents address distinct facets of the HPO problem, from
model selection and hyperparameter suggestion to robust evaluation and
strategic decision-making. By fostering dynamic interactions and knowledge
sharing, OptiMindTune aims to converge to optimal hyperparameter configurations
more rapidly and robustly than existing single-agent or monolithic approaches.
Our framework integrates principles from advanced large language models, and
adaptive search to achieve scalable and intelligent AutoML. We posit that this
multi-agent paradigm offers a promising avenue for tackling the increasing
complexity of modern machine learning model tuning.",2025-05-25,"Meher Bhaskar Madiraju, Meher Sai Preetam Madiraju",http://arxiv.org/pdf/2505.19205v1,cs.LG
Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation,"Adversarial attack reveals the vulnerability of deep learning models. For
about a decade, countless attack and defense methods have been proposed,
leading to robustified classifiers and better understanding of models. Among
these methods, curvature-based approaches have attracted attention because it
is assumed that high curvature may give rise to rough decision boundary.
However, the most commonly used \textit{curvature} is the curvature of loss
function, scores or other parameters from within the model as opposed to
decision boundary curvature, since the former can be relatively easily formed
using second order derivative. In this paper, we propose a new query-efficient
method, dynamic curvature estimation(DCE), to estimate the decision boundary
curvature in a black-box setting. Our approach is based on CGBA, a black-box
adversarial attack. By performing DCE on a wide range of classifiers, we
discovered, statistically, a connection between decision boundary curvature and
adversarial robustness. We also propose a new attack method, curvature dynamic
black-box attack(CDBA) with improved performance using the dynamically
estimated curvature.",2025-05-25,Peiran Sun,http://arxiv.org/pdf/2505.19194v1,cs.LG
Interpretable Graph Learning Over Sets of Temporally-Sparse Data,"Real-world medical data often includes measurements from multiple signals
that are collected at irregular and asynchronous time intervals. For example,
different types of blood tests can be measured at different times and
frequencies, resulting in fragmented and unevenly scattered temporal data.
Similar issues of irregular sampling of different attributes occur in other
domains, such as monitoring of large systems using event log files or the
spread of fake news on social networks. Effectively learning from such data
requires models that can handle sets of temporally sparse and heterogeneous
signals. In this paper, we propose Graph Mixing Additive Networks (GMAN), a
novel and interpretable-by-design model for learning over irregular sets of
temporal signals. Our method achieves state-of-the-art performance in
real-world medical tasks, including a 4-point increase in the AUROC score of
in-hospital mortality prediction, compared to existing methods. We further
showcase GMAN's flexibility by applying it to a fake news detection task. We
demonstrate how its interpretability capabilities, including node-level,
graph-level, and subset-level importance, allow for transition phases detection
and gaining medical insights with real-world high-stakes implications. Finally,
we provide theoretical insights on GMAN expressive power.",2025-05-25,"Andrea Zerio, Maya Bechler-Speicher, Maor Huri, Marie Vibeke Vestergaard, Ran Gilad-Bachrach, Tine Jess, Samir Bhatt, Aleksejs Sazonovs",http://arxiv.org/pdf/2505.19193v1,cs.LG
I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts,"Modality fusion is a cornerstone of multimodal learning, enabling information
integration from diverse data sources. However, vanilla fusion methods are
limited by (1) inability to account for heterogeneous interactions between
modalities and (2) lack of interpretability in uncovering the multimodal
interactions inherent in the data. To this end, we propose I2MoE (Interpretable
Multimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework
designed to enhance modality fusion by explicitly modeling diverse multimodal
interactions, as well as providing interpretation on a local and global level.
First, I2MoE utilizes different interaction experts with weakly supervised
interaction losses to learn multimodal interactions in a data-driven way.
Second, I2MoE deploys a reweighting model that assigns importance scores for
the output of each interaction expert, which offers sample-level and
dataset-level interpretation. Extensive evaluation of medical and general
multimodal datasets shows that I2MoE is flexible enough to be combined with
different fusion techniques, consistently improves task performance, and
provides interpretation across various real-world scenarios. Code is available
at https://github.com/Raina-Xin/I2MoE.",2025-05-25,"Jiayi Xin, Sukwon Yun, Jie Peng, Inyoung Choi, Jenna L. Ballard, Tianlong Chen, Qi Long",http://arxiv.org/pdf/2505.19190v1,cs.LG
Chordless Structure: A Pathway to Simple and Expressive GNNs,"Researchers have proposed various methods of incorporating more structured
information into the design of Graph Neural Networks (GNNs) to enhance their
expressiveness. However, these methods are either computationally expensive or
lacking in provable expressiveness. In this paper, we observe that the chords
increase the complexity of the graph structure while contributing little useful
information in many cases. In contrast, chordless structures are more efficient
and effective for representing the graph. Therefore, when leveraging the
information of cycles, we choose to omit the chords. Accordingly, we propose a
Chordless Structure-based Graph Neural Network (CSGNN) and prove that its
expressiveness is strictly more powerful than the k-hop GNN (KPGNN) with
polynomial complexity. Experimental results on real-world datasets demonstrate
that CSGNN outperforms existing GNNs across various graph tasks while incurring
lower computational costs and achieving better performance than the GNNs of
3-WL expressiveness.",2025-05-25,"Hongxu Pan, Shuxian Hu, Mo Zhou, Zhibin Wang, Rong Gu, Chen Tian, Kun Yang, Sheng Zhong",http://arxiv.org/pdf/2505.19188v1,cs.LG
"Two LLMs debate, both are certain they've won","Can LLMs accurately adjust their confidence when facing opposition? Building
on previous studies measuring calibration on static fact-based
question-answering tasks, we evaluate Large Language Models (LLMs) in a
dynamic, adversarial debate setting, uniquely combining two realistic factors:
(a) a multi-turn format requiring models to update beliefs as new information
emerges, and (b) a zero-sum structure to control for task-related uncertainty,
since mutual high-confidence claims imply systematic overconfidence. We
organized 60 three-round policy debates among ten state-of-the-art LLMs, with
models privately rating their confidence (0-100) in winning after each round.
We observed five concerning patterns: (1) Systematic overconfidence: models
began debates with average initial confidence of 72.9% vs. a rational 50%
baseline. (2) Confidence escalation: rather than reducing confidence as debates
progressed, debaters increased their win probabilities, averaging 83% by the
final round. (3) Mutual overestimation: in 61.7% of debates, both sides
simultaneously claimed >=75% probability of victory, a logical impossibility.
(4) Persistent self-debate bias: models debating identical copies increased
confidence from 64.1% to 75.2%; even when explicitly informed their chance of
winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)
Misaligned private reasoning: models' private scratchpad thoughts sometimes
differed from their public confidence ratings, raising concerns about
faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the
ability to accurately self-assess or update their beliefs in dynamic,
multi-turn tasks; a major concern as LLM outputs are deployed without careful
review in assistant roles or agentic settings.",2025-05-25,"Minh Nhat Nguyen, Pradyumna Shyama Prasad",http://arxiv.org/pdf/2505.19184v1,cs.LG
Federated Learning: From Theory to Practice,"This book offers a hands-on introduction to building and understanding
federated learning (FL) systems. FL enables multiple devices -- such as
smartphones, sensors, or local computers -- to collaboratively train machine
learning (ML) models, while keeping their data private and local. It is a
powerful solution when data cannot or should not be centralized due to privacy,
regulatory, or technical reasons. The book is designed for students, engineers,
and researchers who want to learn how to design scalable, privacy preserving FL
systems. Our main focus is on personalization: enabling each device to train
its own model while still benefiting from collaboration with relevant devices.
This is achieved by leveraging similarities between (the learning tasks
associated with) devices that are encoded by the weighted edges (or links) of a
federated learning network (FL network). The key idea is to represent
real-world FL systems as networks of devices, where nodes correspond to device
and edges represent communication links and data similarities between them. The
training of personalized models for these devices can be naturally framed as a
distributed optimization problem. This optimization problem is referred to as
generalized total variation minimization (GTVMin) and ensures that devices with
similar learning tasks learn similar model parameters. Our approach is both
mathematically principled and practically motivated. While we introduce some
advanced ideas from optimization theory and graph-based learning, we aim to
keep the book accessible. Readers are guided through the core ideas step by
step, with intuitive explanations.",2025-05-25,A. Jung,http://arxiv.org/pdf/2505.19183v1,cs.LG
Saliency-guided Emotion Modeling: Predicting Viewer Reactions from Video Stimuli,"Understanding the emotional impact of videos is crucial for applications in
content creation, advertising, and Human-Computer Interaction (HCI).
Traditional affective computing methods rely on self-reported emotions, facial
expression analysis, and biosensing data, yet they often overlook the role of
visual saliency -- the naturally attention-grabbing regions within a video. In
this study, we utilize deep learning to introduce a novel saliency-based
approach to emotion prediction by extracting two key features: saliency area
and number of salient regions. Using the HD2S saliency model and OpenFace
facial action unit analysis, we examine the relationship between video saliency
and viewer emotions. Our findings reveal three key insights: (1) Videos with
multiple salient regions tend to elicit high-valence, low-arousal emotions, (2)
Videos with a single dominant salient region are more likely to induce
low-valence, high-arousal responses, and (3) Self-reported emotions often
misalign with facial expression-based emotion detection, suggesting limitations
in subjective reporting. By leveraging saliency-driven insights, this work
provides a computationally efficient and interpretable alternative for emotion
modeling, with implications for content creation, personalized media
experiences, and affective computing research.",2025-05-25,"Akhila Yaragoppa, Siddharth",http://arxiv.org/pdf/2505.19178v1,cs.LG
Computational Inertia as a Conserved Quantity in Frictionless and Damped Learning Dynamics,"We identify a conserved quantity in continuous-time optimization dynamics,
termed computational inertia. Defined as the sum of kinetic energy (parameter
velocity) and potential energy (loss), this scalar remains invariant under
idealized, frictionless training. We formalize this conservation law, derive
its analytic decay under damping and stochastic perturbations, and demonstrate
its behavior in a synthetic system. The invariant offers a compact lens for
interpreting learning trajectories, and may inform theoretical tools for
analyzing convergence, stability, and training geometry.",2025-05-25,Atahan Karagoz,http://arxiv.org/pdf/2505.19171v1,cs.LG
JEDI: The Force of Jensen-Shannon Divergence in Disentangling Diffusion Models,"We introduce JEDI, a test-time adaptation method that enhances subject
separation and compositional alignment in diffusion models without requiring
retraining or external supervision. JEDI operates by minimizing semantic
entanglement in attention maps using a novel Jensen-Shannon divergence based
objective. To improve efficiency, we leverage adversarial optimization,
reducing the number of updating steps required.
  JEDI is model-agnostic and applicable to architectures such as Stable
Diffusion 1.5 and 3.5, consistently improving prompt alignment and
disentanglement in complex scenes. Additionally, JEDI provides a lightweight,
CLIP-free disentanglement score derived from internal attention distributions,
offering a principled benchmark for compositional alignment under test-time
conditions. We will publicly release the implementation of our method.",2025-05-25,"Eric Tillmann Bill, Enis Simsar, Thomas Hofmann",http://arxiv.org/pdf/2505.19166v1,cs.LG
BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations,"In the domain of sponsored search advertising, the focus of Keyphrase
recommendation has largely been on exact match types, which pose issues such as
high management expenses, limited targeting scope, and evolving search query
patterns. Alternatives like Broad match types can alleviate certain drawbacks
of exact matches but present challenges like poor targeting accuracy and
minimal supervisory signals owing to limited advertiser usage. This research
defines the criteria for an ideal broad match, emphasizing on both efficiency
and effectiveness, ensuring that a significant portion of matched queries are
relevant. We propose BroadGen, an innovative framework that recommends
efficient and effective broad match keyphrases by utilizing historical search
query data. Additionally, we demonstrate that BroadGen, through token
correspondence modeling, maintains better query stability over time. BroadGen's
capabilities allow it to serve daily, millions of sellers at eBay with over 2.3
billion items.",2025-05-25,"Ashirbad Mishra, Jinyu Zhao, Soumik Dey, Hansi Wu, Binbin Li, Kamesh Madduri",http://arxiv.org/pdf/2505.19164v1,cs.LG
Do Large Language Models (Really) Need Statistical Foundations?,"Large language models (LLMs) represent a new paradigm for processing
unstructured data, with applications across an unprecedented range of domains.
In this paper, we address, through two arguments, whether the development and
application of LLMs would genuinely benefit from foundational contributions
from the statistics discipline. First, we argue affirmatively, beginning with
the observation that LLMs are inherently statistical models due to their
profound data dependency and stochastic generation processes, where statistical
insights are naturally essential for handling variability and uncertainty.
Second, we argue that the persistent black-box nature of LLMs -- stemming from
their immense scale, architectural complexity, and development practices often
prioritizing empirical performance over theoretical interpretability -- renders
closed-form or purely mechanistic analyses generally intractable, thereby
necessitating statistical approaches due to their flexibility and often
demonstrated effectiveness. To substantiate these arguments, the paper outlines
several research areas -- including alignment, watermarking, uncertainty
quantification, evaluation, and data mixture optimization -- where statistical
methodologies are critically needed and are already beginning to make valuable
contributions. We conclude with a discussion suggesting that statistical
research concerning LLMs will likely form a diverse ``mosaic'' of specialized
topics rather than deriving from a single unifying theory, and highlighting the
importance of timely engagement by our statistics community in LLM research.",2025-05-25,Weijie Su,http://arxiv.org/pdf/2505.19145v1,cs.LG
ADGSyn: Dual-Stream Learning for Efficient Anticancer Drug Synergy Prediction,"Drug combinations play a critical role in cancer therapy by significantly
enhancing treatment efficacy and overcoming drug resistance. However, the
combinatorial space of possible drug pairs grows exponentially, making
experimental screening highly impractical. Therefore, developing efficient
computational methods to predict promising drug combinations and guide
experimental validation is of paramount importance. In this work, we propose
ADGSyn, an innovative method for predicting drug synergy. The key components of
our approach include: (1) shared projection matrices combined with attention
mechanisms to enable cross-drug feature alignment; (2) automatic mixed
precision (AMP)-optimized graph operations that reduce memory consumption by
40\% while accelerating training speed threefold; and (3) residual pathways
stabilized by LayerNorm to ensure stable gradient propagation during training.
Evaluated on the O'Neil dataset containing 13,243 drug--cell line combinations,
ADGSyn demonstrates superior performance over eight baseline methods. Moreover,
the framework supports full-batch processing of up to 256 molecular graphs on a
single GPU, setting a new standard for efficiency in drug synergy prediction
within the field of computational oncology.",2025-05-25,"Yuxuan Nie, Yutong Song, Hong Peng",http://arxiv.org/pdf/2505.19144v1,cs.LG
Uncertainty Quantification for Physics-Informed Neural Networks with Extended Fiducial Inference,"Uncertainty quantification (UQ) in scientific machine learning is
increasingly critical as neural networks are widely adopted to tackle complex
problems across diverse scientific disciplines. For physics-informed neural
networks (PINNs), a prominent model in scientific machine learning, uncertainty
is typically quantified using Bayesian or dropout methods. However, both
approaches suffer from a fundamental limitation: the prior distribution or
dropout rate required to construct honest confidence sets cannot be determined
without additional information. In this paper, we propose a novel method within
the framework of extended fiducial inference (EFI) to provide rigorous
uncertainty quantification for PINNs. The proposed method leverages a
narrow-neck hyper-network to learn the parameters of the PINN and quantify
their uncertainty based on imputed random errors in the observations. This
approach overcomes the limitations of Bayesian and dropout methods, enabling
the construction of honest confidence sets based solely on observed data. This
advancement represents a significant breakthrough for PINNs, greatly enhancing
their reliability, interpretability, and applicability to real-world scientific
and engineering challenges. Moreover, it establishes a new theoretical
framework for EFI, extending its application to large-scale models, eliminating
the need for sparse hyper-networks, and significantly improving the
automaticity and robustness of statistical inference.",2025-05-25,"Frank Shih, Zhenghao Jiang, Faming Liang",http://arxiv.org/pdf/2505.19136v1,cs.LG
Incentivizing High-Quality Human Annotations with Golden Questions,"Human-annotated data plays a vital role in training large language models
(LLMs), such as supervised fine-tuning and human preference alignment. However,
it is not guaranteed that paid human annotators produce high-quality data. In
this paper, we study how to incentivize human annotators to do so. We start
from a principal-agent model to model the dynamics between the company (the
principal) and the annotator (the agent), where the principal can only monitor
the annotation quality by examining $n$ samples. We investigate the maximum
likelihood estimators (MLE) and the corresponding hypothesis testing to
incentivize annotators: the agent is given a bonus if the MLE passes the test.
By analyzing the variance of the outcome, we show that the strategic behavior
of the agent makes the hypothesis testing very different from traditional ones:
Unlike the exponential rate proved by the large deviation theory, the
principal-agent model's hypothesis testing rate is of $\Theta(1/\sqrt{n \log
n})$. Our theory implies two criteria for the \emph{golden questions} to
monitor the performance of the annotators: they should be of (1) high certainty
and (2) similar format to normal ones. In that light, we select a set of golden
questions in human preference data. By doing incentive-compatible experiments,
we find out that the annotators' behavior is better revealed by those golden
questions, compared to traditional survey techniques such as instructed
manipulation checks.",2025-05-25,"Shang Liu, Zhongze Cai, Hanzhao Wang, Zhongyao Ma, Xiaocheng Li",http://arxiv.org/pdf/2505.19134v1,cs.LG
Fast and Accurate Power Load Data Completion via Regularization-optimized Low-Rank Factorization,"Low-rank representation learning has emerged as a powerful tool for
recovering missing values in power load data due to its ability to exploit the
inherent low-dimensional structures of spatiotemporal measurements. Among
various techniques, low-rank factorization models are favoured for their
efficiency and interpretability. However, their performance is highly sensitive
to the choice of regularization parameters, which are typically fixed or
manually tuned, resulting in limited generalization capability or slow
convergence in practical scenarios. In this paper, we propose a
Regularization-optimized Low-Rank Factorization, which introduces a
Proportional-Integral-Derivative controller to adaptively adjust the
regularization coefficient. Furthermore, we provide a detailed algorithmic
complexity analysis, showing that our method preserves the computational
efficiency of stochastic gradient descent while improving adaptivity.
Experimental results on real-world power load datasets validate the superiority
of our method in both imputation accuracy and training efficiency compared to
existing baselines.",2025-05-25,"Yan Xia, Hao Feng, Hongwei Sun, Junjie Wang, Qicong Hu",http://arxiv.org/pdf/2505.19133v1,cs.LG
Exploring Magnitude Preservation and Rotation Modulation in Diffusion Transformers,"Denoising diffusion models exhibit remarkable generative capabilities, but
remain challenging to train due to their inherent stochasticity, where
high-variance gradient estimates lead to slow convergence. Previous works have
shown that magnitude preservation helps with stabilizing training in the U-net
architecture. This work explores whether this effect extends to the Diffusion
Transformer (DiT) architecture. As such, we propose a magnitude-preserving
design that stabilizes training without normalization layers. Motivated by the
goal of maintaining activation magnitudes, we additionally introduce rotation
modulation, which is a novel conditioning method using learned rotations
instead of traditional scaling or shifting. Through empirical evaluations and
ablation studies on small-scale models, we show that magnitude-preserving
strategies significantly improve performance, notably reducing FID scores by
$\sim$12.8%. Further, we show that rotation modulation combined with scaling is
competitive with AdaLN, while requiring $\sim$5.4% fewer parameters. This work
provides insights into conditioning strategies and magnitude control. We will
publicly release the implementation of our method.",2025-05-25,"Eric Tillman Bill, Cristian Perez Jensen, Sotiris Anagnostidis, Dimitri von Rütte",http://arxiv.org/pdf/2505.19122v1,cs.LG
FP4 All the Way: Fully Quantized Training of LLMs,"We demonstrate, for the first time, fully quantized training (FQT) of large
language models (LLMs) using predominantly 4-bit floating-point (FP4) precision
for weights, activations, and gradients on datasets up to 200 billion tokens.
We extensively investigate key design choices for FP4, including block sizes,
scaling formats, and rounding methods. Our analysis shows that the NVFP4
format, where each block of 16 FP4 values (E2M1) shares a scale represented in
E4M3, provides optimal results. We use stochastic rounding for backward and
update passes and round-to-nearest for the forward pass to enhance stability.
Additionally, we identify a theoretical and empirical threshold for effective
quantized training: when the gradient norm falls below approximately $\sqrt{3}$
times the quantization noise, quantized training becomes less effective.
Leveraging these insights, we successfully train a 7-billion-parameter model on
256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves
downstream task performance comparable to a standard BF16 baseline, confirming
that FP4 training is a practical and highly efficient approach for large-scale
LLM training. A reference implementation is supplied in
https://github.com/Anonymous1252022/fp4-all-the-way .",2025-05-25,"Brian Chmiel, Maxim Fishman, Ron Banner, Daniel Soudry",http://arxiv.org/pdf/2505.19115v1,cs.LG
An Interpretable Representation Learning Approach for Diffusion Tensor Imaging,"Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the
structural connectivity of the brain, but presents challenges in effective
representation and interpretation in deep learning models. In this work, we
propose a novel 2D representation of DTI tractography that encodes tract-level
fractional anisotropy (FA) values into a 9x9 grayscale image. This
representation is processed through a Beta-Total Correlation Variational
Autoencoder with a Spatial Broadcast Decoder to learn a disentangled and
interpretable latent embedding. We evaluate the quality of this embedding using
supervised and unsupervised representation learning strategies, including
auxiliary classification, triplet loss, and SimCLR-based contrastive learning.
Compared to the 1D Group deep neural network (DNN) baselines, our approach
improves the F1 score in a downstream sex classification task by 15.74% and
shows a better disentanglement than the 3D representation.",2025-05-25,"Vishwa Mohan Singh, Alberto Gaston Villagran Asiares, Luisa Sophie Schuhmacher, Kate Rendall, Simon Weißbrod, David Rügamer, Inga Körte",http://arxiv.org/pdf/2505.19110v1,cs.LG
Optimization-Inspired Few-Shot Adaptation for Large Language Models,"Large Language Models (LLMs) have demonstrated remarkable performance in
real-world applications. However, adapting LLMs to novel tasks via fine-tuning
often requires substantial training data and computational resources that are
impractical in few-shot scenarios. Existing approaches, such as in-context
learning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations:
in-context learning introduces additional inference computational overhead with
limited performance gains, while PEFT models are prone to overfitting on the
few demonstration examples. In this work, we reinterpret the forward pass of
LLMs as an optimization process, a sequence of preconditioned gradient descent
steps refining internal representations. Based on this connection, we propose
Optimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization
that learns preconditioners without introducing additional trainable
parameters, and an objective that improves optimization efficiency by learning
preconditioners based on a convergence bound, while simultaneously steering the
optimization path toward the flat local minimum. Our method overcomes both
issues of ICL-based and PEFT-based methods, and demonstrates superior
performance over the existing methods on a variety of few-shot adaptation tasks
in experiments.",2025-05-25,"Boyan Gao, Xin Wang, Yibo Yang, David Clifton",http://arxiv.org/pdf/2505.19107v1,cs.LG
Latent Mamba Operator for Partial Differential Equations,"Neural operators have emerged as powerful data-driven frameworks for solving
Partial Differential Equations (PDEs), offering significant speedups over
numerical methods. However, existing neural operators struggle with scalability
in high-dimensional spaces, incur high computational costs, and face challenges
in capturing continuous and long-range dependencies in PDE dynamics. To address
these limitations, we introduce the Latent Mamba Operator (LaMO), which
integrates the efficiency of state-space models (SSMs) in latent space with the
expressive power of kernel integral formulations in neural operators. We also
establish a theoretical connection between state-space models (SSMs) and the
kernel integral of neural operators. Extensive experiments across diverse PDE
benchmarks on regular grids, structured meshes, and point clouds covering solid
and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA)
performance, with a 32.3\% improvement over existing baselines in solution
operator approximation, highlighting its efficacy in modeling complex PDE
solutions.",2025-05-25,"Karn Tiwari, Niladri Dutta, N M Anoop Krishnan, Prathosh A P",http://arxiv.org/pdf/2505.19105v1,cs.LG
Statistical inference for Linear Stochastic Approximation with Markovian Noise,"In this paper we derive non-asymptotic Berry-Esseen bounds for Polyak-Ruppert
averaged iterates of the Linear Stochastic Approximation (LSA) algorithm driven
by the Markovian noise. Our analysis yields $\mathcal{O}(n^{-1/4})$ convergence
rates to the Gaussian limit in the Kolmogorov distance. We further establish
the non-asymptotic validity of a multiplier block bootstrap procedure for
constructing the confidence intervals, guaranteeing consistent inference under
Markovian sampling. Our work provides the first non-asymptotic guarantees on
the rate of convergence of bootstrap-based confidence intervals for stochastic
approximation with Markov noise. Moreover, we recover the classical rate of
order $\mathcal{O}(n^{-1/8})$ up to logarithmic factors for estimating the
asymptotic variance of the iterates of the LSA algorithm.",2025-05-25,"Sergey Samsonov, Marina Sheshukova, Eric Moulines, Alexey Naumov",http://arxiv.org/pdf/2505.19102v1,cs.LG
Towards Robust Influence Functions with Flat Validation Minima,"The Influence Function (IF) is a widely used technique for assessing the
impact of individual training samples on model predictions. However, existing
IF methods often fail to provide reliable influence estimates in deep neural
networks, particularly when applied to noisy training data. This issue does not
stem from inaccuracies in parameter change estimation, which has been the
primary focus of prior research, but rather from deficiencies in loss change
estimation, specifically due to the sharpness of validation risk. In this work,
we establish a theoretical connection between influence estimation error,
validation set risk, and its sharpness, underscoring the importance of flat
validation minima for accurate influence estimation. Furthermore, we introduce
a novel estimation form of Influence Function specifically designed for flat
validation minima. Experimental results across various tasks validate the
superiority of our approach.",2025-05-25,"Xichen Ye, Yifan Wu, Weizhong Zhang, Cheng Jin, Yifan Chen",http://arxiv.org/pdf/2505.19097v1,cs.LG
A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random,"Model-based clustering integrated with variable selection is a powerful tool
for uncovering latent structures within complex data. However, its
effectiveness is often hindered by challenges such as identifying relevant
variables that define heterogeneous subgroups and handling data that are
missing not at random, a prevalent issue in fields like transcriptomics. While
several notable methods have been proposed to address these problems, they
typically tackle each issue in isolation, thereby limiting their flexibility
and adaptability. This paper introduces a unified framework designed to address
these challenges simultaneously. Our approach incorporates a data-driven
penalty matrix into penalized clustering to enable more flexible variable
selection, along with a mechanism that explicitly models the relationship
between missingness and latent class membership. We demonstrate that, under
certain regularity conditions, the proposed framework achieves both asymptotic
consistency and selection consistency, even in the presence of missing data.
This unified strategy significantly enhances the capability and efficiency of
model-based clustering, advancing methodologies for identifying informative
variables that define homogeneous subgroups in the presence of complex missing
data patterns. The performance of the framework, including its computational
efficiency, is evaluated through simulations and demonstrated using both
synthetic and real-world transcriptomic datasets.",2025-05-25,"Binh H. Ho, Long Nguyen Chi, TrungTin Nguyen, Binh T. Nguyen, Van Ha Hoang, Christopher Drovandi",http://arxiv.org/pdf/2505.19093v1,cs.LG
ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models,"Recent advancements in Large Vision-Language Models (VLMs), have greatly
enhanced their capability to jointly process text and images. However, despite
extensive benchmarks evaluating visual comprehension (e.g., diagrams, color
schemes, OCR tasks...), there is limited assessment of VLMs' ability to read
and reason about text-rich images effectively. To fill this gap, we introduce
ReadBench, a multimodal benchmark specifically designed to evaluate the reading
comprehension capabilities of VLMs. ReadBench transposes contexts from
established text-only benchmarks into images of text while keeping textual
prompts and questions intact. Evaluating leading VLMs with ReadBench, we find
minimal-but-present performance degradation on short, text-image inputs, while
performance sharply declines for longer, multi-page contexts. Our experiments
further reveal that text resolution has negligible effects on multimodal
performance. These findings highlight needed improvements in VLMs, particularly
their reasoning over visually presented extensive textual content, a capability
critical for practical applications. ReadBench is available at
https://github.com/answerdotai/ReadBench .",2025-05-25,"Benjamin Clavié, Florian Brand",http://arxiv.org/pdf/2505.19091v1,cs.LG
CMoS: Rethinking Time Series Prediction Through the Lens of Chunk-wise Spatial Correlations,"Recent advances in lightweight time series forecasting models suggest the
inherent simplicity of time series forecasting tasks. In this paper, we present
CMoS, a super-lightweight time series forecasting model. Instead of learning
the embedding of the shapes, CMoS directly models the spatial correlations
between different time series chunks. Additionally, we introduce a Correlation
Mixing technique that enables the model to capture diverse spatial correlations
with minimal parameters, and an optional Periodicity Injection technique to
ensure faster convergence. Despite utilizing as low as 1% of the lightweight
model DLinear's parameters count, experimental results demonstrate that CMoS
outperforms existing state-of-the-art models across multiple datasets.
Furthermore, the learned weights of CMoS exhibit great interpretability,
providing practitioners with valuable insights into temporal structures within
specific application scenarios.",2025-05-25,"Haotian Si, Changhua Pei, Jianhui Li, Dan Pei, Gaogang Xie",http://arxiv.org/pdf/2505.19090v1,cs.LG
Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes,"We analyze the generalization gap (gap between the training and test errors)
when training a potentially over-parametrized model using a Markovian
stochastic training algorithm, initialized from some distribution $\theta_0
\sim p_0$. We focus on Langevin dynamics with a positive temperature
$\beta^{-1}$, i.e. gradient descent on a training loss $L$ with infinitesimal
step size, perturbed with $\beta^{-1}$-variances Gaussian noise, and lightly
regularized or bounded. There, we bound the generalization gap, at any time
during training, by $\sqrt{(\beta\mathbb{E} L (\theta_0) + \log(1/\delta))/N}$
with probability $1-\delta$ over the dataset, where $N$ is the sample size, and
$\mathbb{E} L (\theta_0) =O(1)$ with standard initialization scaling. In
contrast to previous guarantees, we have no dependence on either training time
or reliance on mixing, nor a dependence on dimensionality, gradient norms, or
any other properties of the loss or model. This guarantee follows from a
general analysis of any Markov process-based training that has a Gibbs-style
stationary distribution. The proof is surprisingly simple, once we observe that
the marginal distribution divergence from initialization remains bounded, as
implied by a generalized second law of thermodynamics.",2025-05-25,"Itamar Harel, Yonathan Wolanowsky, Gal Vardi, Nathan Srebro, Daniel Soudry",http://arxiv.org/pdf/2505.19087v1,cs.LG
Jodi: Unification of Visual Generation and Understanding via Joint Modeling,"Visual generation and understanding are two deeply interconnected aspects of
human intelligence, yet they have been traditionally treated as separate tasks
in machine learning. In this paper, we propose Jodi, a diffusion framework that
unifies visual generation and understanding by jointly modeling the image
domain and multiple label domains. Specifically, Jodi is built upon a linear
diffusion transformer along with a role switch mechanism, which enables it to
perform three particular types of tasks: (1) joint generation, where the model
simultaneously generates images and multiple labels; (2) controllable
generation, where images are generated conditioned on any combination of
labels; and (3) image perception, where multiple labels can be predicted at
once from a given image. Furthermore, we present the Joint-1.6M dataset, which
contains 200,000 high-quality images collected from public sources, automatic
labels for 7 visual domains, and LLM-generated captions. Extensive experiments
demonstrate that Jodi excels in both generation and understanding tasks and
exhibits strong extensibility to a wider range of visual domains. Code is
available at https://github.com/VIPL-GENUN/Jodi.",2025-05-25,"Yifeng Xu, Zhenliang He, Meina Kan, Shiguang Shan, Xilin Chen",http://arxiv.org/pdf/2505.19084v1,cs.LG
Geometric Determinations Of Characteristic Redshifts From DESI-DR2 BAO and DES-SN5YR Observations: Hints For New Expansion Rate Anomalies,"In this work, we perform a model-agnostic reconstruction of the cosmic
expansion history by combining DESI-DR2 BAO and DES-SN5YR data, with a focus on
geometric determination of characteristic redshifts where notable tensions in
the expansion rate are found to emerge. Employing Gaussian process regression
alongside knot-based spline techniques, we reconstruct cosmic distances and
their derivatives to pinpoint these characteristic redshifts and infer $E(z)$.
Our analysis reveals significant deviations of approximately 4 to 5$\sigma$
from the Planck 2018 $\Lambda$CDM predictions, particularly pronounced in the
redshift range $z \sim 0.35-0.55$. These anomalies are consistently observed
across both reconstruction methods and combined datasets, indicating robust
late-time departures that could signal new physics beyond the standard
cosmological framework. The joint use of BAO and SN probes enhances the
precision of our constraints, allowing us to isolate these deviations without
reliance on specific cosmological assumptions. Our findings underscore the role
of characteristic redshifts as sensitive indicators of expansion rate anomalies
and motivate further scrutiny with forthcoming datasets from DESI-5YR BAO,
Euclid, and LSST. These future surveys will tighten constraints and help
distinguish whether these late-time anomalies arise from new fundamental
physics or unresolved systematics in the data.",2025-05-25,"Purba Mukherjee, Anjan A Sen",http://arxiv.org/pdf/2505.19083v1,cs.LG
"Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs","Large Language Models (LLMs) have demonstrated remarkable general
capabilities, but enhancing skills such as reasoning often demands substantial
computational resources and may compromise their generalization. While
Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious
alternative, they typically requires retraining for each LLM backbone due to
architectural dependencies. To address these challenges, here we propose
Universal Reasoner (UniR) - a single, lightweight, composable, and
plug-and-play reasoning module that can be used with any frozen LLM to endow it
with specialized reasoning capabilities. Specifically, UniR decomposes the
reward into a standalone reasoning module that is trained independently using
predefined rewards, effectively translating trajectory-level signals into
token-level guidance. Once trained, UniR can be combined with any frozen LLM at
inference time by simply adding its output logits to those of the LLM backbone.
This additive structure naturally enables modular composition: multiple UniR
modules trained for different tasks can be jointly applied by summing their
logits, enabling complex reasoning via composition. Experimental results on
mathematical reasoning and machine translation tasks show that UniR
significantly outperforms \add{existing baseline fine-tuning methods using the
Llama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong
generalization: reasoning modules trained on smaller models effectively guide
much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust
solution for enhancing reasoning in LLMs without compromising their core
capabilities. Code is open-sourced at https://github.com/hangeol/UniR",2025-05-25,"Jaemin Kim, Hangeol Chang, Hyunmin Hwang, Choonghan Kim, Jong Chul Ye",http://arxiv.org/pdf/2505.19075v1,cs.LG
Recalibrating binary probabilistic classifiers,"Recalibration of binary probabilistic classifiers to a target prior
probability is an important task in areas like credit risk management. We
analyse methods for recalibration from a distribution shift perspective.
Distribution shift assumptions linked to the area under the curve (AUC) of a
probabilistic classifier are found to be useful for the design of meaningful
recalibration methods. Two new methods called parametric covariate shift with
posterior drift (CSPD) and ROC-based quasi moment matching (QMM) are proposed
and tested together with some other methods in an example setting. The outcomes
of the test suggest that the QMM methods discussed in the paper can provide
appropriately conservative results in evaluations with concave functionals like
for instance risk weights functions for credit risk.",2025-05-25,Dirk Tasche,http://arxiv.org/pdf/2505.19068v1,cs.LG
Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management,"Motivated by dynamic parameter optimization in finite, but large action
(configurations) spaces, this work studies the nonstochastic multi-armed bandit
(MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We
propose ABoB, a hierarchical Adversarial Bandit over Bandits algorithm that can
use state-of-the-art existing ""flat"" algorithms, but additionally clusters
similar configurations to exploit local structures and adapt to changing
environments. We prove that in the worst-case scenario, such clustering
approach cannot hurt too much and ABoB guarantees a standard worst-case regret
bound of $O\left(k^{\frac{1}{2}}T^{\frac{1}{2}}\right)$, where $T$ is the
number of rounds and $k$ is the number of arms, matching the traditional flat
approach. However, under favorable conditions related to the algorithm
properties, clusters properties, and certain Lipschitz conditions, the regret
bound can be improved to $O\left(k^{\frac{1}{4}}T^{\frac{1}{2}}\right)$.
Simulations and experiments on a real storage system demonstrate that ABoB,
using standard algorithms like EXP3 and Tsallis-INF, achieves lower regret and
faster convergence than the flat method, up to 50% improvement in known
previous setups, nonstochastic and stochastic, as well as in our settings.",2025-05-25,"Chen Avin, Zvi Lotker, Shie Mannor, Gil Shabat, Hanan Shteingart, Roey Yadgar",http://arxiv.org/pdf/2505.19061v1,cs.LG
An Initial Exploration of Fine-tuning Small Language Models for Smart Contract Reentrancy Vulnerability Detection,"Large Language Models (LLMs) are being used more and more for various coding
tasks, including to help coders identify bugs and are a promising avenue to
support coders in various tasks including vulnerability detection --
particularly given the flexibility of such generative AI models and tools. Yet
for many tasks it may not be suitable to use LLMs, for which it may be more
suitable to use smaller language models that can fit and easily execute and
train on a developer's computer. In this paper we explore and evaluate whether
smaller language models can be fine-tuned to achieve reasonable results for a
niche area: vulnerability detection -- specifically focusing on detecting the
reentrancy bug in Solidity smart contracts.",2025-05-25,"Ignacio Mariano Andreozzi Pofcher, Joshua Ellul",http://arxiv.org/pdf/2505.19059v1,cs.LG
Distributionally Robust Deep Q-Learning,"We propose a novel distributionally robust $Q$-learning algorithm for the
non-tabular case accounting for continuous state spaces where the state
transition of the underlying Markov decision process is subject to model
uncertainty. The uncertainty is taken into account by considering the
worst-case transition from a ball around a reference probability measure. To
determine the optimal policy under the worst-case state transition, we solve
the associated non-linear Bellman equation by dualising and regularising the
Bellman operator with the Sinkhorn distance, which is then parameterized with
deep neural networks. This approach allows us to modify the Deep Q-Network
algorithm to optimise for the worst case state transition.
  We illustrate the tractability and effectiveness of our approach through
several applications, including a portfolio optimisation task based on
S\&{P}~500 data.",2025-05-25,"Chung I Lu, Julian Sester, Aijia Zhang",http://arxiv.org/pdf/2505.19058v1,cs.LG
An Embarrassingly Simple Defense Against LLM Abliteration Attacks,"Large language models (LLMs) are typically aligned to comply with safety
guidelines by refusing harmful instructions. A recent attack, termed
abliteration, isolates and suppresses the single latent direction most
responsible for refusal behavior, enabling the model to generate unethical
content. We propose a defense that modifies how models generate refusals. We
construct an extended-refusal dataset that contains harmful prompts with a full
response that justifies the reason for refusal. We then fine-tune
Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our
extended-refusal dataset, and evaluate the resulting systems on a set of
harmful prompts. In our experiments, extended-refusal models maintain high
refusal rates, dropping at most by 10%, whereas baseline models' refusal rates
drop by 70-80% after abliteration. A broad evaluation of safety and utility
shows that extended-refusal fine-tuning neutralizes the abliteration attack
while preserving general performance.",2025-05-25,"Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, Bernard Ghanem, George Turkiyyah",http://arxiv.org/pdf/2505.19056v1,cs.LG
Reduce Computational Cost In Deep Reinforcement Learning Via Randomized Policy Learning,"Recent advancements in reinforcement learning (RL) have leveraged neural
networks to achieve state-of-the-art performance across various control tasks.
However, these successes often come at the cost of significant computational
resources, as training deep neural networks requires substantial time and data.
In this paper, we introduce an actor-critic algorithm that utilizes randomized
neural networks to drastically reduce computational costs while maintaining
strong performance. Despite its simple architecture, our method effectively
solves a range of control problems, including the locomotion control of a
highly dynamic 12-motor quadruped robot, and achieves results comparable to
leading algorithms such as Proximal Policy Optimization (PPO). Notably, our
approach does not outperform other algorithms in terms of sample efficnency but
rather in terms of wall-clock training time. That is, although our algorithm
requires more timesteps to converge to an optimal policy, the actual time
required for training turns out to be lower.",2025-05-25,"Zhuochen Liu, Rahul Jain, Quan Nguyen",http://arxiv.org/pdf/2505.19054v1,cs.LG
Structured Reinforcement Learning for Combinatorial Decision-Making,"Reinforcement learning (RL) is increasingly applied to real-world problems
involving complex and structured decisions, such as routing, scheduling, and
assortment planning. These settings challenge standard RL algorithms, which
struggle to scale, generalize, and exploit structure in the presence of
combinatorial action spaces. We propose Structured Reinforcement Learning
(SRL), a novel actor-critic framework that embeds combinatorial optimization
layers into the actor neural network. We enable end-to-end learning of the
actor via Fenchel-Young losses and provide a geometric interpretation of SRL as
a primal-dual algorithm in the dual of the moment polytope. Across six
environments with exogenous and endogenous uncertainty, SRL matches or
surpasses the performance of unstructured RL and imitation learning on static
tasks and improves over these baselines by up to 92% on dynamic problems, with
improved stability and convergence speed.",2025-05-25,"Heiko Hoppe, Léo Baty, Louis Bouvier, Axel Parmentier, Maximilian Schiffer",http://arxiv.org/pdf/2505.19053v1,cs.LG
Efficient Data Selection at Scale via Influence Distillation,"Effective data selection is critical for efficient training of modern Large
Language Models (LLMs). This paper introduces Influence Distillation, a novel,
mathematically-justified framework for data selection that employs second-order
information to optimally weight training samples. By distilling each sample's
influence on a target distribution, our method assigns model-specific weights
that are used to select training data for LLM fine-tuning, guiding it toward
strong performance on the target domain. We derive these optimal weights for
both Gradient Descent and Adam optimizers. To ensure scalability and reduce
computational cost, we propose a $\textit{landmark-based approximation}$:
influence is precisely computed for a small subset of ""landmark"" samples and
then efficiently propagated to all other samples to determine their weights. We
validate Influence Distillation by applying it to instruction tuning on the
Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,
across several models from the Llama and Qwen families. Experiments show that
Influence Distillation matches or outperforms state-of-the-art performance
while achieving up to $3.5\times$ faster selection.",2025-05-25,"Mahdi Nikdan, Vincent Cohen-Addad, Dan Alistarh, Vahab Mirrokni",http://arxiv.org/pdf/2505.19051v1,cs.LG
When Models Don't Collapse: On the Consistency of Iterative MLE,"The widespread use of generative models has created a feedback loop, in which
each generation of models is trained on data partially produced by its
predecessors. This process has raised concerns about \emph{model collapse}: A
critical degradation in performance caused by repeated training on synthetic
data. However, different analyses in the literature have reached different
conclusions as to the severity of model collapse. As such, it remains unclear
how concerning this phenomenon is, and under which assumptions it can be
avoided. To address this, we theoretically study model collapse for maximum
likelihood estimation (MLE), in a natural setting where synthetic data is
gradually added to the original data set. Under standard assumptions (similar
to those long used for proving asymptotic consistency and normality of MLE), we
establish non-asymptotic bounds showing that collapse can be avoided even as
the fraction of real data vanishes. On the other hand, we prove that some
assumptions (beyond MLE consistency) are indeed necessary: Without them, model
collapse can occur arbitrarily quickly, even when the original data is still
present in the training set. To the best of our knowledge, these are the first
rigorous examples of iterative generative modeling with accumulating data that
rapidly leads to model collapse.",2025-05-25,"Daniel Barzilai, Ohad Shamir",http://arxiv.org/pdf/2505.19046v1,cs.LG
Offline Clustering of Linear Bandits: Unlocking the Power of Clusters in Data-Limited Environments,"Contextual linear multi-armed bandits are a learning framework for making a
sequence of decisions, e.g., advertising recommendations for a sequence of
arriving users. Recent works have shown that clustering these users based on
the similarity of their learned preferences can significantly accelerate the
learning. However, prior work has primarily focused on the online setting,
which requires continually collecting user data, ignoring the offline data
widely available in many applications. To tackle these limitations, we study
the offline clustering of bandits (Off-ClusBand) problem, which studies how to
use the offline dataset to learn cluster properties and improve decision-making
across multiple users. The key challenge in Off-ClusBand arises from data
insufficiency for users: unlike the online case, in the offline case, we have a
fixed, limited dataset to work from and thus must determine whether we have
enough data to confidently cluster users together. To address this challenge,
we propose two algorithms: Off-C$^2$LUB, which we analytically show performs
well for arbitrary amounts of user data, and Off-CLUB, which is prone to bias
when data is limited but, given sufficient data, matches a theoretical lower
bound that we derive for the offline clustered MAB problem. We experimentally
validate these results on both real and synthetic datasets.",2025-05-25,"Jingyuan Liu, Zeyu Zhang, Xuchuang Wang, Xutong Liu, John C. S. Lui, Mohammad Hajiesmaili, Carlee Joe-Wong",http://arxiv.org/pdf/2505.19043v1,cs.LG
Turb-L1: Achieving Long-term Turbulence Tracing By Tackling Spectral Bias,"Accurately predicting the long-term evolution of turbulence is crucial for
advancing scientific understanding and optimizing engineering applications.
However, existing deep learning methods face significant bottlenecks in
long-term autoregressive prediction, which exhibit excessive smoothing and fail
to accurately track complex fluid dynamics. Our extensive experimental and
spectral analysis of prevailing methods provides an interpretable explanation
for this shortcoming, identifying Spectral Bias as the core obstacle.
Concretely, spectral bias is the inherent tendency of models to favor
low-frequency, smooth features while overlooking critical high-frequency
details during training, thus reducing fidelity and causing physical
distortions in long-term predictions. Building on this insight, we propose
Turb-L1, an innovative turbulence prediction method, which utilizes a
Hierarchical Dynamics Synthesis mechanism within a multi-grid architecture to
explicitly overcome spectral bias. It accurately captures cross-scale
interactions and preserves the fidelity of high-frequency dynamics, enabling
reliable long-term tracking of turbulence evolution. Extensive experiments on
the 2D turbulence benchmark show that Turb-L1 demonstrates excellent
performance: (I) In long-term predictions, it reduces Mean Squared Error (MSE)
by $80.3\%$ and increases Structural Similarity (SSIM) by over $9\times$
compared to the SOTA baseline, significantly improving prediction fidelity.
(II) It effectively overcomes spectral bias, accurately reproducing the full
enstrophy spectrum and maintaining physical realism in high-wavenumber regions,
thus avoiding the spectral distortions or spurious energy accumulation seen in
other methods.",2025-05-25,"Hao Wu, Yuan Gao, Ruiqi Shu, Zean Han, Fan Xu, Zhihong Zhu, Qingsong Wen, Xian Wu, Kun Wang, Xiaomeng Huang",http://arxiv.org/pdf/2505.19038v1,cs.LG
Optimal Conformal Prediction under Epistemic Uncertainty,"Conformal prediction (CP) is a popular frequentist framework for representing
uncertainty by providing prediction sets that guarantee coverage of the true
label with a user-adjustable probability. In most applications, CP operates on
confidence scores coming from a standard (first-order) probabilistic predictor
(e.g., softmax outputs). Second-order predictors, such as credal set predictors
or Bayesian models, are also widely used for uncertainty quantification and are
known for their ability to represent both aleatoric and epistemic uncertainty.
Despite their popularity, there is still an open question on ``how they can be
incorporated into CP''. In this paper, we discuss the desiderata for CP when
valid second-order predictions are available. We then introduce Bernoulli
prediction sets (BPS), which produce the smallest prediction sets that ensure
conditional coverage in this setting. When given first-order predictions, BPS
reduces to the well-known adaptive prediction sets (APS). Furthermore, when the
validity assumption on the second-order predictions is compromised, we apply
conformal risk control to obtain a marginal coverage guarantee while still
accounting for epistemic uncertainty.",2025-05-25,"Alireza Javanmardi, Soroush H. Zargarbashi, Santo M. A. R. Thies, Willem Waegeman, Aleksandar Bojchevski, Eyke Hüllermeier",http://arxiv.org/pdf/2505.19033v1,cs.LG
Learn Beneficial Noise as Graph Augmentation,"Although graph contrastive learning (GCL) has been widely investigated, it is
still a challenge to generate effective and stable graph augmentations.
Existing methods often apply heuristic augmentation like random edge dropping,
which may disrupt important graph structures and result in unstable GCL
performance. In this paper, we propose Positive-incentive Noise driven Graph
Data Augmentation (PiNGDA), where positive-incentive noise (pi-noise)
scientifically analyzes the beneficial effect of noise under the information
theory. To bridge the standard GCL and pi-noise framework, we design a Gaussian
auxiliary variable to convert the loss function to information entropy. We
prove that the standard GCL with pre-defined augmentations is equivalent to
estimate the beneficial noise via the point estimation. Following our analysis,
PiNGDA is derived from learning the beneficial noise on both topology and
attributes through a trainable noise generator for graph augmentations, instead
of the simple estimation. Since the generator learns how to produce beneficial
perturbations on graph topology and node attributes, PiNGDA is more reliable
compared with the existing methods. Extensive experimental results validate the
effectiveness and stability of PiNGDA.",2025-05-25,"Siqi Huang, Yanchen Xu, Hongyuan Zhang, Xuelong Li",http://arxiv.org/pdf/2505.19024v1,cs.LG
A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking,"Monkeypox is a viral disease characterized by distinctive skin lesions and
has been reported in many countries. The recent global outbreak has emphasized
the urgent need for scalable, accessible, and accurate diagnostic solutions to
support public health responses.
  In this study, we developed ITMAINN, an intelligent, AI-driven healthcare
system specifically designed to detect Monkeypox from skin lesion images using
advanced deep learning techniques. Our system consists of three main
components. First, we trained and evaluated several pretrained models using
transfer learning on publicly available skin lesion datasets to identify the
most effective models. For binary classification (Monkeypox vs. non-Monkeypox),
the Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16
achieved the highest performance, each with an accuracy and F1-score of 97.8%.
For multiclass classification, which contains images of patients with Monkeypox
and five other classes (chickenpox, measles, hand-foot-mouth disease, cowpox,
and healthy), ResNetViT and ViT Hybrid models achieved 92% accuracy, with F1
scores of 92.24% and 92.19%, respectively. The best-performing and most
lightweight model, MobileViT, was deployed within the mobile application. The
second component is a cross-platform smartphone application that enables users
to detect Monkeypox through image analysis, track symptoms, and receive
recommendations for nearby healthcare centers based on their location. The
third component is a real-time monitoring dashboard designed for health
authorities to support them in tracking cases, analyzing symptom trends,
guiding public health interventions, and taking proactive measures.
  This system is fundamental in developing responsive healthcare infrastructure
within smart cities. Our solution, ITMAINN, is part of revolutionizing public
health management.",2025-05-25,"Huda Alghoraibi, Nuha Alqurashi, Sarah Alotaibi, Renad Alkhudaydi, Bdoor Aldajani, Lubna Alqurashi, Jood Batweel, Maha A. Thafar",http://arxiv.org/pdf/2505.19023v1,cs.LG
Querying Kernel Methods Suffices for Reconstructing their Training Data,"Over-parameterized models have raised concerns about their potential to
memorize training data, even when achieving strong generalization. The privacy
implications of such memorization are generally unclear, particularly in
scenarios where only model outputs are accessible. We study this question in
the context of kernel methods, and demonstrate both empirically and
theoretically that querying kernel models at various points suffices to
reconstruct their training data, even without access to model parameters. Our
results hold for a range of kernel methods, including kernel regression,
support vector machines, and kernel density estimation. Our hope is that this
work can illuminate potential privacy concerns for such models.",2025-05-25,"Daniel Barzilai, Yuval Margalit, Eitan Gronich, Gilad Yehudai, Meirav Galun, Ronen Basri",http://arxiv.org/pdf/2505.19019v1,cs.LG
WorldEval: World Model as Real-World Robot Policies Evaluator,"The field of robotics has made significant strides toward developing
generalist robot manipulation policies. However, evaluating these policies in
real-world scenarios remains time-consuming and challenging, particularly as
the number of tasks scales and environmental conditions change. In this work,
we demonstrate that world models can serve as a scalable, reproducible, and
reliable proxy for real-world robot policy evaluation. A key challenge is
generating accurate policy videos from world models that faithfully reflect the
robot actions. We observe that directly inputting robot actions or using
high-dimensional encoding methods often fails to generate action-following
videos. To address this, we propose Policy2Vec, a simple yet effective approach
to turn a video generation model into a world simulator that follows latent
action to generate the robot video. We then introduce WorldEval, an automated
pipeline designed to evaluate real-world robot policies entirely online.
WorldEval effectively ranks various robot policies and individual checkpoints
within a single policy, and functions as a safety detector to prevent dangerous
actions by newly developed robot models. Through comprehensive paired
evaluations of manipulation policies in real-world environments, we demonstrate
a strong correlation between policy performance in WorldEval and real-world
scenarios. Furthermore, our method significantly outperforms popular methods
such as real-to-sim approach.",2025-05-25,"Yaxuan Li, Yichen Zhu, Junjie Wen, Chaomin Shen, Yi Xu",http://arxiv.org/pdf/2505.19017v1,cs.LG
Tokenizing Electron Cloud in Protein-Ligand Interaction Learning,"The affinity and specificity of protein-molecule binding directly impact
functional outcomes, uncovering the mechanisms underlying biological regulation
and signal transduction. Most deep-learning-based prediction approaches focus
on structures of atoms or fragments. However, quantum chemical properties, such
as electronic structures, are the key to unveiling interaction patterns but
remain largely underexplored. To bridge this gap, we propose ECBind, a method
for tokenizing electron cloud signals into quantized embeddings, enabling their
integration into downstream tasks such as binding affinity prediction. By
incorporating electron densities, ECBind helps uncover binding modes that
cannot be fully represented by atom-level models. Specifically, to remove the
redundancy inherent in electron cloud signals, a structure-aware transformer
and hierarchical codebooks encode 3D binding sites enriched with electron
structures into tokens. These tokenized codes are then used for specific tasks
with labels. To extend its applicability to a wider range of scenarios, we
utilize knowledge distillation to develop an electron-cloud-agnostic prediction
model. Experimentally, ECBind demonstrates state-of-the-art performance across
multiple tasks, achieving improvements of 6.42\% and 15.58\% in per-structure
Pearson and Spearman correlation coefficients, respectively.",2025-05-25,"Haitao Lin, Odin Zhang, Jia Xu, Yunfan Liu, Zheng Cheng, Lirong Wu, Yufei Huang, Zhifeng Gao, Stan Z. Li",http://arxiv.org/pdf/2505.19014v1,cs.LG
Faithful Group Shapley Value,"Data Shapley is an important tool for data valuation, which quantifies the
contribution of individual data points to machine learning models. In practice,
group-level data valuation is desirable when data providers contribute data in
batch. However, we identify that existing group-level extensions of Data
Shapley are vulnerable to shell company attacks, where strategic group
splitting can unfairly inflate valuations. We propose Faithful Group Shapley
Value (FGSV) that uniquely defends against such attacks. Building on original
mathematical insights, we develop a provably fast and accurate approximation
algorithm for computing FGSV. Empirical experiments demonstrate that our
algorithm significantly outperforms state-of-the-art methods in computational
efficiency and approximation accuracy, while ensuring faithful group-level
valuation.",2025-05-25,"Kiljae Lee, Ziqi Liu, Weijing Tang, Yuan Zhang",http://arxiv.org/pdf/2505.19013v1,cs.LG
Semi-pessimistic Reinforcement Learning,"Offline reinforcement learning (RL) aims to learn an optimal policy from
pre-collected data. However, it faces challenges of distributional shift, where
the learned policy may encounter unseen scenarios not covered in the offline
data. Additionally, numerous applications suffer from a scarcity of labeled
reward data. Relying on labeled data alone often leads to a narrow state-action
distribution, further amplifying the distributional shift, and resulting in
suboptimal policy learning. To address these issues, we first recognize that
the volume of unlabeled data is typically substantially larger than that of
labeled data. We then propose a semi-pessimistic RL method to effectively
leverage abundant unlabeled data. Our approach offers several advantages. It
considerably simplifies the learning process, as it seeks a lower bound of the
reward function, rather than that of the Q-function or state transition
function. It is highly flexible, and can be integrated with a range of
model-free and model-based RL algorithms. It enjoys the guaranteed improvement
when utilizing vast unlabeled data, but requires much less restrictive
conditions. We compare our method with a number of alternative solutions, both
analytically and numerically, and demonstrate its clear competitiveness. We
further illustrate with an application to adaptive deep brain stimulation for
Parkinson's disease.",2025-05-25,"Jin Zhu, Xin Zhou, Jiaang Yao, Gholamali Aminian, Omar Rivasplata, Simon Little, Lexin Li, Chengchun Shi",http://arxiv.org/pdf/2505.19002v1,cs.LG
Automatic and Structure-Aware Sparsification of Hybrid Neural ODEs,"Hybrid neural ordinary differential equations (neural ODEs) integrate
mechanistic models with neural ODEs, offering strong inductive bias and
flexibility, and are particularly advantageous in data-scarce healthcare
settings. However, excessive latent states and interactions from mechanistic
models can lead to training inefficiency and over-fitting, limiting practical
effectiveness of hybrid neural ODEs. In response, we propose a new hybrid
pipeline for automatic state selection and structure optimization in
mechanistic neural ODEs, combining domain-informed graph modifications with
data-driven regularization to sparsify the model for improving predictive
performance and stability while retaining mechanistic plausibility. Experiments
on synthetic and real-world data show improved predictive performance and
robustness with desired sparsity, establishing an effective solution for hybrid
model reduction in healthcare applications.",2025-05-25,"Bob Junyi Zou, Lu Tian",http://arxiv.org/pdf/2505.18996v1,cs.LG
STRICT: Stress Test of Rendering Images Containing Text,"While diffusion models have revolutionized text-to-image generation with
their ability to synthesize realistic and diverse scenes, they continue to
struggle to generate consistent and legible text within images. This
shortcoming is commonly attributed to the locality bias inherent in
diffusion-based generation, which limits their ability to model long-range
spatial dependencies. In this paper, we introduce $\textbf{STRICT}$, a
benchmark designed to systematically stress-test the ability of diffusion
models to render coherent and instruction-aligned text in images. Our benchmark
evaluates models across multiple dimensions: (1) the maximum length of readable
text that can be generated; (2) the correctness and legibility of the generated
text, and (3) the ratio of not following instructions for generating text. We
evaluate several state-of-the-art models, including proprietary and open-source
variants, and reveal persistent limitations in long-range consistency and
instruction-following capabilities. Our findings provide insights into
architectural bottlenecks and motivate future research directions in multimodal
generative modeling. We release our entire evaluation pipeline at
https://github.com/tianyu-z/STRICT-Bench.",2025-05-25,"Tianyu Zhang, Xinyu Wang, Zhenghan Tai, Lu Li, Jijun Chi, Jingrui Tian, Hailin He, Suyuchen Wang",http://arxiv.org/pdf/2505.18985v1,cs.LG
AmorLIP: Efficient Language-Image Pretraining via Amortization,"Contrastive Language-Image Pretraining (CLIP) has demonstrated strong
zero-shot performance across diverse downstream text-image tasks. Existing CLIP
methods typically optimize a contrastive objective using negative samples drawn
from each minibatch. To achieve robust representation learning, these methods
require extremely large batch sizes and escalate computational demands to
hundreds or even thousands of GPUs. Prior approaches to mitigate this issue
often compromise downstream performance, prolong training duration, or face
scalability challenges with very large datasets. To overcome these limitations,
we propose AmorLIP, an efficient CLIP pretraining framework that amortizes
expensive computations involved in contrastive learning through lightweight
neural networks, which substantially improves training efficiency and
performance. Leveraging insights from a spectral factorization of energy-based
models, we introduce novel amortization objectives along with practical
techniques to improve training stability. Extensive experiments across 38
downstream tasks demonstrate the superior zero-shot classification and
retrieval capabilities of AmorLIP, consistently outperforming standard CLIP
baselines with substantial relative improvements of up to 12.24%.",2025-05-25,"Haotian Sun, Yitong Li, Yuchen Zhuang, Niao He, Hanjun Dai, Bo Dai",http://arxiv.org/pdf/2505.18983v1,cs.LG
FedSKC: Federated Learning with Non-IID Data via Structural Knowledge Collaboration,"With the advancement of edge computing, federated learning (FL) displays a
bright promise as a privacy-preserving collaborative learning paradigm.
However, one major challenge for FL is the data heterogeneity issue, which
refers to the biased labeling preferences among multiple clients, negatively
impacting convergence and model performance. Most previous FL methods attempt
to tackle the data heterogeneity issue locally or globally, neglecting
underlying class-wise structure information contained in each client. In this
paper, we first study how data heterogeneity affects the divergence of the
model and decompose it into local, global, and sampling drift sub-problems. To
explore the potential of using intra-client class-wise structural knowledge in
handling these drifts, we thus propose Federated Learning with Structural
Knowledge Collaboration (FedSKC). The key idea of FedSKC is to extract and
transfer domain preferences from inter-client data distributions, offering
diverse class-relevant knowledge and a fair convergent signal. FedSKC comprises
three components: i) local contrastive learning, to prevent weight divergence
resulting from local training; ii) global discrepancy aggregation, which
addresses the parameter deviation between the server and clients; iii) global
period review, correcting for the sampling drift introduced by the server
randomly selecting devices. We have theoretically analyzed FedSKC under
non-convex objectives and empirically validated its superiority through
extensive experimental results.",2025-05-25,"Huan Wang, Haoran Li, Huaming Chen, Jun Yan, Lijuan Wang, Jiahua Shi, Shiping Chen, Jun Shen",http://arxiv.org/pdf/2505.18981v1,cs.LG
GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization,"Text-to-image (T2I) generation models can inadvertently produce
not-safe-for-work (NSFW) content, prompting the integration of text and image
safety filters. Recent advances employ large language models (LLMs) for
semantic-level detection, rendering traditional token-level perturbation
attacks largely ineffective. However, our evaluation shows that existing
jailbreak methods are ineffective against these modern filters. We introduce
GhostPrompt, the first automated jailbreak framework that combines dynamic
prompt optimization with multimodal feedback. It consists of two key
components: (i) Dynamic Optimization, an iterative process that guides a large
language model (LLM) using feedback from text safety filters and CLIP
similarity scores to generate semantically aligned adversarial prompts; and
(ii) Adaptive Safety Indicator Injection, which formulates the injection of
benign visual cues as a reinforcement learning problem to bypass image-level
filters. GhostPrompt achieves state-of-the-art performance, increasing the
ShieldLM-7B bypass rate from 12.5\% (Sneakyprompt) to 99.0\%, improving CLIP
score from 0.2637 to 0.2762, and reducing the time cost by $4.2 \times$.
Moreover, it generalizes to unseen filters including GPT-4.1 and successfully
jailbreaks DALLE 3 to generate NSFW images in our evaluation, revealing
systemic vulnerabilities in current multimodal defenses. To support further
research on AI safety and red-teaming, we will release code and adversarial
prompts under a controlled-access protocol.",2025-05-25,"Zixuan Chen, Hao Lin, Ke Xu, Xinghao Jiang, Tanfeng Sun",http://arxiv.org/pdf/2505.18979v1,cs.LG
GraSS: Scalable Influence Function with Sparse Gradient Compression,"Gradient-based data attribution methods, such as influence functions, are
critical for understanding the impact of individual training samples without
requiring repeated model retraining. However, their scalability is often
limited by the high computational and memory costs associated with per-sample
gradient computation. In this work, we propose GraSS, a novel gradient
compression algorithm and its variants FactGraSS for linear layers
specifically, that explicitly leverage the inherent sparsity of per-sample
gradients to achieve sub-linear space and time complexity. Extensive
experiments demonstrate the effectiveness of our approach, achieving
substantial speedups while preserving data influence fidelity. In particular,
FactGraSS achieves up to 165% faster throughput on billion-scale models
compared to the previous state-of-the-art baselines. Our code is publicly
available at https://github.com/TRAIS-Lab/GraSS.",2025-05-25,"Pingbang Hu, Joseph Melkonian, Weijing Tang, Han Zhao, Jiaqi W. Ma",http://arxiv.org/pdf/2505.18976v1,cs.LG
Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings,"Selective state-space models have achieved great success in long-sequence
modeling. However, their capacity for language representation, especially in
complex hierarchical reasoning tasks, remains underexplored. Most large
language models rely on flat Euclidean embeddings, limiting their ability to
capture latent hierarchies. To address this limitation, we propose Hierarchical
Mamba (HiM), integrating efficient Mamba2 with exponential growth and curved
nature of hyperbolic geometry to learn hierarchy-aware language embeddings for
deeper linguistic understanding. Mamba2-processed sequences are projected to
the Poincare ball (via tangent-based mapping) or Lorentzian manifold (via
cosine and sine-based mapping) with ""learnable"" curvature, optimized with a
combined hyperbolic loss. Our HiM model facilitates the capture of relational
distances across varying hierarchical levels, enabling effective long-range
reasoning. This makes it well-suited for tasks like mixed-hop prediction and
multi-hop inference in hierarchical classification. We evaluated our HiM with
four linguistic and medical datasets for mixed-hop prediction and multi-hop
inference tasks. Experimental results demonstrated that: 1) Both HiM models
effectively capture hierarchical relationships for four ontological datasets,
surpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic
distinctions with higher h-norms, while HiM-Lorentz provides more stable,
compact, and hierarchy-preserving embeddings favoring robustness over detail.",2025-05-25,"Sarang Patil, Ashish Parmanand Pandey, Ioannis Koutis, Mengjia Xu",http://arxiv.org/pdf/2505.18973v1,cs.LG
Protein Design with Dynamic Protein Vocabulary,"Protein design is a fundamental challenge in biotechnology, aiming to design
novel sequences with specific functions within the vast space of possible
proteins. Recent advances in deep generative models have enabled function-based
protein design from textual descriptions, yet struggle with structural
plausibility. Inspired by classical protein design methods that leverage
natural protein structures, we explore whether incorporating fragments from
natural proteins can enhance foldability in generative models. Our empirical
results show that even random incorporation of fragments improves foldability.
Building on this insight, we introduce ProDVa, a novel protein design approach
that integrates a text encoder for functional descriptions, a protein language
model for designing proteins, and a fragment encoder to dynamically retrieve
protein fragments based on textual functional descriptions. Experimental
results demonstrate that our approach effectively designs protein sequences
that are both functionally aligned and structurally plausible. Compared to
state-of-the-art models, ProDVa achieves comparable function alignment using
less than 0.04% of the training data, while designing significantly more
well-folded proteins, with the proportion of proteins having pLDDT above 70
increasing by 7.38% and those with PAE below 10 increasing by 9.6%.",2025-05-25,"Nuowei Liu, Jiahao Kuang, Yanting Liu, Changzhi Sun, Tao Ji, Yuanbin Wu, Man Lan",http://arxiv.org/pdf/2505.18966v1,cs.LG
How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation,"LiDAR-based 3D panoptic segmentation often struggles with the inherent
sparsity of data from LiDAR sensors, which makes it challenging to accurately
recognize distant or small objects. Recently, a few studies have sought to
overcome this challenge by integrating LiDAR inputs with camera images,
leveraging the rich and dense texture information provided by the latter. While
these approaches have shown promising results, they still face challenges, such
as misalignment during data augmentation and the reliance on post-processing
steps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel
multi-modal 3D panoptic segmentation framework. In IAL, we first introduce a
modality-synchronized data augmentation strategy, PieAug, to ensure alignment
between LiDAR and image inputs from the start. Next, we adopt a transformer
decoder to directly predict panoptic segmentation results. To effectively fuse
LiDAR and image features into tokens for the decoder, we design a
Geometric-guided Token Fusion (GTF) module. Additionally, we leverage the
complementary strengths of each modality as priors for query initialization
through a Prior-based Query Generation (PQG) module, enhancing the decoder's
ability to generate accurate instance masks. Our IAL framework achieves
state-of-the-art performance compared to previous multi-modal 3D panoptic
segmentation methods on two widely used benchmarks. Code and models are
publicly available at <https://github.com/IMPL-Lab/IAL.git>.",2025-05-25,"Yining Pan, Qiongjie Cui, Xulei Yang, Na Zhao",http://arxiv.org/pdf/2505.18956v1,cs.LG
Online Knowledge Distillation with Reward Guidance,"This work studies knowledge distillation (KD) for large language models
(LLMs) through preference optimization. We propose a reward-guided imitation
learning framework for sequential KD, formulating a min-max optimization
problem between the policy and reward model (RM) to minimize the performance
gap between the student and teacher policies. Specifically, the reward
optimization is constrained to achieve near-optimality within a confidence set
for preference alignment. For preference data construction, we explore both
offline and online preference-based KD. Additionally, we reformulate the RM
using the $Q$-value function and extend the framework to white-box KD, where
the teacher policy's predicted probabilities are accessible. Theoretical
analysis and empirical results demonstrate the effectiveness of the proposed
framework.",2025-05-25,Chen Jia,http://arxiv.org/pdf/2505.18952v1,cs.LG
The Price of Format: Diversity Collapse in LLMs,"Instruction-tuned large language models (LLMs) employ structured templates,
such as role markers and special tokens, to enforce format consistency during
inference. However, we identify a critical limitation of such formatting: it
induces a phenomenon we term diversity collapse, where the model generates
semantically similar outputs for open-ended inputs, undermining creativity and
variability. We systematically evaluate this effect across tasks like story
completion and free-form generation, finding that (1) diversity collapse
persists even under high-temperature sampling, and (2) structural tokens in
templates significantly constrain the model's output space. To contextualize
these findings, we fine-tune the same model using a range of structured prompts
and then evaluate them across three axes: downstream task performance,
alignment behavior, and output diversity. Our analysis shows that format
consistency between fine-tuning and inference is crucial for
structure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on
knowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity
is primarily governed by the presence or absence of structural tokens, with
minimal formatting yielding the most diverse outputs. These findings reveal
that current prompting conventions, while beneficial for alignment, may
inadvertently suppress output diversity, underscoring the need for
diversity-aware prompt design and instruction tuning.",2025-05-25,"Longfei Yun, Chenyang An, Zilong Wang, Letian Peng, Jingbo Shang",http://arxiv.org/pdf/2505.18949v1,cs.LG
Exact Expressive Power of Transformers with Padding,"Chain of thought is a natural inference-time method for increasing the
computational power of transformer-based large language models (LLMs), but
comes at the cost of sequential decoding. Are there more efficient alternatives
to expand a transformer's expressive power without adding parameters? We
consider transformers with padding tokens as a form of parallelizable test-time
compute. We show that averaging-hard-attention, masked-pre-norm transformers
with polynomial padding converge to precisely the class $\mathsf{TC}^0$ of
extremely parallelizable problems. While the $\mathsf{TC}^0$ upper bound was
known, proving a matching lower bound had been elusive. Further, our novel
analysis reveals the precise expanded power of padded transformers when coupled
with another form of inference-time compute, namely dynamically increasing
depth via looping. Our core technical contribution is to show how padding helps
bring the notions of complete problems and reductions, which have been a
cornerstone of classical complexity theory, to the formal study of
transformers. Armed with this new tool, we prove that padded transformers with
$O(\log^d n)$ looping on inputs of length $n$ recognize exactly the class
$\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and
looping together systematically expand transformers' expressive power: with
polylogarithmic looping, padded transformers converge to the class
$\mathsf{NC}$, the best that could be expected without losing parallelism
(unless $\mathsf{NC} = \mathsf{P}$). Our results thus motivate further
exploration of padding and looping as parallelizable alternatives to chain of
thought.",2025-05-25,"William Merrill, Ashish Sabharwal",http://arxiv.org/pdf/2505.18948v1,cs.LG
Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection,"Graph Anomaly Detection (GAD) in heterogeneous networks presents unique
challenges due to node and edge heterogeneity. Existing Graph Neural Network
(GNN) methods primarily focus on homogeneous GAD and thus fail to address three
key issues: (C1) Capturing abnormal signal and rich semantics across diverse
meta-paths; (C2) Retaining high-frequency content in HIN dimension alignment;
and (C3) Learning effectively from difficult anomaly samples with class
imbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based
on a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse
domains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter,
which captures anomalous information via applying dedicated Chi-Square filters
to each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns
features while preserving high-frequency information and incorporates
heterogeneous messages by a unified Chi-Square Filter; and (3)
Contribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies
to address class imbalance. Extensive experiments on public and industrial
datasets show that ChiGAD outperforms state-of-the-art models on multiple
metrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD
datasets, validating the effectiveness of Chi-Square filters. Our code is
available at https://github.com/HsipingLi/ChiGAD.",2025-05-25,"Xiping Li, Xiangyu Dong, Xingyi Zhang, Kun Xie, Yuanhao Feng, Bo Wang, Guilin Li, Wuxiong Zeng, Xiujun Shu, Sibo Wang",http://arxiv.org/pdf/2505.18934v1,cs.LG
Can Large Language Models Infer Causal Relationships from Real-World Text?,"Understanding and inferring causal relationships from texts is a core aspect
of human cognition and is essential for advancing large language models (LLMs)
towards artificial general intelligence. Existing work primarily focuses on
synthetically generated texts which involve simple causal relationships
explicitly mentioned in the text. This fails to reflect the complexities of
real-world tasks. In this paper, we investigate whether LLMs are capable of
inferring causal relationships from real-world texts. We develop a benchmark
drawn from real-world academic literature which includes diverse texts with
respect to length, complexity of relationships (different levels of
explicitness, number of events, and causal relationships), and domains and
sub-domains. To the best of our knowledge, our benchmark is the first-ever
real-world dataset for this task. Our experiments on state-of-the-art LLMs
evaluated on our proposed benchmark demonstrate significant challenges, with
the best-performing model achieving an average F1 score of only 0.477. Analysis
reveals common pitfalls: difficulty with implicitly stated information, in
distinguishing relevant causal factors from surrounding contextual details, and
with connecting causally relevant information spread across lengthy textual
passages. By systematically characterizing these deficiencies, our benchmark
offers targeted insights for further research into advancing LLM causal
reasoning.",2025-05-25,"Ryan Saklad, Aman Chadha, Oleg Pavlov, Raha Moraffah",http://arxiv.org/pdf/2505.18931v1,cs.LG
Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time,"We propose a neural physics system for real-time, interactive fluid
simulations. Traditional physics-based methods, while accurate, are
computationally intensive and suffer from latency issues. Recent
machine-learning methods reduce computational costs while preserving fidelity;
yet most still fail to satisfy the latency constraints for real-time use and
lack support for interactive applications. To bridge this gap, we introduce a
novel hybrid method that integrates numerical simulation, neural physics, and
generative control. Our neural physics jointly pursues low-latency simulation
and high physical fidelity by employing a fallback safeguard to classical
numerical solvers. Furthermore, we develop a diffusion-based controller that is
trained using a reverse modeling strategy to generate external dynamic force
fields for fluid manipulation. Our system demonstrates robust performance
across diverse 2D/3D scenarios, material types, and obstacle interactions,
achieving real-time simulations at high frame rates (11~29% latency) while
enabling fluid control guided by user-friendly freehand sketches. We present a
significant step towards practical, controllable, and physically plausible
fluid simulations for real-time interactive applications. We promise to release
both models and data upon acceptance.",2025-05-25,"Jingxuan Xu, Hong Huang, Chuhang Zou, Manolis Savva, Yunchao Wei, Wuyang Chen",http://arxiv.org/pdf/2505.18926v1,cs.LG
Graph-Based Operator Learning from Limited Data on Irregular Domains,"Operator learning seeks to approximate mappings from input functions to
output solutions, particularly in the context of partial differential equations
(PDEs). While recent advances such as DeepONet and Fourier Neural Operator
(FNO) have demonstrated strong performance, they often rely on regular grid
discretizations, limiting their applicability to complex or irregular domains.
In this work, we propose a Graph-based Operator Learning with Attention (GOLA)
framework that addresses this limitation by constructing graphs from
irregularly sampled spatial points and leveraging attention-enhanced Graph
Neural Netwoks (GNNs) to model spatial dependencies with global information. To
improve the expressive capacity, we introduce a Fourier-based encoder that
projects input functions into a frequency space using learnable complex
coefficients, allowing for flexible embeddings even with sparse or nonuniform
samples. We evaluated our approach across a range of 2D PDEs, including Darcy
Flow, Advection, Eikonal, and Nonlinear Diffusion, under varying sampling
densities. Our method consistently outperforms baselines, particularly in
data-scarce regimes, demonstrating strong generalization and efficiency on
irregular domains.",2025-05-25,"Yile Li, Shandian Zhe",http://arxiv.org/pdf/2505.18923v1,cs.LG
ALPCAHUS: Subspace Clustering for Heteroscedastic Data,"Principal component analysis (PCA) is a key tool in the field of data
dimensionality reduction. Various methods have been proposed to extend PCA to
the union of subspace (UoS) setting for clustering data that come from multiple
subspaces like K-Subspaces (KSS). However, some applications involve
heterogeneous data that vary in quality due to noise characteristics associated
with each data sample. Heteroscedastic methods aim to deal with such mixed data
quality. This paper develops a heteroscedastic-focused subspace clustering
method, named ALPCAHUS, that can estimate the sample-wise noise variances and
use this information to improve the estimate of the subspace bases associated
with the low-rank structure of the data. This clustering algorithm builds on
K-Subspaces (KSS) principles by extending the recently proposed heteroscedastic
PCA method, named LR-ALPCAH, for clusters with heteroscedastic noise in the UoS
setting. Simulations and real-data experiments show the effectiveness of
accounting for data heteroscedasticity compared to existing clustering
algorithms. Code available at https://github.com/javiersc1/ALPCAHUS.",2025-05-25,"Javier Salazar Cavazos, Jeffrey A Fessler, Laura Balzano",http://arxiv.org/pdf/2505.18918v1,cs.LG
Behavior Injection: Preparing Language Models for Reinforcement Learning,"Reinforcement fine-tuning (RFT) has emerged as a powerful post-training
technique to incentivize the reasoning ability of large language models (LLMs).
However, LLMs can respond very inconsistently to RFT: some show substantial
performance gains, while others plateau or even degrade. To understand this
divergence, we analyze the per-step influence of the RL objective and identify
two key conditions for effective post-training: (1) RL-informative rollout
accuracy, and (2) strong data co-influence, which quantifies how much the
training data affects performance on other samples. Guided by these insights,
we propose behavior injection, a task-agnostic data-augmentation scheme applied
prior to RL. Behavior injection enriches the supervised finetuning (SFT) data
by seeding exploratory and exploitative behaviors, effectively making the model
more RL-ready. We evaluate our method across two reasoning benchmarks with
multiple base models. The results demonstrate that our theoretically motivated
augmentation can significantly increases the performance gain from RFT over the
pre-RL model.",2025-05-25,"Zhepeng Cen, Yihang Yao, William Han, Zuxin Liu, Ding Zhao",http://arxiv.org/pdf/2505.18917v1,cs.LG
On the Role of Label Noise in the Feature Learning Process,"Deep learning with noisy labels presents significant challenges. In this
work, we theoretically characterize the role of label noise from a feature
learning perspective. Specifically, we consider a signal-noise data
distribution, where each sample comprises a label-dependent signal and
label-independent noise, and rigorously analyze the training dynamics of a
two-layer convolutional neural network under this data setup, along with the
presence of label noise. Our analysis identifies two key stages. In Stage I,
the model perfectly fits all the clean samples (i.e., samples without label
noise) while ignoring the noisy ones (i.e., samples with noisy labels). During
this stage, the model learns the signal from the clean samples, which
generalizes well on unseen data. In Stage II, as the training loss converges,
the gradient in the direction of noise surpasses that of the signal, leading to
overfitting on noisy samples. Eventually, the model memorizes the noise present
in the noisy samples and degrades its generalization ability. Furthermore, our
analysis provides a theoretical basis for two widely used techniques for
tackling label noise: early stopping and sample selection. Experiments on both
synthetic and real-world setups validate our theory.",2025-05-25,"Andi Han, Wei Huang, Zhanpeng Zhou, Gang Niu, Wuyang Chen, Junchi Yan, Akiko Takeda, Taiji Suzuki",http://arxiv.org/pdf/2505.18909v1,cs.LG
Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations,"Prompt injection attacks are a critical security vulnerability in large
language models (LLMs), allowing attackers to hijack model behavior by
injecting malicious instructions within the input context. Recent defense
mechanisms have leveraged an Instruction Hierarchy (IH) Signal, often
implemented through special delimiter tokens or additive embeddings to denote
the privilege level of input tokens. However, these prior works typically
inject the IH signal exclusively at the initial input layer, which we
hypothesize limits its ability to effectively distinguish the privilege levels
of tokens as it propagates through the different layers of the model. To
overcome this limitation, we introduce a novel approach that injects the IH
signal into the intermediate token representations within the network. Our
method augments these representations with layer-specific trainable embeddings
that encode the privilege information. Our evaluations across multiple models
and training methods reveal that our proposal yields between $1.6\times$ and
$9.2\times$ reduction in attack success rate on gradient-based prompt injection
attacks compared to state-of-the-art methods, without significantly degrading
the model's utility.",2025-05-25,"Sanjay Kariyappa, G. Edward Suh",http://arxiv.org/pdf/2505.18907v1,cs.LG
PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models,"The rapid advancement of generative AI models has provided users with
numerous options to address their prompts. When selecting a generative AI model
for a given prompt, users should consider not only the performance of the
chosen model but also its associated service cost. The principle guiding such
consideration is to select the least expensive model among the available
satisfactory options. However, existing model-selection approaches typically
prioritize performance, overlooking pricing differences between models. In this
paper, we introduce PromptWise, an online learning framework designed to assign
a sequence of prompts to a group of large language models (LLMs) in a
cost-effective manner. PromptWise strategically queries cheaper models first,
progressing to more expensive options only if the lower-cost models fail to
adequately address a given prompt. Through numerical experiments, we
demonstrate PromptWise's effectiveness across various tasks, including puzzles
of varying complexity and code generation/translation tasks. The results
highlight that PromptWise consistently outperforms cost-unaware baseline
methods, emphasizing that directly assigning prompts to the most expensive
models can lead to higher costs and potentially lower average performance.",2025-05-24,"Xiaoyan Hu, Lauren Pick, Ho-fung Leung, Farzan Farnia",http://arxiv.org/pdf/2505.18901v1,cs.LG
Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos,"Imitation from videos often fails when expert demonstrations and learner
environments exhibit domain shifts, such as discrepancies in lighting, color,
or texture. While visual randomization partially addresses this problem by
augmenting training data, it remains computationally intensive and inherently
reactive, struggling with unseen scenarios. We propose a different approach:
instead of randomizing appearances, we eliminate their influence entirely by
rethinking the sensory representation itself. Inspired by biological vision
systems that prioritize temporal transients (e.g., retinal ganglion cells) and
by recent sensor advancements, we introduce event-inspired perception for
visually robust imitation. Our method converts standard RGB videos into a
sparse, event-based representation that encodes temporal intensity gradients,
discarding static appearance features. This biologically grounded approach
disentangles motion dynamics from visual style, enabling robust visual
imitation from observations even in the presence of visual mismatches between
expert and agent environments. By training policies on event streams, we
achieve invariance to appearance-based distractors without requiring
computationally expensive and environment-specific data augmentation
techniques. Experiments across the DeepMind Control Suite and the Adroit
platform for dynamic dexterous manipulation show the efficacy of our method.
Our code is publicly available at Eb-LAIfO.",2025-05-24,"Andrea Ramazzina, Vittorio Giammarino, Matteo El-Hariry, Mario Bijelic",http://arxiv.org/pdf/2505.18899v1,cs.LG
Marginal Fairness: Fair Decision-Making under Risk Measures,"This paper introduces marginal fairness, a new individual fairness notion for
equitable decision-making in the presence of protected attributes such as
gender, race, and religion. This criterion ensures that decisions based on
generalized distortion risk measures are insensitive to distributional
perturbations in protected attributes, regardless of whether these attributes
are continuous, discrete, categorical, univariate, or multivariate. To
operationalize this notion and reflect real-world regulatory environments (such
as the EU gender-neutral pricing regulation), we model business decision-making
in highly regulated industries (such as insurance and finance) as a two-step
process: (i) a predictive modeling stage, in which a prediction function for
the target variable (e.g., insurance losses) is estimated based on both
protected and non-protected covariates; and (ii) a decision-making stage, in
which a generalized distortion risk measure is applied to the target variable,
conditional only on non-protected covariates, to determine the decision. In
this second step, we modify the risk measure such that the decision becomes
insensitive to the protected attribute, thus enforcing fairness to ensure
equitable outcomes under risk-sensitive, regulatory constraints. Furthermore,
by utilizing the concept of cascade sensitivity, we extend the marginal
fairness framework to capture how dependencies between covariates propagate the
influence of protected attributes through the modeling pipeline. A numerical
study and an empirical implementation using an auto insurance dataset
demonstrate how the framework can be applied in practice.",2025-05-24,"Fei Huang, Silvana M. Pesenti",http://arxiv.org/pdf/2505.18895v1,cs.LG
Conformal Prediction for Uncertainty Estimation in Drug-Target Interaction Prediction,"Accurate drug-target interaction (DTI) prediction with machine learning
models is essential for drug discovery. Such models should also provide a
credible representation of their uncertainty, but applying classical marginal
conformal prediction (CP) in DTI prediction often overlooks variability across
drug and protein subgroups. In this work, we analyze three cluster-conditioned
CP methods for DTI prediction, and compare them with marginal and
group-conditioned CP. Clusterings are obtained via nonconformity scores,
feature similarity, and nearest neighbors, respectively. Experiments on the
KIBA dataset using four data-splitting strategies show that nonconformity-based
clustering yields the tightest intervals and most reliable subgroup coverage,
especially in random and fully unseen drug-protein splits. Group-conditioned CP
works well when one entity is familiar, but residual-driven clustering provides
robust uncertainty estimates even in sparse or novel scenarios. These results
highlight the potential of cluster-based CP for improving DTI prediction under
uncertainty.",2025-05-24,"Morteza Rakhshaninejad, Mira Jurgens, Nicolas Dewolf, Willem Waegeman",http://arxiv.org/pdf/2505.18890v1,cs.LG
KerZOO: Kernel Function Informed Zeroth-Order Optimization for Accurate and Accelerated LLM Fine-Tuning,"Large language models (LLMs) have demonstrated impressive capabilities across
numerous NLP tasks. Nevertheless, conventional first-order fine-tuning
techniques impose heavy memory demands, creating practical obstacles to
real-world applications. Zeroth-order (ZO) optimization has recently emerged as
a promising memory-efficient alternative, as it circumvents the need for
backpropagation by estimating gradients solely through forward passes--making
it particularly suitable for resource-limited environments. Despite its
efficiency, ZO optimization suffers from gradient estimation bias, which
significantly hinders convergence speed. To address this, we analytically
identify and characterize the lower-order bias introduced during ZO-based
gradient estimation in LLM fine-tuning. Motivated by tools in mathematical
physics, we introduce a kernel-function-based ZO framework aimed at mitigating
this bias and improving optimization stability. KerZOO achieves comparable or
superior performance to existing ZO baselines in both full-parameter and
parameter-efficient fine-tuning settings of LLMs, while significantly reducing
the number of iterations required to reach convergence. For example, KerZOO
reduces total GPU training hours by as much as 74% and 44% on WSC and MultiRC
datasets in fine-tuning OPT-2.7B model and can exceed the MeZO baseline by 2.9%
and 2.6% in accuracy. We show that the kernel function is an effective avenue
for reducing estimation bias in ZO methods.",2025-05-24,"Zhendong Mi, Qitao Tan, Xiaodong Yu, Zining Zhu, Geng Yuan, Shaoyi Huang",http://arxiv.org/pdf/2505.18886v1,cs.LG
LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders,"Visual encoders have become fundamental components in modern computer vision
pipelines. However, ensuring robustness against adversarial perturbations
remains a critical challenge. Recent efforts have explored both supervised and
unsupervised adversarial fine-tuning strategies. We identify two key
limitations in these approaches: (i) they often suffer from instability,
especially during the early stages of fine-tuning, resulting in suboptimal
convergence and degraded performance on clean data, and (ii) they exhibit a
suboptimal trade-off between robustness and clean data accuracy, hindering the
simultaneous optimization of both objectives. To overcome these challenges, we
propose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised
adversarial fine-tuning framework. LORE utilizes constrained optimization,
which offers a principled approach to balancing competing goals, such as
improving robustness while preserving nominal performance. By enforcing
embedding-space proximity constraints, LORE effectively maintains clean data
performance throughout adversarial fine-tuning. Extensive experiments show that
LORE significantly improves zero-shot adversarial robustness with minimal
degradation in clean data accuracy. Furthermore, we demonstrate the
effectiveness of the adversarially fine-tuned CLIP image encoder in
out-of-distribution generalization and enhancing the interpretability of image
embeddings.",2025-05-24,"Borna Khodabandeh, Amirabbas Afzali, Amirhossein Afsharrad, Seyed Shahabeddin Mousavi, Sanjay Lall, Sajjad Amini, Seyed-Mohsen Moosavi-Dezfooli",http://arxiv.org/pdf/2505.18884v1,cs.LG
Partition Generative Modeling: Masked Modeling Without Masks,"We introduce ``Partition Generative Models'' (PGMs), a novel approach to
masked generative modeling (MGMs), particularly effective for masked diffusion
language modeling (MDLMs). PGM divides tokens into two distinct groups and
employs sparse attention patterns to prevent cross-group information exchange.
Hence, the model is trained to predict tokens in one group based solely on
information from the other group. This partitioning strategy eliminates the
need for MASK tokens entirely. While traditional MGMs inefficiently process
MASK tokens during generation, PGMs achieve greater computational efficiency by
operating exclusively on unmasked tokens. Our experiments on OpenWebText with a
context length of 1024 tokens demonstrate that PGMs deliver at least 5x
improvements in both latency and throughput compared to MDLM when using the
same number of sampling steps, while generating samples with better generative
perplexity than MDLM. Finally, we show that PGMs can be distilled with
Self-Distillation Through Time (SDTT), a method originally devised for MDLM, in
order to achieve further inference gains.",2025-05-24,"Justin Deschenaux, Lan Tran, Caglar Gulcehre",http://arxiv.org/pdf/2505.18883v1,cs.LG
RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models,"Low-Rank Adaptation (LoRA) lowers the computational and memory overhead of
fine-tuning large models by updating a low-dimensional subspace of the
pre-trained weight matrix. Albeit efficient, LoRA exhibits suboptimal
convergence and noticeable performance degradation, due to inconsistent and
imbalanced weight updates induced by its nonunique low-rank factorizations. To
overcome these limitations, this article identifies the optimal low-rank
factorization per step that minimizes an upper bound on the loss. The resultant
refactored low-rank adaptation (RefLoRA) method promotes a flatter loss
landscape, along with consistent and balanced weight updates, thus speeding up
stable convergence. Extensive experiments evaluate RefLoRA on natural language
understanding, and commonsense reasoning tasks with popular large language
models including DeBERTaV3, LLaMA-7B, LLaMA2-7B and LLaMA3-8B. The numerical
tests corroborate that RefLoRA converges faster, outperforms various
benchmarks, and enjoys negligible computational overhead compared to
state-of-the-art LoRA variants.",2025-05-24,"Yilang Zhang, Bingcong Li, Georgios B. Giannakis",http://arxiv.org/pdf/2505.18877v1,cs.LG
Non-Stationary Lipschitz Bandits,"We study the problem of non-stationary Lipschitz bandits, where the number of
actions is infinite and the reward function, satisfying a Lipschitz assumption,
can change arbitrarily over time. We design an algorithm that adaptively tracks
the recently introduced notion of significant shifts, defined by large
deviations of the cumulative reward function. To detect such reward changes,
our algorithm leverages a hierarchical discretization of the action space.
Without requiring any prior knowledge of the non-stationarity, our algorithm
achieves a minimax-optimal dynamic regret bound of
$\mathcal{\widetilde{O}}(\tilde{L}^{1/3}T^{2/3})$, where $\tilde{L}$ is the
number of significant shifts and $T$ the horizon. This result provides the
first optimal guarantee in this setting.",2025-05-24,"Nicolas Nguyen, Solenne Gaucher, Claire Vernade",http://arxiv.org/pdf/2505.18871v1,cs.LG
Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing,"Lay paraphrasing aims to make scientific information accessible to audiences
without technical backgrounds. However, most existing studies focus on a single
domain, such as biomedicine. With the rise of interdisciplinary research, it is
increasingly necessary to comprehend knowledge spanning multiple technical
fields. To address this, we propose Sci-LoRA, a model that leverages a mixture
of LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA
dynamically generates and applies weights for each LoRA, enabling it to adjust
the impact of different domains based on the input text, without requiring
explicit domain labels. To balance domain-specific knowledge and generalization
across various domains, Sci-LoRA integrates information at both the data and
model levels. This dynamic fusion enhances the adaptability and performance
across various domains. Experimental results across twelve domains on five
public datasets show that Sci-LoRA significantly outperforms state-of-the-art
large language models and demonstrates flexible generalization and adaptability
in cross-domain lay paraphrasing.",2025-05-24,"Ming Cheng, Jiaying Gong, Hoda Eldardiry",http://arxiv.org/pdf/2505.18867v1,cs.LG
Distribution-Aware Mobility-Assisted Decentralized Federated Learning,"Decentralized federated learning (DFL) has attracted significant attention
due to its scalability and independence from a central server. In practice,
some participating clients can be mobile, yet the impact of user mobility on
DFL performance remains largely unexplored, despite its potential to facilitate
communication and model convergence. In this work, we demonstrate that
introducing a small fraction of mobile clients, even with random movement, can
significantly improve the accuracy of DFL by facilitating information flow. To
further enhance performance, we propose novel distribution-aware mobility
patterns, where mobile clients strategically navigate the network, leveraging
knowledge of data distributions and static client locations. The proposed
moving strategies mitigate the impact of data heterogeneity and boost learning
convergence. Extensive experiments validate the effectiveness of induced
mobility in DFL and demonstrate the superiority of our proposed mobility
patterns over random movement.",2025-05-24,"Md Farhamdur Reza, Reza Jahani, Richeng Jin, Huaiyu Dai",http://arxiv.org/pdf/2505.18866v1,cs.LG
Guided by Guardrails: Control Barrier Functions as Safety Instructors for Robotic Learning,"Safety stands as the primary obstacle preventing the widespread adoption of
learning-based robotic systems in our daily lives. While reinforcement learning
(RL) shows promise as an effective robot learning paradigm, conventional RL
frameworks often model safety by using single scalar negative rewards with
immediate episode termination, failing to capture the temporal consequences of
unsafe actions (e.g., sustained collision damage). In this work, we introduce a
novel approach that simulates these temporal effects by applying continuous
negative rewards without episode termination. Our experiments reveal that
standard RL methods struggle with this model, as the accumulated negative
values in unsafe zones create learning barriers. To address this challenge, we
demonstrate how Control Barrier Functions (CBFs), with their proven safety
guarantees, effectively help robots avoid catastrophic regions while enhancing
learning outcomes. We present three CBF-based approaches, each integrating
traditional RL methods with Control Barrier Functions, guiding the agent to
learn safe behavior. Our empirical analysis, conducted in both simulated
environments and real-world settings using a four-wheel differential drive
robot, explores the possibilities of employing these approaches for safe
robotic learning.",2025-05-24,"Maeva Guerrier, Karthik Soma, Hassan Fouad, Giovanni Beltrame",http://arxiv.org/pdf/2505.18858v1,cs.LG
On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization,"Reinforcement learning (RL) has become popular in enhancing the reasoning
capabilities of large language models (LLMs), with Group Relative Policy
Optimization (GRPO) emerging as a widely used algorithm in recent systems.
Despite GRPO's widespread adoption, we identify a previously unrecognized
phenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood
of correct responses marginally increases or even decreases during training.
This behavior mirrors a recently discovered misalignment issue in Direct
Preference Optimization (DPO), attributed to the influence of negative
gradients. We provide a theoretical analysis of GRPO's learning dynamic,
identifying the source of LLD as the naive penalization of all tokens in
incorrect responses with the same strength. To address this, we develop a
method called NTHR, which downweights penalties on tokens contributing to the
LLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's
group-based structure, using correct responses as anchors to identify
influential tokens. Experiments on math reasoning benchmarks demonstrate that
NTHR effectively mitigates LLD, yielding consistent performance gains across
models ranging from 0.5B to 3B parameters.",2025-05-24,"Wenlong Deng, Yi Ren, Muchen Li, Danica J. Sutherland, Xiaoxiao Li, Christos Thrampoulidis",http://arxiv.org/pdf/2505.18830v1,cs.LG
Improved Regret and Contextual Linear Extension for Pandora's Box and Prophet Inequality,"We study the Pandora's Box problem in an online learning setting with
semi-bandit feedback. In each round, the learner sequentially pays to open up
to $n$ boxes with unknown reward distributions, observes rewards upon opening,
and decides when to stop. The utility of the learner is the maximum observed
reward minus the cumulative cost of opened boxes, and the goal is to minimize
regret defined as the gap between the cumulative expected utility and that of
the optimal policy. We propose a new algorithm that achieves
$\widetilde{O}(\sqrt{nT})$ regret after $T$ rounds, which improves the
$\widetilde{O}(n\sqrt{T})$ bound of Agarwal et al. [2024] and matches the known
lower bound up to logarithmic factors. To better capture real-life
applications, we then extend our results to a natural but challenging
contextual linear setting, where each box's expected reward is linear in some
known but time-varying $d$-dimensional context and the noise distribution is
fixed over time. We design an algorithm that learns both the linear function
and the noise distributions, achieving $\widetilde{O}(nd\sqrt{T})$ regret.
Finally, we show that our techniques also apply to the online Prophet
Inequality problem, where the learner must decide immediately whether or not to
accept a revealed reward. In both non-contextual and contextual settings, our
approach achieves similar improvements and regret bounds.",2025-05-24,"Junyan Liu, Ziyun Chen, Kun Wang, Haipeng Luo, Lillian J. Ratliff",http://arxiv.org/pdf/2505.18828v1,cs.LG
How to build a consistency model: Learning flow maps via self-distillation,"Building on the framework proposed in Boffi et al. (2024), we present a
systematic approach for learning flow maps associated with flow and diffusion
models. Flow map-based models, commonly known as consistency models, encompass
recent efforts to improve the efficiency of generative models based on
solutions to differential equations. By exploiting a relationship between the
velocity field underlying a continuous-time flow and the instantaneous rate of
change of the flow map, we show how to convert existing distillation schemes
into direct training algorithms via self-distillation, eliminating the need for
pre-trained models. We empirically evaluate several instantiations of our
framework, finding that high-dimensional tasks like image synthesis benefit
from objective functions that avoid temporal and spatial derivatives of the
flow map, while lower-dimensional tasks can benefit from objectives
incorporating higher-order derivatives to capture sharp features.",2025-05-24,"Nicholas M. Boffi, Michael S. Albergo, Eric Vanden-Eijnden",http://arxiv.org/pdf/2505.18825v1,cs.LG
Governing Equation Discovery from Data Based on Differential Invariants,"The explicit governing equation is one of the simplest and most intuitive
forms for characterizing physical laws. However, directly discovering partial
differential equations (PDEs) from data poses significant challenges, primarily
in determining relevant terms from a vast search space. Symmetry, as a crucial
prior knowledge in scientific fields, has been widely applied in tasks such as
designing equivariant networks and guiding neural PDE solvers. In this paper,
we propose a pipeline for governing equation discovery based on differential
invariants, which can losslessly reduce the search space of existing equation
discovery methods while strictly adhering to symmetry. Specifically, we compute
the set of differential invariants corresponding to the infinitesimal
generators of the symmetry group and select them as the relevant terms for
equation discovery. Taking DI-SINDy (SINDy based on Differential Invariants) as
an example, we demonstrate that its success rate and accuracy in PDE discovery
surpass those of other symmetry-informed governing equation discovery methods
across a series of PDEs.",2025-05-24,"Lexiang Hu, Yikang Li, Zhouchen Lin",http://arxiv.org/pdf/2505.18798v1,cs.LG
Leveraging Per-Instance Privacy for Machine Unlearning,"We present a principled, per-instance approach to quantifying the difficulty
of unlearning via fine-tuning. We begin by sharpening an analysis of noisy
gradient descent for unlearning (Chien et al., 2024), obtaining a better
utility-unlearning tradeoff by replacing worst-case privacy loss bounds with
per-instance privacy losses (Thudi et al., 2024), each of which bounds the
(Renyi) divergence to retraining without an individual data point. To
demonstrate the practical applicability of our theory, we present empirical
results showing that our theoretical predictions are born out both for
Stochastic Gradient Langevin Dynamics (SGLD) as well as for standard
fine-tuning without explicit noise. We further demonstrate that per-instance
privacy losses correlate well with several existing data difficulty metrics,
while also identifying harder groups of data points, and introduce novel
evaluation methods based on loss barriers. All together, our findings provide a
foundation for more efficient and adaptive unlearning strategies tailored to
the unique properties of individual data points.",2025-05-24,"Nazanin Mohammadi Sepahvand, Anvith Thudi, Berivan Isik, Ashmita Bhattacharyya, Nicolas Papernot, Eleni Triantafillou, Daniel M. Roy, Gintare Karolina Dziugaite",http://arxiv.org/pdf/2505.18786v1,cs.LG
A physics-guided smoothing method for material modeling with digital image correlation (DIC) measurements,"In this work, we present a novel approach to process the DIC measurements of
multiple biaxial stretching protocols. In particular, we develop a
optimization-based approach, which calculates the smoothed nodal displacements
using a moving least-squares algorithm subject to positive strain constraints.
As such, physically consistent displacement and strain fields are obtained.
Then, we further deploy a data-driven workflow to heterogeneous material
modeling from these physically consistent DIC measurements, by estimating a
nonlocal constitutive law together with the material microstructure. To
demonstrate the applicability of our approach, we apply it in learning a
material model and fiber orientation field from DIC measurements of a porcine
tricuspid valve anterior leaflet. Our results demonstrate that the proposed DIC
data processing approach can significantly improve the accuracy of modeling
biological materials.",2025-05-24,"Jihong Wang, Chung-Hao Lee, William Richardson, Yue Yu",http://arxiv.org/pdf/2505.18784v1,cs.LG
Soft Weighted Machine Unlearning,"Machine unlearning, as a post-hoc processing technique, has gained widespread
adoption in addressing challenges like bias mitigation and robustness
enhancement, colloquially, machine unlearning for fairness and robustness.
However, existing non-privacy unlearning-based solutions persist in using
binary data removal framework designed for privacy-driven motivation, leading
to significant information loss, a phenomenon known as over-unlearning. While
over-unlearning has been largely described in many studies as primarily causing
utility degradation, we investigate its fundamental causes and provide deeper
insights in this work through counterfactual leave-one-out analysis. In this
paper, we introduce a weighted influence function that assigns tailored weights
to each sample by solving a convex quadratic programming problem analytically.
Building on this, we propose a soft-weighted framework enabling fine-grained
model adjustments to address the over-unlearning challenge. We demonstrate that
the proposed soft-weighted scheme is versatile and can be seamlessly integrated
into most existing unlearning algorithms. Extensive experiments show that in
fairness- and robustness-driven tasks, the soft-weighted scheme significantly
outperforms hard-weighted schemes in fairness/robustness metrics and alleviates
the decline in utility metric, thereby enhancing machine unlearning algorithm
as an effective correction solution.",2025-05-24,"Xinbao Qiao, Ningning Ding, Yushi Cheng, Meng Zhang",http://arxiv.org/pdf/2505.18783v1,cs.LG
Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains,"The very challenging task of learning solution operators of PDEs on arbitrary
domains accurately and efficiently is of vital importance to engineering and
industrial simulations. Despite the existence of many operator learning
algorithms to approximate such PDEs, we find that accurate models are not
necessarily computationally efficient and vice versa. We address this issue by
proposing a geometry aware operator transformer (GAOT) for learning PDEs on
arbitrary domains. GAOT combines novel multiscale attentional graph neural
operator encoders and decoders, together with geometry embeddings and (vision)
transformer processors to accurately map information about the domain and the
inputs into a robust approximation of the PDE solution. Multiple innovations in
the implementation of GAOT also ensure computational efficiency and
scalability. We demonstrate this significant gain in both accuracy and
efficiency of GAOT over several baselines on a large number of learning tasks
from a diverse set of PDEs, including achieving state of the art performance on
a large scale three-dimensional industrial CFD dataset.",2025-05-24,"Shizheng Wen, Arsh Kumbhat, Levi Lingsch, Sepehr Mousavi, Praveen Chandrashekar, Siddhartha Mishra",http://arxiv.org/pdf/2505.18781v1,cs.LG
One Policy but Many Worlds: A Scalable Unified Policy for Versatile Humanoid Locomotion,"Humanoid locomotion faces a critical scalability challenge: traditional
reinforcement learning (RL) methods require task-specific rewards and struggle
to leverage growing datasets, even as more training terrains are introduced. We
propose DreamPolicy, a unified framework that enables a single policy to master
diverse terrains and generalize zero-shot to unseen scenarios by systematically
integrating offline data and diffusion-driven motion synthesis. At its core,
DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions
synthesized through an autoregressive terrain-aware diffusion planner curated
by aggregating rollouts from specialized policies across various distinct
terrains. Unlike human motion datasets requiring laborious retargeting, our
data directly captures humanoid kinematics, enabling the diffusion planner to
synthesize ""dreamed"" trajectories that encode terrain-specific physical
constraints. These trajectories act as dynamic objectives for our
HMI-conditioned policy, bypassing manual reward engineering and enabling
cross-terrain generalization. DreamPolicy addresses the scalability limitations
of prior methods: while traditional RL fails to exploit growing datasets, our
framework scales seamlessly with more offline data. As the dataset expands, the
diffusion prior learns richer locomotion skills, which the policy leverages to
master new terrains without retraining. Experiments demonstrate that
DreamPolicy achieves average 90% success rates in training environments and an
average of 20% higher success on unseen terrains than the prevalent method. It
also generalizes to perturbed and composite scenarios where prior approaches
collapse. By unifying offline data, diffusion-based trajectory synthesis, and
policy optimization, DreamPolicy overcomes the ""one task, one policy""
bottleneck, establishing a paradigm for scalable, data-driven humanoid control.",2025-05-24,"Yahao Fan, Tianxiang Gui, Kaiyang Ji, Shutong Ding, Chixuan Zhang, Jiayuan Gu, Jingyi Yu, Jingya Wang, Ye Shi",http://arxiv.org/pdf/2505.18780v1,cs.LG
HD-PiSSA: High-Rank Distributed Orthogonal Adaptation,"Existing parameter-efficient fine-tuning (PEFT) methods for large language
models (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank
subspaces, limiting their expressiveness and leading to suboptimal performance
on complex tasks. To address this, we introduce High-rank Distributed PiSSA
(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters
across different devices and aggregates their delta updates collectively on W
for fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical
adapters across all devices, HD-PiSSA assigns different principal components of
the pre-trained weights to each GPU, significantly expanding the range of
update directions. This results in over 16x higher effective updated ranks than
data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device
adapter rank. Empirically, we evaluate HD-PiSSA across various challenging
downstream tasks, including mathematics, code generation, and multi-task
learning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0
absolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12
benchmarks, demonstrating its benefits from the extra optimization flexibility.",2025-05-24,"Yiding Wang, Fauxu meng, Xuefeng Zhang, Fan Jiang, Pingzhi Tang, Muhan Zhang",http://arxiv.org/pdf/2505.18777v1,cs.LG
Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models,"State-of-the-art membership inference attacks (MIAs) typically require
training many reference models, making it difficult to scale these attacks to
large pre-trained language models (LLMs). As a result, prior research has
either relied on weaker attacks that avoid training reference models (e.g.,
fine-tuning attacks), or on stronger attacks applied to small-scale models and
datasets. However, weaker attacks have been shown to be brittle - achieving
close-to-arbitrary success - and insights from strong attacks in simplified
settings do not translate to today's LLMs. These challenges have prompted an
important question: are the limitations observed in prior work due to attack
design choices, or are MIAs fundamentally ineffective on LLMs? We address this
question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures
ranging from 10M to 1B parameters, training reference models on over 20B tokens
from the C4 dataset. Our results advance the understanding of MIAs on LLMs in
three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their
effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;
and, (3) the relationship between MIA success and related privacy metrics is
not as straightforward as prior work has suggested.",2025-05-24,"Jamie Hayes, Ilia Shumailov, Christopher A. Choquette-Choo, Matthew Jagielski, George Kaissis, Katherine Lee, Milad Nasr, Sahra Ghalebikesabi, Niloofar Mireshghallah, Meenatchi Sundaram Mutu Selva Annamalai, Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye, Franziska Boenisch, Adam Dziedzic, A. Feder Cooper",http://arxiv.org/pdf/2505.18773v1,cs.LG
CageNet: A Meta-Framework for Learning on Wild Meshes,"Learning on triangle meshes has recently proven to be instrumental to a
myriad of tasks, from shape classification, to segmentation, to deformation and
animation, to mention just a few. While some of these applications are tackled
through neural network architectures which are tailored to the application at
hand, many others use generic frameworks for triangle meshes where the only
customization required is the modification of the input features and the loss
function. Our goal in this paper is to broaden the applicability of these
generic frameworks to ""wild"", i.e. meshes in-the-wild which often have multiple
components, non-manifold elements, disrupted connectivity, or a combination of
these. We propose a configurable meta-framework based on the concept of caged
geometry: Given a mesh, a cage is a single component manifold triangle mesh
that envelopes it closely. Generalized barycentric coordinates map between
functions on the cage, and functions on the mesh, allowing us to learn and test
on a variety of data, in different applications. We demonstrate this concept by
learning segmentation and skinning weights on difficult data, achieving better
performance to state of the art techniques on wild meshes.",2025-05-24,"Michal Edelstein, Hsueh-Ti Derek Liu, Mirela Ben-Chen",http://arxiv.org/pdf/2505.18772v1,cs.LG
Dual-Path Stable Soft Prompt Generation for Domain Generalization,"Domain generalization (DG) aims to learn a model using data from one or
multiple related but distinct source domains that can generalize well to unseen
out-of-distribution target domains. Inspired by the success of large
pre-trained vision-language models (VLMs), prompt tuning has emerged as an
effective generalization strategy. However, it often struggles to capture
domain-specific features due to its reliance on manually or fixed prompt
inputs. Recently, some prompt generation methods have addressed this limitation
by dynamically generating instance-specific and domain-specific prompts for
each input, enriching domain information and demonstrating potential for
enhanced generalization. Through further investigation, we identify a notable
issue in existing prompt generation methods: the same input often yields
significantly different and suboptimal prompts across different random seeds, a
phenomenon we term Prompt Variability. To address this, we introduce negative
learning into the prompt generation process and propose Dual-Path Stable Soft
Prompt Generation (DPSPG), a transformer-based framework designed to improve
both the stability and generalization of prompts. Specifically, DPSPG
incorporates a complementary prompt generator to produce negative prompts,
thereby reducing the risk of introducing misleading information. Both
theoretical and empirical analyses demonstrate that negative learning leads to
more robust and effective prompts by increasing the effective margin and
reducing the upper bound of the gradient norm. Extensive experiments on five DG
benchmark datasets show that DPSPG consistently outperforms state-of-the-art
methods while maintaining prompt stability.",2025-05-24,"Yuedi Zhang, Shuanghao Bai, Wanqi Zhou, Zhirong Luan, Badong Chen",http://arxiv.org/pdf/2505.18770v1,cs.LG
Multiple Wasserstein Gradient Descent Algorithm for Multi-Objective Distributional Optimization,"We address the optimization problem of simultaneously minimizing multiple
objective functionals over a family of probability distributions. This type of
Multi-Objective Distributional Optimization commonly arises in machine learning
and statistics, with applications in areas such as multiple target sampling,
multi-task learning, and multi-objective generative modeling. To solve this
problem, we propose an iterative particle-based algorithm, which we call
Muliple Wasserstein Gradient Descent (MWGraD), which constructs a flow of
intermediate empirical distributions, each being represented by a set of
particles, which gradually minimize the multiple objective functionals
simultaneously. Specifically, MWGraD consists of two key steps at each
iteration. First, it estimates the Wasserstein gradient for each objective
functional based on the current particles. Then, it aggregates these gradients
into a single Wasserstein gradient using dynamically adjusted weights and
updates the particles accordingly. In addition, we provide theoretical analysis
and present experimental results on both synthetic and real-world datasets,
demonstrating the effectiveness of MWGraD.",2025-05-24,"Dai Hai Nguyen, Hiroshi Mamitsuka, Atsuyoshi Nakamura",http://arxiv.org/pdf/2505.18765v1,cs.LG
GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning,"Recent advances in reinforcement learning (RL) have demonstrated the powerful
exploration capabilities and multimodality of generative diffusion-based
policies. While substantial progress has been made in offline RL and off-policy
RL settings, integrating diffusion policies into on-policy frameworks like PPO
remains underexplored. This gap is particularly significant given the
widespread use of large-scale parallel GPU-accelerated simulators, such as
IsaacLab, which are optimized for on-policy RL algorithms and enable rapid
training of complex robotic tasks. A key challenge lies in computing
state-action log-likelihoods under diffusion policies, which is straightforward
for Gaussian policies but intractable for flow-based models due to irreversible
forward-reverse processes and discretization errors (e.g., Euler-Maruyama
approximations). To bridge this gap, we propose GenPO, a generative policy
optimization framework that leverages exact diffusion inversion to construct
invertible action mappings. GenPO introduces a novel doubled dummy action
mechanism that enables invertibility via alternating updates, resolving
log-likelihood computation barriers. Furthermore, we also use the action
log-likelihood for unbiased entropy and KL divergence estimation, enabling
KL-adaptive learning rates and entropy regularization in on-policy updates.
Extensive experiments on eight IsaacLab benchmarks, including legged locomotion
(Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow
Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate
GenPO's superiority over existing RL baselines. Notably, GenPO is the first
method to successfully integrate diffusion policies into on-policy RL,
unlocking their potential for large-scale parallelized training and real-world
robotic deployment.",2025-05-24,"Shutong Ding, Ke Hu, Shan Zhong, Haoyang Luo, Weinan Zhang, Jingya Wang, Jun Wang, Ye Shi",http://arxiv.org/pdf/2505.18763v1,cs.LG
How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark,"We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic
benchmark to evaluate Large Language Models' (LLMs) reasoning robustness
against systematically controlled irrelevant context (IC). GSM-DC constructs
symbolic reasoning graphs with precise distractor injections, enabling
rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are
significantly sensitive to IC, affecting both reasoning path selection and
arithmetic accuracy. Additionally, training models with strong distractors
improves performance in both in-distribution and out-of-distribution scenarios.
We further propose a stepwise tree search guided by a process reward model,
which notably enhances robustness in out-of-distribution conditions.",2025-05-24,"Minglai Yang, Ethan Huang, Liang Zhang, Mihai Surdeanu, William Wang, Liangming Pan",http://arxiv.org/pdf/2505.18761v1,cs.LG
The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation,"Data-centric distillation, including data augmentation, selection, and
mixing, offers a promising path to creating smaller, more efficient student
Large Language Models (LLMs) that retain strong reasoning abilities. However,
there still lacks a comprehensive benchmark to systematically assess the effect
of each distillation approach. This paper introduces DC-CoT, the first
data-centric benchmark that investigates data manipulation in chain-of-thought
(CoT) distillation from method, model and data perspectives. Utilizing various
teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student
architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of
these data manipulations on student model performance across multiple reasoning
datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)
generalization, and cross-domain transfer. Our findings aim to provide
actionable insights and establish best practices for optimizing CoT
distillation through data-centric techniques, ultimately facilitating the
development of more accessible and capable reasoning models. The dataset can be
found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is
shared in https://anonymous.4open.science/r/DC-COT-FF4C/.",2025-05-24,"Ruichen Zhang, Rana Muhammad Shahroz Khan, Zhen Tan, Dawei Li, Song Wang, Tianlong Chen",http://arxiv.org/pdf/2505.18759v1,cs.LG
Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding,"The ever-growing size of neural networks poses serious challenges on
resource-constrained devices, such as embedded sensors. Compression algorithms
that reduce their size can mitigate these problems, provided that model
performance stays close to the original. We propose a novel post-training
compression framework that combines rate-aware quantization with entropy coding
by (1) extending the well-known layer-wise loss by a quadratic rate estimation,
and (2) providing locally exact solutions to this modified objective following
the Optimal Brain Surgeon (OBS) method. Our method allows for very fast
decoding and is compatible with arbitrary quantization grids. We verify our
results empirically by testing on various computer-vision networks, achieving a
20-40\% decrease in bit rate at the same performance as the popular compression
algorithm NNCodec. Our code is available at https://github.com/Conzel/cerwu.",2025-05-24,"Alexander Conzelmann, Robert Bamler",http://arxiv.org/pdf/2505.18758v1,cs.LG
Smart Energy Guardian: A Hybrid Deep Learning Model for Detecting Fraudulent PV Generation,"With the proliferation of smart grids, smart cities face growing challenges
due to cyber-attacks and sophisticated electricity theft behaviors,
particularly in residential photovoltaic (PV) generation systems. Traditional
Electricity Theft Detection (ETD) methods often struggle to capture complex
temporal dependencies and integrating multi-source data, limiting their
effectiveness. In this work, we propose an efficient ETD method that accurately
identifies fraudulent behaviors in residential PV generation, thus ensuring the
supply-demand balance in smart cities. Our hybrid deep learning model,
combining multi-scale Convolutional Neural Network (CNN), Long Short-Term
Memory (LSTM), and Transformer, excels in capturing both short-term and
long-term temporal dependencies. Additionally, we introduce a data embedding
technique that seamlessly integrates time-series data with discrete temperature
variables, enhancing detection robustness. Extensive simulation experiments
using real-world data validate the effectiveness of our approach, demonstrating
significant improvements in the accuracy of detecting sophisticated energy
theft activities, thereby contributing to the stability and fairness of energy
systems in smart cities.",2025-05-24,"Xiaolu Chen, Chenghao Huang, Yanru Zhang, Hao Wang",http://arxiv.org/pdf/2505.18755v1,cs.LG
Season-Independent PV Disaggregation Using Multi-Scale Net Load Temporal Feature Extraction and Weather Factor Fusion,"With the advancement of energy Internet and energy system integration, the
increasing adoption of distributed photovoltaic (PV) systems presents new
challenges on smart monitoring and measurement for utility companies,
particularly in separating PV generation from net electricity load. Existing
methods struggle with feature extraction from net load and capturing the
relevance between weather factors. This paper proposes a PV disaggregation
method that integrates Hierarchical Interpolation (HI) and multi-head
self-attention mechanisms. By using HI to extract net load features and
multi-head self-attention to capture the complex dependencies between weather
factors, the method achieves precise PV generation predictions. Simulation
experiments demonstrate the effectiveness of the proposed method in real-world
data, supporting improved monitoring and management of distributed energy
systems.",2025-05-24,"Xiaolu Chen, Chenghao Huang, Yanru Zhang, Hao Wang",http://arxiv.org/pdf/2505.18747v1,cs.LG
C3R: Channel Conditioned Cell Representations for unified evaluation in microscopy imaging,"Immunohistochemical (IHC) images reveal detailed information about structures
and functions at the subcellular level. However, unlike natural images, IHC
datasets pose challenges for deep learning models due to their inconsistencies
in channel count and configuration, stemming from varying staining protocols
across laboratories and studies. Existing approaches build channel-adaptive
models, which unfortunately fail to support out-of-distribution (OOD)
evaluation across IHC datasets and cannot be applied in a true zero-shot
setting with mismatched channel counts. To address this, we introduce a
structured view of cellular image channels by grouping them into either context
or concept, where we treat the context channels as a reference to the concept
channels in the image. We leverage this context-concept principle to develop
Channel Conditioned Cell Representations (C3R), a framework designed for
unified evaluation on in-distribution (ID) and OOD datasets. C3R is a two-fold
framework comprising a channel-adaptive encoder architecture and a masked
knowledge distillation training strategy, both built around the context-concept
principle. We find that C3R outperforms existing benchmarks on both ID and OOD
tasks, while a trivial implementation of our core idea also outperforms the
channel-adaptive methods reported on the CHAMMI benchmark. Our method opens a
new pathway for cross-dataset generalization between IHC datasets, without
requiring dataset-specific adaptation or retraining.",2025-05-24,"Umar Marikkar, Syed Sameed Husain, Muhammad Awais, Sara Atito",http://arxiv.org/pdf/2505.18745v1,cs.LG
AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping,"Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA
faces an inherent low-rank bottleneck: narrowing its performance gap with full
finetuning requires increasing the rank of its parameter matrix, resulting in
significant parameter overhead. Recent linear LoRA variants have attempted to
enhance expressiveness by introducing additional linear mappings; however,
their composition remains inherently linear and fails to fundamentally improve
LoRA's representational capacity. To address this limitation, we propose
AuroRA, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear
projectors to capture fixed and learnable nonlinearities. This combination
forms an MLP-like structure with a compressed rank, enabling flexible and
precise approximation of diverse target functions while theoretically
guaranteeing lower approximation errors and bounded gradients. Extensive
experiments on 22 datasets and 6 pretrained models demonstrate that AuroRA: (I)
not only matches or surpasses full fine-tuning performance with only 6.18% ~
25% of LoRA's parameters but also (II) outperforms state-of-the-art PEFT
methods by up to 10.88% in both NLP and CV tasks, and (III) exhibits robust
performance across various rank configurations.",2025-05-24,"Haonan Dong, Wenhao Zhu, Guojie Song, Liang Wang",http://arxiv.org/pdf/2505.18738v1,cs.LG
MADCAT: Combating Malware Detection Under Concept Drift with Test-Time Adaptation,"We present MADCAT, a self-supervised approach designed to address the concept
drift problem in malware detection. MADCAT employs an encoder-decoder
architecture and works by test-time training of the encoder on a small,
balanced subset of the test-time data using a self-supervised objective. During
test-time training, the model learns features that are useful for detecting
both previously seen (old) data and newly arriving samples. We demonstrate the
effectiveness of MADCAT in continuous Android malware detection settings.
MADCAT consistently outperforms baseline methods in detection performance at
test time. We also show the synergy between MADCAT and prior approaches in
addressing concept drift in malware detection",2025-05-24,"Eunjin Roh, Yigitcan Kaya, Christopher Kruegel, Giovanni Vigna, Sanghyun Hong",http://arxiv.org/pdf/2505.18734v1,cs.LG
Reward-Driven Interaction: Enhancing Proactive Dialogue Agents through User Satisfaction Prediction,"Reward-driven proactive dialogue agents require precise estimation of user
satisfaction as an intrinsic reward signal to determine optimal interaction
strategies. Specifically, this framework triggers clarification questions when
detecting potential user dissatisfaction during interactions in the industrial
dialogue system. Traditional works typically rely on training a neural network
model based on weak labels which are generated by a simple model trained on
user actions after current turn. However, existing methods suffer from two
critical limitations in real-world scenarios: (1) Noisy Reward Supervision,
dependence on weak labels derived from post-hoc user actions introduces bias,
particularly failing to capture satisfaction signals in ASR-error-induced
utterances; (2) Long-Tail Feedback Sparsity, the power-law distribution of user
queries causes reward prediction accuracy to drop in low-frequency domains. The
noise in the weak labels and a power-law distribution of user utterances
results in that the model is hard to learn good representation of user
utterances and sessions. To address these limitations, we propose two auxiliary
tasks to improve the representation learning of user utterances and sessions
that enhance user satisfaction prediction. The first one is a contrastive
self-supervised learning task, which helps the model learn the representation
of rare user utterances and identify ASR errors. The second one is a
domain-intent classification task, which aids the model in learning the
representation of user sessions from long-tailed domains and improving the
model's performance on such domains. The proposed method is evaluated on
DuerOS, demonstrating significant improvements in the accuracy of error
recognition on rare user utterances and long-tailed domains.",2025-05-24,"Wei Shen, Xiaonan He, Chuheng Zhang, Xuyun Zhang, Xiaolong Xu, Wanchun Dou",http://arxiv.org/pdf/2505.18731v1,cs.LG
Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling,"The recent success of State-Space Models (SSMs) in sequence modeling has
motivated their adaptation to graph learning, giving rise to Graph State-Space
Models (GSSMs). However, existing GSSMs operate by applying SSM modules to
sequences extracted from graphs, often compromising core properties such as
permutation equivariance, message-passing compatibility, and computational
efficiency. In this paper, we introduce a new perspective by embedding the key
principles of modern SSM computation directly into the Message-Passing Neural
Network framework, resulting in a unified methodology for both static and
temporal graphs. Our approach, MP-SSM, enables efficient,
permutation-equivariant, and long-range information propagation while
preserving the architectural simplicity of message passing. Crucially, MP-SSM
enables an exact sensitivity analysis, which we use to theoretically
characterize information flow and evaluate issues like vanishing gradients and
over-squashing in the deep regime. Furthermore, our design choices allow for a
highly optimized parallel implementation akin to modern SSMs. We validate
MP-SSM across a wide range of tasks, including node classification, graph
property prediction, long-range benchmarks, and spatiotemporal forecasting,
demonstrating both its versatility and strong empirical performance.",2025-05-24,"Andrea Ceni, Alessio Gravina, Claudio Gallicchio, Davide Bacciu, Carola-Bibiane Schonlieb, Moshe Eliasof",http://arxiv.org/pdf/2505.18728v1,cs.LG
Audio Geolocation: A Natural Sounds Benchmark,"Can we determine someone's geographic location purely from the sounds they
hear? Are acoustic signals enough to localize within a country, state, or even
city? We tackle the challenge of global-scale audio geolocation, formalize the
problem, and conduct an in-depth analysis with wildlife audio from the
iNatSounds dataset. Adopting a vision-inspired approach, we convert audio
recordings to spectrograms and benchmark existing image geolocation techniques.
We hypothesize that species vocalizations offer strong geolocation cues due to
their defined geographic ranges and propose an approach that integrates species
range prediction with retrieval-based geolocation. We further evaluate whether
geolocation improves when analyzing species-rich recordings or when aggregating
across spatiotemporal neighborhoods. Finally, we introduce case studies from
movies to explore multimodal geolocation using both audio and visual content.
Our work highlights the advantages of integrating audio and visual cues, and
sets the stage for future research in audio geolocation.",2025-05-24,"Mustafa Chasmai, Wuao Liu, Subhransu Maji, Grant Van Horn",http://arxiv.org/pdf/2505.18726v1,cs.LG
LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning,"Quantization and fine-tuning are crucial for deploying large language models
(LLMs) on resource-constrained edge devices. However, fine-tuning quantized
models presents significant challenges, primarily stemming from: First, the
mismatch in data types between the low-precision quantized weights (e.g.,
4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch
limits the computational efficiency advantage offered by quantized weights
during inference. Second, potential accuracy degradation when merging these
high-precision adaptation weights into the low-precision quantized weights, as
the adaptation weights often necessitate approximation or truncation. Third, as
far as we know, no existing methods support the lossless merging of adaptation
while adjusting all quantized weights. To address these challenges, we
introduce lossless ternary adaptation for quantization-aware fine-tuning
(LoTA-QAF). This is a novel fine-tuning method specifically designed for
quantized LLMs, enabling the lossless merging of ternary adaptation weights
into quantized weights and the adjustment of all quantized weights. LoTA-QAF
operates through a combination of: i) A custom-designed ternary adaptation (TA)
that aligns ternary weights with the quantization grid and uses these ternary
weights to adjust quantized weights. ii) A TA-based mechanism that enables the
lossless merging of adaptation weights. iii) Ternary signed gradient descent
(t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and
Qwen-2.5 model families and validate its effectiveness on several downstream
tasks. On the MMLU benchmark, our method effectively recovers performance for
quantized models, surpassing 16-bit LoRA by up to 5.14\%. For task-specific
fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still
outperforms other methods.",2025-05-24,"Junyu Chen, Junzhuo Li, Zhen Peng, Wenjie Wang, Yuxiang Ren, Long Shi, Xuming Hu",http://arxiv.org/pdf/2505.18724v1,cs.LG
Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization,"Direct Preference Optimization (DPO) has emerged as a promising framework for
aligning Large Language Models (LLMs) with human preferences by directly
optimizing the log-likelihood difference between chosen and rejected responses.
However, existing methods assign equal importance to all tokens in the
response, while humans focus on more meaningful parts. This leads to suboptimal
preference optimization, as irrelevant or noisy tokens disproportionately
influence DPO loss. To address this limitation, we propose \textbf{O}ptimal
\textbf{T}ransport-based token weighting scheme for enhancing direct
\textbf{P}reference \textbf{O}ptimization (OTPO). By emphasizing semantically
meaningful token pairs and de-emphasizing less relevant ones, our method
introduces a context-aware token weighting scheme that yields a more
contrastive reward difference estimate. This adaptive weighting enhances reward
stability, improves interpretability, and ensures that preference optimization
focuses on meaningful differences between responses. Extensive experiments have
validated OTPO's effectiveness in improving instruction-following ability
across various settings\footnote{Code is available at
https://github.com/Mimasss2/OTPO.}.",2025-05-24,"Meng Li, Guangda Huzhang, Haibo Zhang, Xiting Wang, Anxiang Zeng",http://arxiv.org/pdf/2505.18720v1,cs.LG
Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer,"Foundation models and their checkpoints have significantly advanced deep
learning, boosting performance across various applications. However, fine-tuned
models often struggle outside their specific domains and exhibit considerable
redundancy. Recent studies suggest that combining a pruned fine-tuned model
with the original pre-trained model can mitigate forgetting, reduce
interference when merging model parameters across tasks, and improve
compression efficiency. In this context, developing an effective pruning
strategy for fine-tuned models is crucial. Leveraging the advantages of the
task vector mechanism, we preprocess fine-tuned models by calculating the
differences between them and the original model. Recognizing that different
task vector subspaces contribute variably to model performance, we introduce a
novel method called Neural Parameter Search (NPS-Pruning) for slimming down
fine-tuned models. This method enhances pruning efficiency by searching through
neural parameters of task vectors within low-rank subspaces. Our method has
three key applications: enhancing knowledge transfer through pairwise model
interpolation, facilitating effective knowledge fusion via model merging, and
enabling the deployment of compressed models that retain near-original
performance while significantly reducing storage costs. Extensive experiments
across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness
and robustness of our approach, resulting in substantial performance gains. The
code is publicly available at: https://github.com/duguodong7/NPS-Pruning.",2025-05-24,"Guodong Du, Zitao Fang, Jing Li, Junlin Li, Runhua Jiang, Shuyang Yu, Yifei Guo, Yangneng Chen, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Honghai Liu, Min Zhang",http://arxiv.org/pdf/2505.18713v1,cs.LG
Steering LLM Reasoning Through Bias-Only Adaptation,"Recent work on reasoning-oriented language models, exemplified by o1-like
systems, suggests that reinforcement-learning (RL) finetuning does not create
new capabilities but instead strengthens reasoning patterns already latent in
the pretrained network. We test this claim by training steering vectors:
layer-wise biases that additively amplify selected hidden features while
leaving all original weights unchanged. Experiments on four base models across
the GSM8K and MATH benchmarks show that steering vectors recover, and in
several cases exceed, the accuracy of fully-tuned counterparts. This result
supports the view that the required reasoning skills pre-exist in the base
model. Further, logit-lens analysis reveals that the trained vectors
consistently boost token groups linked to structured languages and logical
connectors, providing an interpretable account that aligns with the demands of
quantitative reasoning tasks.",2025-05-24,"Viacheslav Sinii, Alexey Gorbatovski, Artem Cherepanov, Boris Shaposhnikov, Nikita Balagansky, Daniil Gavrilov",http://arxiv.org/pdf/2505.18706v1,cs.LG
"MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention","Transformers have achieved state-of-the-art performance across various tasks,
but suffer from a notable quadratic complexity in sequence length due to the
attention mechanism. In this work, we propose MonarchAttention -- a novel
approach to sub-quadratic attention approximation via Monarch matrices, an
expressive class of structured matrices. Based on the variational form of
softmax, we describe an efficient optimization-based algorithm to compute an
approximate projection of softmax attention onto the class of Monarch matrices
with $\Theta(N\sqrt{N} d)$ computational complexity and $\Theta(Nd)$ memory/IO
complexity. Unlike previous approaches, MonarchAttention is both (1)
transferable, yielding minimal performance loss with no additional training,
even when replacing every attention layer of the transformer, and (2)
hardware-efficient, utilizing the highest-throughput tensor core units on
modern GPUs. With optimized kernels, MonarchAttention achieves substantial
speed-ups in wall-time over FlashAttention-2: $1.4\times$ for shorter sequences
$(N=256)$, $4.5\times$ for medium-length sequences $(N=4K)$, and $8.2\times$
for longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention
on diverse tasks and architectures in vision and language problems, showing
that it flexibly and accurately approximates softmax attention in a variety of
contexts. Our code is available at
https://github.com/cjyaras/monarch-attention.",2025-05-24,"Can Yaras, Alec S. Xu, Pierre Abillama, Changwoo Lee, Laura Balzano",http://arxiv.org/pdf/2505.18698v1,cs.LG
Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study,"Nowadays, real-world data, including graph-structure data, often arrives in a
streaming manner, which means that learning systems need to continuously
acquire new knowledge without forgetting previously learned information.
Although substantial existing works attempt to address catastrophic forgetting
in graph machine learning, they are all based on training from scratch with
streaming data. With the rise of pretrained models, an increasing number of
studies have leveraged their strong generalization ability for continual
learning. Therefore, in this work, we attempt to answer whether large language
models (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning
(GCL). We first point out that current experimental setups for GCL have
significant flaws, as the evaluation stage may lead to task ID leakage. Then,
we evaluate the performance of LLMs in more realistic scenarios and find that
even minor modifications can lead to outstanding results. Finally, based on
extensive experiments, we propose a simple-yet-effective method, Simple Graph
Continual Learning (SimGCL), that surpasses the previous state-of-the-art
GNN-based baseline by around 20% under the rehearsal-free constraint. To
facilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL
for training and evaluating existing GCL methods. The code is available at:
https://github.com/ZhixunLEE/LLM4GCL.",2025-05-24,"Ziyang Cheng, Zhixun Li, Yuhan Li, Yixin Song, Kangyi Zhao, Dawei Cheng, Jia Li, Jeffrey Xu Yu",http://arxiv.org/pdf/2505.18697v1,cs.LG
Simultaneous Optimization of Efficiency and Degradation in Tunable HTL-Free Perovskite Solar Cells with MWCNT-Integrated Back Contact Using a Machine Learning-Derived Polynomial Regressor,"Perovskite solar cells (PSCs) without a hole transport layer (HTL) offer a
cost-effective and stable alternative to conventional architectures, utilizing
only an absorber layer and an electron transport layer (ETL). This study
presents a machine learning (ML)-driven framework to optimize the efficiency
and stability of HTL-free PSCs by integrating experimental validation with
numerical simulations. Excellent agreement is achieved between a fabricated
device and its simulated counterpart at a molar fraction \( x = 68.7\% \) in
\(\mathrm{MAPb}_{1-x}\mathrm{Sb}_{2x/3}\mathrm{I}_3\), where MA is
methylammonium. A dataset of 1650 samples is generated by varying molar
fraction, absorber defect density, thickness, and ETL doping, with
corresponding efficiency and 50-hour degradation as targets. A fourth-degree
polynomial regressor (PR-4) shows the best performance, achieving RMSEs of
0.0179 and 0.0117, and \( R^2 \) scores of 1 and 0.999 for efficiency and
degradation, respectively. The derived model generalizes beyond the training
range and is used in an L-BFGS-B optimization algorithm with a weighted
objective function to maximize efficiency and minimize degradation. This
improves device efficiency from 13.7\% to 16.84\% and reduces degradation from
6.61\% to 2.39\% over 1000 hours. Finally, the dataset is labeled into superior
and inferior classes, and a multilayer perceptron (MLP) classifier achieves
100\% accuracy, successfully identifying optimal configurations.",2025-05-24,"Ihtesham Ibn Malek, Hafiz Imtiaz, Samia Subrina",http://arxiv.org/pdf/2505.18693v1,cs.LG
Large Language Models in the Task of Automatic Validation of Text Classifier Predictions,"Machine learning models for text classification are trained to predict a
class for a given text. To do this, training and validation samples must be
prepared: a set of texts is collected, and each text is assigned a class. These
classes are usually assigned by human annotators with different expertise
levels, depending on the specific classification task. Collecting such samples
from scratch is labor-intensive because it requires finding specialists and
compensating them for their work; moreover, the number of available specialists
is limited, and their productivity is constrained by human factors. While it
may not be too resource-intensive to collect samples once, the ongoing need to
retrain models (especially in incremental learning pipelines) to address data
drift (also called model drift) makes the data collection process crucial and
costly over the model's entire lifecycle. This paper proposes several
approaches to replace human annotators with Large Language Models (LLMs) to
test classifier predictions for correctness, helping ensure model quality and
support high-quality incremental learning.",2025-05-24,Aleksandr Tsymbalov,http://arxiv.org/pdf/2505.18688v1,cs.LG
Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?,"Representation intervention aims to locate and modify the representations
that encode the underlying concepts in Large Language Models (LLMs) to elicit
the aligned and expected behaviors. Despite the empirical success, it has never
been examined whether one could locate the faithful concepts for intervention.
In this work, we explore the question in safety alignment. If the interventions
are faithful, the intervened LLMs should erase the harmful concepts and be
robust to both in-distribution adversarial prompts and the out-of-distribution
(OOD) jailbreaks. While it is feasible to erase harmful concepts without
degrading the benign functionalities of LLMs in linear settings, we show that
it is infeasible in the general non-linear setting. To tackle the issue, we
propose Concept Concentration (COCA). Instead of identifying the faithful
locations to intervene, COCA refractors the training data with an explicit
reasoning process, which firstly identifies the potential unsafe concepts and
then decides the responses. Essentially, COCA simplifies the decision boundary
between harmful and benign representations, enabling more effective linear
erasure. Extensive experiments with multiple representation intervention
methods and model architectures demonstrate that COCA significantly reduces
both in-distribution and OOD jailbreak success rates, and meanwhile maintaining
strong performance on regular tasks such as math and code generation.",2025-05-24,"Hongzheng Yang, Yongqiang Chen, Zeyu Qin, Tongliang Liu, Chaowei Xiao, Kun Zhang, Bo Han",http://arxiv.org/pdf/2505.18672v1,cs.LG
Self-Supervised Evolution Operator Learning for High-Dimensional Dynamical Systems,"We introduce an encoder-only approach to learn the evolution operators of
large-scale non-linear dynamical systems, such as those describing complex
natural phenomena. Evolution operators are particularly well-suited for
analyzing systems that exhibit complex spatio-temporal patterns and have become
a key analytical tool across various scientific communities. As terabyte-scale
weather datasets and simulation tools capable of running millions of molecular
dynamics steps per day are becoming commodities, our approach provides an
effective tool to make sense of them from a data-driven perspective. The core
of it lies in a remarkable connection between self-supervised representation
learning methods and the recently established learning theory of evolution
operators. To show the usefulness of the proposed method, we test it across
multiple scientific domains: explaining the folding dynamics of small proteins,
the binding process of drug-like molecules in host sites, and autonomously
finding patterns in climate data. Code and data to reproduce the experiments
are made available open source.",2025-05-24,"Giacomo Turri, Luigi Bonati, Kai Zhu, Massimiliano Pontil, Pietro Novelli",http://arxiv.org/pdf/2505.18671v1,cs.LG
Memory-Efficient Super-Resolution of 3D Micro-CT Images Using Octree-Based GANs: Enhancing Resolution and Segmentation Accuracy,"We present a memory-efficient algorithm for significantly enhancing the
quality of segmented 3D micro-Computed Tomography (micro-CT) images of rocks
using a generative model. The proposed model achieves a 16x increase in
resolution and corrects inaccuracies in segmentation caused by the overlapping
X-ray attenuation in micro-CT measurements across different minerals. The
generative model employed is a 3D Octree-based convolutional Wasserstein
generative adversarial network with gradient penalty. To address the challenge
of high memory consumption inherent in standard 3D convolutional layers, we
implemented an Octree structure within the 3D progressive growing generator
model. This enabled the use of memory-efficient 3D Octree-based convolutional
layers. The approach is pivotal in overcoming the long-standing memory
bottleneck in volumetric deep learning, making it possible to reach 16x
super-resolution in 3D, a scale that is challenging to attain due to cubic
memory scaling. For training, we utilized segmented 3D low-resolution micro-CT
images along with unpaired segmented complementary 2D high-resolution laser
scanning microscope images. Post-training, resolution improved from 7 to 0.44
micro-m/voxel with accurate segmentation of constituent minerals. Validated on
Berea sandstone, this framework demonstrates substantial improvements in pore
characterization and mineral differentiation, offering a robust solution to one
of the primary computational limitations in modern geoscientific imaging.",2025-05-24,"Evgeny Ugolkov, Xupeng He, Hyung Kwak, Hussein Hoteit",http://arxiv.org/pdf/2505.18664v1,cs.LG
Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees,"Selecting artificial intelligence (AI) models, such as large language models
(LLMs), from multiple candidates requires accurate performance estimation. This
is ideally achieved through empirical evaluations involving abundant real-world
data. However, such evaluations are costly and impractical at scale. To address
this challenge, autoevaluation methods leverage synthetic data produced by
automated evaluators, such as LLMs-as-judges, reducing variance but potentially
introducing bias. Recent approaches have employed semi-supervised
prediction-powered inference (\texttt{PPI}) to correct for the bias of
autoevaluators. However, the use of autoevaluators may lead in practice to a
degradation in sample efficiency compared to conventional methods using only
real-world data. In this paper, we propose \texttt{R-AutoEval+}, a novel
framework that provides finite-sample reliability guarantees on the model
evaluation, while also ensuring an enhanced (or at least no worse) sample
efficiency compared to conventional methods. The key innovation of
\texttt{R-AutoEval+} is an adaptive construction of the model evaluation
variable, which dynamically tunes its reliance on synthetic data, reverting to
conventional methods when the autoevaluator is insufficiently accurate.
Experiments on the use of LLMs-as-judges for the optimization of quantization
settings for the weights of an LLM, and for prompt design in LLMs confirm the
reliability and efficiency of \texttt{R-AutoEval+}.",2025-05-24,"Sangwoo Park, Matteo Zecchin, Osvaldo Simeone",http://arxiv.org/pdf/2505.18659v1,cs.LG
Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics,"Large Language Models (LLMs) have emerged as a promising cornerstone for the
development of natural language processing (NLP) and artificial intelligence
(AI). However, ensuring the robustness of LLMs remains a critical challenge. To
address these challenges and advance the field, this survey provides a
comprehensive overview of current studies in this area. First, we
systematically examine the nature of robustness in LLMs, including its
conceptual foundations, the importance of consistent performance across diverse
inputs, and the implications of failure modes in real-world applications. Next,
we analyze the sources of non-robustness, categorizing intrinsic model
limitations, data-driven vulnerabilities, and external adversarial factors that
compromise reliability. Following this, we review state-of-the-art mitigation
strategies, and then we discuss widely adopted benchmarks, emerging metrics,
and persistent gaps in assessing real-world reliability. Finally, we synthesize
findings from existing surveys and interdisciplinary studies to highlight
trends, unresolved issues, and pathways for future research.",2025-05-24,"Pankaj Kumar, Subhankar Mishra",http://arxiv.org/pdf/2505.18658v1,cs.LG
LLM-QFL: Distilling Large Language Model for Quantum Federated Learning,"Inspired by the power of large language models (LLMs), our research adapts
them to quantum federated learning (QFL) to boost efficiency and performance.
We propose a federated fine-tuning method that distills an LLM within QFL,
allowing each client to locally adapt the model to its own data while
preserving privacy and reducing unnecessary global updates. The fine-tuned LLM
also acts as a reinforcement agent, optimizing QFL by adjusting optimizer
steps, cutting down communication rounds, and intelligently selecting clients.
Experiments show significant efficiency gains. We pioneer a synergy between LLM
and QFL, offering: i) practical efficiency: Reduced communication costs and
faster convergence. ii) theoretical rigor: Provable guarantees for adaptive
federated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable
deployment on resource-constrained quantum devices. Code implementation is
available here 1.",2025-05-24,"Dev Gurung, Shiva Raj Pokhrel",http://arxiv.org/pdf/2505.18656v1,cs.LG
On the Emergence of Linear Analogies in Word Embeddings,"Models such as Word2Vec and GloVe construct word embeddings based on the
co-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The
resulting vectors $W_i$ not only group semantically similar words but also
exhibit a striking linear analogy structure -- for example, $W_{\text{king}} -
W_{\text{man}} + W_{\text{woman}} \approx W_{\text{queen}}$ -- whose
theoretical origin remains unclear. Previous observations indicate that this
analogy structure: (i) already emerges in the top eigenvectors of the matrix
$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more
eigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are
included, (iii) is enhanced when using $\log M(i,j)$ rather than $M(i,j)$, and
(iv) persists even when all word pairs involved in a specific analogy relation
(e.g., king-queen, man-woman) are removed from the corpus. To explain these
phenomena, we introduce a theoretical generative model in which words are
defined by binary semantic attributes, and co-occurrence probabilities are
derived from attribute-based interactions. This model analytically reproduces
the emergence of linear analogy structure and naturally accounts for properties
(i)-(iv). It can be viewed as giving fine-grained resolution into the role of
each additional embedding dimension. It is robust to various forms of noise and
agrees well with co-occurrence statistics measured on Wikipedia and the analogy
benchmark introduced by Mikolov et al.",2025-05-24,"Daniel J. Korchinski, Dhruva Karkada, Yasaman Bahri, Matthieu Wyart",http://arxiv.org/pdf/2505.18651v1,cs.LG
Flow Matching for Geometric Trajectory Simulation,"The simulation of N-body systems is a fundamental problem with applications
in a wide range of fields, such as molecular dynamics, biochemistry, and
pedestrian dynamics. Machine learning has become an invaluable tool for scaling
physics-based simulators and developing models directly from experimental data.
In particular, recent advances based on deep generative modeling and geometric
deep learning have enabled probabilistic simulation by modeling complex
distributions over trajectories while respecting the permutation symmetry that
is fundamental to N-body systems. However, to generate realistic trajectories,
existing methods must learn complex transformations starting from uninformed
noise and do not allow for the exploitation of domain-informed priors. In this
work, we propose STFlow to address this limitation. By leveraging flow matching
and data-dependent couplings, STFlow facilitates physics-informed simulation of
geometric trajectories without sacrificing model expressivity or scalability.
Our evaluation on N-body dynamical systems, molecular dynamics, and pedestrian
dynamics benchmarks shows that STFlow produces significantly lower prediction
errors while enabling more efficient inference, highlighting the benefits of
employing physics-informed prior distributions in probabilistic geometric
trajectory modeling.",2025-05-24,"Kiet Bennema ten Brinke, Koen Minartz, Vlado Menkovski",http://arxiv.org/pdf/2505.18647v1,cs.LG
ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation,"Low-Rank Adaptation (LoRA) is widely adopted for downstream fine-tuning of
foundation models due to its efficiency and zero additional inference cost.
Many real-world applications require foundation models to specialize in
multiple tasks simultaneously, motivating the need for efficient multi-task
adaptation. While recent approaches integrate LoRA with mixture-of-experts
(MoE) to address this, the use of routers prevents parameter mergeability,
which increases inference overhead and hinders unified multi-task adaptation,
thereby limiting deployment practicality. In this work, we propose ThanoRA, a
Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation framework that enables
multi-task adaptation while preserving the inference efficiency of LoRA.
ThanoRA jointly models task heterogeneity and mitigates subspace interference
throughout training. Specifically, motivated by inherent differences in
complexity and heterogeneity across tasks, ThanoRA constructs task-specific
LoRA subspaces at initialization, enabling fine-grained knowledge injection
aligned with task heterogeneity. Furthermore, to prevent task interference and
subspace collapse during multi-task training, ThanoRA introduces a
subspace-preserving regularization that maintains the independence of
task-specific representations. With the synergy of both components, ThanoRA
enables efficient and unified multi-task adaptation. Extensive experiments
across multimodal and text-only benchmarks under varying multi-task mixtures
demonstrate that ThanoRA consistently achieves robust and superior performance
over strong baselines without introducing additional inference overhead. Our
code is publicly available at: https://github.com/LiangJian24/ThanoRA.",2025-05-24,"Jian Liang, Wenke Huang, Xianda Guo, Guancheng Wan, Bo Du, Mang Ye",http://arxiv.org/pdf/2505.18640v1,cs.LG
Asymmetric Duos: Sidekicks Improve Uncertainty,"The go-to strategy to apply deep networks in settings where uncertainty
informs decisions--ensembling multiple training runs with random
initializations--is ill-suited for the extremely large-scale models and
practical fine-tuning workflows of today. We introduce a new cost-effective
strategy for improving the uncertainty quantification and downstream decisions
of a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate
but much smaller ""sidekick"" (e.g. a fine-tuned ResNet-34) with a fraction of
the computational cost. We propose aggregating the predictions of this
\emph{Asymmetric Duo} by simple learned weighted averaging. Surprisingly,
despite their inherent asymmetry, the sidekick model almost never harms the
performance of the larger model. In fact, across five image classification
benchmarks and a variety of model architectures and training schemes (including
soups), Asymmetric Duos significantly improve accuracy, uncertainty
quantification, and selective classification metrics with only ${\sim}10-20\%$
more computation.",2025-05-24,"Tim G. Zhou, Evan Shelhamer, Geoff Pleiss",http://arxiv.org/pdf/2505.18636v1,cs.LG
Think Before You Accept: Semantic Reflective Verification for Faster Speculative Decoding,"Large language models (LLMs) suffer from high inference latency due to the
auto-regressive decoding process. Speculative decoding accelerates inference by
generating multiple draft tokens using a lightweight model and verifying them
in parallel. However, existing verification methods rely heavily on
distributional consistency while overlooking semantic correctness, thereby
limiting the potential speedup of speculative decoding. While some methods
employ additional models for relaxed verification of draft tokens, they often
fail to generalize effectively to more diverse or open-domain settings. In this
work, we propose Reflective Verification, a training-free and semantics-aware
approach that achieves a better trade-off between correctness and efficiency.
Specifically, we leverage the inherent reflective capacity of LLMs to
semantically assess the correctness of draft tokens in parallel during
verification. Using prompt-based probing, we obtain both the original and
reflective distributions of draft tokens in a single forward pass. The fusion
of these distributions enables semantic-level verification of draft tokens that
incorporates both consistency and correctness. Experiments across multiple
domain benchmarks and model scales demonstrate that our method significantly
increases the acceptance length of draft tokens without compromising model
performance. Furthermore, we find that the proposed Reflective Verification is
orthogonal to existing statistical verification methods, and their combination
yields additional 5$\sim$15\% improvements in decoding speed.",2025-05-24,"Yixuan Wang, Yijun Liu, Shiyu ji, Yuzhuang Xu, Yang Xu, Qingfu Zhu, Wanxiang Che",http://arxiv.org/pdf/2505.18629v1,cs.LG
Mind The Gap: Deep Learning Doesn't Learn Deeply,"This paper aims to understand how neural networks learn algorithmic reasoning
by addressing two questions: How faithful are learned algorithms when they are
effective, and why do neural networks fail to learn effective algorithms
otherwise? To answer these questions, we use neural compilation, a technique
that directly encodes a source algorithm into neural network parameters,
enabling the network to compute the algorithm exactly. This enables comparison
between compiled and conventionally learned parameters, intermediate vectors,
and behaviors. This investigation is crucial for developing neural networks
that robustly learn complexalgorithms from data. Our analysis focuses on graph
neural networks (GNNs), which are naturally aligned with algorithmic reasoning
tasks, specifically our choices of BFS, DFS, and Bellman-Ford, which cover the
spectrum of effective, faithful, and ineffective learned algorithms. Commonly,
learning algorithmic reasoning is framed as induction over synthetic data,
where a parameterized model is trained on inputs, traces, and outputs produced
by an underlying ground truth algorithm. In contrast, we introduce a neural
compilation method for GNNs, which sets network parameters analytically,
bypassing training. Focusing on GNNs leverages their alignment with algorithmic
reasoning, extensive algorithmic induction literature, and the novel
application of neural compilation to GNNs. Overall, this paper aims to
characterize expressability-trainability gaps - a fundamental shortcoming in
learning algorithmic reasoning. We hypothesize that inductive learning is most
effective for parallel algorithms contained within the computational class
\texttt{NC}.",2025-05-24,"Lucas Saldyt, Subbarao Kambhampati",http://arxiv.org/pdf/2505.18623v1,cs.LG
"Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation","In recent machine learning systems, confidence scores are being utilized more
and more to manage selective prediction, whereby a model can abstain from
making a prediction when it is unconfident. Yet, conventional metrics like
accuracy, expected calibration error (ECE), and area under the risk-coverage
curve (AURC) do not capture the actual reliability of predictions. These
metrics either disregard confidence entirely, dilute valuable localized
information through averaging, or neglect to suitably penalize overconfident
misclassifications, which can be particularly detrimental in real-world
systems. We introduce two new metrics Confidence-Weighted Selective Accuracy
(CWSA) and its normalized variant CWSA+ that offer a principled and
interpretable way to evaluate predictive models under confidence thresholds.
Unlike existing methods, our metrics explicitly reward confident accuracy and
penalize overconfident mistakes. They are threshold-local, decomposable, and
usable in both evaluation and deployment settings where trust and risk must be
quantified. Through exhaustive experiments on both real-world data sets (MNIST,
CIFAR-10) and artificial model variants (calibrated, overconfident,
underconfident, random, perfect), we show that CWSA and CWSA+ both effectively
detect nuanced failure modes and outperform classical metrics in
trust-sensitive tests. Our results confirm that CWSA is a sound basis for
developing and assessing selective prediction systems for safety-critical
domains.",2025-05-24,"Kourosh Shahnazari, Seyed Moein Ayyoubzadeh, Mohammadali Keshtparvar, Pegah Ghaffari",http://arxiv.org/pdf/2505.18622v1,cs.LG
MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation,"Lyrics translation requires both accurate semantic transfer and preservation
of musical rhythm, syllabic structure, and poetic style. In animated musicals,
the challenge intensifies due to alignment with visual and auditory cues. We
introduce Multilingual Audio-Video Lyrics Benchmark for Animated Song
Translation (MAVL), the first multilingual, multimodal benchmark for singable
lyrics translation. By integrating text, audio, and video, MAVL enables richer
and more expressive translations than text-only approaches. Building on this,
we propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought
SylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints
to produce natural-sounding lyrics. Experimental results demonstrate that
SylAVL-CoT significantly outperforms text-based models in singability and
contextual accuracy, emphasizing the value of multimodal, multilingual
approaches for lyrics translation.",2025-05-24,"Woohyun Cho, Youngmin Kim, Sunghyun Lee, Youngjae Yu",http://arxiv.org/pdf/2505.18614v1,cs.LG
MLRan: A Behavioural Dataset for Ransomware Analysis and Detection,"Ransomware remains a critical threat to cybersecurity, yet publicly available
datasets for training machine learning-based ransomware detection models are
scarce and often have limited sample size, diversity, and reproducibility. In
this paper, we introduce MLRan, a behavioural ransomware dataset, comprising
over 4,800 samples across 64 ransomware families and a balanced set of goodware
samples. The samples span from 2006 to 2024 and encompass the four major types
of ransomware: locker, crypto, ransomware-as-a-service, and modern variants. We
also propose guidelines (GUIDE-MLRan), inspired by previous work, for
constructing high-quality behavioural ransomware datasets, which informed the
curation of our dataset. We evaluated the ransomware detection performance of
several machine learning (ML) models using MLRan. For this purpose, we
performed feature selection by conducting mutual information filtering to
reduce the initial 6.4 million features to 24,162, followed by recursive
feature elimination, yielding 483 highly informative features. The ML models
achieved an accuracy, precision and recall of up to 98.7%, 98.9%, 98.5%,
respectively. Using SHAP and LIME, we identified critical indicators of
malicious behaviour, including registry tampering, strings, and API misuse. The
dataset and source code for feature extraction, selection, ML training, and
evaluation are available publicly to support replicability and encourage future
research, which can be found at https://github.com/faithfulco/mlran.",2025-05-24,"Faithful Chiagoziem Onwuegbuche, Adelodun Olaoluwa, Anca Delia Jurcut, Liliana Pasquale",http://arxiv.org/pdf/2505.18613v1,cs.LG
Exemplar-Free Continual Learning for State Space Models,"State-Space Models (SSMs) excel at capturing long-range dependencies with
structured recurrence, making them well-suited for sequence modeling. However,
their evolving internal states pose challenges in adapting them under Continual
Learning (CL). This is particularly difficult in exemplar-free settings, where
the absence of prior data leaves updates to the dynamic SSM states
unconstrained, resulting in catastrophic forgetting. To address this, we
propose Inf-SSM, a novel and simple geometry-aware regularization method that
utilizes the geometry of the infinite-dimensional Grassmannian to constrain
state evolution during CL. Unlike classical continual learning methods that
constrain weight updates, Inf-SSM regularizes the infinite-horizon evolution of
SSMs encoded in their extended observability subspace. We show that enforcing
this regularization requires solving a matrix equation known as the Sylvester
equation, which typically incurs $\mathcal{O}(n^3)$ complexity. We develop a
$\mathcal{O}(n^2)$ solution by exploiting the structure and properties of SSMs.
This leads to an efficient regularization mechanism that can be seamlessly
integrated into existing CL methods. Comprehensive experiments on challenging
benchmarks, including ImageNet-R and Caltech-256, demonstrate a significant
reduction in forgetting while improving accuracy across sequential tasks.",2025-05-24,"Isaac Ning Lee, Leila Mahmoodi, Trung Le, Mehrtash Harandi",http://arxiv.org/pdf/2505.18604v1,cs.LG
LLM-Meta-SR: Learning to Evolve Selection Operators for Symbolic Regression,"Large language models (LLMs) have revolutionized algorithm development, yet
their application in symbolic regression, where algorithms automatically
discover symbolic expressions from data, remains constrained and is typically
designed manually by human experts. In this paper, we propose a
learning-to-evolve framework that enables LLMs to automatically design
selection operators for evolutionary symbolic regression algorithms. We first
identify two key limitations in existing LLM-based algorithm evolution
techniques: code bloat and a lack of semantic guidance. Bloat results in
unnecessarily complex components, and the absence of semantic awareness can
lead to ineffective exchange of useful code components, both of which can
reduce the interpretability of the designed algorithm or hinder evolutionary
learning progress. To address these issues, we enhance the LLM-based evolution
framework for meta symbolic regression with two key innovations: bloat control
and a complementary, semantics-aware selection operator. Additionally, we embed
domain knowledge into the prompt, enabling the LLM to generate more effective
and contextually relevant selection operators. Our experimental results on
symbolic regression benchmarks show that LLMs can devise selection operators
that outperform nine expert-designed baselines, achieving state-of-the-art
performance. This demonstrates that LLMs can exceed expert-level algorithm
design for symbolic regression.",2025-05-24,"Hengzhe Zhang, Qi Chen, Bing Xue, Mengjie Zhang",http://arxiv.org/pdf/2505.18602v1,cs.LG
Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment,"Modern single-image super-resolution (SISR) models deliver photo-realistic
results at the scale factors on which they are trained, but collapse when asked
to magnify far beyond that regime. We address this scalability bottleneck with
Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an
autoregressive chain of intermediate scale-states with multi-scale-aware
prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the
conditional probability into tractable sub-problems to achieve extreme
resolutions without additional training. Because visual cues diminish at high
magnifications, we augment each zoom step with multi-scale-aware text prompts
generated by a vision-language model (VLM). The prompt extractor itself is
fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic
VLM, aligning text guidance towards human preference. Experiments show that a
standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement
with high perceptual quality and fidelity.",2025-05-24,"Bryan Sangwoo Kim, Jeongsol Kim, Jong Chul Ye",http://arxiv.org/pdf/2505.18600v1,cs.LG
LLMs for Supply Chain Management,"The development of large language models (LLMs) has provided new tools for
research in supply chain management (SCM). In this paper, we introduce a
retrieval-augmented generation (RAG) framework that dynamically integrates
external knowledge into the inference process, and develop a domain-specialized
SCM LLM, which demonstrates expert-level competence by passing standardized SCM
examinations and beer game tests. We further employ the use of LLMs to conduct
horizontal and vertical supply chain games, in order to analyze competition and
cooperation within supply chains. Our experiments show that RAG significantly
improves performance on SCM tasks. Moreover, game-theoretic analysis reveals
that the LLM can reproduce insights from the classical SCM literature, while
also uncovering novel behaviors and offering fresh perspectives on phenomena
such as the bullwhip effect. This paper opens the door for exploring
cooperation and competition for complex supply chain network through the lens
of LLMs.",2025-05-24,"Haojie Wang, Jiuyun Jiang, L. Jeff Hong, Guangxin Jiang",http://arxiv.org/pdf/2505.18597v1,cs.LG
MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations,"We study offline imitation learning (IL) in cooperative multi-agent settings,
where demonstrations have unlabeled mixed quality - containing both expert and
suboptimal trajectories. Our proposed solution is structured in two stages:
trajectory labeling and multi-agent imitation learning, designed jointly to
enable effective learning from heterogeneous, unlabeled data. In the first
stage, we combine advances in large language models and preference-based
reinforcement learning to construct a progressive labeling pipeline that
distinguishes expert-quality trajectories. In the second stage, we introduce
MisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn
robust policies while addressing the computational complexity of large joint
state-action spaces. By extending the popular single-agent DICE framework to
multi-agent settings with a new value decomposition and mixing architecture,
our method yields a convex policy optimization objective and ensures
consistency between global and local policies. We evaluate MisoDICE on multiple
standard multi-agent RL benchmarks and demonstrate superior performance,
especially when expert data is scarce.",2025-05-24,"The Viet Bui, Tien Mai, Hong Thanh Nguyen",http://arxiv.org/pdf/2505.18595v1,cs.LG
Bayesian Meta-Reinforcement Learning with Laplace Variational Recurrent Networks,"Meta-reinforcement learning trains a single reinforcement learning agent on a
distribution of tasks to quickly generalize to new tasks outside of the
training set at test time. From a Bayesian perspective, one can interpret this
as performing amortized variational inference on the posterior distribution
over training tasks. Among the various meta-reinforcement learning approaches,
a common method is to represent this distribution with a point-estimate using a
recurrent neural network. We show how one can augment this point estimate to
give full distributions through the Laplace approximation, either at the start
of, during, or after learning, without modifying the base model architecture.
With our approximation, we are able to estimate distribution statistics (e.g.,
the entropy) of non-Bayesian agents and observe that point-estimate based
methods produce overconfident estimators while not satisfying consistency.
Furthermore, when comparing our approach to full-distribution based learning of
the task posterior, our method performs on par with variational baselines while
having much fewer parameters.",2025-05-24,"Joery A. de Vries, Jinke He, Mathijs M. de Weerdt, Matthijs T. J. Spaan",http://arxiv.org/pdf/2505.18591v1,cs.LG
Mechanical in-sensor computing: a programmable meta-sensor for structural damage classification without external electronic power,"Structural health monitoring (SHM) involves sensor deployment, data
acquisition, and data interpretation, commonly implemented via a tedious wired
system. The information processing in current practice majorly depends on
electronic computers, albeit with universal applications, delivering challenges
such as high energy consumption and low throughput due to the nature of digital
units. In recent years, there has been a renaissance interest in shifting
computations from electronic computing units to the use of real physical
systems, a concept known as physical computation. This approach provides the
possibility of thinking out of the box for SHM, seamlessly integrating sensing
and computing into a pure-physical entity, without relying on external
electronic power supplies, thereby properly coping with resource-restricted
scenarios. The latest advances of metamaterials (MM) hold great promise for
this proactive idea. In this paper, we introduce a programmable
metamaterial-based sensor (termed as MM-sensor) for physically processing
structural vibration information to perform specific SHM tasks, such as
structural damage warning (binary classification) in this initiation, without
the need for further information processing or resource-consuming, that is, the
data collection and analysis are completed in-situ at the sensor level. We
adopt the configuration of a locally resonant metamaterial plate (LRMP) to
achieve the first fabrication of the MM-sensor. We take advantage of the
bandgap properties of LRMP to physically differentiate the dynamic behavior of
structures before and after damage. By inversely designing the geometric
parameters, our current approach allows for adjustments to the bandgap
features. This is effective for engineering systems with a first natural
frequency ranging from 9.54 Hz to 81.86 Hz.",2025-05-24,"Tingpeng Zhang, Xuzhang Peng, Mingyuan Zhou, Guobiao Hu, Zhilu Lai",http://arxiv.org/pdf/2505.18579v1,cs.LG
Autocomp: LLM-Driven Code Optimization for Tensor Accelerators,"Hardware accelerators, especially those designed for tensor processing, have
become ubiquitous in today's computing landscape. However, even with
significant efforts in building compilers, programming these tensor
accelerators remains challenging, leaving much of their potential
underutilized. Recently, large language models (LLMs), trained on large amounts
of code, have shown significant promise in code generation and optimization
tasks, but generating low-resource languages like specialized tensor
accelerator code still poses a significant challenge. We tackle this challenge
with Autocomp, an approach that empowers accelerator programmers to leverage
domain knowledge and hardware feedback to optimize code via an automated
LLM-driven search. We accomplish this by: 1) formulating each optimization pass
as a structured two-phase prompt, divided into planning and code generation
phases, 2) inserting domain knowledge during planning via a concise and
adaptable optimization menu, and 3) integrating correctness and performance
metrics from hardware as feedback at each search iteration. Across three
categories of representative workloads and two different accelerators, we
demonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x
(convolution) faster than the vendor-provided library, and outperforms
expert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x
(fine-grained linear algebra). Additionally, we demonstrate that optimization
schedules generated from Autocomp can be reused across similar tensor
operations, improving speedups by up to 24% under a fixed sample budget.",2025-05-24,"Charles Hong, Sahil Bhatia, Alvin Cheung, Yakun Sophia Shao",http://arxiv.org/pdf/2505.18574v1,cs.LG
Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs,"Reasoning large language models (LLMs) excel in complex tasks, which has
drawn significant attention to reinforcement learning (RL) for LLMs. However,
existing approaches allocate an equal number of rollouts to all questions
during the RL process, which is inefficient. This inefficiency stems from the
fact that training on simple questions yields limited gains, whereas more
rollouts are needed for challenging questions to sample correct answers.
Furthermore, while RL improves response precision, it limits the model's
exploration ability, potentially resulting in a performance cap below that of
the base model prior to RL. To address these issues, we propose a mechanism for
dynamically allocating rollout budgets based on the difficulty of the problems,
enabling more efficient RL training. Additionally, we introduce an adaptive
dynamic temperature adjustment strategy to maintain the entropy at a stable
level, thereby encouraging sufficient exploration. This enables LLMs to improve
response precision while preserving their exploratory ability to uncover
potential correct pathways. The code and data is available on:
https://github.com/LiaoMengqi/E3-RL4LLMs",2025-05-24,"Mengqi Liao, Xiangyu Xi, Ruinian Chen, Jia Leng, Yangen Hu, Ke Zeng, Shuai Liu, Huaiyu Wan",http://arxiv.org/pdf/2505.18573v1,cs.LG
VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis,"Stock price prediction remains a complex and high-stakes task in financial
analysis, traditionally addressed using statistical models or, more recently,
language models. In this work, we introduce VISTA (Vision-Language Inference
for Stock Time-series Analysis), a novel, training-free framework that
leverages Vision-Language Models (VLMs) for multi-modal stock forecasting.
VISTA prompts a VLM with both textual representations of historical stock
prices and their corresponding line charts to predict future price values. By
combining numerical and visual modalities in a zero-shot setting and using
carefully designed chain-of-thought prompts, VISTA captures complementary
patterns that unimodal approaches often miss. We benchmark VISTA against
standard baselines, including ARIMA and text-only LLM-based prompting methods.
Experimental results show that VISTA outperforms these baselines by up to
89.83%, demonstrating the effectiveness of multi-modal inference for stock
time-series analysis and highlighting the potential of VLMs in financial
forecasting tasks without requiring task-specific training.",2025-05-24,"Tina Khezresmaeilzadeh, Parsa Razmara, Seyedarmin Azizi, Mohammad Erfan Sadeghi, Erfan Baghaei Portaghloo",http://arxiv.org/pdf/2505.18570v1,cs.LG
Learning without Isolation: Pathway Protection for Continual Learning,"Deep networks are prone to catastrophic forgetting during sequential task
learning, i.e., losing the knowledge about old tasks upon learning new tasks.
To this end, continual learning(CL) has emerged, whose existing methods focus
mostly on regulating or protecting the parameters associated with the previous
tasks. However, parameter protection is often impractical, since the size of
parameters for storing the old-task knowledge increases linearly with the
number of tasks, otherwise it is hard to preserve the parameters related to the
old-task knowledge. In this work, we bring a dual opinion from neuroscience and
physics to CL: in the whole networks, the pathways matter more than the
parameters when concerning the knowledge acquired from the old tasks. Following
this opinion, we propose a novel CL framework, learning without isolation(LwI),
where model fusion is formulated as graph matching and the pathways occupied by
the old tasks are protected without being isolated. Thanks to the sparsity of
activation channels in a deep network, LwI can adaptively allocate available
pathways for a new task, realizing pathway protection and addressing
catastrophic forgetting in a parameter-efficient manner. Experiments on popular
benchmark datasets demonstrate the superiority of the proposed LwI.",2025-05-24,"Zhikang Chen, Abudukelimu Wuerkaixi, Sen Cui, Haoxuan Li, Ding Li, Jingfeng Zhang, Bo Han, Gang Niu, Houfang Liu, Yi Yang, Sifan Yang, Changshui Zhang, Tianling Ren",http://arxiv.org/pdf/2505.18568v1,cs.LG
Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods,"We introduce neural network architectures that combine physics-informed
neural networks (PINNs) with the immersed boundary method (IBM) to solve
fluid-structure interaction (FSI) problems. Our approach features two distinct
architectures: a Single-FSI network with a unified parameter space, and an
innovative Eulerian-Lagrangian network that maintains separate parameter spaces
for fluid and structure domains. We study each architecture using standard Tanh
and adaptive B-spline activation functions. Empirical studies on a 2D cavity
flow problem involving a moving solid structure show that the
Eulerian-Lagrangian architecture performs significantly better. The adaptive
B-spline activation further enhances accuracy by providing locality-aware
representation near boundaries. While our methodology shows promising results
in predicting the velocity field, pressure recovery remains challenging due to
the absence of explicit force-coupling constraints in the current formulation.
Our findings underscore the importance of domain-specific architectural design
and adaptive activation functions for modeling FSI problems within the PINN
framework.",2025-05-24,"Afrah Farea, Saiful Khan, Reza Daryani, Emre Cenk Ersan, Mustafa Serdar Celebi",http://arxiv.org/pdf/2505.18565v1,cs.LG
Joint-stochastic-approximation Autoencoders with Application to Semi-supervised Learning,"Our examination of existing deep generative models (DGMs), including VAEs and
GANs, reveals two problems. First, their capability in handling discrete
observations and latent codes is unsatisfactory, though there are interesting
efforts. Second, both VAEs and GANs optimize some criteria that are indirectly
related to the data likelihood. To address these problems, we formally present
Joint-stochastic-approximation (JSA) autoencoders - a new family of algorithms
for building deep directed generative models, with application to
semi-supervised learning. The JSA learning algorithm directly maximizes the
data log-likelihood and simultaneously minimizes the inclusive KL divergence
the between the posteriori and the inference model. We provide theoretical
results and conduct a series of experiments to show its superiority such as
being robust to structure mismatch between encoder and decoder, consistent
handling of both discrete and continuous variables. Particularly we empirically
show that JSA autoencoders with discrete latent space achieve comparable
performance to other state-of-the-art DGMs with continuous latent space in
semi-supervised tasks over the widely adopted datasets - MNIST and SVHN. To the
best of our knowledge, this is the first demonstration that discrete latent
variable models are successfully applied in the challenging semi-supervised
tasks.",2025-05-24,"Wenbo He, Zhijian Ou",http://arxiv.org/pdf/2505.18558v1,cs.LG
LAMDA: A Longitudinal Android Malware Benchmark for Concept Drift Analysis,"Machine learning (ML)-based malware detection systems often fail to account
for the dynamic nature of real-world training and test data distributions. In
practice, these distributions evolve due to frequent changes in the Android
ecosystem, adversarial development of new malware families, and the continuous
emergence of both benign and malicious applications. Prior studies have shown
that such concept drift -- distributional shifts in benign and malicious
samples, leads to significant degradation in detection performance over time.
Despite the practical importance of this issue, existing datasets are often
outdated and limited in temporal scope, diversity of malware families, and
sample scale, making them insufficient for the systematic evaluation of concept
drift in malware detection.
  To address this gap, we present LAMDA, the largest and most temporally
diverse Android malware benchmark to date, designed specifically for concept
drift analysis. LAMDA spans 12 years (2013-2025, excluding 2015), includes over
1 million samples (approximately 37% labeled as malware), and covers 1,380
malware families and 150,000 singleton samples, reflecting the natural
distribution and evolution of real-world Android applications. We empirically
demonstrate LAMDA's utility by quantifying the performance degradation of
standard ML models over time and analyzing feature stability across years. As
the most comprehensive Android malware dataset to date, LAMDA enables in-depth
research into temporal drift, generalization, explainability, and evolving
detection challenges. The dataset and code are available at:
https://iqsec-lab.github.io/LAMDA/.",2025-05-24,"Md Ahsanul Haque, Ismail Hossain, Md Mahmuduzzaman Kamol, Md Jahangir Alam, Suresh Kumar Amalapuram, Sajedul Talukder, Mohammad Saidur Rahman",http://arxiv.org/pdf/2505.18551v1,cs.LG
ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery,"Soil organic carbon (SOC) is a critical indicator of soil health, but its
accurate estimation from satellite imagery is hindered in vegetated regions due
to spectral contamination from plant cover, which obscures soil reflectance and
reduces model reliability. This study proposes the Reflectance Transformation
Generative Adversarial Network (ReflectGAN), a novel paired GAN-based framework
designed to reconstruct accurate bare soil reflectance from vegetated soil
satellite observations. By learning the spectral transformation between
vegetated and bare soil reflectance, ReflectGAN facilitates more precise SOC
estimation under mixed land cover conditions. Using the LUCAS 2018 dataset and
corresponding Landsat 8 imagery, we trained multiple learning-based models on
both original and ReflectGAN-reconstructed reflectance inputs. Models trained
on ReflectGAN outputs consistently outperformed those using existing vegetation
correction methods. For example, the best-performing model (RF) achieved an
$R^2$ of 0.54, RMSE of 3.95, and RPD of 2.07 when applied to the
ReflectGAN-generated signals, representing a 35\% increase in $R^2$, a 43\%
reduction in RMSE, and a 43\% improvement in RPD compared to the best existing
method (PMM-SU). The performance of the models with ReflectGAN is also better
compared to their counterparts when applied to another dataset, i.e.,
Sentinel-2 imagery. These findings demonstrate the potential of ReflectGAN to
improve SOC estimation accuracy in vegetated landscapes, supporting more
reliable soil monitoring.",2025-05-24,"Dristi Datta, Manoranjan Paul, Manzur Murshed, Shyh Wei Teng, Leigh M. Schmidtke",http://arxiv.org/pdf/2505.18546v1,cs.LG
B-score: Detecting biases in large language models using response history,"Large language models (LLMs) often exhibit strong biases, e.g, against women
or in favor of the number 7. We investigate whether LLMs would be able to
output less biased answers when allowed to observe their prior answers to the
same question in a multi-turn conversation. To understand which types of
questions invite more biased answers, we test LLMs on our proposed set of
questions that span 9 topics and belong to three types: (1) Subjective; (2)
Random; and (3) Objective. Interestingly, LLMs are able to ""de-bias"" themselves
in a multi-turn conversation in response to questions that seek an Random,
unbiased answer. Furthermore, we propose B-score, a novel metric that is
effective in detecting biases to Subjective, Random, Easy, and Hard questions.
On MMLU, HLE, and CSQA, leveraging B-score substantially improves the
verification accuracy of LLM answers (i.e, accepting LLM correct answers and
rejecting incorrect ones) compared to using verbalized confidence scores or the
frequency of single-turn answers alone. Code and data are available at:
https://b-score.github.io.",2025-05-24,"An Vo, Mohammad Reza Taesiri, Daeyoung Kim, Anh Totti Nguyen",http://arxiv.org/pdf/2505.18545v1,cs.LG
Benchmarking Poisoning Attacks against Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has proven effective in mitigating
hallucinations in large language models by incorporating external knowledge
during inference. However, this integration introduces new security
vulnerabilities, particularly to poisoning attacks. Although prior work has
explored various poisoning strategies, a thorough assessment of their practical
threat to RAG systems remains missing. To address this gap, we propose the
first comprehensive benchmark framework for evaluating poisoning attacks on
RAG. Our benchmark covers 5 standard question answering (QA) datasets and 10
expanded variants, along with 13 poisoning attack methods and 7 defense
mechanisms, representing a broad spectrum of existing techniques. Using this
benchmark, we conduct a comprehensive evaluation of all included attacks and
defenses across the full dataset spectrum. Our findings show that while
existing attacks perform well on standard QA datasets, their effectiveness
drops significantly on the expanded versions. Moreover, our results demonstrate
that various advanced RAG architectures, such as sequential, branching,
conditional, and loop RAG, as well as multi-turn conversational RAG, multimodal
RAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning
attacks. Notably, current defense techniques fail to provide robust protection,
underscoring the pressing need for more resilient and generalizable defense
strategies.",2025-05-24,"Baolei Zhang, Haoran Xin, Jiatong Li, Dongzhe Zhang, Minghong Fang, Zhuqing Liu, Lihai Nie, Zheli Liu",http://arxiv.org/pdf/2505.18543v1,cs.LG
Mind Your Vision: Multimodal Estimation of Refractive Disorders Using Electrooculography and Eye Tracking,"Refractive errors are among the most common visual impairments globally, yet
their diagnosis often relies on active user participation and clinical
oversight. This study explores a passive method for estimating refractive power
using two eye movement recording techniques: electrooculography (EOG) and
video-based eye tracking. Using a publicly available dataset recorded under
varying diopter conditions, we trained Long Short-Term Memory (LSTM) models to
classify refractive power from unimodal (EOG or eye tracking) and multimodal
configuration. We assess performance in both subject-dependent and
subject-independent settings to evaluate model personalization and
generalizability across individuals. Results show that the multimodal model
consistently outperforms unimodal models, achieving the highest average
accuracy in both settings: 96.207\% in the subject-dependent scenario and
8.882\% in the subject-independent scenario. However, generalization remains
limited, with classification accuracy only marginally above chance in the
subject-independent evaluations. Statistical comparisons in the
subject-dependent setting confirmed that the multimodal model significantly
outperformed the EOG and eye-tracking models. However, no statistically
significant differences were found in the subject-independent setting. Our
findings demonstrate both the potential and current limitations of eye movement
data-based refractive error estimation, contributing to the development of
continuous, non-invasive screening methods using EOG signals and eye-tracking
data.",2025-05-24,"Xin Wei, Huakun Liu, Yutaro Hirao, Monica Perusquia-Hernandez, Katsutoshi Masai, Hideaki Uchiyama, Kiyoshi Kiyokawa",http://arxiv.org/pdf/2505.18538v1,cs.LG
"Convergence, Sticking and Escape: Stochastic Dynamics Near Critical Points in SGD","We study the convergence properties and escape dynamics of Stochastic
Gradient Descent (SGD) in one-dimensional landscapes, separately considering
infinite- and finite-variance noise. Our main focus is to identify the time
scales on which SGD reliably moves from an initial point to the local minimum
in the same ''basin''. Under suitable conditions on the noise distribution, we
prove that SGD converges to the basin's minimum unless the initial point lies
too close to a local maximum. In that near-maximum scenario, we show that SGD
can linger for a long time in its neighborhood. For initial points near a
''sharp'' maximum, we show that SGD does not remain stuck there, and we provide
results to estimate the probability that it will reach each of the two
neighboring minima. Overall, our findings present a nuanced view of SGD's
transitions between local maxima and minima, influenced by both noise
characteristics and the underlying function geometry.",2025-05-24,"Dmitry Dudukalov, Artem Logachov, Vladimir Lotov, Timofei Prasolov, Evgeny Prokopenko, Anton Tarasenko",http://arxiv.org/pdf/2505.18535v1,cs.LG
Preserving AUC Fairness in Learning with Noisy Protected Groups,"The Area Under the ROC Curve (AUC) is a key metric for classification,
especially under class imbalance, with growing research focus on optimizing AUC
over accuracy in applications like medical image analysis and deepfake
detection. This leads to fairness in AUC optimization becoming crucial as
biases can impact protected groups. While various fairness mitigation
techniques exist, fairness considerations in AUC optimization remain in their
early stages, with most research focusing on improving AUC fairness under the
assumption of clean protected groups. However, these studies often overlook the
impact of noisy protected groups, leading to fairness violations in practice.
To address this, we propose the first robust AUC fairness approach under noisy
protected groups with fairness theoretical guarantees using distributionally
robust optimization. Extensive experiments on tabular and image datasets show
that our method outperforms state-of-the-art approaches in preserving AUC
fairness. The code is in
https://github.com/Purdue-M2/AUC_Fairness_with_Noisy_Groups.",2025-05-24,"Mingyang Wu, Li Lin, Wenbin Zhang, Xin Wang, Zhenhuan Yang, Shu Hu",http://arxiv.org/pdf/2505.18532v1,cs.LG
CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs,"Many existing models for clinical trial outcome prediction are optimized
using task-specific loss functions on trial phase-specific data. While this
scheme may boost prediction for common diseases and drugs, it can hinder
learning of generalizable representations, leading to more false
positives/negatives. To address this limitation, we introduce CLaDMoP, a new
pre-training approach for clinical trial outcome prediction, alongside the
Successful Clinical Trials dataset(SCT), specifically designed for this task.
CLaDMoP leverages a Large Language Model-to encode trials' eligibility
criteria-linked to a lightweight Drug-Molecule branch through a novel
multi-level fusion technique. To efficiently fuse long embeddings across
levels, we incorporate a grouping block, drastically reducing computational
overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training
on a ""pair matching"" proxy task. Compared to established zero-shot and few-shot
baselines, our method significantly improves both PR-AUC and ROC-AUC,
especially for phase I and phase II trials. We further evaluate and perform
ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to
state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome
Prediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC
and 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP,
highlighting its potential for clinical trial outcome prediction. Code and SCT
dataset can be downloaded from https://github.com/murai-lab/CLaDMoP.",2025-05-24,"Yiqing Zhang, Xiaozhong Liu, Fabricio Murai",http://arxiv.org/pdf/2505.18527v1,cs.LG
Scalable Gaussian Processes with Low-Rank Deep Kernel Decomposition,"Kernels are key to encoding prior beliefs and data structures in Gaussian
process (GP) models. The design of expressive and scalable kernels has garnered
significant research attention. Deep kernel learning enhances kernel
flexibility by feeding inputs through a neural network before applying a
standard parametric form. However, this approach remains limited by the choice
of base kernels, inherits high inference costs, and often demands sparse
approximations. Drawing on Mercer's theorem, we introduce a fully data-driven,
scalable deep kernel representation where a neural network directly represents
a low-rank kernel through a small set of basis functions. This construction
enables highly efficient exact GP inference in linear time and memory without
invoking inducing points. It also supports scalable mini-batch training based
on a principled variational inference framework. We further propose a simple
variance correction procedure to guard against overconfidence in uncertainty
estimates. Experiments on synthetic and real-world data demonstrate the
advantages of our deep kernel GP in terms of predictive accuracy, uncertainty
quantification, and computational efficiency.",2025-05-24,"Yunqin Zhu, Henry Shaowu Yuchi, Yao Xie",http://arxiv.org/pdf/2505.18526v1,cs.LG
LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs,"Foundation models based on large language models (LLMs) have shown great
success in handling various tasks and modalities. However, adapting these
models for general-purpose audio-language tasks is challenging due to
differences in acoustic environments and task variations. In this work, we
introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a
framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic
prompt selection strategy with learnable key-value pairs, allowing the model to
balance general and task-specific knowledge while avoiding overfitting in a
multitask setting. Our approach reduces dependence on large-scale ASR or
captioning datasets, achieves competitive performance with fewer trainable
parameters, and simplifies training by using a single-stage process.
Additionally, LiSTEN enhances interpretability by analyzing the diversity and
overlap of selected prompts across different tasks.",2025-05-24,"Pooneh Mousavi, Shubham Gupta, Cem Subakan, Mirco Ravanelli",http://arxiv.org/pdf/2505.18517v1,cs.LG
Test-Time Adaptation with Binary Feedback,"Deep learning models perform poorly when domain shifts exist between training
and test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue
by adapting pre-trained models using only unlabeled test samples. However,
existing TTA methods can fail under severe domain shifts, while recent active
TTA approaches requiring full-class labels are impractical due to high labeling
costs. To address this issue, we introduce a new setting of TTA with binary
feedback. This setting uses a few binary feedback inputs from annotators to
indicate whether model predictions are correct, thereby significantly reducing
the labeling burden of annotators. Under the setting, we propose BiTTA, a novel
dual-path optimization framework that leverages reinforcement learning to
balance binary feedback-guided adaptation on uncertain samples with
agreement-based self-adaptation on confident predictions. Experiments show
BiTTA achieves 13.3%p accuracy improvements over state-of-the-art baselines,
demonstrating its effectiveness in handling severe distribution shifts with
minimal labeling effort. The source code is available at
https://github.com/taeckyung/BiTTA.",2025-05-24,"Taeckyung Lee, Sorn Chottananurak, Junsu Kim, Jinwoo Shin, Taesik Gong, Sung-Ju Lee",http://arxiv.org/pdf/2505.18514v1,cs.LG
Enhancing Training Data Attribution with Representational Optimization,"Training data attribution (TDA) methods aim to measure how training data
impacts a model's predictions. While gradient-based attribution methods, such
as influence functions, offer theoretical grounding, their computational costs
make them impractical for large-scale applications. Representation-based
approaches are far more scalable, but typically rely on heuristic embeddings
that are not optimized for attribution, limiting their fidelity. To address
these challenges, we propose AirRep, a scalable, representation-based approach
that closes this gap by learning task-specific and model-aligned
representations optimized explicitly for TDA. AirRep introduces two key
innovations: a trainable encoder tuned for attribution quality, and an
attention-based pooling mechanism that enables accurate estimation of
group-wise influence. We train AirRep using a ranking objective over
automatically constructed training subsets labeled by their empirical effect on
target predictions. Experiments on instruction-tuned LLMs demonstrate that
AirRep achieves performance on par with state-of-the-art gradient-based
approaches while being nearly two orders of magnitude more efficient at
inference time. Further analysis highlights its robustness and generalization
across tasks and models. Our code is available at
https://github.com/sunnweiwei/AirRep.",2025-05-24,"Weiwei Sun, Haokun Liu, Nikhil Kandpal, Colin Raffel, Yiming Yang",http://arxiv.org/pdf/2505.18513v1,cs.LG
AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking,"Listwise reranking with large language models (LLMs) enhances top-ranked
results in retrieval-based applications. Due to the limit in context size and
high inference cost of long context, reranking is typically performed over a
fixed size of small subsets, with the final ranking aggregated from these
partial results. This fixed computation disregards query difficulty and
document distribution, leading to inefficiencies. We propose AcuRank, an
adaptive reranking framework that dynamically adjusts both the amount and
target of computation based on uncertainty estimates over document relevance.
Using a Bayesian TrueSkill model, we iteratively refine relevance estimates
until reaching sufficient confidence levels, and our explicit modeling of
ranking uncertainty enables principled control over reranking behavior and
avoids unnecessary updates to confident predictions. Results on the TREC-DL and
BEIR benchmarks show that our method consistently achieves a superior
accuracy-efficiency trade-off and scales better with compute than
fixed-computation baselines. These results highlight the effectiveness and
generalizability of our method across diverse retrieval tasks and LLM-based
reranking models.",2025-05-24,"Soyoung Yoon, Gyuwan Kim, Gyu-Hwung Cho, Seung-won Hwang",http://arxiv.org/pdf/2505.18512v1,cs.LG
SPDEBench: An Extensive Benchmark for Learning Regular and Singular Stochastic PDEs,"Stochastic Partial Differential Equations (SPDEs) driven by random noise play
a central role in modelling physical processes whose spatio-temporal dynamics
can be rough, such as turbulence flows, superconductors, and quantum dynamics.
To efficiently model these processes and make predictions, machine learning
(ML)-based surrogate models are proposed, with their network architectures
incorporating the spatio-temporal roughness in their design. However, it lacks
an extensive and unified datasets for SPDE learning; especially, existing
datasets do not account for the computational error introduced by noise
sampling and the necessary renormalization required for handling singular
SPDEs. We thus introduce SPDEBench, which is designed to solve typical SPDEs of
physical significance (e.g., the $\Phi^4_d$, wave, incompressible
Navier--Stokes, and KdV equations) on 1D or 2D tori driven by white noise via
ML methods. New datasets for singular SPDEs based on the renormalization
process have been constructed, and novel ML models achieving the best results
to date have been proposed. In particular, we investigate the impact of
computational error introduced by noise sampling and renormalization on the
performance comparison of ML models and highlight the importance of selecting
high-quality test data for accurate evaluation. Results are benchmarked with
traditional numerical solvers and ML-based models, including FNO, NSPDE and
DLR-Net, etc. It is shown that, for singular SPDEs, naively applying ML models
on data without specifying the numerical schemes can lead to significant errors
and misleading conclusions. Our SPDEBench provides an open-source codebase that
ensures full reproducibility of benchmarking across a variety of SPDE datasets
while offering the flexibility to incorporate new datasets and machine learning
baselines, making it a valuable resource for the community.",2025-05-24,"Zheyan Li, Yuantu Zhu, Hao Ni, Siran Li, Bingguang Chen, Qi Meng",http://arxiv.org/pdf/2505.18511v1,cs.LG
How Particle System Theory Enhances Hypergraph Message Passing,"Hypergraphs effectively model higher-order relationships in natural
phenomena, capturing complex interactions beyond pairwise connections. We
introduce a novel hypergraph message passing framework inspired by interacting
particle systems, where hyperedges act as fields inducing shared node dynamics.
By incorporating attraction, repulsion, and Allen-Cahn forcing terms, particles
of varying classes and features achieve class-dependent equilibrium, enabling
separability through the particle-driven message passing. We investigate both
first-order and second-order particle system equations for modeling these
dynamics, which mitigate over-smoothing and heterophily thus can capture
complete interactions. The more stable second-order system permits deeper
message passing. Furthermore, we enhance deterministic message passing with
stochastic element to account for interaction uncertainties. We prove
theoretically that our approach mitigates over-smoothing by maintaining a
positive lower bound on the hypergraph Dirichlet energy during propagation and
thus to enable hypergraph message passing to go deep. Empirically, our models
demonstrate competitive performance on diverse real-world hypergraph node
classification tasks, excelling on both homophilic and heterophilic datasets.",2025-05-24,"Yixuan Ma, Kai Yi, Pietro Lio, Shi Jin, Yu Guang Wang",http://arxiv.org/pdf/2505.18505v1,cs.LG
Knowledge Grafting of Large Language Models,"Cross-capability transfer is a key challenge in large language model (LLM)
research, with applications in multi-task integration, model compression, and
continual learning. Recent works like FuseLLM and FuseChat have demonstrated
the potential of transferring multiple model capabilities to lightweight
models, enhancing adaptability and efficiency, which motivates our
investigation into more efficient cross-capability transfer methods. However,
existing approaches primarily focus on small, homogeneous models, limiting
their applicability. For large, heterogeneous models, knowledge distillation
with full-parameter fine-tuning often overlooks the student model's intrinsic
capacity and risks catastrophic forgetting, while PEFT methods struggle to
effectively absorb knowledge from source LLMs. To address these issues, we
introduce GraftLLM, a novel method that stores source model capabilities in a
target model with SkillPack format. This approach preserves general
capabilities, reduces parameter conflicts, and supports forget-free continual
learning and model fusion. We employ a module-aware adaptive compression
strategy to compress parameter updates, ensuring efficient storage while
maintaining task-specific knowledge. The resulting SkillPack serves as a
compact and transferable knowledge carrier, ideal for heterogeneous model
fusion and continual learning. Experiments across various scenarios demonstrate
that GraftLLM outperforms existing techniques in knowledge transfer, knowledge
fusion, and forget-free learning, providing a scalable and efficient solution
for cross-capability transfer. The code is publicly available at:
https://github.com/duguodong7/GraftLLM.",2025-05-24,"Guodong Du, Xuanning Zhou, Junlin Li, Zhuo Li, Zesheng Shi, Wanyu Lin, Ho-Kin Tang, Xiucheng Li, Fangming Liu, Wenya Wang, Min Zhang, Jing Li",http://arxiv.org/pdf/2505.18502v1,cs.LG
G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning,"Although Large Language Models (LLMs) have demonstrated remarkable progress,
their proficiency in graph-related tasks remains notably limited, hindering the
development of truly general-purpose models. Previous attempts, including
pretraining graph foundation models or employing supervised fine-tuning, often
face challenges such as the scarcity of large-scale, universally represented
graph data. We introduce G1, a simple yet effective approach demonstrating that
Reinforcement Learning (RL) on synthetic graph-theoretic tasks can
significantly scale LLMs' graph reasoning abilities. To enable RL training, we
curate Erd\~os, the largest graph reasoning dataset to date comprising 50
diverse graph-theoretic tasks of varying difficulty levels, 100k training data
and 5k test data, all drived from real-world graphs. With RL on Erd\~os, G1
obtains substantial improvements in graph reasoning, where our finetuned 3B
model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also
show strong zero-shot generalization to unseen tasks, domains, and graph
encoding schemes, including other graph-theoretic benchmarks as well as
real-world node classification and link prediction tasks, without compromising
general reasoning abilities. Our findings offer an efficient, scalable path for
building strong graph reasoners by finetuning LLMs with RL on graph-theoretic
tasks, which combines the strengths of pretrained LLM capabilities with
abundant, automatically generated synthetic data, suggesting that LLMs possess
graph understanding abilities that RL can elicit successfully.",2025-05-24,"Xiaojun Guo, Ang Li, Yifei Wang, Stefanie Jegelka, Yisen Wang",http://arxiv.org/pdf/2505.18499v1,cs.LG
Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking,"Masked diffusion models (MDM) are powerful generative models for discrete
data that generate samples by progressively unmasking tokens in a sequence.
Each token can take one of two states: masked or unmasked. We observe that
token sequences often remain unchanged between consecutive sampling steps;
consequently, the model repeatedly processes identical inputs, leading to
redundant computation. To address this inefficiency, we propose the Partial
masking scheme (Prime), which augments MDM by allowing tokens to take
intermediate states interpolated between the masked and unmasked states. This
design enables the model to make predictions based on partially observed token
information, and facilitates a fine-grained denoising process. We derive a
variational training objective and introduce a simple architectural design to
accommodate intermediate-state inputs. Our method demonstrates superior
performance across a diverse set of generative modeling tasks. On text data, it
achieves a perplexity of 15.36 on OpenWebText, outperforming previous MDM
(21.52), autoregressive models (17.54), and their hybrid variants (17.58),
without relying on an autoregressive formulation. On image data, it attains
competitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32, comparable
to leading continuous generative models.",2025-05-24,"Chen-Hao Chao, Wei-Fang Sun, Hanwen Liang, Chun-Yi Lee, Rahul G. Krishnan",http://arxiv.org/pdf/2505.18495v1,cs.LG
FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased Aggregation,"Federated Learning (FL) facilitates the fine-tuning of Foundation Models
(FMs) using distributed data sources, with Low-Rank Adaptation (LoRA) gaining
popularity due to its low communication costs and strong performance. While
recent work acknowledges the benefits of heterogeneous LoRA in FL and
introduces flexible algorithms to support its implementation, our theoretical
analysis reveals a critical gap: existing methods lack formal convergence
guarantees due to parameter truncation and biased gradient updates.
Specifically, adapting client-specific LoRA ranks necessitates truncating
global parameters, which introduces inherent truncation errors and leads to
subsequent inaccurate gradient updates that accumulate over training rounds,
ultimately degrading performance. To address the above issues, we propose
\textbf{FedHL}, a simple yet effective \textbf{Fed}erated Learning framework
tailored for \textbf{H}eterogeneous \textbf{L}oRA. By leveraging the full-rank
global model as a calibrated aggregation basis, FedHL eliminates the direct
truncation bias from initial alignment with client-specific ranks. Furthermore,
we derive the theoretically optimal aggregation weights by minimizing the
gradient drift term in the convergence upper bound. Our analysis shows that
FedHL guarantees $\mathcal{O}(1/\sqrt{T})$ convergence rate, and experiments on
multiple real-world datasets demonstrate a 1-3\% improvement over several
state-of-the-art methods.",2025-05-24,"Zihao Peng, Jiandian Zeng, Boyuan Li, Guo Li, Shengbo Chen, Tian Wang",http://arxiv.org/pdf/2505.18494v1,cs.LG
Statistical Inference under Performativity,"Performativity of predictions refers to the phenomena that
prediction-informed decisions may influence the target they aim to predict,
which is widely observed in policy-making in social sciences and economics. In
this paper, we initiate the study of statistical inference under
performativity. Our contribution is two-fold. First, we build a central limit
theorem for estimation and inference under performativity, which enables
inferential purposes in policy-making such as constructing confidence intervals
or testing hypotheses. Second, we further leverage the derived central limit
theorem to investigate prediction-powered inference (PPI) under performativity,
which is based on a small labeled dataset and a much larger dataset of
machine-learning predictions. This enables us to obtain more precise estimation
and improved confidence regions for the model parameter (i.e., policy) of
interest in performative prediction. We demonstrate the power of our framework
by numerical experiments. To the best of our knowledge, this paper is the first
one to establish statistical inference under performativity, which brings up
new challenges and inference settings that we believe will add significant
values to policy-making, statistics, and machine learning.",2025-05-24,"Xiang Li, Yunai Li, Huiying Zhong, Lihua Lei, Zhun Deng",http://arxiv.org/pdf/2505.18493v1,cs.LG
Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications,"Error correction is an important capability when applying large language
models (LLMs) to facilitate user typing on mobile devices. In this paper, we
use LLMs to synthesize a high-quality dataset of error correction pairs to
evaluate and improve LLMs for mobile applications. We first prompt LLMs with
error correction domain knowledge to build a scalable and reliable addition to
the existing data synthesis pipeline. We then adapt the synthetic data
distribution to match the mobile application domain by reweighting the samples.
The reweighting model is learnt by predicting (a handful of) live A/B test
metrics when deploying LLMs in production, given the LLM performance on offline
evaluation data and scores from a small privacy-preserving on-device language
model. Finally, we present best practices for mixing our synthetic data with
other data sources to improve model performance on error correction in both
offline evaluation and production live A/B testing.",2025-05-24,"Yanxiang Zhang, Zheng Xu, Shanshan Wu, Yuanbo Zhang, Daniel Ramage",http://arxiv.org/pdf/2505.18488v1,cs.LG
Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning,"Learning effective visual representations for robotic manipulation remains a
fundamental challenge due to the complex body dynamics involved in action
execution. In this paper, we study how visual representations that carry
body-relevant cues can enable efficient policy learning for downstream robotic
manipulation tasks. We present $\textbf{I}$nter-token $\textbf{Con}$trast
($\textbf{ICon}$), a contrastive learning method applied to the token-level
representations of Vision Transformers (ViTs). ICon enforces a separation in
the feature space between agent-specific and environment-specific tokens,
resulting in agent-centric visual representations that embed body-specific
inductive biases. This framework can be seamlessly integrated into end-to-end
policy learning by incorporating the contrastive loss as an auxiliary
objective. Our experiments show that ICon not only improves policy performance
across various manipulation tasks but also facilitates policy transfer across
different robots. The project website: https://github.com/HenryWJL/icon",2025-05-24,"Junlin Wang, Zhiyun Lin",http://arxiv.org/pdf/2505.18487v1,cs.LG
"Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek","Large language models (LLMs) have been widely explored for automated scoring
in low-stakes assessment to facilitate learning and instruction. Empirical
evidence related to which LLM produces the most reliable scores and induces
least rater effects needs to be collected before the use of LLMs for automated
scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,
ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini
2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in
scoring two types of writing tasks. The accuracy of the holistic and analytic
scores from LLMs compared with human raters was evaluated in terms of Quadratic
Weighted Kappa. Intra-rater consistency across prompts was compared in terms of
Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human
raters using the Many-Facet Rasch model. The results in general supported the
use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring
accuracy, better rater reliability, and less rater effects.",2025-05-24,"Hong Jiao, Dan Song, Won-Chan Lee",http://arxiv.org/pdf/2505.18486v1,cs.LG
The Prompt is Mightier than the Example,"Numerous recent prompt optimization approaches like chain-of-thought, have
been demonstrated to significantly improve the quality of content generated by
large language models (LLMs). In-context learning (ICL), a recent paradigm
where a few representative examples guide content generation has also led to
strong improvements in generation quality of LLM generated content. This idea
has been applied to great effect in synthetic tabular data generation, where
LLMs, through effective use of ICL and prompt optimization, can generate data
that approximate samples from complex, heterogeneous distributions based on
representative examples. However, ensuring high-fidelity synthetic data often
requires a very large number of ICL examples which may be unavailable or costly
to obtain. At the same time, as LLMs get larger and larger, their in-built
prior knowledge becomes vast and can potentially substitute for specific data
examples. In this paper, we introduce Knowledge-Guided Prompting (KGP) as a new
knob in prompt optimization and explore the ability of KGP-based prompt
optimization to offset the cost of ICL. Specifically, we explore the question
`how many examples can a prompt substitute for?' and explore knowledge-guided
prompting (KGP) where domain knowledge, either inferred or available, is
explicitly injected into the prompt, reducing dependence on ICL examples. Our
experiments systematically explore the trade-off between ICL and KGP, revealing
an empirical scaling law that quantifies how quality of generated synthetic
data varies with increasing domain knowledge and decreasing example count. Our
results demonstrate that knowledge-guided prompting can be a scalable
alternative, or addition, to in-context examples, unlocking new approaches to
synthetic data generation.",2025-05-24,"Shengzhe Xu, Nikhil Muralidhar, Naren Ramakrishnan",http://arxiv.org/pdf/2505.18485v1,cs.LG
Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey,"Graphs are a widely used paradigm for representing non-Euclidean data, with
applications ranging from social network analysis to biomolecular prediction.
Conventional graph learning approaches typically rely on fixed structural
assumptions or fully observed data, limiting their effectiveness in more
complex, noisy, or evolving settings. Consequently, real-world graph data often
violates the assumptions of traditional graph learning methods, in particular,
it leads to four fundamental challenges: (1) Incompleteness, real-world graphs
have missing nodes, edges, or attributes; (2) Imbalance, the distribution of
the labels of nodes or edges and their structures for real-world graphs are
highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains
exhibit incompatible feature spaces or structural patterns; and (4) Dynamic
Instability, graphs evolve over time in unpredictable ways. Recent advances in
Large Language Models (LLMs) offer the potential to tackle these challenges by
leveraging rich semantic reasoning and external knowledge. This survey provides
a comprehensive review of how LLMs can be integrated with graph learning to
address the aforementioned challenges. For each challenge, we review both
traditional solutions and modern LLM-driven approaches, highlighting how LLMs
contribute unique advantages. Finally, we discuss open research questions and
promising future directions in this emerging interdisciplinary field. To
support further exploration, we have curated a repository of recent advances on
graph learning challenges:
https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.",2025-05-24,"Mengran Li, Pengyu Zhang, Wenbin Xing, Yijia Zheng, Klim Zaporojets, Junzhou Chen, Ronghui Zhang, Yong Zhang, Siyuan Gong, Jia Hu, Xiaolei Ma, Zhiyuan Liu, Paul Groth, Marcel Worring",http://arxiv.org/pdf/2505.18475v1,cs.LG
Performance and Generalizability Impacts of Incorporating Geolocation into Deep Learning for Dynamic PM2.5 Estimation,"Deep learning models have demonstrated success in geospatial applications,
yet quantifying the role of geolocation information in enhancing model
performance and geographic generalizability remains underexplored. A new
generation of location encoders have emerged with the goal of capturing
attributes present at any given location for downstream use in predictive
modeling. Being a nascent area of research, their evaluation has remained
largely limited to static tasks such as species distributions or average
temperature mapping. In this paper, we discuss and quantify the impact of
incorporating geolocation into deep learning for a real-world application
domain that is characteristically dynamic (with fast temporal change) and
spatially heterogeneous at high resolutions: estimating surface-level daily
PM2.5 levels using remotely sensed and ground-level data. We build on a
recently published deep learning-based PM2.5 estimation model that achieves
state-of-the-art performance on data observed in the contiguous United States.
We examine three approaches for incorporating geolocation: excluding
geolocation as a baseline, using raw geographic coordinates, and leveraging
pretrained location encoders. We evaluate each approach under within-region
(WR) and out-of-region (OoR) evaluation scenarios. Aggregate performance
metrics indicate that while na\""ive incorporation of raw geographic coordinates
improves within-region performance by retaining the interpolative value of
geographic location, it can hinder generalizability across regions. In
contrast, pretrained location encoders like GeoCLIP enhance predictive
performance and geographic generalizability for both WR and OoR scenarios.
However, qualitative analysis reveals artifact patterns caused by high-degree
basis functions and sparse upstream samples in certain areas, and ablation
results indicate varying performance among location encoders...",2025-05-24,"Morteza Karimzadeh, Zhongying Wang, James L. Crooks",http://arxiv.org/pdf/2505.18461v1,cs.LG
A Survey of LLM $\times$ DATA,"The integration of large language model (LLM) and data management (DATA) is
rapidly redefining both domains. In this survey, we comprehensively review the
bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale
data processing, storage, and serving, feeds LLMs with high quality, diversity,
and timeliness of data required for stages like pre-training, post-training,
retrieval-augmented generation, and agentic workflows: (i) Data processing for
LLMs includes scalable acquisition, deduplication, filtering, selection, domain
mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on
efficient data and model formats, distributed and heterogeneous storage
hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data
serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),
LLM inference (e.g., prompt compression, data provenance), and training
strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,
LLMs are emerging as general-purpose engines for data management. We review
recent advances in (i) data manipulation, including automatic data cleaning,
integration, discovery; (ii) data analysis, covering reasoning over structured,
semi-structured, and unstructured data, and (iii) system optimization (e.g.,
configuration tuning, query rewriting, anomaly diagnosis), powered by LLM
techniques like retrieval-augmented prompting, task-specialized fine-tuning,
and multi-agent collaboration.",2025-05-24,"Xuanhe Zhou, Junxuan He, Wei Zhou, Haodong Chen, Zirui Tang, Haoyu Zhao, Xin Tong, Guoliang Li, Youmin Chen, Jun Zhou, Zhaojun Sun, Binyuan Hui, Shuo Wang, Conghui He, Zhiyuan Liu, Jingren Zhou, Fan Wu",http://arxiv.org/pdf/2505.18458v1,cs.LG
EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks,"This paper introduces EdgeAgentX, a novel framework integrating federated
learning (FL), multi-agent reinforcement learning (MARL), and adversarial
defense mechanisms, tailored for military communication networks. EdgeAgentX
significantly improves autonomous decision-making, reduces latency, enhances
throughput, and robustly withstands adversarial disruptions, as evidenced by
comprehensive simulations.",2025-05-24,Abir Ray,http://arxiv.org/pdf/2505.18457v1,cs.LG
Anchored Diffusion Language Model,"Diffusion Language Models (DLMs) promise parallel generation and
bidirectional context, yet they underperform autoregressive (AR) models in both
likelihood modeling and generated text quality. We identify that this
performance gap arises when important tokens (e.g., key words or low-frequency
words that anchor a sentence) are masked early in the forward process, limiting
contextual information for accurate reconstruction. To address this, we
introduce the Anchored Diffusion Language Model (ADLM), a novel two-stage
framework that first predicts distributions over important tokens via an anchor
network, and then predicts the likelihoods of missing tokens conditioned on the
anchored predictions. ADLM significantly improves test perplexity on LM1B and
OpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap
with strong AR baselines. It also achieves state-of-the-art performance in
zero-shot generalization across seven benchmarks and surpasses AR models in
MAUVE score, which marks the first time a DLM generates better human-like text
than an AR model. Theoretically, we derive an Anchored Negative Evidence Lower
Bound (ANELBO) objective and show that anchoring improves sample complexity and
likelihood modeling. Beyond diffusion, anchoring boosts performance in AR
models and enhances reasoning in math and logic tasks, outperforming existing
chain-of-thought approaches",2025-05-24,"Litu Rout, Constantine Caramanis, Sanjay Shakkottai",http://arxiv.org/pdf/2505.18456v1,cs.LG
On Minimax Estimation of Parameters in Softmax-Contaminated Mixture of Experts,"The softmax-contaminated mixture of experts (MoE) model is deployed when a
large-scale pre-trained model, which plays the role of a fixed expert, is
fine-tuned for learning downstream tasks by including a new contamination part,
or prompt, functioning as a new, trainable expert. Despite its popularity and
relevance, the theoretical properties of the softmax-contaminated MoE have
remained unexplored in the literature. In the paper, we study the convergence
rates of the maximum likelihood estimator of gating and prompt parameters in
order to gain insights into the statistical properties and potential challenges
of fine-tuning with a new prompt. We find that the estimability of these
parameters is compromised when the prompt acquires overlapping knowledge with
the pre-trained model, in the sense that we make precise by formulating a novel
analytic notion of distinguishability. Under distinguishability of the
pre-trained and prompt models, we derive minimax optimal estimation rates for
all the gating and prompt parameters. By contrast, when the distinguishability
condition is violated, these estimation rates become significantly slower due
to their dependence on the prompt convergence rate to the pre-trained model.
Finally, we empirically corroborate our theoretical findings through several
numerical experiments.",2025-05-24,"Fanqi Yan, Huy Nguyen, Dung Le, Pedram Akbarian, Nhat Ho, Alessandro Rinaldo",http://arxiv.org/pdf/2505.18455v1,cs.LG
$μ$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts,"To tackle the huge computational demand of large foundation models,
activation-aware compression techniques without retraining have been
introduced. However, since these rely on calibration data, domain shift may
arise for unknown downstream tasks. With a computationally efficient
calibration, activation-aware pruning can be executed for every prompt
adaptively, yet achieving reduced complexity at inference. We formulate it as a
mixture of micro-experts, called $\mu$-MoE. Several experiments demonstrate
that $\mu$-MoE can dynamically adapt to task/prompt-dependent structured
sparsity on the fly.",2025-05-24,"Toshiaki Koike-Akino, Jing Liu, Ye Wang",http://arxiv.org/pdf/2505.18451v1,cs.LG
Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning,"Transfer reinforcement learning aims to derive a near-optimal policy for a
target environment with limited data by leveraging abundant data from related
source domains. However, it faces two key challenges: the lack of performance
guarantees for the transferred policy, which can lead to undesired actions, and
the risk of negative transfer when multiple source domains are involved. We
propose a novel framework based on the pessimism principle, which constructs
and optimizes a conservative estimation of the target domain's performance. Our
framework effectively addresses the two challenges by providing an optimized
lower bound on target performance, ensuring safe and reliable decisions, and by
exhibiting monotonic improvement with respect to the quality of the source
domains, thereby avoiding negative transfer. We construct two types of
conservative estimations, rigorously characterize their effectiveness, and
develop efficient distributed algorithms with convergence guarantees. Our
framework provides a theoretically sound and practically robust solution for
transfer learning in reinforcement learning.",2025-05-24,"Chi Zhang, Ziying Jia, George K. Atia, Sihong He, Yue Wang",http://arxiv.org/pdf/2505.18447v1,cs.LG
Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting,"Time-series forecasting plays a critical role in many real-world
applications. Although increasingly powerful models have been developed and
achieved superior results on benchmark datasets, through a fine-grained
sample-level inspection, we find that (i) no single model consistently
outperforms others across different test samples, but instead (ii) each model
excels in specific cases. These findings prompt us to explore how to adaptively
leverage the distinct strengths of various forecasting models for different
samples. We introduce TimeFuse, a framework for collective time-series
forecasting with sample-level adaptive fusion of heterogeneous models. TimeFuse
utilizes meta-features to characterize input time series and trains a learnable
fusor to predict optimal model fusion weights for any given input. The fusor
can leverage samples from diverse datasets for joint training, allowing it to
adapt to a wide variety of temporal patterns and thus generalize to new inputs,
even from unseen datasets. Extensive experiments demonstrate the effectiveness
of TimeFuse in various long-/short-term forecasting tasks, achieving
near-universal improvement over the state-of-the-art individual models. Code is
available at https://github.com/ZhiningLiu1998/TimeFuse.",2025-05-24,"Zhining Liu, Ze Yang, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Yada Zhu, Hendrik Hamann, Jingrui He, Hanghang Tong",http://arxiv.org/pdf/2505.18442v1,cs.LG
DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces,"Dictionary learning has recently emerged as a promising approach for
mechanistic interpretability of large transformer models. Disentangling
high-dimensional transformer embeddings, however, requires algorithms that
scale to high-dimensional data with large sample sizes. Recent work has
explored sparse autoencoders (SAEs) for this problem. However, SAEs use a
simple linear encoder to solve the sparse encoding subproblem, which is known
to be NP-hard. It is therefore interesting to understand whether this structure
is sufficient to find good solutions to the dictionary learning problem or if a
more sophisticated algorithm could find better solutions. In this work, we
propose Double-Batch KSVD (DB-KSVD), a scalable dictionary learning algorithm
that adapts the classic KSVD algorithm. DB-KSVD is informed by the rich
theoretical foundations of KSVD but scales to datasets with millions of samples
and thousands of dimensions. We demonstrate the efficacy of DB-KSVD by
disentangling embeddings of the Gemma-2-2B model and evaluating on six metrics
from the SAEBench benchmark, where we achieve competitive results when compared
to established approaches based on SAEs. By matching SAE performance with an
entirely different optimization approach, our results suggest that (i) SAEs do
find strong solutions to the dictionary learning problem and (ii) that
traditional optimization approaches can be scaled to the required problem
sizes, offering a promising avenue for further research. We provide an
implementation of DB-KSVD at https://github.com/RomeoV/KSVD.jl.",2025-05-24,"Romeo Valentin, Sydney M. Katz, Vincent Vanhoucke, Mykel J. Kochenderfer",http://arxiv.org/pdf/2505.18441v1,cs.LG
Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning,"Actor-critic methods for decentralized multi-agent reinforcement learning
(MARL) facilitate collaborative optimal decision making without centralized
coordination, thus enabling a wide range of applications in practice. To date,
however, most theoretical convergence studies for existing actor-critic
decentralized MARL methods are limited to the guarantee of a stationary
solution under the linear function approximation. This leaves a significant gap
between the highly successful use of deep neural actor-critic for decentralized
MARL in practice and the current theoretical understanding. To bridge this gap,
in this paper, we make the first attempt to develop a deep neural actor-critic
method for decentralized MARL, where both the actor and critic components are
inherently non-linear. We show that our proposed method enjoys a global
optimality guarantee with a finite-time convergence rate of O(1/T), where T is
the total iteration times. This marks the first global convergence result for
deep neural actor-critic methods in the MARL literature. We also conduct
extensive numerical experiments, which verify our theoretical results.",2025-05-24,"Zhiyao Zhang, Myeung Suk Oh, FNU Hairi, Ziyue Luo, Alvaro Velasquez, Jia Liu",http://arxiv.org/pdf/2505.18433v1,cs.LG
Development of Interactive Nomograms for Predicting Short-Term Survival in ICU Patients with Aplastic Anemia,"Aplastic anemia is a rare, life-threatening hematologic disorder
characterized by pancytopenia and bone marrow failure. ICU admission in these
patients often signals critical complications or disease progression, making
early risk assessment crucial for clinical decision-making and resource
allocation. In this study, we used the MIMIC-IV database to identify ICU
patients diagnosed with aplastic anemia and extracted clinical features from
five domains: demographics, synthetic indicators, laboratory results,
comorbidities, and medications. Over 400 variables were reduced to seven key
predictors through machine learning-based feature selection. Logistic
regression and Cox regression models were constructed to predict 7-, 14-, and
28-day mortality, and their performance was evaluated using AUROC. External
validation was conducted using the eICU Collaborative Research Database to
assess model generalizability. Among 1,662 included patients, the logistic
regression model demonstrated superior performance, with AUROC values of
0.8227, 0.8311, and 0.8298 for 7-, 14-, and 28-day mortality, respectively,
compared to the Cox model. External validation yielded AUROCs of 0.7391,
0.7119, and 0.7093. Interactive nomograms were developed based on the logistic
regression model to visually estimate individual patient risk. In conclusion,
we identified a concise set of seven predictors, led by APS III, to build
validated and generalizable nomograms that accurately estimate short-term
mortality in ICU patients with aplastic anemia. These tools may aid clinicians
in personalized risk stratification and decision-making at the point of care.",2025-05-23,"Junyi Fan, Shuheng Chen, Li Sun, Yong Si, Elham Pishgar, Kamiar Alaei, Greg Placencia, Maryam Pishgar",http://arxiv.org/pdf/2505.18421v1,cs.LG
LocalKMeans: Convergence of Lloyd's Algorithm with Distributed Local Iterations,"In this paper, we analyze the classical $K$-means alternating-minimization
algorithm, also known as Lloyd's algorithm (Lloyd, 1956), for a mixture of
Gaussians in a data-distributed setting that incorporates local iteration
steps. Assuming unlabeled data distributed across multiple machines, we propose
an algorithm, LocalKMeans, that performs Lloyd's algorithm in parallel in the
machines by running its iterations on local data, synchronizing only every $L$
of such local steps. We characterize the cost of these local iterations against
the non-distributed setting, and show that the price paid for the local steps
is a higher required signal-to-noise ratio. While local iterations were
theoretically studied in the past for gradient-based learning methods, the
analysis of unsupervised learning methods is more involved owing to the
presence of latent variables, e.g. cluster identities, than that of an
iterative gradient-based algorithm. To obtain our results, we adapt a virtual
iterate method to work with a non-convex, non-smooth objective function, in
conjunction with a tight statistical analysis of Lloyd steps.",2025-05-23,"Harsh Vardhan, Heng Zhu, Avishek Ghosh, Arya Mazumdar",http://arxiv.org/pdf/2505.18420v1,cs.LG
Reinforcement Learning for Ballbot Navigation in Uneven Terrain,"Ballbot (i.e. Ball balancing robot) navigation usually relies on methods
rooted in control theory (CT), and works that apply Reinforcement learning (RL)
to the problem remain rare while generally being limited to specific subtasks
(e.g. balance recovery). Unlike CT based methods, RL does not require
(simplifying) assumptions about environment dynamics (e.g. the absence of
slippage between the ball and the floor). In addition to this increased
accuracy in modeling, RL agents can easily be conditioned on additional
observations such as depth-maps without the need for explicit formulations from
first principles, leading to increased adaptivity. Despite those advantages,
there has been little to no investigation into the capabilities,
data-efficiency and limitations of RL based methods for ballbot control and
navigation. Furthermore, there is a notable absence of an open-source,
RL-friendly simulator for this task. In this paper, we present an open-source
ballbot simulation based on MuJoCo, and show that with appropriate conditioning
on exteroceptive observations as well as reward shaping, policies learned by
classical model-free RL methods are capable of effectively navigating through
randomly generated uneven terrain, using a reasonable amount of data (four to
five hours on a system operating at 500hz).",2025-05-23,Achkan Salehi,http://arxiv.org/pdf/2505.18417v1,cs.LG
A Dual Basis Approach for Structured Robust Euclidean Distance Geometry,"Euclidean Distance Matrix (EDM), which consists of pairwise squared Euclidean
distances of a given point configuration, finds many applications in modern
machine learning. This paper considers the setting where only a set of anchor
nodes is used to collect the distances between themselves and the rest. In the
presence of potential outliers, it results in a structured partial observation
on EDM with partial corruptions. Note that an EDM can be connected to a
positive semi-definite Gram matrix via a non-orthogonal dual basis. Inspired by
recent development of non-orthogonal dual basis in optimization, we propose a
novel algorithmic framework, dubbed Robust Euclidean Distance Geometry via Dual
Basis (RoDEoDB), for recovering the Euclidean distance geometry, i.e., the
underlying point configuration. The exact recovery guarantees have been
established in terms of both the Gram matrix and point configuration, under
some mild conditions. Empirical experiments show superior performance of
RoDEoDB on sensor localization and molecular conformation datasets.",2025-05-23,"Chandra Kundu, Abiy Tasissa, HanQin Cai",http://arxiv.org/pdf/2505.18414v1,cs.LG
LatentLLM: Attention-Aware Joint Tensor Compression,"Modern foundation models such as large language models (LLMs) and large
multi-modal models (LMMs) require a massive amount of computational and memory
resources. We propose a new framework to convert such LLMs/LMMs into a
reduced-dimension latent structure. Our method extends a local activation-aware
tensor decomposition to a global attention-aware joint tensor de-composition.
Our framework can significantly improve the model accuracy over the existing
model compression methods when reducing the latent dimension to realize
computationally/memory-efficient LLMs/LLMs. We show the benefit on several
benchmark including multi-modal reasoning tasks.",2025-05-23,"Toshiaki Koike-Akino, Xiangyu Chen, Jing Liu, Ye Wang, Pu, Wang, Matthew Brand",http://arxiv.org/pdf/2505.18413v1,cs.LG
DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding,"We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance
multi-modal Temporal Point Process (TPP) modeling in the era of Large Language
Models (LLMs). While TPPs have been widely studied for modeling temporal event
sequences, existing datasets are predominantly unimodal, hindering progress in
models that require joint reasoning over temporal, textual, and visual
information. To address this gap, DanmakuTPPBench comprises two complementary
components: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili
video platform, where user-generated bullet comments (Danmaku) naturally form
multi-modal events annotated with precise timestamps, rich textual content, and
corresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering
dataset constructed via a novel multi-agent pipeline powered by
state-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex
temporal-textual-visual reasoning. We conduct extensive evaluations using both
classical TPP models and recent MLLMs, revealing significant performance gaps
and limitations in current methods' ability to model multi-modal event
dynamics. Our benchmark establishes strong baselines and calls for further
integration of TPP modeling into the multi-modal language modeling landscape.
The code and dataset have been released at
https://github.com/FRENKIE-CHIANG/DanmakuTPPBench",2025-05-23,"Yue Jiang, Jichu Li, Yang Liu, Dingkang Yang, Feng Zhou, Quyu Kong",http://arxiv.org/pdf/2505.18411v1,cs.LG
Identifiability of latent causal graphical models without pure children,"This paper considers a challenging problem of identifying a causal graphical
model under the presence of latent variables. While various identifiability
conditions have been proposed in the literature, they often require multiple
pure children per latent variable or restrictions on the latent causal graph.
Furthermore, it is common for all observed variables to exhibit the same
modality. Consequently, the existing identifiability conditions are often too
stringent for complex real-world data. We consider a general nonparametric
measurement model with arbitrary observed variable types and binary latent
variables, and propose a double triangular graphical condition that guarantees
identifiability of the entire causal graphical model. The proposed condition
significantly relaxes the popular pure children condition. We also establish
necessary conditions for identifiability and provide valuable insights into
fundamental limits of identifiability. Simulation studies verify that latent
structures satisfying our conditions can be accurately estimated from data.",2025-05-23,"Seunghyun Lee, Yuqi Gu",http://arxiv.org/pdf/2505.18410v1,cs.LG
KL-regularization Itself is Differentially Private in Bandits and RLHF,"Differential Privacy (DP) provides a rigorous framework for privacy, ensuring
the outputs of data-driven algorithms remain statistically indistinguishable
across datasets that differ in a single entry. While guaranteeing DP generally
requires explicitly injecting noise either to the algorithm itself or to its
outputs, the intrinsic randomness of existing algorithms presents an
opportunity to achieve DP ``for free''. In this work, we explore the role of
regularization in achieving DP across three different decision-making problems:
multi-armed bandits, linear contextual bandits, and reinforcement learning from
human feedback (RLHF), in offline data settings. We show that adding
KL-regularization to the learning objective (a common approach in optimization
algorithms) makes the action sampled from the resulting stochastic policy
itself differentially private. This offers a new route to privacy guarantees
without additional noise injection, while also preserving the inherent
advantage of regularization in enhancing performance.",2025-05-23,"Yizhou Zhang, Kishan Panaganti, Laixi Shi, Juba Ziani, Adam Wierman",http://arxiv.org/pdf/2505.18407v1,cs.LG
Thought calibration: Efficient and confident test-time scaling,"Reasoning large language models achieve impressive test-time scaling by
thinking for longer, but this performance gain comes at significant compute
cost. Directly limiting test-time budget hurts overall performance, but not all
problems are equally difficult. We propose thought calibration to decide
dynamically when thinking can be terminated. To calibrate our decision rule, we
view a language model's growing body of thoughts as a nested sequence of
reasoning trees, where the goal is to identify the point at which novel
reasoning plateaus. We realize this framework through lightweight probes that
operate on top of the language model's hidden representations, which are
informative of both the reasoning structure and overall consistency of
response. Based on three reasoning language models and four datasets, thought
calibration preserves model performance with up to a 60% reduction in thinking
tokens on in-distribution data, and up to 20% in out-of-distribution data.",2025-05-23,"Menghua Wu, Cai Zhou, Stephen Bates, Tommi Jaakkola",http://arxiv.org/pdf/2505.18404v1,cs.LG
Taming Diffusion for Dataset Distillation with High Representativeness,"Recent deep learning models demand larger datasets, driving the need for
dataset distillation to create compact, cost-efficient datasets while
maintaining performance. Due to the powerful image generation capability of
diffusion, it has been introduced to this field for generating distilled
images. In this paper, we systematically investigate issues present in current
diffusion-based dataset distillation methods, including inaccurate distribution
matching, distribution deviation with random noise, and separate sampling.
Building on this, we propose D^3HR, a novel diffusion-based framework to
generate distilled datasets with high representativeness. Specifically, we
adopt DDIM inversion to map the latents of the full dataset from a
low-normality latent domain to a high-normality Gaussian domain, preserving
information and ensuring structural consistency to generate representative
latents for the distilled dataset. Furthermore, we propose an efficient
sampling scheme to better align the representative latents with the
high-normality Gaussian distribution. Our comprehensive experiments demonstrate
that D^3HR can achieve higher accuracy across different model architectures
compared with state-of-the-art baselines in dataset distillation. Source code:
https://github.com/lin-zhao-resoLve/D3HR.",2025-05-23,"Lin Zhao, Yushu Wu, Xinru Jiang, Jianyang Gu, Yanzhi Wang, Xiaolin Xu, Pu Zhao, Xue Lin",http://arxiv.org/pdf/2505.18399v1,cs.LG
An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems,"Multi-agent AI systems (MAS) offer a promising framework for distributed
intelligence, enabling collaborative reasoning, planning, and decision-making
across autonomous agents. This paper provides a systematic outlook on the
current opportunities and challenges of MAS, drawing insights from recent
advances in large language models (LLMs), federated optimization, and human-AI
interaction. We formalize key concepts including agent topology, coordination
protocols, and shared objectives, and identify major risks such as dependency,
misalignment, and vulnerabilities arising from training data overlap. Through a
biologically inspired simulation and comprehensive theoretical framing, we
highlight critical pathways for developing robust, scalable, and secure MAS in
real-world settings.",2025-05-23,"Fangqiao Tian, An Luo, Jin Du, Xun Xian, Robert Specht, Ganghua Wang, Xuan Bi, Jiawei Zhou, Jayanth Srinivasa, Ashish Kundu, Charles Fleming, Rui Zhang, Zirui Liu, Mingyi Hong, Jie Ding",http://arxiv.org/pdf/2505.18397v1,cs.LG
Applications of Modular Co-Design for De Novo 3D Molecule Generation,"De novo 3D molecule generation is a pivotal task in drug discovery. However,
many recent geometric generative models struggle to produce high-quality 3D
structures, even if they maintain 2D validity and topological stability. To
tackle this issue and enhance the learning of effective molecular generation
dynamics, we present Megalodon-a family of scalable transformer models. These
models are enhanced with basic equivariant layers and trained using a joint
continuous and discrete denoising co-design objective. We assess Megalodon's
performance on established molecule generation benchmarks and introduce new 3D
structure benchmarks that evaluate a model's capability to generate realistic
molecular structures, particularly focusing on energetics. We show that
Megalodon achieves state-of-the-art results in 3D molecule generation,
conditional structure generation, and structure energy benchmarks using
diffusion and flow matching. Furthermore, doubling the number of parameters in
Megalodon to 40M significantly enhances its performance, generating up to 49x
more valid large molecules and achieving energy levels that are 2-10x lower
than those of the best prior generative models.",2025-05-23,"Danny Reidenbach, Filipp Nikitin, Olexandr Isayev, Saee Paliwal",http://arxiv.org/pdf/2505.18392v1,cs.LG
RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification,"Ensuring clinical data privacy while preserving utility is critical for
AI-driven healthcare and data analytics. Existing de-identification (De-ID)
methods, including rule-based techniques, deep learning models, and large
language models (LLMs), often suffer from recall errors, limited
generalization, and inefficiencies, limiting their real-world applicability. We
propose a fully automated, multi-modal framework, RedactOR for de-identifying
structured and unstructured electronic health records, including clinical audio
records. Our framework employs cost-efficient De-ID strategies, including
intelligent routing, hybrid rule and LLM based approaches, and a two-step audio
redaction approach. We present a retrieval-based entity relexicalization
approach to ensure consistent substitutions of protected entities, thereby
enhancing data coherence for downstream applications. We discuss key design
desiderata, de-identification and relexicalization methodology, and modular
architecture of RedactX and its integration with the Oracle Health Clinical AI
system. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with
strict recall, our approach achieves competitive performance while optimizing
token usage to reduce LLM costs. Finally, we discuss key lessons and insights
from deployment in real-world AI- driven healthcare data pipelines.",2025-05-23,"Praphul Singh, Charlotte Dzialo, Jangwon Kim, Sumana Srivatsa, Irfan Bulu, Sri Gadde, Krishnaram Kenthapadi",http://arxiv.org/pdf/2505.18380v1,cs.LG
"SP2RINT: Spatially-Decoupled Physics-Inspired Progressive Inverse Optimization for Scalable, PDE-Constrained Meta-Optical Neural Network Training","DONNs harness the physics of light propagation for efficient analog
computation, with applications in AI and signal processing. Advances in
nanophotonic fabrication and metasurface-based wavefront engineering have
opened new pathways to realize high-capacity DONNs across various spectral
regimes. Training such DONN systems to determine the metasurface structures
remains challenging. Heuristic methods are fast but oversimplify metasurfaces
modulation, often resulting in physically unrealizable designs and significant
performance degradation. Simulation-in-the-loop training methods directly
optimize a physically implementable metasurface using adjoint methods during
end-to-end DONN training, but are inherently computationally prohibitive and
unscalable.To address these limitations, we propose SP2RINT, a spatially
decoupled, progressive training framework that formulates DONN training as a
PDE-constrained learning problem. Metasurface responses are first relaxed into
freely trainable transfer matrices with a banded structure. We then
progressively enforce physical constraints by alternating between transfer
matrix training and adjoint-based inverse design, avoiding per-iteration PDE
solves while ensuring final physical realizability. To further reduce runtime,
we introduce a physics-inspired, spatially decoupled inverse design strategy
based on the natural locality of field interactions. This approach partitions
the metasurface into independently solvable patches, enabling scalable and
parallel inverse design with system-level calibration. Evaluated across diverse
DONN training tasks, SP2RINT achieves digital-comparable accuracy while being
1825 times faster than simulation-in-the-loop approaches. By bridging the gap
between abstract DONN models and implementable photonic hardware, SP2RINT
enables scalable, high-performance training of physically realizable
meta-optical neural systems.",2025-05-23,"Pingchuan Ma, Ziang Yin, Qi Jing, Zhengqi Gao, Nicholas Gangi, Boyang Zhang, Tsung-Wei Huang, Zhaoran Huang, Duane S. Boning, Yu Yao, Jiaqi Gu",http://arxiv.org/pdf/2505.18377v1,cs.LG
ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation,"Command-line interfaces (CLIs) provide structured textual environments for
system administration. Explorations have been performed using pre-trained
language models (PLMs) to simulate these environments for safe interaction in
high-risk environments. However, their use has been constrained to frozen,
large parameter models like GPT. For smaller architectures to reach a similar
level of believability, a rich dataset of CLI interactions is required.
Existing public datasets focus on mapping natural-language tasks to commands,
omitting crucial execution data such as exit codes, outputs, and environmental
side effects, limiting their usability for behavioral modeling. We introduce a
Shell Input -Output Environment (ShIOEnv), which casts command construction as
a Markov Decision Process whose state is the partially built sequence and whose
actions append arguments. After each action, ShIOEnv executes the candidate and
returns its exit status, output, and progress toward a minimal-length
behavioral objective. Due to the intractable nature of the combinatorial
argument state-action space, we derive a context-free grammar from man pages to
mask invalid arguments from being emitted. We explore random and
proximal-policy optimization (PPO)-optimized sampling of unrestricted and
grammar-masked action spaces to produce four exploration strategies. We
observed that grammar masking and PPO significantly improve sample efficiency
to produce a higher quality dataset (maximizing the number of arguments while
minimizing redundancies). Policy-generated datasets of shell input-output
behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements
in BLEU-4 when constraining the action space to grammar productions with an
additional 26% improvement when applying PPO. The ShIOEnv environment and
curated command behavior datasets are released for use in future research.",2025-05-23,"Jarrod Ragsdale, Rajendra Boppana",http://arxiv.org/pdf/2505.18374v1,cs.LG
Next-token pretraining implies in-context learning,"We argue that in-context learning (ICL) predictably arises from standard
self-supervised next-token pretraining, rather than being an exotic emergent
property. This work establishes the foundational principles of this emergence
by focusing on in-distribution ICL, demonstrating how models necessarily adapt
to context when trained on token sequences, especially from non-ergodic
sources. Our information-theoretic framework precisely predicts these
in-distribution ICL dynamics (i.e., context-dependent loss reduction). We
verify this with experiments using synthetic datasets of differing types of
correlational structure, reproducing characteristic phenomena like phase
transitions in training loss for induction head formation and power-law scaling
of in-context loss. We further show that a model's in-context performance on
any task is mathematically coupled to the ensemble of tasks seen in
pretraining, offering a fundamental explanation, grounded in architecture- and
modality-independent principles, for such inference-time learning.",2025-05-23,"Paul M. Riechers, Henry R. Bigelow, Eric A. Alt, Adam Shai",http://arxiv.org/pdf/2505.18373v1,cs.LG
"Small Models, Smarter Learning: The Power of Joint Task Training","The ability of a model to learn a task depends strongly on both the task
difficulty and the model size. We aim to understand how task difficulty relates
to the minimum number of parameters required for learning specific tasks in
small transformer models. Our study focuses on the ListOps dataset, which
consists of nested mathematical operations. We gradually increase task
difficulty by introducing new operations or combinations of operations into the
training data. We observe that sum modulo n is the hardest to learn. Curiously,
when combined with other operations such as maximum and median, the sum
operation becomes easier to learn and requires fewer parameters. We show that
joint training not only improves performance but also leads to qualitatively
different model behavior. We show evidence that models trained only on SUM
might be memorizing and fail to capture the number structure in the embeddings.
In contrast, models trained on a mixture of SUM and other operations exhibit
number-like representations in the embedding space, and a strong ability to
distinguish parity. Furthermore, the SUM-only model relies more heavily on its
feedforward layers, while the jointly trained model activates the attention
mechanism more. Finally, we show that learning pure SUM can be induced in
models below the learning threshold of pure SUM, by pretraining them on
MAX+MED. Our findings indicate that emergent abilities in language models
depend not only on model size, but also the training curriculum.",2025-05-23,"Csaba Both, Benjamin Hoover, Hendrik Strobelt, Dmitry Krotov, Daniel Karl I. Weidele, Mauro Martino, Nima Dehmamy",http://arxiv.org/pdf/2505.18369v1,cs.LG
Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems,"Enterprise search systems often struggle to retrieve accurate,
domain-specific information due to semantic mismatches and overlapping
terminologies. These issues can degrade the performance of downstream
applications such as knowledge management, customer support, and
retrieval-augmented generation agents. To address this challenge, we propose a
scalable hard-negative mining framework tailored specifically for
domain-specific enterprise data. Our approach dynamically selects semantically
challenging but contextually irrelevant documents to enhance deployed
re-ranking models.
  Our method integrates diverse embedding models, performs dimensionality
reduction, and uniquely selects hard negatives, ensuring computational
efficiency and semantic precision. Evaluation on our proprietary enterprise
corpus (cloud services domain) demonstrates substantial improvements of 15\% in
MRR@3 and 19\% in MRR@10 compared to state-of-the-art baselines and other
negative sampling techniques. Further validation on public domain-specific
datasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability
and readiness for real-world applications.",2025-05-23,"Hansa Meghwani, Amit Agarwal, Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Srikant Panda",http://arxiv.org/pdf/2505.18366v1,cs.LG
Hamiltonian Theory and Computation of Optimal Probability Density Control in High Dimensions,"We develop a general theoretical framework for optimal probability density
control and propose a numerical algorithm that is scalable to solve the control
problem in high dimensions. Specifically, we establish the Pontryagin Maximum
Principle (PMP) for optimal density control and construct the
Hamilton-Jacobi-Bellman (HJB) equation of the value functional through rigorous
derivations without any concept from Wasserstein theory. To solve the density
control problem numerically, we propose to use reduced-order models, such as
deep neural networks (DNNs), to parameterize the control vector-field and the
adjoint function, which allows us to tackle problems defined on
high-dimensional state spaces. We also prove several convergence properties of
the proposed algorithm. Numerical results demonstrate promising performances of
our algorithm on a variety of density control problems with obstacles and
nonlinear interaction challenges in high dimensions.",2025-05-23,"Nathan Gaby, Xiaojing Ye",http://arxiv.org/pdf/2505.18362v1,cs.LG
Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain,"Tactile sensing remains far less understood in neuroscience and less
effective in artificial systems compared to more mature modalities such as
vision and language. We bridge these gaps by introducing a novel
Encoder-Attender-Decoder (EAD) framework to systematically explore the space of
task-optimized temporal neural networks trained on realistic tactile input
sequences from a customized rodent whisker-array simulator. We identify
convolutional recurrent neural networks (ConvRNNs) as superior encoders to
purely feedforward and state-space architectures for tactile categorization.
Crucially, these ConvRNN-encoder-based EAD models achieve neural
representations closely matching rodent somatosensory cortex, saturating the
explainable neural variability and revealing a clear linear relationship
between supervised categorization performance and neural alignment.
Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained
with tactile-specific augmentations, match supervised neural fits, serving as
an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as
important for general-purpose tactile representations in somatosensory cortex,
providing the first quantitative characterization of the underlying inductive
biases in this system. For embodied AI, our results emphasize the importance of
recurrent EAD architectures to handle realistic tactile inputs, along with
tailored self-supervised learning methods for achieving robust tactile
perception with the same type of sensors animals use to sense in unstructured
environments.",2025-05-23,"Trinity Chung, Yuchen Shen, Nathan C. L. Kong, Aran Nayebi",http://arxiv.org/pdf/2505.18361v1,cs.LG
The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs,"Large language models (LLMs) still struggle across tasks outside of
high-resource languages. In this work, we investigate cross-lingual transfer to
lower-resource languages where task-specific post-training data is scarce.
Building on prior work, we first validate that the subsets of model parameters
that matter most for mathematical reasoning and multilingual capabilities are
distinctly non-overlapping. To exploit this implicit separability between task
and target language parameterization, we develop and analyze numerous modular
frameworks to improve the composition of the two during fine-tuning. These
methods generally employ freezing parameters or post hoc model merging to
assign math and language improvement to different key parts of the LLM. In the
absence of in-language math data, we demonstrate that the modular approaches
successfully improve upon baselines across three languages, four models, and
two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most
consistently successful modular method to be fine-tuning separate language and
math experts and model merging via Layer-Swapping, somewhat surprisingly. We
offer possible explanations for this result via recent works on the linearity
of task vectors. We further explain this by empirically showing that reverting
less useful fine-tuning updates after training often outperforms freezing them
from the start.",2025-05-23,"Lucas Bandarkar, Nanyun Peng",http://arxiv.org/pdf/2505.18356v1,cs.LG
X-MethaneWet: A Cross-scale Global Wetland Methane Emission Benchmark Dataset for Advancing Science Discovery with AI,"Methane (CH$_4$) is the second most powerful greenhouse gas after carbon
dioxide and plays a crucial role in climate change due to its high global
warming potential. Accurately modeling CH$_4$ fluxes across the globe and at
fine temporal scales is essential for understanding its spatial and temporal
variability and developing effective mitigation strategies. In this work, we
introduce the first-of-its-kind cross-scale global wetland methane benchmark
dataset (X-MethaneWet), which synthesizes physics-based model simulation data
from TEM-MDM and the real-world observation data from FLUXNET-CH$_4$. This
dataset can offer opportunities for improving global wetland CH$_4$ modeling
and science discovery with new AI algorithms. To set up AI model baselines for
methane flux prediction, we evaluate the performance of various sequential deep
learning models on X-MethaneWet. Furthermore, we explore four different
transfer learning techniques to leverage simulated data from TEM-MDM to improve
the generalization of deep learning models on real-world FLUXNET-CH$_4$
observations. Our extensive experiments demonstrate the effectiveness of these
approaches, highlighting their potential for advancing methane emission
modeling and contributing to the development of more accurate and scalable
AI-driven climate models.",2025-05-23,"Yiming Sun, Shuo Chen, Shengyu Chen, Chonghao Qiu, Licheng Liu, Youmi Oh, Sparkle L. Malone, Gavin McNicol, Qianlai Zhuang, Chris Smith, Yiqun Xie, Xiaowei Jia",http://arxiv.org/pdf/2505.18355v1,cs.LG
Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?,"As Large Language Models (LLMs) are increasingly being adopted for narrow
tasks - such as medical question answering or sentiment analysis - and deployed
in resource-constrained settings, a key question arises: how many parameters
does a task actually need? In this work, we present LLM-Sieve, the first
comprehensive framework for task-specific pruning of LLMs that achieves 20-75%
parameter reduction with only 1-5% accuracy degradation across diverse domains.
Unlike prior methods that apply uniform pruning or rely on low-rank
approximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns
task-aware joint projections to better approximate output behavior, and (ii)
employs a Genetic Algorithm to discover differentiated pruning levels for each
matrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization,
and uniquely demonstrates strong generalization across datasets within the same
task domain. Together, these results establish a practical and robust mechanism
to generate smaller performant task-specific models.",2025-05-23,"Waleed Reda, Abhinav Jangda, Krishna Chintalapudi",http://arxiv.org/pdf/2505.18350v1,cs.LG
The Cell Must Go On: Agar.io for Continual Reinforcement Learning,"Continual reinforcement learning (RL) concerns agents that are expected to
learn continually, rather than converge to a policy that is then fixed for
evaluation. Such an approach is well suited to environments the agent perceives
as changing, which renders any static policy ineffective over time. The few
simulators explicitly designed for empirical research in continual RL are often
limited in scope or complexity, and it is now common for researchers to modify
episodic RL environments by artificially incorporating abrupt task changes
during interaction. In this paper, we introduce AgarCL, a research platform for
continual RL that allows for a progression of increasingly sophisticated
behaviour. AgarCL is based on the game Agar.io, a non-episodic,
high-dimensional problem featuring stochastic, ever-evolving dynamics,
continuous actions, and partial observability. Additionally, we provide
benchmark results reporting the performance of DQN, PPO, and SAC in both the
primary, challenging continual RL problem, and across a suite of smaller tasks
within AgarCL, each of which isolates aspects of the full environment and allow
us to characterize the challenges posed by different aspects of the game.",2025-05-23,"Mohamed A. Mohamed, Kateryna Nekhomiazh, Vedant Vyas, Marcos M. Jose, Andrew Patterson, Marlos C. Machado",http://arxiv.org/pdf/2505.18347v1,cs.LG
On the Mechanisms of Weak-to-Strong Generalization: A Theoretical Perspective,"Weak-to-strong generalization, where a student model trained on imperfect
labels generated by a weaker teacher nonetheless surpasses that teacher, has
been widely observed but the mechanisms that enable it have remained poorly
understood. In this paper, through a theoretical analysis of simple models, we
uncover three core mechanisms that can drive this phenomenon. First, by
analyzing ridge regression, we study the interplay between the teacher and
student regularization and prove that a student can compensate for a teacher's
under-regularization and achieve lower test error. We also analyze the role of
the parameterization regime of the models. Second, by analyzing weighted ridge
regression, we show that a student model with a regularization structure more
aligned to the target, can outperform its teacher. Third, in a nonlinear
multi-index setting, we demonstrate that a student can learn easy,
task-specific features from the teacher while leveraging its own broader
pre-training to learn hard-to-learn features that the teacher cannot capture.",2025-05-23,"Behrad Moniri, Hamed Hassani",http://arxiv.org/pdf/2505.18346v1,cs.LG
Diffusion Self-Weighted Guidance for Offline Reinforcement Learning,"Offline reinforcement learning (RL) recovers the optimal policy $\pi$ given
historical observations of an agent. In practice, $\pi$ is modeled as a
weighted version of the agent's behavior policy $\mu$, using a weight function
$w$ working as a critic of the agent's behavior. Though recent approaches to
offline RL based on diffusion models have exhibited promising results, the
computation of the required scores is challenging due to their dependence on
the unknown $w$. In this work, we alleviate this issue by constructing a
diffusion over both the actions and the weights. With the proposed setting, the
required scores are directly obtained from the diffusion model without learning
extra networks. Our main conceptual contribution is a novel guidance method,
where guidance (which is a function of $w$) comes from the same diffusion
model, therefore, our proposal is termed Self-Weighted Guidance (SWG). We show
that SWG generates samples from the desired distribution on toy examples and
performs on par with state-of-the-art methods on D4RL's challenging
environments, while maintaining a streamlined training pipeline. We further
validate SWG through ablation studies on weight formulations and scalability.",2025-05-23,"Augusto Tagle, Javier Ruiz-del-Solar, Felipe Tobar",http://arxiv.org/pdf/2505.18345v1,cs.LG
Sample Complexity of Diffusion Model Training Without Empirical Risk Minimizer Access,"Diffusion models have demonstrated state-of-the-art performance across
vision, language, and scientific domains. Despite their empirical success,
prior theoretical analyses of the sample complexity suffer from poor scaling
with input data dimension or rely on unrealistic assumptions such as access to
exact empirical risk minimizers. In this work, we provide a principled analysis
of score estimation, establishing a sample complexity bound of
$\widetilde{\mathcal{O}}(\epsilon^{-6})$. Our approach leverages a structured
decomposition of the score estimation error into statistical, approximation,
and optimization errors, enabling us to eliminate the exponential dependence on
neural network parameters that arises in prior analyses. It is the first such
result which achieves sample complexity bounds without assuming access to the
empirical risk minimizer of score function estimation loss.",2025-05-23,"Mudit Gaur, Prashant Trivedi, Sasidhar Kunapuli, Amrit Singh Bedi, Vaneet Aggarwal",http://arxiv.org/pdf/2505.18344v1,cs.LG
Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance,"Accurate and scalable quantification of animal pose and appearance is crucial
for studying behavior. Current 3D pose estimation techniques, such as keypoint-
and mesh-based techniques, often face challenges including limited
representational detail, labor-intensive annotation requirements, and expensive
per-frame optimization. These limitations hinder the study of subtle movements
and can make large-scale analyses impractical. We propose Pose Splatter, a
novel framework leveraging shape carving and 3D Gaussian splatting to model the
complete pose and appearance of laboratory animals without prior knowledge of
animal geometry, per-frame optimization, or manual annotations. We also propose
a novel rotation-invariant visual embedding technique for encoding pose and
appearance, designed to be a plug-in replacement for 3D keypoint data in
downstream behavioral analyses. Experiments on datasets of mice, rats, and
zebra finches show Pose Splatter learns accurate 3D animal geometries. Notably,
Pose Splatter represents subtle variations in pose, provides better
low-dimensional pose embeddings over state-of-the-art as evaluated by humans,
and generalizes to unseen data. By eliminating annotation and per-frame
optimization bottlenecks, Pose Splatter enables analysis of large-scale,
longitudinal behavior needed to map genotype, neural activity, and
micro-behavior at unprecedented resolution.",2025-05-23,"Jack Goffinet, Youngjo Min, Carlo Tomasi, David E. Carlson",http://arxiv.org/pdf/2505.18342v1,cs.LG
An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs,"Recent advances in Large Language Models (LLMs) have led to the widespread
adoption of third-party inference services, raising critical privacy concerns.
Existing methods of performing private third-party inference, such as Secure
Multiparty Computation (SMPC), often rely on cryptographic methods. However,
these methods are thousands of times slower than standard unencrypted
inference, and fail to scale to large modern LLMs. Therefore, recent lines of
work have explored the replacement of expensive encrypted nonlinear
computations in SMPC with statistical obfuscation methods - in particular,
revealing permuted hidden states to the third parties, with accompanying strong
claims of the difficulty of reversal into the unpermuted states. In this work,
we begin by introducing a novel reconstruction technique that can recover
original prompts from hidden states with nearly perfect accuracy across
multiple state-of-the-art LLMs. We then show that extensions of our attack are
nearly perfectly effective in reversing permuted hidden states of LLMs,
demonstrating the insecurity of three recently proposed privacy schemes. We
further dissect the shortcomings of prior theoretical `proofs' of permuation
security which allow our attack to succeed. Our findings highlight the
importance of rigorous security analysis in privacy-preserving LLM inference.",2025-05-23,"Rahul Thomas, Louai Zahran, Erica Choi, Akilesh Potti, Micah Goldblum, Arka Pal",http://arxiv.org/pdf/2505.18332v1,cs.LG
Online Statistical Inference of Constrained Stochastic Optimization via Random Scaling,"Constrained stochastic nonlinear optimization problems have attracted
significant attention for their ability to model complex real-world scenarios
in physics, economics, and biology. As datasets continue to grow, online
inference methods have become crucial for enabling real-time decision-making
without the need to store historical data. In this work, we develop an online
inference procedure for constrained stochastic optimization by leveraging a
method called Sketched Stochastic Sequential Quadratic Programming (SSQP). As a
direct generalization of sketched Newton methods, SSQP approximates the
objective with a quadratic model and the constraints with a linear model at
each step, then applies a sketching solver to inexactly solve the resulting
subproblem. Building on this design, we propose a new online inference
procedure called random scaling. In particular, we construct a test statistic
based on SSQP iterates whose limiting distribution is free of any unknown
parameters. Compared to existing online inference procedures, our approach
offers two key advantages: (i) it enables the construction of asymptotically
valid confidence intervals; and (ii) it is matrix-free, i.e. the computation
involves only primal-dual SSQP iterates $(\boldsymbol{x}_t,
\boldsymbol{\lambda}_t)$ without requiring any matrix inversions. We validate
our theory through numerical experiments on nonlinearly constrained regression
problems and demonstrate the superior performance of our random scaling method
over existing inference procedures.",2025-05-23,"Xinchen Du, Wanrong Zhu, Wei Biao Wu, Sen Na",http://arxiv.org/pdf/2505.18327v1,cs.LG
Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary,"Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they often refuse to answer legitimate queries-a
phenomenon known as overrefusal. Overrefusal typically stems from
over-conservative safety alignment, causing models to treat many reasonable
prompts as potentially risky. To systematically understand this issue, we probe
and leverage the models'safety decision boundaries to analyze and mitigate
overrefusal. Our findings reveal that overrefusal is closely tied to
misalignment at these boundary regions, where models struggle to distinguish
subtle differences between benign and harmful content. Building on these
insights, we present RASS, an automated framework for prompt generation and
selection that strategically targets overrefusal prompts near the safety
boundary. By harnessing steering vectors in the representation space, RASS
efficiently identifies and curates boundary-aligned prompts, enabling more
effective and targeted mitigation of overrefusal. This approach not only
provides a more precise and interpretable view of model safety decisions but
also seamlessly extends to multilingual scenarios.We have explored the safety
decision boundaries of various LLMs and construct the MORBench evaluation set
to facilitate robust assessment of model safety and helpfulness across multiple
languages. Code and datasets will be released at
https://anonymous.4open.science/r/RASS-80D3.",2025-05-23,"Licheng Pan, Yongqi Tong, Xin Zhang, Xiaolu Zhang, Jun Zhou, Zhixuan Chu",http://arxiv.org/pdf/2505.18325v1,cs.LG
Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation,"For nearly a decade the academic community has investigated backdoors in
neural networks, primarily focusing on classification tasks where adversaries
manipulate the model prediction. While demonstrably malicious, the immediate
real-world impact of such prediction-altering attacks has remained unclear. In
this paper we introduce a novel and significantly more potent class of
backdoors that builds upon recent advancements in architectural backdoors. We
demonstrate how these backdoors can be specifically engineered to exploit
batched inference, a common technique for hardware utilization, enabling
large-scale user data manipulation and theft. By targeting the batching
process, these architectural backdoors facilitate information leakage between
concurrent user requests and allow attackers to fully control model responses
directed at other users within the same batch. In other words, an attacker who
can change the model architecture can set and steal model inputs and outputs of
other users within the same batch. We show that such attacks are not only
feasible but also alarmingly effective, can be readily injected into prevalent
model architectures, and represent a truly malicious threat to user privacy and
system integrity. Critically, to counteract this new class of vulnerabilities,
we propose a deterministic mitigation strategy that provides formal guarantees
against this new attack vector, unlike prior work that relied on Large Language
Models to find the backdoors. Our mitigation strategy employs a novel
Information Flow Control mechanism that analyzes the model graph and proves
non-interference between different user inputs within the same batch. Using our
mitigation strategy we perform a large scale analysis of models hosted through
Hugging Face and find over 200 models that introduce (unintended) information
leakage between batch entries due to the use of dynamic quantization.",2025-05-23,"Nicolas Küchler, Ivan Petrov, Conrad Grobler, Ilia Shumailov",http://arxiv.org/pdf/2505.18323v1,cs.LG
PLUMAGE: Probabilistic Low rank Unbiased Min Variance Gradient Estimator for Efficient Large Model Training,"Accelerator memory and networking constraints have emerged as dominant
bottlenecks when training large language models LLMs with billions of
parameters. Existing low rank gradient estimators such as GaLoRE and FLORA
compress gradients and optimizer tensors by projecting weight gradients onto a
rank r subspace, enabling LLM training on consumer hardware. Yet, these methods
are either biased or subject to high estimator variance. Moreover, the
optimizer state based on the first and second moments estimates expressed in
the previous subspace becomes misaligned whenever the projection is updated,
leading to instabilities during training. We propose PLUMAGE: Probabilistic Low
rank Unbiased Minimum vAriance Gradient Estimator. PLUMAGE is a drop in
replacement for existing low rank gradient estimators. It does not introduce
new hyperparameters beyond the chosen rank r and the update interval. In
addition, we resolve optimizer state misalignment issues to prevent spurious
weight updates and enhance training stability. We empirically demonstrate that
PLUMAGE shrinks the full rank optimization's gap over the pre training
evaluation loss by 33% on average across models and the average training loss
across the GLUE benchmark by 28% within a similar computational and memory
footprint as GaloRE.",2025-05-23,"Matan Haroush, Daniel Soudry",http://arxiv.org/pdf/2505.18313v1,cs.LG
Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs,"We propose a history-driven target (HDT) framework in Markov Chain Monte
Carlo (MCMC) to improve any random walk algorithm on discrete state spaces,
such as general undirected graphs, for efficient sampling from target
distribution $\boldsymbol{\mu}$. With broad applications in network science and
distributed optimization, recent innovations like the self-repellent random
walk (SRRW) achieve near-zero variance by prioritizing under-sampled states
through transition kernel modifications based on past visit frequencies.
However, SRRW's reliance on explicit computation of transition probabilities
for all neighbors at each step introduces substantial computational overhead,
while its strict dependence on time-reversible Markov chains excludes advanced
non-reversible MCMC methods. To overcome these limitations, instead of direct
modification of transition kernel, HDT introduces a history-dependent target
distribution $\boldsymbol{\pi}[\mathbf{x}]$ to replace the original target
$\boldsymbol{\mu}$ in any graph sampler, where $\mathbf{x}$ represents the
empirical measure of past visits. This design preserves lightweight
implementation by requiring only local information between the current and
proposed states and achieves compatibility with both reversible and
non-reversible MCMC samplers, while retaining unbiased samples with target
distribution $\boldsymbol{\mu}$ and near-zero variance performance. Extensive
experiments in graph sampling demonstrate consistent performance gains, and a
memory-efficient Least Recently Used (LRU) cache ensures scalability to large
general graphs.",2025-05-23,"Jie Hu, Yi-Ting Ma, Do Young Eun",http://arxiv.org/pdf/2505.18300v1,cs.LG
A deep solver for backward stochastic Volterra integral equations,"We present the first deep-learning solver for backward stochastic Volterra
integral equations (BSVIEs) and their fully-coupled forward-backward variants.
The method trains a neural network to approximate the two solution fields in a
single stage, avoiding the use of nested time-stepping cycles that limit
classical algorithms. For the decoupled case we prove a non-asymptotic error
bound composed of an a posteriori residual plus the familiar square root
dependence on the time step. Numerical experiments confirm this rate and reveal
two key properties: \emph{scalability}, in the sense that accuracy remains
stable from low dimension up to 500 spatial variables while GPU batching keeps
wall-clock time nearly constant; and \emph{generality}, since the same method
handles coupled systems whose forward dynamics depend on the backward solution.
These results open practical access to a family of high-dimensional,
path-dependent problems in stochastic control and quantitative finance.",2025-05-23,"Kristoffer Andersson, Alessandro Gnoatto, Camilo Andrés García Trillos",http://arxiv.org/pdf/2505.18297v1,cs.LG
Convexified Message-Passing Graph Neural Networks,"Graph Neural Networks (GNNs) have become prominent methods for graph
representation learning, demonstrating strong empirical results on diverse
graph prediction tasks. In this paper, we introduce Convexified Message Passing
Graph Neural Networks (CGNNs), a novel and general framework that combines the
power of message-passing GNNs with the tractability of convex optimization. By
mapping their nonlinear filters into a reproducing kernel Hilbert space, CGNNs
transform training into a convex optimization problem, which can be solved
efficiently and optimally by projected gradient methods. This convexity further
allows the statistical properties of CGNNs to be analyzed accurately and
rigorously. For two-layer CGNNs, we establish rigorous generalization
guarantees, showing convergence to the performance of the optimal GNN. To scale
to deeper architectures, we adopt a principled layer-wise training strategy.
Experiments on benchmark datasets show that CGNNs significantly exceed the
performance of leading GNN models, achieving 10 to 40 percent higher accuracy
in most cases, underscoring their promise as a powerful and principled method
with strong theoretical foundations. In rare cases where improvements are not
quantitatively substantial, the convex models either slightly exceed or match
the baselines, stressing their robustness and wide applicability. Though
over-parameterization is often employed to enhance performance in nonconvex
models, we show that our CGNNs framework yields shallow convex models that can
surpass these models in both accuracy and resource efficiency.",2025-05-23,"Saar Cohen, Noa Agmon, Uri Shaham",http://arxiv.org/pdf/2505.18289v1,cs.LG
"Operator Learning for Schrödinger Equation: Unitarity, Error Bounds, and Time Generalization","We consider the problem of learning the evolution operator for the
time-dependent Schr\""{o}dinger equation, where the Hamiltonian may vary with
time. Existing neural network-based surrogates often ignore fundamental
properties of the Schr\""{o}dinger equation, such as linearity and unitarity,
and lack theoretical guarantees on prediction error or time generalization. To
address this, we introduce a linear estimator for the evolution operator that
preserves a weak form of unitarity. We establish both upper and lower bounds on
the prediction error that hold uniformly over all sufficiently smooth initial
wave functions. Additionally, we derive time generalization bounds that
quantify how the estimator extrapolates beyond the time points seen during
training. Experiments across real-world Hamiltonians -- including hydrogen
atoms, ion traps for qubit design, and optical lattices -- show that our
estimator achieves relative errors $10^{-2}$ to $10^{-3}$ times smaller than
state-of-the-art methods such as the Fourier Neural Operator and DeepONet.",2025-05-23,"Yash Patel, Unique Subedi, Ambuj Tewari",http://arxiv.org/pdf/2505.18288v1,cs.LG
Single-agent or Multi-agent Systems? Why Not Both?,"Multi-agent systems (MAS) decompose complex tasks and delegate subtasks to
different large language model (LLM) agents and tools. Prior studies have
reported the superior accuracy performance of MAS across diverse domains,
enabled by long-horizon context tracking and error correction through
role-specific agents. However, the design and deployment of MAS incur higher
complexity and runtime cost compared to single-agent systems (SAS). Meanwhile,
frontier LLMs, such as OpenAI-o3 and Gemini-2.5-Pro, have rapidly advanced in
long-context reasoning, memory retention, and tool usage, mitigating many
limitations that originally motivated MAS designs. In this paper, we conduct an
extensive empirical study comparing MAS and SAS across various popular agentic
applications. We find that the benefits of MAS over SAS diminish as LLM
capabilities improve, and we propose efficient mechanisms to pinpoint the
error-prone agent in MAS. Furthermore, the performance discrepancy between MAS
and SAS motivates our design of a hybrid agentic paradigm, request cascading
between MAS and SAS, to improve both efficiency and capability. Our design
improves accuracy by 1.1-12% while reducing deployment costs by up to 20%
across various agentic applications.",2025-05-23,"Mingyan Gao, Yanzi Li, Banruo Liu, Yifan Yu, Phillip Wang, Ching-Yu Lin, Fan Lai",http://arxiv.org/pdf/2505.18286v1,cs.LG
Tube Loss based Deep Networks For Improving the Probabilistic Forecasting of Wind Speed,"Uncertainty Quantification (UQ) in wind speed forecasting is a critical
challenge in wind power production due to the inherently volatile nature of
wind. By quantifying the associated risks and returns, UQ supports more
effective decision-making for grid operations and participation in the
electricity market. In this paper, we design a sequence of deep learning based
probabilistic forecasting methods by using the Tube loss function for wind
speed forecasting. The Tube loss function is a simple and model agnostic
Prediction Interval (PI) estimation approach and can obtain the narrow PI with
asymptotical coverage guarantees without any distribution assumption. Our deep
probabilistic forecasting models effectively incorporate popular architectures
such as LSTM, GRU, and TCN within the Tube loss framework. We further design a
simple yet effective heuristic for tuning the $\delta$ parameter of the Tube
loss function so that our deep forecasting models obtain the narrower PI
without compromising its calibration ability. We have considered three wind
datasets, containing the hourly recording of the wind speed, collected from
three distinct location namely Jaisalmer, Los Angeles and San Fransico. Our
numerical results demonstrate that the proposed deep forecasting models produce
more reliable and narrower PIs compared to recently developed probabilistic
wind forecasting methods.",2025-05-23,"Pritam Anand, Aadesh Minz, Asish Joel",http://arxiv.org/pdf/2505.18284v1,cs.LG
Feature Preserving Shrinkage on Bayesian Neural Networks via the R2D2 Prior,"Bayesian neural networks (BNNs) treat neural network weights as random
variables, which aim to provide posterior uncertainty estimates and avoid
overfitting by performing inference on the posterior weights. However, the
selection of appropriate prior distributions remains a challenging task, and
BNNs may suffer from catastrophic inflated variance or poor predictive
performance when poor choices are made for the priors. Existing BNN designs
apply different priors to weights, while the behaviours of these priors make it
difficult to sufficiently shrink noisy signals or they are prone to
overshrinking important signals in the weights. To alleviate this problem, we
propose a novel R2D2-Net, which imposes the R^2-induced Dirichlet Decomposition
(R2D2) prior to the BNN weights. The R2D2-Net can effectively shrink irrelevant
coefficients towards zero, while preventing key features from over-shrinkage.
To approximate the posterior distribution of weights more accurately, we
further propose a variational Gibbs inference algorithm that combines the Gibbs
updating procedure and gradient-based optimization. This strategy enhances
stability and consistency in estimation when the variational objective
involving the shrinkage parameters is non-convex. We also analyze the evidence
lower bound (ELBO) and the posterior concentration rates from a theoretical
perspective. Experiments on both natural and medical image classification and
uncertainty estimation tasks demonstrate satisfactory performance of our
method.",2025-05-23,"Tsai Hor Chan, Dora Yan Zhang, Guosheng Yin, Lequan Yu",http://arxiv.org/pdf/2505.18280v1,cs.LG
Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control,"Complex tasks are increasingly delegated to ensembles of specialized
LLM-based agents that reason, communicate, and coordinate actions-both among
themselves and through interactions with external tools, APIs, and databases.
While persistent memory has been shown to enhance single-agent performance,
most approaches assume a monolithic, single-user context-overlooking the
benefits and challenges of knowledge transfer across users under dynamic,
asymmetric permissions. We introduce Collaborative Memory, a framework for
multi-user, multi-agent environments with asymmetric, time-evolving access
controls encoded as bipartite graphs linking users, agents, and resources. Our
system maintains two memory tiers: (1) private memory-private fragments visible
only to their originating user; and (2) shared memory-selectively shared
fragments. Each fragment carries immutable provenance attributes (contributing
agents, accessed resources, and timestamps) to support retrospective permission
checks. Granular read policies enforce current user-agent-resource constraints
and project existing memory fragments into filtered transformed views. Write
policies determine fragment retention and sharing, applying context-aware
transformations to update the memory. Both policies may be designed conditioned
on system, agent, and user-level information. Our framework enables safe,
efficient, and interpretable cross-user knowledge sharing, with provable
adherence to asymmetric, time-varying policies and full auditability of memory
operations.",2025-05-23,"Alireza Rezazadeh, Zichao Li, Ange Lou, Yuying Zhao, Wei Wei, Yujia Bao",http://arxiv.org/pdf/2505.18279v1,cs.LG
Preconditioned Langevin Dynamics with Score-Based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems,"Designing algorithms for solving high-dimensional Bayesian inverse problems
directly in infinite-dimensional function spaces - where such problems are
naturally formulated - is crucial to ensure stability and convergence as the
discretization of the underlying problem is refined. In this paper, we
contribute to this line of work by analyzing a widely used sampler for linear
inverse problems: Langevin dynamics driven by score-based generative models
(SGMs) acting as priors, formulated directly in function space. Building on the
theoretical framework for SGMs in Hilbert spaces, we give a rigorous definition
of this sampler in the infinite-dimensional setting and derive, for the first
time, error estimates that explicitly depend on the approximation error of the
score. As a consequence, we obtain sufficient conditions for global convergence
in Kullback-Leibler divergence on the underlying function space. Preventing
numerical instabilities requires preconditioning of the Langevin algorithm and
we prove the existence and the form of an optimal preconditioner. The
preconditioner depends on both the score error and the forward operator and
guarantees a uniform convergence rate across all posterior modes. Our analysis
applies to both Gaussian and a general class of non-Gaussian priors. Finally,
we present examples that illustrate and validate our theoretical findings.",2025-05-23,"Lorenzo Baldassari, Josselin Garnier, Knut Solna, Maarten V. de Hoop",http://arxiv.org/pdf/2505.18276v1,cs.LG
Representative Action Selection for Large Action-Space Meta-Bandits,"We study the problem of selecting a subset from a large action space shared
by a family of bandits, with the goal of achieving performance nearly matching
that of using the full action space. We assume that similar actions tend to
have related payoffs, modeled by a Gaussian process. To exploit this structure,
we propose a simple epsilon-net algorithm to select a representative subset. We
provide theoretical guarantees for its performance and compare it empirically
to Thompson Sampling and Upper Confidence Bound.",2025-05-23,"Quan Zhou, Mark Kozdoba, Shie Mannor",http://arxiv.org/pdf/2505.18269v1,cs.LG
Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks,"We propose a testable universality hypothesis, asserting that seemingly
disparate neural network solutions observed in the simple task of modular
addition are unified under a common abstract algorithm. While prior work
interpreted variations in neuron-level representations as evidence for distinct
algorithms, we demonstrate - through multi-level analyses spanning neurons,
neuron clusters, and entire networks - that multilayer perceptrons and
transformers universally implement the abstract algorithm we call the
approximate Chinese Remainder Theorem. Crucially, we introduce approximate
cosets and show that neurons activate exclusively on them. Furthermore, our
theory works for deep neural networks (DNNs). It predicts that universally
learned solutions in DNNs with trainable embeddings or more than one hidden
layer require only O(log n) features, a result we empirically confirm. This
work thus provides the first theory-backed interpretation of multilayer
networks solving modular addition. It advances generalizable interpretability
and opens a testable universality hypothesis for group multiplication beyond
modular addition.",2025-05-23,"Gavin McCracken, Gabriela Moisescu-Pareja, Vincent Letourneau, Doina Precup, Jonathan Love",http://arxiv.org/pdf/2505.18266v1,cs.LG
Generative Distribution Embeddings,"Many real-world problems require reasoning across multiple scales, demanding
models which operate not on single data points, but on entire distributions. We
introduce generative distribution embeddings (GDE), a framework that lifts
autoencoders to the space of distributions. In GDEs, an encoder acts on sets of
samples, and the decoder is replaced by a generator which aims to match the
input distribution. This framework enables learning representations of
distributions by coupling conditional generative models with encoder networks
which satisfy a criterion we call distributional invariance. We show that GDEs
learn predictive sufficient statistics embedded in the Wasserstein space, such
that latent GDE distances approximately recover the $W_2$ distance, and latent
interpolation approximately recovers optimal transport trajectories for
Gaussian and Gaussian mixture distributions. We systematically benchmark GDEs
against existing approaches on synthetic datasets, demonstrating consistently
stronger performance. We then apply GDEs to six key problems in computational
biology: learning representations of cell populations from lineage-tracing data
(150K cells), predicting perturbation effects on single-cell transcriptomes (1M
cells), predicting perturbation effects on cellular phenotypes (20M single-cell
images), modeling tissue-specific DNA methylation patterns (253M sequences),
designing synthetic yeast promoters (34M sequences), and spatiotemporal
modeling of viral protein sequences (1M sequences).",2025-05-23,"Nic Fishman, Gokul Gowri, Peng Yin, Jonathan Gootenberg, Omar Abudayyeh",http://arxiv.org/pdf/2505.18150v1,cs.LG
Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find,"Large language models (LLMs) face significant challenges with
needle-in-a-haystack tasks, where relevant information (""the needle"") must be
drawn from a large pool of irrelevant context (""the haystack""). Previous
studies have highlighted positional bias and distractor quantity as critical
factors affecting model performance, yet the influence of gold context size has
received little attention. We address this gap by systematically studying how
variations in gold context length impact LLM performance on long-context
question answering tasks. Our experiments reveal that LLM performance drops
sharply when the gold context is shorter, i.e., smaller gold contexts
consistently degrade model performance and amplify positional sensitivity,
posing a major challenge for agentic systems that must integrate scattered,
fine-grained information of varying lengths. This pattern holds across three
diverse domains (general knowledge, biomedical reasoning, and mathematical
reasoning) and seven state-of-the-art LLMs of various sizes and architectures.
Our work provides clear insights to guide the design of robust, context-aware
LLM-driven systems.",2025-05-23,"Owen Bianchi, Mathew J. Koretsky, Maya Willey, Chelsea X. Alvarado, Tanay Nayak, Adi Asija, Nicole Kuznetsov, Mike A. Nalls, Faraz Faghri, Daniel Khashabi",http://arxiv.org/pdf/2505.18148v1,cs.LG
Boosting Open Set Recognition Performance through Modulated Representation Learning,"The open set recognition (OSR) problem aims to identify test samples from
novel semantic classes that are not part of the training classes, a task that
is crucial in many practical scenarios. However, existing OSR methods use a
constant scaling factor (the temperature) to the logits before applying a loss
function, which hinders the model from exploring both ends of the spectrum in
representation learning -- from instance-level to semantic-level features. In
this paper, we address this problem by enabling temperature-modulated
representation learning using our novel negative cosine scheduling scheme. Our
scheduling lets the model form a coarse decision boundary at the beginning of
training by focusing on fewer neighbors, and gradually prioritizes more
neighbors to smooth out rough edges. This gradual task switching leads to a
richer and more generalizable representation space. While other OSR methods
benefit by including regularization or auxiliary negative samples, such as with
mix-up, thereby adding a significant computational overhead, our scheme can be
folded into any existing OSR method with no overhead. We implement the proposed
scheme on top of a number of baselines, using both cross-entropy and
contrastive loss functions as well as a few other OSR methods, and find that
our scheme boosts both the OSR performance and the closed set performance in
most cases, especially on the tougher semantic shift benchmarks.",2025-05-23,"Amit Kumar Kundu, Vaishnavi Patil, Joseph Jaja",http://arxiv.org/pdf/2505.18137v1,cs.LG
Gaming Tool Preferences in Agentic LLMs,"Large language models (LLMs) can now access a wide range of external tools,
thanks to the Model Context Protocol (MCP). This greatly expands their
abilities as various agents. However, LLMs rely entirely on the text
descriptions of tools to decide which ones to use--a process that is
surprisingly fragile. In this work, we expose a vulnerability in prevalent
tool/function-calling protocols by investigating a series of edits to tool
descriptions, some of which can drastically increase a tool's usage from LLMs
when competing with alternatives. Through controlled experiments, we show that
tools with properly edited descriptions receive over 10 times more usage from
GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further
evaluate how various edits to tool descriptions perform when competing directly
with one another and how these trends generalize or differ across a broader set
of 10 different models. These phenomenons, while giving developers a powerful
way to promote their tools, underscore the need for a more reliable foundation
for agentic LLMs to select and utilize tools and resources.",2025-05-23,"Kazem Faghih, Wenxiao Wang, Yize Cheng, Siddhant Bharti, Gaurang Sriramanan, Sriram Balasubramanian, Parsa Hosseini, Soheil Feizi",http://arxiv.org/pdf/2505.18135v1,cs.LG
Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement,"Multilayer perceptrons (MLPs) are a workhorse machine learning architecture,
used in a variety of modern deep learning frameworks. However, recently
Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their
success on a range of problems, particularly for scientific machine learning
tasks. In this paper, we exploit the relationship between KANs and multichannel
MLPs to gain structural insight into how to train MLPs faster. We demonstrate
the KAN basis (1) provides geometric localized support, and (2) acts as a
preconditioned descent in the ReLU basis, overall resulting in expedited
training and improved accuracy. Our results show the equivalence between
free-knot spline KAN architectures, and a class of MLPs that are refined
geometrically along the channel dimension of each weight tensor. We exploit
this structural equivalence to define a hierarchical refinement scheme that
dramatically accelerates training of the multi-channel MLP architecture. We
show further accuracy improvements can be had by allowing the $1$D locations of
the spline knots to be trained simultaneously with the weights. These advances
are demonstrated on a range of benchmark examples for regression and scientific
machine learning.",2025-05-23,"Jonas A. Actor, Graham Harper, Ben Southworth, Eric C. Cyr",http://arxiv.org/pdf/2505.18131v1,cs.LG
Reward Model Overoptimisation in Iterated RLHF,"Reinforcement learning from human feedback (RLHF) is a widely used method for
aligning large language models with human preferences. However, RLHF often
suffers from reward model overoptimisation, in which models overfit to the
reward function, resulting in non-generalisable policies that exploit the
idiosyncrasies and peculiarities of the reward function. A common mitigation is
iterated RLHF, in which reward models are repeatedly retrained with updated
human feedback and policies are re-optimised. Despite its increasing adoption,
the dynamics of overoptimisation in this setting remain poorly understood. In
this work, we present the first comprehensive study of overoptimisation in
iterated RLHF. We systematically analyse key design choices - how reward model
training data is transferred across iterations, which reward function is used
for optimisation, and how policies are initialised. Using the controlled
AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over
successive iterations, as reward models increasingly approximate ground-truth
preferences. However, performance gains diminish over time, and while
reinitialising from the base policy is robust, it limits optimisation
flexibility. Other initialisation strategies often fail to recover from early
overoptimisation. These findings offer actionable insights for building more
stable and generalisable RLHF pipelines.",2025-05-23,"Lorenz Wolf, Robert Kirk, Mirco Musolesi",http://arxiv.org/pdf/2505.18126v1,cs.LG
TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations,"While deep learning has achieved remarkable success across many domains, it
has historically underperformed on tabular learning tasks, which remain
dominated by gradient boosting decision trees (GBDTs). However, recent
advancements are paving the way for Tabular Foundation Models, which can
leverage real-world knowledge and generalize across diverse datasets,
particularly when the data contains free-text. Although incorporating language
model capabilities into tabular tasks has been explored, most existing methods
utilize static, target-agnostic textual representations, limiting their
effectiveness. We introduce TabSTAR: a Foundation Tabular Model with
Semantically Target-Aware Representations. TabSTAR is designed to enable
transfer learning on tabular data with textual features, with an architecture
free of dataset-specific parameters. It unfreezes a pretrained text encoder and
takes as input target tokens, which provide the model with the context needed
to learn task-specific embeddings. TabSTAR achieves state-of-the-art
performance for both medium- and large-sized datasets across known benchmarks
of classification tasks with text features, and its pretraining phase exhibits
scaling laws in the number of datasets, offering a pathway for further
performance improvements.",2025-05-23,"Alan Arazi, Eilam Shapira, Roi Reichart",http://arxiv.org/pdf/2505.18125v1,cs.LG
ProgRM: Build Better GUI Agents with Progress Rewards,"LLM-based (Large Language Model) GUI (Graphical User Interface) agents can
potentially reshape our daily lives significantly. However, current LLM-based
GUI agents suffer from the scarcity of high-quality training data owing to the
difficulties of trajectory collection and reward annotation. Existing works
have been exploring LLMs to collect trajectories for imitation learning or to
offer reward signals for online RL training. However, the Outcome Reward Model
(ORM) used in existing works cannot provide finegrained feedback and can
over-penalize the valuable steps in finally failed trajectories. To this end,
we propose Progress Reward Model (ProgRM) to provide dense informative
intermediate rewards by predicting a task completion progress for each step in
online training. To handle the challenge of progress reward label annotation,
we further design an efficient LCS-based (Longest Common Subsequence)
self-annotation algorithm to discover the key steps in trajectories and assign
progress labels accordingly. ProgRM is evaluated with extensive experiments and
analyses. Actors trained with ProgRM outperform leading proprietary LLMs and
ORM-trained actors, illustrating the effectiveness of ProgRM. The codes for
experiments will be made publicly available upon acceptance.",2025-05-23,"Danyang Zhang, Situo Zhang, Ziyue Yang, Zichen Zhu, Zihan Zhao, Ruisheng Cao, Lu Chen, Kai Yu",http://arxiv.org/pdf/2505.18121v1,cs.LG
Scalable Policy Maximization Under Network Interference,"Many interventions, such as vaccines in clinical trials or coupons in online
marketplaces, must be assigned sequentially without full knowledge of their
effects. Multi-armed bandit algorithms have proven successful in such settings.
However, standard independence assumptions fail when the treatment status of
one individual impacts the outcomes of others, a phenomenon known as
interference. We study optimal-policy learning under interference on a dynamic
network. Existing approaches to this problem require repeated observations of
the same fixed network and struggle to scale in sample size beyond as few as
fifteen connected units -- both limit applications. We show that under common
assumptions on the structure of interference, rewards become linear. This
enables us to develop a scalable Thompson sampling algorithm that maximizes
policy impact when a new $n$-node network is observed each round. We prove a
Bayesian regret bound that is sublinear in $n$ and the number of rounds.
Simulation experiments show that our algorithm learns quickly and outperforms
existing methods. The results close a key scalability gap between causal
inference methods for interference and practical bandit algorithms, enabling
policy optimization in large-scale networked systems.",2025-05-23,"Aidan Gleich, Eric Laber, Alexander Volfovsky",http://arxiv.org/pdf/2505.18118v1,cs.LG
MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning,"Despite the widespread exploration of Retrieval-Augmented Generation (RAG),
its deployment in enterprises for domain-specific datasets remains limited due
to poor answer accuracy. These corpora, often shielded behind firewalls in
private enterprise knowledge bases, having complex, domain-specific
terminology, rarely seen by LLMs during pre-training; exhibit significant
semantic variability across domains (like networking, military, or legal,
etc.), or even within a single domain like medicine, and thus result in poor
context precision for RAG systems. Currently, in such situations, fine-tuning
or RAG with fine-tuning is attempted, but these approaches are slow, expensive,
and lack generalization for accuracy as the new domain-specific data emerges.
We propose an approach for Enterprise Search that focuses on enhancing the
retriever for a domain-specific corpus through hybrid query indexes and
metadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata
generation pipeline using key concepts, topics, and acronyms, and then creates
a metadata-enriched hybrid index with boosted search queries. This approach
avoids overfitting and generalizes effectively across domains. On the PubMedQA
benchmark for the biomedical domain, the proposed method achieves 82% retrieval
accuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results
without fine-tuning and sets a new benchmark for zero-shot results while
outperforming much larger models like GPT3.5. The results are even comparable
to the best fine-tuned models on this dataset, and we further demonstrate the
robustness and scalability of the approach by evaluating it on other Q&A
datasets like SQuAD, NQ etc.",2025-05-23,"Kunal Sawarkar, Shivam R. Solanki, Abhilasha Mangal",http://arxiv.org/pdf/2505.18247v1,cs.LG
Bridging Supervised Learning and Reinforcement Learning in Math Reasoning,"Reinforcement Learning (RL) has played a central role in the recent surge of
LLMs' math abilities by enabling self-improvement through binary verifier
signals. In contrast, Supervised Learning (SL) is rarely considered for such
verification-driven training, largely due to its heavy reliance on reference
answers and inability to reflect on mistakes. In this work, we challenge the
prevailing notion that self-improvement is exclusive to RL and propose
Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to
reflect on their failures and improve autonomously with no external teachers.
In online training, instead of throwing away self-generated negative answers,
NFT constructs an implicit negative policy to model them. This implicit policy
is parameterized with the same positive LLM we target to optimize on positive
data, enabling direct policy optimization on all LLMs' generations. We conduct
experiments on 7B and 32B models in math reasoning tasks. Results consistently
show that through the additional leverage of negative feedback, NFT
significantly improves over SL baselines like Rejection sampling Fine-Tuning,
matching or even surpassing leading RL algorithms like GRPO and DAPO.
Furthermore, we demonstrate that NFT and GRPO are actually equivalent in
strict-on-policy training, even though they originate from entirely different
theoretical foundations. Our experiments and theoretical findings bridge the
gap between SL and RL methods in binary-feedback learning systems.",2025-05-23,"Huayu Chen, Kaiwen Zheng, Qinsheng Zhang, Ganqu Cui, Yin Cui, Haotian Ye, Tsung-Yi Lin, Ming-Yu Liu, Jun Zhu, Haoxiang Wang",http://arxiv.org/pdf/2505.18116v1,cs.LG
Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization,"Training quantized neural networks requires addressing the non-differentiable
and discrete nature of the underlying optimization problem. To tackle this
challenge, the straight-through estimator (STE) has become the most widely
adopted heuristic, allowing backpropagation through discrete operations by
introducing surrogate gradients. However, its theoretical properties remain
largely unexplored, with few existing works simplifying the analysis by
assuming an infinite amount of training data. In contrast, this work presents
the first finite-sample analysis of STE in the context of neural network
quantization. Our theoretical results highlight the critical role of sample
size in the success of STE, a key insight absent from existing studies.
Specifically, by analyzing the quantization-aware training of a two-layer
neural network with binary weights and activations, we derive the sample
complexity bound in terms of the data dimensionality that guarantees the
convergence of STE-based optimization to the global minimum. Moreover, in the
presence of label noises, we uncover an intriguing recurrence property of
STE-gradient method, where the iterate repeatedly escape from and return to the
optimal binary weights. Our analysis leverages tools from compressed sensing
and dynamical systems theory.",2025-05-23,"Halyun Jeong, Jack Xin, Penghang Yin",http://arxiv.org/pdf/2505.18113v1,cs.LG
F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles,"Nanomaterial research is becoming a vital area for energy, medicine, and
materials science, and accurate analysis of the nanoparticle topology is
essential to determine their properties. Unfortunately, the lack of
high-quality annotated datasets drastically hinders the creation of strong
segmentation models for nanoscale imaging. To alleviate this problem, we
introduce F-ANcGAN, an attention-enhanced cycle consistent generative
adversarial system that can be trained using a limited number of data samples
and generates realistic scanning electron microscopy (SEM) images directly from
segmentation maps. Our model uses a Style U-Net generator and a U-Net
segmentation network equipped with self-attention to capture structural
relationships and applies augmentation methods to increase the variety of the
dataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset
generation, with a further reduction in FID score to nearly 10.39 by using
efficient post-processing techniques. By facilitating scalable high-fidelity
synthetic dataset generation, our approach can improve the effectiveness of
downstream segmentation task training, overcoming severe data shortage issues
in nanoparticle analysis, thus extending its applications to resource-limited
fields.",2025-05-23,"Varun Ajith, Anindya Pal, Saumik Bhattacharya, Sayantari Ghosh",http://arxiv.org/pdf/2505.18106v1,cs.LG
Decomposition of Water Demand Patterns Using Skewed Gaussian Distributions for Behavioral Insights and Operational Planning,"This study presents a novel approach for decomposing urban water demand
patterns using Skewed Gaussian Distributions (SGD) to derive behavioral
insights and support operational planning. Hourly demand profiles contain
critical information for both long-term infrastructure design and daily
operations, influencing network pressures, water quality, energy consumption,
and overall reliability. By breaking down each daily demand curve into a
baseline component and distinct peak components, the proposed SGD method
characterizes each peak with interpretable parameters, including peak
amplitude, timing (mean), spread (duration), and skewness (asymmetry), thereby
reconstructing the observed pattern and uncovering latent usage dynamics. This
detailed peak-level decomposition enables both operational applications, e.g.
anomaly and leakage detection, real-time demand management, and strategic
analyses, e.g. identifying behavioral shifts, seasonal influences, or policy
impacts on consumption patterns. Unlike traditional symmetric Gaussian or
purely statistical time-series models, SGDs explicitly capture asymmetric peak
shapes such as sharp morning surges followed by gradual declines, improving the
fidelity of synthetic pattern generation and enhancing the detection of
irregular consumption behavior. The method is demonstrated on several
real-world datasets, showing that SGD outperforms symmetric Gaussian models in
reconstruction accuracy, reducing root-mean-square error by over 50% on
average, while maintaining physical interpretability. The SGD framework can
also be used to construct synthetic demand scenarios by designing daily peak
profiles with chosen characteristics. All implementation code is publicly
available at: https://github.com/Relkayam/water-demand-decomposition-sgd",2025-05-23,Roy Elkayam,http://arxiv.org/pdf/2505.18245v1,cs.LG
How Can I Publish My LLM Benchmark Without Giving the True Answers Away?,"Publishing a large language model (LLM) benchmark on the Internet risks
contaminating future LLMs: the benchmark may be unintentionally (or
intentionally) used to train or select a model. A common mitigation is to keep
the benchmark private and let participants submit their models or predictions
to the organizers. However, this strategy will require trust in a single
organization and still permits test-set overfitting through repeated queries.
To overcome this issue, we propose a way to publish benchmarks without
completely disclosing the ground-truth answers to the questions, while still
maintaining the ability to openly evaluate LLMs. Our main idea is to inject
randomness to the answers by preparing several logically correct answers, and
only include one of them as the solution in the benchmark. This reduces the
best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is
this helpful to keep us from disclosing the ground truth, but this approach
also offers a test for detecting data contamination. In principle, even fully
capable models should not surpass the Bayes accuracy. If a model surpasses this
ceiling despite this expectation, this is a strong signal of data
contamination. We present experimental evidence that our method can detect data
contamination accurately on a wide range of benchmarks, models, and training
methodologies.",2025-05-23,"Takashi Ishida, Thanawat Lodkaew, Ikko Yamane",http://arxiv.org/pdf/2505.18102v1,cs.LG
Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning,"Online Continual Learning (OCL) presents a complex learning environment in
which new data arrives in a batch-to-batch online format, and the risk of
catastrophic forgetting can significantly impair model efficacy. In this study,
we address OCL by introducing an innovative memory framework that incorporates
a short-term memory system to retain dynamic information and a long-term memory
system to archive enduring knowledge. Specifically, the long-term memory system
comprises a collection of sub-memory buffers, each linked to a cluster
prototype and designed to retain data samples from distinct categories. We
propose a novel $K$-means-based sample selection method to identify cluster
prototypes for each encountered category. To safeguard essential and critical
samples, we introduce a novel memory optimisation strategy that selectively
retains samples in the appropriate sub-memory buffer by evaluating each cluster
prototype against incoming samples through an optimal transportation mechanism.
This approach specifically promotes each sub-memory buffer to retain data
samples that exhibit significant discrepancies from the corresponding cluster
prototype, thereby ensuring the preservation of semantically rich information.
In addition, we propose a novel Divide-and-Conquer (DAC) approach that
formulates the memory updating as an optimisation problem and divides it into
several subproblems. As a result, the proposed DAC approach can solve these
subproblems separately and thus can significantly reduce computations of the
proposed memory updating process. We conduct a series of experiments across
standard and imbalanced learning settings, and the empirical findings indicate
that the proposed memory framework achieves state-of-the-art performance in
both learning contexts.",2025-05-23,"Congren Dai, Huichi Zhou, Jiahao Huang, Zhenxuan Zhang, Fanwen Wang, Guang Yang, Fei Ye",http://arxiv.org/pdf/2505.18101v1,cs.LG
Towards more transferable adversarial attack in black-box manner,"Adversarial attacks have become a well-explored domain, frequently serving as
evaluation baselines for model robustness. Among these, black-box attacks based
on transferability have received significant attention due to their practical
applicability in real-world scenarios. Traditional black-box methods have
generally focused on improving the optimization framework (e.g., utilizing
momentum in MI-FGSM) to enhance transferability, rather than examining the
dependency on surrogate white-box model architectures. Recent state-of-the-art
approach DiffPGD has demonstrated enhanced transferability by employing
diffusion-based adversarial purification models for adaptive attacks. The
inductive bias of diffusion-based adversarial purification aligns naturally
with the adversarial attack process, where both involving noise addition,
reducing dependency on surrogate white-box model selection. However, the
denoising process of diffusion models incurs substantial computational costs
through chain rule derivation, manifested in excessive VRAM consumption and
extended runtime. This progression prompts us to question whether introducing
diffusion models is necessary. We hypothesize that a model sharing similar
inductive bias to diffusion-based adversarial purification, combined with an
appropriate loss function, could achieve comparable or superior transferability
while dramatically reducing computational overhead. In this paper, we propose a
novel loss function coupled with a unique surrogate model to validate our
hypothesis. Our approach leverages the score of the time-dependent classifier
from classifier-guided diffusion models, effectively incorporating natural data
distribution knowledge into the adversarial optimization process. Experimental
results demonstrate significantly improved transferability across diverse model
architectures while maintaining robustness against diffusion-based defenses.",2025-05-23,"Chun Tong Lei, Zhongliang Guo, Hon Chung Lee, Minh Quoc Duong, Chun Pong Lau",http://arxiv.org/pdf/2505.18097v1,cs.LG
Data Mixing Can Induce Phase Transitions in Knowledge Acquisition,"Large Language Models (LLMs) are typically trained on data mixtures: most
data come from web scrapes, while a small portion is curated from high-quality
sources with dense domain-specific knowledge. In this paper, we show that when
training LLMs on such data mixtures, knowledge acquisition from knowledge-dense
datasets, unlike training exclusively on knowledge-dense data
(arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit
phase transitions with respect to the mixing ratio and model size. Through
controlled experiments on a synthetic biography dataset mixed with web-scraped
data, we demonstrate that: (1) as we increase the model size to a critical
value, the model suddenly transitions from memorizing very few to most of the
biographies; (2) below a critical mixing ratio, the model memorizes almost
nothing even with extensive training, but beyond this threshold, it rapidly
memorizes more biographies. We attribute these phase transitions to a capacity
allocation phenomenon: a model with bounded capacity must act like a knapsack
problem solver to minimize the overall test loss, and the optimal allocation
across datasets can change discontinuously as the model size or mixing ratio
varies. We formalize this intuition in an information-theoretic framework and
reveal that these phase transitions are predictable, with the critical mixing
ratio following a power-law relationship with the model size. Our findings
highlight a concrete case where a good mixing recipe for large models may not
be optimal for small models, and vice versa.",2025-05-23,"Xinran Gu, Kaifeng Lyu, Jiazheng Li, Jingzhao Zhang",http://arxiv.org/pdf/2505.18091v1,cs.LG
Early-Exit Graph Neural Networks,"Early-exit mechanisms allow deep neural networks to halt inference as soon as
classification confidence is high enough, adaptively trading depth for
confidence, and thereby cutting latency and energy on easy inputs while
retaining full-depth accuracy for harder ones. Similarly, adding early exit
mechanisms to Graph Neural Networks (GNNs), the go-to models for
graph-structured data, allows for dynamic trading depth for confidence on
simple graphs while maintaining full-depth accuracy on harder and more complex
graphs to capture intricate relationships. Although early exits have proven
effective across various deep learning domains, their potential within GNNs in
scenarios that require deep architectures while resisting over-smoothing and
over-squashing remains largely unexplored. We unlock that potential by first
introducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose
symmetry-based inductive biases mitigate these issues and yield stable
intermediate representations that can be useful to allow early exiting in GNNs.
Building on this backbone, we present Early-Exit Graph Neural Networks
(EEGNNs), which append confidence-aware exit heads that allow on-the-fly
termination of propagation based on each node or the entire graph. Experiments
show that EEGNNs preserve robust performance as depth grows and deliver
competitive accuracy on heterophilic and long-range benchmarks, matching
attention-based and asynchronous message-passing models while substantially
reducing computation and latency. We plan to release the code to reproduce our
experiments.",2025-05-23,"Andrea Giuseppe Di Francesco, Maria Sofia Bucarelli, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Fabrizio Silvestri",http://arxiv.org/pdf/2505.18088v1,cs.LG
Stable Reinforcement Learning for Efficient Reasoning,"The success of Deepseek-R1 has drawn the LLM community's attention to
reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1
outcome reward methods lack the capability to regulate the intermediate
reasoning processes during chain-of-thought (CoT) generation, leading to severe
overthinking phenomena. In response, recent studies have designed reward
functions to reinforce models' behaviors in producing shorter yet correct
completions. Nevertheless, we observe that these length-penalty reward
functions exacerbate RL training instability: as the completion length
decreases, model accuracy abruptly collapses, often occurring early in
training. To address this issue, we propose a simple yet effective solution
GRPO-$\lambda$, an efficient and stabilized variant of GRPO, which dynamically
adjusts the reward strategy by monitoring the correctness ratio among
completions within each query-sampled group. A low correctness ratio indicates
the need to avoid length penalty that compromises CoT quality, triggering a
switch to length-agnostic 0/1 rewards that prioritize reasoning capability. A
high ratio maintains length penalties to boost efficiency. Experimental results
show that our approach avoids training instability caused by length penalty
while maintaining the optimal accuracy-efficiency trade-off. On the GSM8K,
GPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average
accuracy by 1.48% while reducing CoT sequence length by 47.3%.",2025-05-23,"Muzhi Dai, Shixuan Liu, Qingyi Si",http://arxiv.org/pdf/2505.18086v1,cs.LG
What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?,"In planning, stitching is an ability of algorithms to piece together
sub-trajectories of data they are trained on to generate new and diverse
behaviours. While stitching is historically a strength of offline reinforcement
learning, recent generative behavioural cloning (BC) methods have also shown
proficiency at stitching. However, the main factors behind this are poorly
understood, hindering the development of new algorithms that can reliably
stitch. Focusing on diffusion planners trained via BC, we find two properties
are needed to compose: \emph{positional equivariance} and \emph{local
receptiveness}. We use these two properties to explain architecture, data, and
inference choices in existing generative BC methods based on diffusion
planning, including replanning frequency, data augmentation, and data scaling.
Experimental comparisions show that (1) while locality is more important than
positional equivariance in creating a diffusion planner capable of composition,
both are crucial (2) enabling these properties through relatively simple
architecture choices can be competitive with more computationally expensive
methods such as replanning or scaling data, and (3) simple inpainting-based
guidance can guide architecturally compositional models to enable
generalization in goal-conditioned settings.",2025-05-23,"Quentin Clark, Florian Shkurti",http://arxiv.org/pdf/2505.18083v1,cs.LG
An Iterative Framework for Generative Backmapping of Coarse Grained Proteins,"The techniques of data-driven backmapping from coarse-grained (CG) to
fine-grained (FG) representation often struggle with accuracy, unstable
training, and physical realism, especially when applied to complex systems such
as proteins. In this work, we introduce a novel iterative framework by using
conditional Variational Autoencoders and graph-based neural networks,
specifically designed to tackle the challenges associated with such large-scale
biomolecules. Our method enables stepwise refinement from CG beads to full
atomistic details. We outline the theory of iterative generative backmapping
and demonstrate via numerical experiments the advantages of multistep schemes
by applying them to proteins of vastly different structures with very coarse
representations. This multistep approach not only improves the accuracy of
reconstructions but also makes the training process more computationally
efficient for proteins with ultra-CG representations.",2025-05-23,"Georgios Kementzidis, Erin Wong, John Nicholson, Ruichen Xu, Yuefan Deng",http://arxiv.org/pdf/2505.18082v1,cs.LG
Backpropagation-Free Metropolis-Adjusted Langevin Algorithm,"Recent work on backpropagation-free learning has shown that it is possible to
use forward-mode automatic differentiation (AD) to perform optimization on
differentiable models. Forward-mode AD requires sampling a tangent vector for
each forward pass of a model. The result is the model evaluation with the
directional derivative along the tangent. In this paper, we illustrate how the
sampling of this tangent vector can be incorporated into the proposal mechanism
for the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the
first to introduce a backpropagation-free gradient-based Markov chain Monte
Carlo (MCMC) algorithm. We also extend to a novel backpropagation-free
position-specific preconditioned forward-mode MALA that leverages Hessian
information. Overall, we propose four new algorithms: Forward MALA; Line
Forward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward
MALA. We highlight the reduced computational cost of the forward-mode samplers
and show that forward-mode is competitive with the original MALA, while even
outperforming it depending on the probabilistic model. We include Bayesian
inference results on a range of probabilistic models, including hierarchical
distributions and Bayesian neural networks.",2025-05-23,"Adam D. Cobb, Susmit Jha",http://arxiv.org/pdf/2505.18081v1,cs.LG
AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction,"This paper presents AFD-STA Net, a neural framework integrating adaptive
filtering and spatiotemporal dynamics learning for predicting high-dimensional
chaotic systems governed by partial differential equations. The architecture
combines: 1) An adaptive exponential smoothing module with position-aware decay
coefficients for robust attractor reconstruction, 2) Parallel attention
mechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated
fusion of multiscale features, and 4) Deep projection networks with
dimension-scaling capabilities. Numerical experiments on nonlinear PDE systems
demonstrate the model's effectiveness in maintaining prediction accuracy under
both smooth and strongly chaotic regimes while exhibiting noise tolerance
through adaptive filtering. Component ablation studies confirm critical
contributions from each module, particularly highlighting the essential role of
spatiotemporal attention in learning complex dynamical interactions. The
framework shows promising potential for real-world applications requiring
simultaneous handling of measurement uncertainties and high-dimensional
nonlinear dynamics.",2025-05-23,"Chunlin Gong, Yin Wang, Jingru Li, Hanleran Zhang",http://arxiv.org/pdf/2505.18080v1,cs.LG
Bayesian Deep Learning for Discrete Choice,"Discrete choice models (DCMs) are used to analyze individual decision-making
in contexts such as transportation choices, political elections, and consumer
preferences. DCMs play a central role in applied econometrics by enabling
inference on key economic variables, such as marginal rates of substitution,
rather than focusing solely on predicting choices on new unlabeled data.
However, while traditional DCMs offer high interpretability and support for
point and interval estimation of economic quantities, these models often
underperform in predictive tasks compared to deep learning (DL) models. Despite
their predictive advantages, DL models remain largely underutilized in discrete
choice due to concerns about their lack of interpretability, unstable parameter
estimates, and the absence of established methods for uncertainty
quantification. Here, we introduce a deep learning model architecture
specifically designed to integrate with approximate Bayesian inference methods,
such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model
collapses to behaviorally informed hypotheses when data is limited, mitigating
overfitting and instability in underspecified settings while retaining the
flexibility to capture complex nonlinear relationships when sufficient data is
available. We demonstrate our approach using SGLD through a Monte Carlo
simulation study, evaluating both predictive metrics--such as out-of-sample
balanced accuracy--and inferential metrics--such as empirical coverage for
marginal rates of substitution interval estimates. Additionally, we present
results from two empirical case studies: one using revealed mode choice data in
NYC, and the other based on the widely used Swiss train choice stated
preference data.",2025-05-23,"Daniel F. Villarraga, Ricardo A. Daziano",http://arxiv.org/pdf/2505.18077v1,cs.LG
Emergence of Hebbian Dynamics in Regularized Non-Local Learners,"Stochastic Gradient Descent (SGD) has emerged as a remarkably effective
learning algorithm, underpinning nearly all state-of-the-art machine learning
models, from large language models to autonomous vehicles. Despite its
practical success, SGD appears fundamentally distinct from biological learning
mechanisms. It is widely believed that the biological brain can not implement
gradient descent because it is nonlocal, and we have found little (if any)
experimental evidence for it. In contrast, the brain is widely thought to learn
via local Hebbian learning principles, which have been seen as incompatible
with gradient descent. In this paper, we establish a theoretical and empirical
connection between the learning signals of neural networks trained using SGD
with weight decay and those trained with Hebbian learning near convergence. We
show that SGD with regularization can appear to learn according to a Hebbian
rule, and SGD with injected noise according to an anti-Hebbian rule. We also
provide empirical evidence that Hebbian learning properties can emerge in a
network with weight decay from virtually any learning rule--even random ones.
These results may bridge a long-standing gap between artificial and biological
learning, revealing Hebbian properties as an epiphenomenon of deeper
optimization principles and cautioning against interpreting their presence in
neural data as evidence against more complex hetero-synaptic mechanisms.",2025-05-23,"David Koplow, Tomaso Poggio, Liu Ziyin",http://arxiv.org/pdf/2505.18069v1,cs.LG
Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making,"Despite the growing promise of artificial intelligence (AI) in supporting
decision-making across domains, fostering appropriate human reliance on AI
remains a critical challenge. In this paper, we investigate the utility of
exploring distance-based uncertainty scores for task delegation to AI and
describe how these scores can be visualized through embedding representations
for human-AI decision-making. After developing an AI-based system for physical
stroke rehabilitation assessment, we conducted a study with 19 health
professionals and 10 students in medicine/health to understand the effect of
exploring distance-based uncertainty scores on users' reliance on AI. Our
findings showed that distance-based uncertainty scores outperformed traditional
probability-based uncertainty scores in identifying uncertain cases. In
addition, after exploring confidence scores for task delegation and reviewing
embedding-based visualizations of distance-based uncertainty scores,
participants achieved an 8.20% higher rate of correct decisions, a 7.15% higher
rate of changing their decisions to correct ones, and a 7.14% lower rate of
incorrect changes after reviewing AI outputs than those reviewing
probability-based uncertainty scores ($p<0.01$). Our findings highlight the
potential of distance-based uncertainty scores to enhance decision accuracy and
appropriate reliance on AI while discussing ongoing challenges for human-AI
collaborative decision-making.",2025-05-23,"Min Hun Lee, Martyn Zhe Yu Tok",http://arxiv.org/pdf/2505.18066v1,cs.LG
Reward Model Generalization for Compute-Aware Test-Time Reasoning,"External test-time reasoning enhances large language models (LLMs) by
decoupling generation and selection. At inference time, the model generates
multiple reasoning paths, and an auxiliary process reward model (PRM) is used
to score and select the best one. A central challenge in this setting is
test-time compute optimality (TCO), i.e., how to maximize answer accuracy under
a fixed inference budget. In this work, we establish a theoretical framework to
analyze how the generalization error of the PRM affects compute efficiency and
reasoning performance. Leveraging PAC-Bayes theory, we derive generalization
bounds and show that a lower generalization error of PRM leads to fewer samples
required to find correct answers. Motivated by this analysis, we propose
Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically
controls search behavior. The actor outputs sampling hyperparameters based on
reward distributions and sparsity statistics, while the critic estimates their
utility to guide budget allocation. Experiments on the MATH and AIME benchmarks
with various LLMs and PRMs demonstrate that CATS consistently outperforms other
external TTS methods, validating our theoretical predictions.",2025-05-23,"Zeen Song, Wenwen Qiang, Siyu Zhao, Changwen Zheng, Gang Hua",http://arxiv.org/pdf/2505.18065v1,cs.LG
Asymptotically optimal regret in communicating Markov decision processes,"In this paper, we present a learning algorithm that achieves asymptotically
optimal regret for Markov decision processes in average reward under a
communicating assumption. That is, given a communicating Markov decision
process $M$, our algorithm has regret $K(M) \log(T) + \mathrm{o}(\log(T))$
where $T$ is the number of learning steps and $K(M)$ is the best possible
constant. This algorithm works by explicitly tracking the constant $K(M)$ to
learn optimally, then balances the trade-off between exploration (playing
sub-optimally to gain information), co-exploration (playing optimally to gain
information) and exploitation (playing optimally to score maximally). We
further show that the function $K(M)$ is discontinuous, which is a consequence
challenge for our approach. To that end, we describe a regularization mechanism
to estimate $K(M)$ with arbitrary precision from empirical data.",2025-05-23,Victor Boone,http://arxiv.org/pdf/2505.18064v1,cs.LG
Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions,"The Restricted Boltzmann Machine (RBM) is one of the simplest generative
neural networks capable of learning input distributions. Despite its
simplicity, the analysis of its performance in learning from the training data
is only well understood in cases that essentially reduce to singular value
decomposition of the data. Here, we consider the limit of a large dimension of
the input space and a constant number of hidden units. In this limit, we
simplify the standard RBM training objective into a form that is equivalent to
the multi-index model with non-separable regularization. This opens a path to
analyze training of the RBM using methods that are established for multi-index
models, such as Approximate Message Passing (AMP) and its state evolution, and
the analysis of Gradient Descent (GD) via the dynamical mean-field theory. We
then give rigorous asymptotics of the training dynamics of RBM on data
generated by the spiked covariance model as a prototype of a structure suitable
for unsupervised learning. We show in particular that RBM reaches the optimal
computational weak recovery threshold, aligning with the BBP transition, in the
spiked covariance model.",2025-05-23,"Yizhou Xu, Florent Krzakala, Lenka Zdeborová",http://arxiv.org/pdf/2505.18046v1,cs.LG
Linear Mixture Distributionally Robust Markov Decision Processes,"Many real-world decision-making problems face the off-dynamics challenge: the
agent learns a policy in a source domain and deploys it in a target domain with
different state transitions. The distributionally robust Markov decision
process (DRMDP) addresses this challenge by finding a robust policy that
performs well under the worst-case environment within a pre-specified
uncertainty set of transition dynamics. Its effectiveness heavily hinges on the
proper design of these uncertainty sets, based on prior knowledge of the
dynamics. In this work, we propose a novel linear mixture DRMDP framework,
where the nominal dynamics is assumed to be a linear mixture model. In contrast
with existing uncertainty sets directly defined as a ball centered around the
nominal kernel, linear mixture DRMDPs define the uncertainty sets based on a
ball around the mixture weighting parameter. We show that this new framework
provides a more refined representation of uncertainties compared to
conventional models based on $(s,a)$-rectangularity and $d$-rectangularity,
when prior knowledge about the mixture model is present. We propose a meta
algorithm for robust policy learning in linear mixture DRMDPs with general
$f$-divergence defined uncertainty sets, and analyze its sample complexities
under three divergence metrics instantiations: total variation,
Kullback-Leibler, and $\chi^2$ divergences. These results establish the
statistical learnability of linear mixture DRMDPs, laying the theoretical
foundation for future research on this new setting.",2025-05-23,"Zhishuai Liu, Pan Xu",http://arxiv.org/pdf/2505.18044v1,cs.LG
Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach,"Clustering is a fundamental task in both machine learning and data mining.
Among various methods, edge-colored clustering (ECC) has emerged as a useful
approach for handling categorical data. Given a hypergraph with (hyper)edges
labeled by colors, ECC aims to assign vertex colors to minimize the number of
edges where the vertex color differs from the edge's color. However,
traditional ECC has inherent limitations, as it enforces a nonoverlapping and
exhaustive clustering. To tackle these limitations, three versions of ECC have
been studied: Local ECC and Global ECC, which allow overlapping clusters, and
Robust ECC, which accounts for vertex outliers. For these problems, both linear
programming (LP) rounding algorithms and greedy combinatorial algorithms have
been proposed. While these LP-rounding algorithms provide high-quality
solutions, they demand substantial computation time; the greedy algorithms, on
the other hand, run very fast but often compromise solution quality. In this
paper, we present an algorithmic framework that combines the strengths of LP
with the computational efficiency of combinatorial algorithms. Both
experimental and theoretical analyses show that our algorithms efficiently
produce high-quality solutions for all three problems: Local, Global, and
Robust ECC. We complement our algorithmic contributions with
complexity-theoretic inapproximability results and integrality gap bounds,
which suggest that significant theoretical improvements are unlikely. Our
results also answer two open questions previously raised in the literature.",2025-05-23,"Changyeol Lee, Yongho Shin, Hyung-Chan An",http://arxiv.org/pdf/2505.18043v1,cs.LG
Mahalanobis++: Improving OOD Detection via Feature Normalization,"Detecting out-of-distribution (OOD) examples is an important task for
deploying reliable machine learning models in safety-critial applications.
While post-hoc methods based on the Mahalanobis distance applied to pre-logit
features are among the most effective for ImageNet-scale OOD detection, their
performance varies significantly across models. We connect this inconsistency
to strong variations in feature norms, indicating severe violations of the
Gaussian assumption underlying the Mahalanobis distance estimation. We show
that simple $\ell_2$-normalization of the features mitigates this problem
effectively, aligning better with the premise of normally distributed data with
shared covariance matrix. Extensive experiments on 44 models across diverse
architectures and pretraining schemes show that $\ell_2$-normalization improves
the conventional Mahalanobis distance-based approaches significantly and
consistently, and outperforms other recently proposed OOD detection methods.",2025-05-23,"Maximilian Mueller, Matthias Hein",http://arxiv.org/pdf/2505.18032v1,cs.LG
Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons,"Many preference elicitation algorithms consider preference over propositional
logic formulas or items with different attributes. In sequential decision
making, a user's preference can be a preorder over possible outcomes, each of
which is a temporal sequence of events. This paper considers a class of
preference inference problems where the user's unknown preference is
represented by a preorder over regular languages (sets of temporal sequences),
referred to as temporal goals. Given a finite set of pairwise comparisons
between finite words, the objective is to learn both the set of temporal goals
and the preorder over these goals. We first show that a preference relation
over temporal goals can be modeled by a Preference Deterministic Finite
Automaton (PDFA), which is a deterministic finite automaton augmented with a
preorder over acceptance conditions. The problem of preference inference
reduces to learning the PDFA. This problem is shown to be computationally
challenging, with the problem of determining whether there exists a PDFA of
size smaller than a given integer $k$, consistent with the sample, being
NP-Complete. We formalize the properties of characteristic samples and develop
an algorithm that guarantees to learn, given a characteristic sample, the
minimal PDFA equivalent to the true PDFA from which the sample is drawn. We
present the method through a running example and provide detailed analysis
using a robotic motion planning problem.",2025-05-23,"Hazhar Rahmani, Jie Fu",http://arxiv.org/pdf/2505.18030v1,cs.LG
Knot So Simple: A Minimalistic Environment for Spatial Reasoning,"We propose KnotGym, an interactive environment for complex, spatial reasoning
and manipulation. KnotGym includes goal-oriented rope manipulation tasks with
varying levels of complexity, all requiring acting from pure image
observations. Tasks are defined along a clear and quantifiable axis of
complexity based on the number of knot crossings, creating a natural
generalization test. KnotGym has a simple observation space, allowing for
scalable development, yet it highlights core challenges in integrating acute
perception, spatial reasoning, and grounded manipulation. We evaluate methods
of different classes, including model-based RL, model-predictive control, and
chain-of-thought reasoning, and illustrate the challenges KnotGym presents.
KnotGym is available at https://github.com/lil-lab/knotgym.",2025-05-23,"Zizhao Chen, Yoav Artzi",http://arxiv.org/pdf/2505.18028v1,cs.LG
Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time,"Recent years have seen significant progress in developing spiking neural
networks (SNNs) as a potential solution to the energy challenges posed by
conventional artificial neural networks (ANNs). However, our theoretical
understanding of SNNs remains relatively limited compared to the ever-growing
body of literature on ANNs. In this paper, we study a discrete-time model of
SNNs based on leaky integrate-and-fire (LIF) neurons, referred to as
discrete-time LIF-SNNs, a widely used framework that still lacks solid
theoretical foundations. We demonstrate that discrete-time LIF-SNNs with static
inputs and outputs realize piecewise constant functions defined on polyhedral
regions, and more importantly, we quantify the network size required to
approximate continuous functions. Moreover, we investigate the impact of
latency (number of time steps) and depth (number of layers) on the complexity
of the input space partitioning induced by discrete-time LIF-SNNs. Our analysis
highlights the importance of latency and contrasts these networks with ANNs
employing piecewise linear activation functions. Finally, we present numerical
experiments to support our theoretical findings.",2025-05-23,"Duc Anh Nguyen, Ernesto Araya, Adalbert Fono, Gitta Kutyniok",http://arxiv.org/pdf/2505.18023v1,cs.LG
Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling,"Deep generative models hold great promise for representing complex physical
systems, but their deployment is currently limited by the lack of guarantees on
the physical plausibility of the generated outputs. Ensuring that known
physical constraints are enforced is therefore critical when applying
generative models to scientific and engineering problems. We address this
limitation by developing a principled framework for sampling from a target
distribution while rigorously satisfying physical constraints. Leveraging the
variational formulation of Langevin dynamics, we propose Split Augmented
Langevin (SAL), a novel primal-dual sampling algorithm that enforces
constraints progressively through variable splitting, with convergence
guarantees. While the method is developed theoretically for Langevin dynamics,
we demonstrate its effective applicability to diffusion models. In particular,
we use constrained diffusion models to generate physical fields satisfying
energy and mass conservation laws. We apply our method to diffusion-based data
assimilation on a complex physical system, where enforcing physical constraints
substantially improves both forecast accuracy and the preservation of critical
conserved quantities. We also demonstrate the potential of SAL for challenging
feasibility problems in optimal control.",2025-05-23,"Matthieu Blanke, Yongquan Qu, Sara Shamekh, Pierre Gentine",http://arxiv.org/pdf/2505.18017v2,cs.LG
SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification,"Reliability and generalization in deep learning are predominantly studied in
the context of image classification. Yet, real-world applications in
safety-critical domains involve a broader set of semantic tasks, such as
semantic segmentation and object detection, which come with a diverse set of
dedicated model architectures. To facilitate research towards robust model
design in segmentation and detection, our primary objective is to provide
benchmarking tools regarding robustness to distribution shifts and adversarial
manipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH,
along with the most extensive evaluation to date on the reliability and
generalization of semantic segmentation and object detection models. In
particular, we benchmark 76 segmentation models across four datasets and 61
object detectors across two datasets, evaluating their performance under
diverse adversarial attacks and common corruptions. Our findings reveal
systematic weaknesses in state-of-the-art models and uncover key trends based
on architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are
open-sourced in our GitHub repository
(https://github.com/shashankskagnihotri/benchmarking_reliability_generalization)
along with our complete set of total 6139 evaluations. We anticipate the
collected data to foster and encourage future research towards improved model
reliability beyond classification.",2025-05-23,"Shashank Agnihotri, David Schader, Jonas Jakubassa, Nico Sharei, Simon Kral, Mehmet Ege Kaçar, Ruben Weber, Margret Keuper",http://arxiv.org/pdf/2505.18015v1,cs.LG
Deep Operator Neural Network Model Predictive Control,"In this paper, we consider the design of model predictive control (MPC)
algorithms based on deep operator neural networks (DeepONets). These neural
networks are capable of accurately approximating real and complex valued
solutions of continuous time nonlinear systems without relying on recurrent
architectures. The DeepONet architecture is made up of two feedforward neural
networks: the branch network, which encodes the input function space, and the
trunk network, which represents dependencies on temporal variables or initial
conditions. Utilizing the original DeepONet architecture as a predictor within
MPC for Multi Input Multi Output (MIMO) systems requires multiple branch
networks, to generate multi output predictions, one for each input. Moreover,
to predict multiple time steps into the future, the network has to be evaluated
multiple times. Motivated by this, we introduce a multi step DeepONet
(MS-DeepONet) architecture that computes in one shot multi step predictions of
system outputs from multi step input sequences, which is better suited for MPC.
We prove that the MS DeepONet is a universal approximator in terms of multi
step sequence prediction. Additionally, we develop automated hyper parameter
selection strategies and implement MPC frameworks using both the standard
DeepONet and the proposed MS DeepONet architectures in PyTorch. The
implementation is publicly available on GitHub. Simulation results demonstrate
that MS-DeepONet consistently outperforms the standard DeepONet in learning and
predictive control tasks across several nonlinear benchmark systems: the van
der Pol oscillator, the quadruple tank process, and a cart pendulum unstable
system, where it successfully learns and executes multiple swing up and
stabilization policies.",2025-05-23,"Thomas Oliver de Jong, Khemraj Shukla, Mircea Lazar",http://arxiv.org/pdf/2505.18008v1,cs.LG
Distances for Markov chains from sample streams,"Bisimulation metrics are powerful tools for measuring similarities between
stochastic processes, and specifically Markov chains. Recent advances have
uncovered that bisimulation metrics are, in fact, optimal-transport distances,
which has enabled the development of fast algorithms for computing such metrics
with provable accuracy and runtime guarantees. However, these recent methods,
as well as all previously known methods, assume full knowledge of the
transition dynamics. This is often an impractical assumption in most real-world
scenarios, where typically only sample trajectories are available. In this
work, we propose a stochastic optimization method that addresses this
limitation and estimates bisimulation metrics based on sample access, without
requiring explicit transition models. Our approach is derived from a new linear
programming (LP) formulation of bisimulation metrics, which we solve using a
stochastic primal-dual optimization method. We provide theoretical guarantees
on the sample complexity of the algorithm and validate its effectiveness
through a series of empirical evaluations.",2025-05-23,"Sergio Calo, Anders Jonsson, Gergely Neu, Ludovic Schwartz, Javier Segovia-Aguas",http://arxiv.org/pdf/2505.18005v1,cs.LG
An Example Safety Case for Safeguards Against Misuse,"Existing evaluations of AI misuse safeguards provide a patchwork of evidence
that is often difficult to connect to real-world decisions. To bridge this gap,
we describe an end-to-end argument (a ""safety case"") that misuse safeguards
reduce the risk posed by an AI assistant to low levels. We first describe how a
hypothetical developer red teams safeguards, estimating the effort required to
evade them. Then, the developer plugs this estimate into a quantitative ""uplift
model"" to determine how much barriers introduced by safeguards dissuade misuse
(https://www.aimisusemodel.com/). This procedure provides a continuous signal
of risk during deployment that helps the developer rapidly respond to emerging
threats. Finally, we describe how to tie these components together into a
simple safety case. Our work provides one concrete path -- though not the only
path -- to rigorously justifying AI misuse risks are low.",2025-05-23,"Joshua Clymer, Jonah Weinbaum, Robert Kirk, Kimberly Mai, Selena Zhang, Xander Davies",http://arxiv.org/pdf/2505.18003v1,cs.LG
Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View Perspective,"Graph anomaly detection aims to identify unusual patterns in graph-based
data, with wide applications in fields such as web security and financial fraud
detection. Existing methods typically rely on contrastive learning, assuming
that a lower similarity between a node and its local subgraph indicates
abnormality. However, these approaches overlook a crucial limitation: the
presence of interfering edges invalidates this assumption, since it introduces
disruptive noise that compromises the contrastive learning process.
Consequently, this limitation impairs the ability to effectively learn
meaningful representations of normal patterns, leading to suboptimal detection
performance. To address this issue, we propose a Clean-View Enhanced Graph
Anomaly Detection framework (CVGAD), which includes a multi-scale anomaly
awareness module to identify key sources of interference in the contrastive
learning process. Moreover, to mitigate bias from the one-step edge removal
process, we introduce a novel progressive purification module. This module
incrementally refines the graph by iteratively identifying and removing
interfering edges, thereby enhancing model performance. Extensive experiments
on five benchmark datasets validate the effectiveness of our approach.",2025-05-23,"Di Jin, Jingyi Cao, Xiaobao Wang, Bingdao Feng, Dongxiao He, Longbiao Wang, Jianwu Dang",http://arxiv.org/pdf/2505.18002v1,cs.LG
"Anytime-valid, Bayes-assisted,Prediction-Powered Inference","Given a large pool of unlabelled data and a smaller amount of labels,
prediction-powered inference (PPI) leverages machine learning predictions to
increase the statistical efficiency of standard confidence interval procedures
based solely on labelled data, while preserving their fixed-time validity.
  In this paper, we extend the PPI framework to the sequential setting, where
labelled and unlabelled datasets grow over time.
  Exploiting Ville's inequality and the method of mixtures, we propose
prediction-powered confidence sequence procedures that are valid uniformly over
time and naturally accommodate prior knowledge on the quality of the
predictions to further boost efficiency.
  We carefully illustrate the design choices behind our method and demonstrate
its effectiveness in real and synthetic examples.",2025-05-23,"Valentin Kilian, Stefano Cortinovis, François Caron",http://arxiv.org/pdf/2505.18000v1,cs.LG
Revisiting Feature Interactions from the Perspective of Quadratic Neural Networks for Click-through Rate Prediction,"Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR)
prediction tasks due to its simplicity, effectiveness, and ability to capture
feature interactions without additional parameters. However, the underlying
reasons for its effectiveness remain unclear. In this paper, we revisit HP from
the perspective of Quadratic Neural Networks (QNN), which leverage quadratic
interaction terms to model complex feature relationships. We further reveal
QNN's ability to expand the feature space and provide smooth nonlinear
approximations without relying on activation functions. Meanwhile, we find that
traditional post-activation does not further improve the performance of the
QNN. Instead, mid-activation is a more suitable alternative. Through
theoretical analysis and empirical evaluation of 25 QNN neuron formats, we
identify a good-performing variant and make further enhancements on it.
Specifically, we propose the Multi-Head Khatri-Rao Product as a superior
alternative to HP and a Self-Ensemble Loss with dynamic ensemble capability
within the same network to enhance computational efficiency and performance.
Ultimately, we propose a novel neuron format, QNN-alpha, which is tailored for
CTR prediction tasks. Experimental results show that QNN-alpha achieves new
state-of-the-art performance on six public datasets while maintaining low
inference latency, good scalability, and excellent compatibility. The code,
running logs, and detailed hyperparameter configurations are available at:
https://github.com/salmon1802/QNN.",2025-05-23,"Honghao Li, Yiwen Zhang, Yi Zhang, Lei Sang, Jieming Zhu",http://arxiv.org/pdf/2505.17999v1,cs.LG
Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective,"The VAPO framework has demonstrated significant empirical success in
enhancing the efficiency and reliability of reinforcement learning for long
chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By
systematically addressing challenges such as value model bias, heterogeneous
sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art
performance. While its practical benefits are evident, a deeper theoretical
understanding of its underlying mechanisms and potential limitations is crucial
for guiding future advancements. This paper aims to initiate such a discussion
by exploring VAPO from a theoretical perspective, highlighting areas where its
assumptions might be challenged and where further investigation could yield
more robust and generalizable reasoning agents. We delve into the intricacies
of value function approximation in complex reasoning spaces, the optimality of
adaptive advantage estimation, the impact of token-level optimization, and the
enduring challenges of exploration and generalization.",2025-05-23,"Jintian Shao, Yiming Cheng, Hongyi Huang, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng",http://arxiv.org/pdf/2505.17997v1,cs.LG
Outcome-based Reinforcement Learning to Predict the Future,"Reinforcement learning with verifiable rewards (RLVR) has boosted math and
coding in large language models, yet there has been little effort to extend
RLVR into messier, real-world domains like forecasting. One sticking point is
that outcome-based reinforcement learning for forecasting must learn from
binary, delayed, and noisy rewards, a regime where standard fine-tuning is
brittle. We show that outcome-only online RL on a 14B model can match
frontier-scale accuracy and surpass it in calibration and hypothetical
prediction market betting by adapting two leading algorithms, Group-Relative
Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our
adaptations remove per-question variance scaling in GRPO, apply
baseline-subtracted advantages in ReMax, hydrate training with 100k temporally
consistent synthetic questions, and introduce lightweight guard-rails that
penalise gibberish, non-English responses and missing rationales, enabling a
single stable pass over 110k events. Scaling ReMax to 110k questions and
ensembling seven predictions yields a 14B model that matches frontier baseline
o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in
calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this
calibration edge into \$127 of hypothetical profit versus \$92 for o1 (p =
0.037). This demonstrates that refined RLVR methods can convert small-scale
LLMs into potentially economically valuable forecasting tools, with
implications for scaling this to larger models.",2025-05-23,"Benjamin Turtel, Danny Franklin, Kris Skotheim, Luke Hewitt, Philipp Schoenegger",http://arxiv.org/pdf/2505.17989v2,cs.LG
Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning,"R1-style Reinforcement Learning (RL) significantly enhances Large Language
Models' reasoning capabilities, yet the mechanism behind rule-based RL remains
unclear. We found that small-scale SFT has significant influence on RL but
shows poor efficiency. To explain our observations, we propose an analytical
framework and compare the efficiency of SFT and RL by measuring sample effect.
Hypothetical analysis show that SFT efficiency is limited by training data.
Guided by our analysis, we propose Re-distillation, a technique that fine-tunes
pretrain model through small-scale distillation from the RL-trained policy.
Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's
surprising efficiency: re-distilled models match RL performance with far fewer
samples and less computation. Empirical verification shows that sample effect
is a good indicator of performance improvements. As a result, on K&K dataset,
our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT
samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches
its instruct-tuned variant without RL. Our work explains several interesting
phenomena in R1-style RL, shedding light on the mechanisms behind its empirical
success. Code is available at: https://github.com/on1262/deep-reasoning",2025-05-23,"Yutong Chen, Jiandong Gao, Ji Wu",http://arxiv.org/pdf/2505.17988v1,cs.LG
"ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling","Real world collection of Activities of Daily Living data is challenging due
to privacy concerns, costly deployment and labeling, and the inherent sparsity
and imbalance of human behavior. We present ADLGen, a generative framework
specifically designed to synthesize realistic, event triggered, and symbolic
sensor sequences for ambient assistive environments. ADLGen integrates a
decoder only Transformer with sign based symbolic temporal encoding, and a
context and layout aware sampling mechanism to guide generation toward
semantically rich and physically plausible sensor event sequences. To enhance
semantic fidelity and correct structural inconsistencies, we further
incorporate a large language model into an automatic generate evaluate refine
loop, which verifies logical, behavioral, and temporal coherence and generates
correction rules without manual intervention or environment specific tuning.
Through comprehensive experiments with novel evaluation metrics, ADLGen is
shown to outperform baseline generators in statistical fidelity, semantic
richness, and downstream activity recognition, offering a scalable and
privacy-preserving solution for ADL data synthesis.",2025-05-23,"Weihang You, Hanqi Jiang, Zishuai Liu, Zihang Xie, Tianming Liu, Jin Lu, Fei Dou",http://arxiv.org/pdf/2505.17987v1,cs.LG
Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models,"The Fisher information is a fundamental concept for characterizing the
sensitivity of parameters in neural networks. However, leveraging the full
observed Fisher information is too expensive for large models, so most methods
rely on simple diagonal approximations. While efficient, this approach ignores
parameter correlations, often resulting in reduced performance on downstream
tasks. In this work, we mitigate these limitations and propose Generalized
Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that
accounts for both diagonal and off-diagonal elements of the Fisher information
matrix, providing a more accurate reflection of parameter importance. To make
the method tractable, we introduce a scalable adaptation of the
Kronecker-factored approximation algorithm for the observed Fisher information.
We demonstrate the effectiveness of our method on LLM compression, showing
improvements over existing compression baselines. For example, at a 20
compression rate on the MMLU benchmark, our method outperforms FWSVD, which is
based on a diagonal approximation of the Fisher information, by 5 percent,
SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.",2025-05-23,"Viktoriia Chekalina, Daniil Moskovskiy, Daria Cherniuk, Maxim Kurkin, Andrey Kuznetsov, Evgeny Frolov",http://arxiv.org/pdf/2505.17974v1,cs.LG
To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models,"Feature matching is a necessary step for many computer vision and
photogrammetry applications such as image registration, structure-from-motion,
and visual localization. Classical handcrafted methods such as SIFT feature
detection and description combined with nearest neighbour matching and RANSAC
outlier removal have been state-of-the-art for mobile mapping cameras. With
recent advances in deep learning, learnable methods have been introduced and
proven to have better robustness and performance under complex conditions.
Despite their growing adoption, a comprehensive comparison between classical
and learnable feature matching methods for the specific task of semantic 3D
building camera-to-model matching is still missing. This submission
systematically evaluates the effectiveness of different feature-matching
techniques in visual localization using textured CityGML LoD2 models. We use
standard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets
consisting of facade textures and corresponding camera images (terrestrial and
drone). For the latter, we evaluate the achievable accuracy of the absolute
pose estimated using a Perspective-n-Point (PnP) algorithm, with geometric
ground truth derived from geo-referenced trajectory data. The results indicate
that the learnable feature matching methods vastly outperform traditional
approaches regarding accuracy and robustness on our challenging custom datasets
with zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We
believe that this work will foster the development of model-based visual
localization methods. Link to the code:
https://github.com/simBauer/To\_Glue\_or\_not\_to\_Glue",2025-05-23,"Simone Gaisbauer, Prabin Gyawali, Qilin Zhang, Olaf Wysocki, Boris Jutzi",http://arxiv.org/pdf/2505.17973v1,cs.LG
MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings,"Feature engineering for generalized seizure detection models remains a
significant challenge. Recently proposed models show variable performance
depending on the training data and remain ineffective at accurately
distinguishing artifacts from seizure data. In this study, we propose a novel
end-to-end model, ''Multiresolutional EEGWaveNet (MR-EEGWaveNet),'' which
efficiently distinguishes seizure events from background electroencephalogram
(EEG) and artifacts/noise by capturing both temporal dependencies across
different time frames and spatial relationships between channels. The model has
three modules: convolution, feature extraction, and predictor. The convolution
module extracts features through depth-wise and spatio-temporal convolution.
The feature extraction module individually reduces the feature dimension
extracted from EEG segments and their sub-segments. Subsequently, the extracted
features are concatenated into a single vector for classification using a fully
connected classifier called the predictor module. In addition, an anomaly
score-based post-classification processing technique was introduced to reduce
the false-positive rates of the model. Experimental results were reported and
analyzed using different parameter settings and datasets (Siena (public) and
Juntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the
conventional non-multiresolution approach, improving the F1 scores from 0.177
to 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9%
and 20.62%, respectively.",2025-05-23,"Kazi Mahmudul Hassan, Xuyang Zhao, Hidenori Sugano, Toshihisa Tanaka",http://arxiv.org/pdf/2505.17972v1,cs.LG
Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems,"Using AI to create autonomous researchers has the potential to accelerate
scientific discovery. A prerequisite for this vision is understanding how well
an AI model can identify the underlying structure of a black-box system from
its behavior. In this paper, we explore how well a large language model (LLM)
learns to identify a black-box function from passively observed versus actively
collected data. We investigate the reverse-engineering capabilities of LLMs
across three distinct types of black-box systems, each chosen to represent
different problem domains where future autonomous AI researchers may have
considerable impact: Program, Formal Language, and Math Equation. Through
extensive experiments, we show that LLMs fail to extract information from
observations, reaching a performance plateau that falls short of the ideal of
Bayesian inference. However, we demonstrate that prompting LLMs to not only
observe but also intervene -- actively querying the black-box with specific
inputs to observe the resulting output -- improves performance by allowing LLMs
to test edge cases and refine their beliefs. By providing the intervention data
from one LLM to another, we show that this improvement is partly a result of
engaging in the process of generating effective interventions, paralleling
results in the literature on human learning. Further analysis reveals that
engaging in intervention can help LLMs escape from two common failure modes:
overcomplication, where the LLM falsely assumes prior knowledge about the
black-box, and overlooking, where the LLM fails to incorporate observations.
These insights provide practical guidance for helping LLMs more effectively
reverse-engineer black-box systems, supporting their use in making new
discoveries.",2025-05-23,"Jiayi Geng, Howard Chen, Dilip Arumugam, Thomas L. Griffiths",http://arxiv.org/pdf/2505.17968v1,cs.LG
SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models,"Low-rank optimization has emerged as a promising direction in training large
language models (LLMs) to reduce the memory usage of adaptive optimizers by
constraining learning to a lower-dimensional space. Prior work typically
projects gradients of linear layers using approaches based on Singular Value
Decomposition (SVD). However, applying SVD-based procedures individually to
each layer in large models is computationally expensive and incurs additional
memory costs due to storing the projection matrices. In this work, we propose a
computationally efficient and conceptually simple two-step procedure to
approximate SVD-based gradient projections into lower-dimensional spaces.
First, we construct a complete orthogonal basis using predefined orthogonal
matrices of the Discrete Cosine Transform (DCT). Second, we adaptively select
basis columns based on their alignment with the gradient of each layer. Each
projection matrix in our method is obtained via a single matrix multiplication
followed by a lightweight sorting step to identify the most relevant basis
vectors. Due to the predefined nature of the orthogonal bases, they are
computed once at the start of training. During training, we store only the
indices of the selected columns, avoiding the need to store full projection
matrices for each layer. Our numerical experiments on both pre-training and
fine-tuning tasks demonstrate the effectiveness of our dual strategy in
approximating optimal low-rank projections, matching the performance of costly
SVD-based methods while achieving faster runtime and reduced memory usage.",2025-05-23,"Ionut-Vlad Modoranu, Mher Safaryan, Erik Schultheis, Dan Alistarh",http://arxiv.org/pdf/2505.17967v1,cs.LG
New Tight Bounds for SGD without Variance Assumption: A Computer-Aided Lyapunov Analysis,"The analysis of Stochastic Gradient Descent (SGD) often relies on making some
assumption on the variance of the stochastic gradients, which is usually not
satisfied or difficult to verify in practice. This paper contributes to a
recent line of works which attempt to provide guarantees without making any
variance assumption, leveraging only the (strong) convexity and smoothness of
the loss functions. In this context, we prove new theoretical bounds derived
from the monotonicity of a simple Lyapunov energy, improving the current
state-of-the-art and extending their validity to larger step-sizes. Our
theoretical analysis is backed by a Performance Estimation Problem analysis,
which allows us to claim that, empirically, the bias term in our bounds is
tight within our framework.",2025-05-23,"Daniel Cortild, Lucas Ketels, Juan Peypouquet, Guillaume Garrigos",http://arxiv.org/pdf/2505.17965v1,cs.LG
A Principled Bayesian Framework for Training Binary and Spiking Neural Networks,"We propose a Bayesian framework for training binary and spiking neural
networks that achieves state-of-the-art performance without normalisation
layers. Unlike commonly used surrogate gradient methods -- often heuristic and
sensitive to hyperparameter choices -- our approach is grounded in a
probabilistic model of noisy binary networks, enabling fully end-to-end
gradient-based optimisation. We introduce importance-weighted straight-through
(IW-ST) estimators, a unified class generalising straight-through and
relaxation-based estimators. We characterise the bias-variance trade-off in
this family and derive a bias-minimising objective implemented via an auxiliary
loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs),
a variational inference framework that uses posterior noise to train Binary and
Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient
bias, regularises parameters, and introduces dropout-like noise. By linking
low-bias conditions, vanishing gradients, and the KL term, we enable training
of deep residual networks without normalisation. Experiments on CIFAR-10, DVS
Gesture, and SHD show our method matches or exceeds existing approaches without
normalisation or hand-tuned gradients.",2025-05-23,"James A. Walker, Moein Khajehnejad, Adeel Razi",http://arxiv.org/pdf/2505.17962v1,cs.LG
Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development,"Owing to the typical long-tail data distribution issues, simulating
domain-gap-free synthetic data is crucial in robotics, photogrammetry, and
computer vision research. The fundamental challenge pertains to credibly
measuring the difference between real and simulated data. Such a measure is
vital for safety-critical applications, such as automated driving, where
out-of-domain samples may impact a car's perception and cause fatal accidents.
Previous work has commonly focused on simulating data on one scene and
analyzing performance on a different, real-world scene, hampering the disjoint
analysis of domain gap coming from networks' deficiencies, class definitions,
and object representation. In this paper, we propose a novel approach to
measuring the domain gap between the real world sensor observations and
simulated data representing the same location, enabling comprehensive domain
gap analysis. To measure such a domain gap, we introduce a novel metric
DoGSS-PCL and evaluation assessing the geometric and semantic quality of the
simulated point cloud. Our experiments corroborate that the introduced approach
can be used to measure the domain gap. The tests also reveal that synthetic
semantic point clouds may be used for training deep neural networks,
maintaining the performance at the 50/50 real-to-synthetic ratio. We strongly
believe that this work will facilitate research on credible data simulation and
allow for at-scale deployment in automated driving testing and digital
twinning.",2025-05-23,"Nguyen Duc, Yan-Ling Lai, Patrick Madlindl, Xinyuan Zhu, Benedikt Schwab, Olaf Wysocki, Ludwig Hoegner, Thomas H. Kolbe",http://arxiv.org/pdf/2505.17959v1,cs.LG
The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks,"We study the high-dimensional asymptotics of empirical risk minimization
(ERM) in over-parametrized two-layer neural networks with quadratic activations
trained on synthetic data. We derive sharp asymptotics for both training and
test errors by mapping the $\ell_2$-regularized learning problem to a convex
matrix sensing task with nuclear norm penalization. This reveals that capacity
control in such networks emerges from a low-rank structure in the learned
feature maps. Our results characterize the global minima of the loss and yield
precise generalization thresholds, showing how the width of the target function
governs learnability. This analysis bridges and extends ideas from spin-glass
methods, matrix factorization, and convex optimization and emphasizes the deep
link between low-rank matrix sensing and learning in quadratic neural networks.",2025-05-23,"Vittorio Erba, Emanuele Troiani, Lenka Zdeborová, Florent Krzakala",http://arxiv.org/pdf/2505.17958v1,cs.LG
VeriThinker: Learning to Verify Makes Reasoning Model Efficient,"Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought
(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily
lengthy reasoning chains, dramatically increasing inference costs. To mitigate
this issue, we introduce VeriThinker, a novel approach for CoT compression.
Unlike conventional methods that fine-tune LRMs directly on the original
reasoning task using synthetic concise CoT data, we innovatively fine-tune the
model solely through an auxiliary verification task. By training LRMs to
accurately verify the correctness of CoT solutions, the LRMs inherently become
more discerning about the necessity of subsequent self-reflection steps,
thereby effectively suppressing overthinking. Extensive experiments validate
that VeriThinker substantially reduces reasoning chain lengths while
maintaining or even slightly improving accuracy. When applied to
DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500
from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on
AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to
40.8%). Additionally, our experiments demonstrate that VeriThinker can also be
zero-shot generalized to speculative reasoning. Code is available at
https://github.com/czg1225/VeriThinker",2025-05-23,"Zigeng Chen, Xinyin Ma, Gongfan Fang, Ruonan Yu, Xinchao Wang",http://arxiv.org/pdf/2505.17941v1,cs.LG
Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding,"Graph Neural Networks (GNNs) excel at learning from pairwise interactions but
often overlook multi-way and hierarchical relationships. Topological Deep
Learning (TDL) addresses this limitation by leveraging combinatorial
topological spaces. However, existing TDL models are restricted to undirected
settings and fail to capture the higher-order directed patterns prevalent in
many complex systems, e.g., brain networks, where such interactions are both
abundant and functionally significant. To fill this gap, we introduce
Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that
operate on semi-simplicial sets -- combinatorial structures that encode
directed higher-order motifs and their directional relationships. To enhance
scalability, we propose Routing-SSNs, which dynamically select the most
informative relations in a learnable manner. We prove that SSNs are strictly
more expressive than standard graph and TDL models. We then introduce a new
principled framework for brain dynamics representation learning, grounded in
the ability of SSNs to provably recover topological descriptors shown to
successfully characterize brain activity. Empirically, SSNs achieve
state-of-the-art performance on brain dynamics classification tasks,
outperforming the second-best model by up to 27%, and message passing GNNs by
up to 50% in accuracy. Our results highlight the potential of principled
topological models for learning from structured brain data, establishing a
unique real-world case study for TDL. We also test SSNs on standard node
classification and edge regression tasks, showing competitive performance. We
will make the code and data publicly available.",2025-05-23,"Manuel Lecha, Andrea Cavallo, Francesca Dominici, Ran Levi, Alessio Del Bue, Elvin Isufi, Pietro Morerio, Claudio Battiloro",http://arxiv.org/pdf/2505.17939v1,cs.LG
LMask: Learn to Solve Constrained Routing Problems with Lazy Masking,"Routing problems are canonical combinatorial optimization tasks with
wide-ranging applications in logistics, transportation, and supply chain
management. However, solving these problems becomes significantly more
challenging when complex constraints are involved. In this paper, we propose
LMask, a novel learning framework that utilizes dynamic masking to generate
high-quality feasible solutions for constrained routing problems. LMask
introduces the LazyMask decoding method, which lazily refines feasibility masks
with the backtracking mechanism. In addition, it employs the refinement
intensity embedding to encode the search trace into the model, mitigating
representation ambiguities induced by backtracking. To further reduce sampling
cost, LMask sets a backtracking budget during decoding, while constraint
violations are penalized in the loss function during training to counteract
infeasibility caused by this budget. We provide theoretical guarantees for the
validity and probabilistic optimality of our approach. Extensive experiments on
the traveling salesman problem with time windows (TSPTW) and TSP with draft
limits (TSPDL) demonstrate that LMask achieves state-of-the-art feasibility
rates and solution quality, outperforming existing neural methods.",2025-05-23,"Tianyou Li, Haijun Zou, Jiayuan Wu, Zaiwen Wen",http://arxiv.org/pdf/2505.17938v1,cs.LG
Understanding Gated Neurons in Transformers from Their Input-Output Functionality,"Interpretability researchers have attempted to understand MLP neurons of
language models based on both the contexts in which they activate and their
output weight vectors. They have paid little attention to a complementary
aspect: the interactions between input and output. For example, when neurons
detect a direction in the input, they might add much the same direction to the
residual stream (""enrichment neurons"") or reduce its presence (""depletion
neurons""). We address this aspect by examining the cosine similarity between
input and output weights of a neuron. We apply our method to 12 models and find
that enrichment neurons dominate in early-middle layers whereas later layers
tend more towards depletion. To explain this finding, we argue that enrichment
neurons are largely responsible for enriching concept representations, one of
the first steps of factual recall. Our input-output perspective is a complement
to activation-dependent analyses and to approaches that treat input and output
separately.",2025-05-23,"Sebastian Gerstner, Hinrich Schütze",http://arxiv.org/pdf/2505.17936v1,cs.LG
Selection Mechanisms for Sequence Modeling using Linear State Space Models,"Recent advancements in language modeling tasks have been driven by
architectures such as Transformers and, more recently, by Selective State Space
Models (SSMs). In this paper, we introduce an alternative selection mechanism
inspired by control theory methodologies. Specifically, we propose a novel
residual generator for selection, drawing an analogy to fault detection
strategies in Linear Time-Invariant (LTI) systems. Unlike Mamba, which utilizes
Linear Time-Varying (LTV) systems, our approach combines multiple LTI systems,
preserving their beneficial properties during training while achieving
comparable selectivity. To evaluate the effectiveness of the proposed
architecture, we test its performance on synthetic tasks. While these tasks are
not inherently critical, they serve as benchmarks to test the selectivity
properties of different cores architecture. This work highlights the potential
of integrating theoretical insights with experimental advancements, offering a
complementary perspective to deep learning innovations at the intersection of
control theory and machine learning.",2025-05-23,"Umberto Casti, Sandro Zampieri, Fabio Pasqualetti",http://arxiv.org/pdf/2505.17932v1,cs.LG
AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models,"Medical image segmentation is vital for clinical diagnosis, yet current deep
learning methods often demand extensive expert effort, i.e., either through
annotating large training datasets or providing prompts at inference time for
each new case. This paper introduces a zero-shot and automatic segmentation
pipeline that combines off-the-shelf vision-language and segmentation
foundation models. Given a medical image and a task definition (e.g., ""segment
the optic disc in an eye fundus image""), our method uses a grounding model to
generate an initial bounding box, followed by a visual prompt boosting module
that enhance the prompts, which are then processed by a promptable segmentation
model to produce the final mask. To address the challenges of domain gap and
result verification, we introduce a test-time adaptation framework featuring a
set of learnable adaptors that align the medical inputs with foundation model
representations. Its hyperparameters are optimized via Bayesian Optimization,
guided by a proxy validation model without requiring ground-truth labels. Our
pipeline offers an annotation-efficient and scalable solution for zero-shot
medical image segmentation across diverse tasks. Our pipeline is evaluated on
seven diverse medical imaging datasets and shows promising results. By proper
decomposition and test-time adaptation, our fully automatic pipeline performs
competitively with weakly-prompted interactive foundation models.",2025-05-23,"Xingjian Li, Qifeng Wu, Colleen Que, Yiran Ding, Adithya S. Ubaradka, Jianhua Xing, Tianyang Wang, Min Xu",http://arxiv.org/pdf/2505.17931v1,cs.LG
Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV,"Intensive care unit (ICU) is a crucial hospital department that handles
life-threatening cases. Nowadays machine learning (ML) is being leveraged in
healthcare ubiquitously. In recent years, management of ICU became one of the
most significant parts of the hospital functionality (largely but not only due
to the worldwide COVID-19 pandemic). This study explores multiple ML approaches
for predicting LOS in ICU specifically for the patients with neurological
diseases based on the MIMIC-IV dataset. The evaluated models include classic ML
algorithms (K-Nearest Neighbors, Random Forest, XGBoost and CatBoost) and
Neural Networks (LSTM, BERT and Temporal Fusion Transformer). Given that LOS
prediction is often framed as a classification task, this study categorizes LOS
into three groups: less than two days, less than a week, and a week or more. As
the first ML-based approach targeting LOS prediction for neurological disorder
patients, this study does not aim to outperform existing methods but rather to
assess their effectiveness in this specific context. The findings provide
insights into the applicability of ML techniques for improving ICU resource
management and patient care. According to the results, Random Forest model
proved to outperform others on static, achieving an accuracy of 0.68, a
precision of 0.68, a recall of 0.68, and F1-score of 0.67. While BERT model
outperformed LSTM model on time-series data with an accuracy of 0.80, a
precision of 0.80, a recall of 0.80 and F1-score 0.80.",2025-05-23,"Alexander Gabitashvili, Philipp Kellmeyer",http://arxiv.org/pdf/2505.17929v1,cs.LG
Towards Practical Defect-Focused Automated Code Review,"The complexity of code reviews has driven efforts to automate review
comments, but prior approaches oversimplify this task by treating it as
snippet-level code-to-text generation and relying on text similarity metrics
like BLEU for evaluation. These methods overlook repository context, real-world
merge request evaluation, and defect detection, limiting their practicality. To
address these issues, we explore the full automation pipeline within the online
recommendation service of a company with nearly 400 million daily active users,
analyzing industry-grade C++ codebases comprising hundreds of thousands of
lines of code. We identify four key challenges: 1) capturing relevant context,
2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and
4) integrating human workflows. To tackle these, we propose 1) code slicing
algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a
filtering mechanism for FAR reduction, and 4) a novel prompt design for better
human interaction. Our approach, validated on real-world merge requests from
historical fault reports, achieves a 2x improvement over standard LLMs and a
10x gain over previous baselines. While the presented results focus on C++, the
underlying framework design leverages language-agnostic principles (e.g.,
AST-based analysis), suggesting potential for broader applicability.",2025-05-23,"Junyi Lu, Lili Jiang, Xiaojia Li, Jianbing Fang, Fengjun Zhang, Li Yang, Chun Zuo",http://arxiv.org/pdf/2505.17928v1,cs.LG
Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy,"Determining the type of kidney stones is crucial for prescribing appropriate
treatments to prevent recurrence. Currently, various approaches exist to
identify the type of kidney stones. However, obtaining results through the
reference ex vivo identification procedure can take several weeks, while in
vivo visual recognition requires highly trained specialists. For this reason,
deep learning models have been developed to provide urologists with an
automated classification of kidney stones during ureteroscopies. Nevertheless,
a common issue with these models is the lack of training data. This
contribution presents a deep learning method based on few-shot learning, aimed
at producing sufficiently discriminative features for identifying kidney stone
types in endoscopic images, even with a very limited number of samples. This
approach was specifically designed for scenarios where endoscopic images are
scarce or where uncommon classes are present, enabling classification even with
a limited training dataset. The results demonstrate that Prototypical Networks,
using up to 25% of the training data, can achieve performance equal to or
better than traditional deep learning models trained with the complete dataset.",2025-05-23,"Carlos Salazar-Ruiz, Francisco Lopez-Tiro, Ivan Reyes-Amezcua, Clement Larose, Gilberto Ochoa-Ruiz, Christian Daul",http://arxiv.org/pdf/2505.17921v1,cs.LG
KITINet: Kinetics Theory Inspired Network Architectures with PDE Simulation Approaches,"Despite the widely recognized success of residual connections in modern
neural networks, their design principles remain largely heuristic. This paper
introduces KITINet (Kinetics Theory Inspired Network), a novel architecture
that reinterprets feature propagation through the lens of non-equilibrium
particle dynamics and partial differential equation (PDE) simulation. At its
core, we propose a residual module that models feature updates as the
stochastic evolution of a particle system, numerically simulated via a
discretized solver for the Boltzmann transport equation (BTE). This formulation
mimics particle collisions and energy exchange, enabling adaptive feature
refinement via physics-informed interactions. Additionally, we reveal that this
mechanism induces network parameter condensation during training, where
parameters progressively concentrate into a sparse subset of dominant channels.
Experiments on scientific computation (PDE operator), image classification
(CIFAR-10/100), and text classification (IMDb/SNLI) show consistent
improvements over classic network baselines, with negligible increase of FLOPs.",2025-05-23,"Mingquan Feng, Yifan Fu, Tongcheng Zhang, Yu Jiang, Yixin Huang, Junchi Yan",http://arxiv.org/pdf/2505.17919v1,cs.LG
LLM Meeting Decision Trees on Tabular Data,"Tabular data have been playing a vital role in diverse real-world fields,
including healthcare, finance, etc. With the recent success of Large Language
Models (LLMs), early explorations of extending LLMs to the domain of tabular
data have been developed. Most of these LLM-based methods typically first
serialize tabular data into natural language descriptions, and then tune LLMs
or directly infer on these serialized data. However, these methods suffer from
two key inherent issues: (i) data perspective: existing data serialization
methods lack universal applicability for structured tabular data, and may pose
privacy risks through direct textual exposure, and (ii) model perspective: LLM
fine-tuning methods struggle with tabular data, and in-context learning
scalability is bottle-necked by input length constraints (suitable for few-shot
learning). This work explores a novel direction of integrating LLMs into
tabular data throughough logical decision tree rules as intermediaries,
proposes a decision tree enhancer with LLM-derived rule for tabular prediction,
DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied
to full data learning setting without LLM fine-tuning. Specifically, we
leverage the reasoning ability of LLMs to redesign an improved rule given a set
of decision tree rules. Furthermore, we provide a calibration method for
original decision trees via new generated rule by LLM, which approximates the
error correction vector to steer the original decision tree predictions in the
direction of ``errors'' reducing. Finally, extensive experiments on diverse
tabular benchmarks show that our method achieves state-of-the-art performance.",2025-05-23,"Hangting Ye, Jinmeng Li, He Zhao, Dandan Guo, Yi Chang",http://arxiv.org/pdf/2505.17918v1,cs.LG
M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model,"We propose a novel method, termed the M-learner, for estimating heterogeneous
indirect and total treatment effects and identifying relevant subgroups within
a mediation framework. The procedure comprises four key steps. First, we
compute individual-level conditional average indirect/total treatment effect
Second, we construct a distance matrix based on pairwise differences. Third, we
apply tSNE to project this matrix into a low-dimensional Euclidean space,
followed by K-means clustering to identify subgroup structures. Finally, we
calibrate and refine the clusters using a threshold-based procedure to
determine the optimal configuration. To the best of our knowledge, this is the
first approach specifically designed to capture treatment effect heterogeneity
in the presence of mediation. Experimental results validate the robustness and
effectiveness of the proposed framework. Application to the real-world Jobs II
dataset highlights the broad adaptability and potential applicability of our
method.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.",2025-05-23,"Xingyu Li, Qing Liu, Tony Jiang, Hong Amy Xia, Brian P. Hobbs, Peng Wei",http://arxiv.org/pdf/2505.17917v1,cs.LG
Flexible MOF Generation with Torsion-Aware Flow Matching,"Designing metal-organic frameworks (MOFs) with novel chemistries is a
long-standing challenge due to their large combinatorial space and the complex
3D arrangements of building blocks. While recent deep generative models have
enabled scalable MOF generation, they assume (1) a fixed set of building blocks
and (2) known ground-truth local block-wise 3D coordinates. However, this
limits their ability to (1) design novel MOFs and (2) generate the structure
using novel building blocks. We propose a two-stage de novo MOF generation
framework that overcomes these limitations by modeling both chemical and
geometric degrees of freedom. First, we train a SMILES-based autoregressive
model to generate novel metal and organic building blocks, paired with
cheminformatics for 3D structure initialization. Second, we introduce a
flow-matching model that predicts translations, rotations, and torsional angles
to assemble flexible blocks into valid 3D frameworks. Our experiments
demonstrate improved reconstruction accuracy, the generation of valid, novel,
and unique MOFs, and the ability of our model to create novel building blocks.",2025-05-23,"Nayoung Kim, Seongsu Kim, Sungsoo Ahn",http://arxiv.org/pdf/2505.17914v1,cs.LG
NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling,"Model ensembles have long been a cornerstone for improving generalization and
robustness in deep learning. However, their effectiveness often comes at the
cost of substantial computational overhead. To address this issue,
state-of-the-art methods aim to replicate ensemble-class performance without
requiring multiple independently trained networks. Unfortunately, these
algorithms often still demand considerable compute at inference. In response to
these limitations, we introduce $\textbf{NeuroTrails}$, a sparse multi-head
architecture with dynamically evolving topology. This unexplored model-agnostic
training paradigm improves ensemble performance while reducing the required
resources. We analyze the underlying reason for its effectiveness and observe
that the various neural trails induced by dynamic sparsity attain a
$\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays
efficacy with convolutional and transformer-based architectures on computer
vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4,
among many others, demonstrate increased accuracy and stronger robustness in
zero-shot generalization, while requiring significantly fewer parameters.",2025-05-23,"Bram Grooten, Farid Hasanov, Chenxiang Zhang, Qiao Xiao, Boqian Wu, Zahra Atashgahi, Ghada Sokar, Shiwei Liu, Lu Yin, Elena Mocanu, Mykola Pechenizkiy, Decebal Constantin Mocanu",http://arxiv.org/pdf/2505.17909v1,cs.LG
Function Forms of Simple ReLU Networks with Random Hidden Weights,"We investigate the function space dynamics of a two-layer ReLU neural network
in the infinite-width limit, highlighting the Fisher information matrix (FIM)'s
role in steering learning. Extending seminal works on approximate
eigendecomposition of the FIM, we derive the asymptotic behavior of basis
functions ($f_v(x) = X^{\top} v $) for four groups of approximate eigenvectors,
showing their convergence to distinct function forms. These functions,
prioritized by gradient descent, exhibit FIM-induced inner products that
approximate orthogonality in the function space, forging a novel connection
between parameter and function spaces. Simulations validate the accuracy of
these theoretical approximations, confirming their practical relevance. By
refining the function space inner product's role, we advance the theoretical
framework for ReLU networks, illuminating their optimization and expressivity.
Overall, this work offers a robust foundation for understanding wide neural
networks and enhances insights into scalable deep learning architectures,
paving the way for improved design and analysis of neural networks.",2025-05-23,"Ka Long Keith Ho, Yoshinari Takeishi, Junichi Takeuchi",http://arxiv.org/pdf/2505.17907v1,cs.LG
Evolving Machine Learning: A Survey,"In an era defined by rapid data evolution, traditional machine learning (ML)
models often fall short in adapting to dynamic environments. Evolving Machine
Learning (EML) has emerged as a critical paradigm, enabling continuous learning
and adaptation in real-time data streams. This survey presents a comprehensive
analysis of EML, focusing on five core challenges: data drift, concept drift,
catastrophic forgetting, skewed learning, and network adaptation. We
systematically review over 120 studies, categorizing state-of-the-art methods
across supervised, unsupervised, and semi-supervised approaches. The survey
explores diverse evaluation metrics, benchmark datasets, and real-world
applications, offering a comparative lens on the effectiveness and limitations
of current techniques. Additionally, we highlight the growing role of adaptive
neural architectures, meta-learning, and ensemble strategies in addressing
evolving data complexities. By synthesizing insights from recent literature,
this work not only maps the current landscape of EML but also identifies
critical gaps and opportunities for future research. Our findings aim to guide
researchers and practitioners in developing robust, ethical, and scalable EML
systems for real-world deployment.",2025-05-23,"Ignacio Cabrera Martin, Subhaditya Mukherjee, Almas Baimagambetov, Joaquin Vanschoren, Nikolaos Polatidis",http://arxiv.org/pdf/2505.17902v1,cs.LG
Universal Domain Adaptation Benchmark for Time Series Data Representation,"Deep learning models have significantly improved the ability to detect
novelties in time series (TS) data. This success is attributed to their strong
representation capabilities. However, due to the inherent variability in TS
data, these models often struggle with generalization and robustness. To
address this, a common approach is to perform Unsupervised Domain Adaptation,
particularly Universal Domain Adaptation (UniDA), to handle domain shifts and
emerging novel classes. While extensively studied in computer vision, UniDA
remains underexplored for TS data. This work provides a comprehensive
implementation and comparison of state-of-the-art TS backbones in a UniDA
framework. We propose a reliable protocol to evaluate their robustness and
generalization across different domains. The goal is to provide practitioners
with a framework that can be easily extended to incorporate future advancements
in UniDA and TS architectures. Our results highlight the critical influence of
backbone selection in UniDA performance and enable a robustness analysis across
various datasets and architectures.",2025-05-23,"Romain Mussard, Fannia Pacheco, Maxime Berar, Gilles Gasso, Paul Honeine",http://arxiv.org/pdf/2505.17899v1,cs.LG
DataRater: Meta-Learned Dataset Curation,"The quality of foundation models depends heavily on their training data.
Consequently, great efforts have been put into dataset curation. Yet most
approaches rely on manual tuning of coarse-grained mixtures of large buckets of
data, or filtering by hand-crafted heuristics. An approach that is ultimately
more scalable (let alone more satisfying) is to \emph{learn} which data is
actually valuable for training. This type of meta-learning could allow more
sophisticated, fine-grained, and effective curation. Our proposed
\emph{DataRater} is an instance of this idea. It estimates the value of
training on any particular data point. This is done by meta-learning using
`meta-gradients', with the objective of improving training efficiency on held
out data. In extensive experiments across a range of model scales and datasets,
we find that using our DataRater to filter data is highly effective, resulting
in significantly improved compute efficiency.",2025-05-23,"Dan A. Calian, Gregory Farquhar, Iurii Kemaev, Luisa M. Zintgraf, Matteo Hessel, Jeremy Shar, Junhyuk Oh, András György, Tom Schaul, Jeffrey Dean, Hado van Hasselt, David Silver",http://arxiv.org/pdf/2505.17895v1,cs.LG
FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks,"Concepts such as objects, patterns, and shapes are how humans understand the
world. Building on this intuition, concept-based explainability methods aim to
study representations learned by deep neural networks in relation to
human-understandable concepts. Here, Concept Activation Vectors (CAVs) are an
important tool and can identify whether a model learned a concept or not.
However, the computational cost and time requirements of existing CAV
computation pose a significant challenge, particularly in large-scale,
high-dimensional architectures. To address this limitation, we introduce
FastCAV, a novel approach that accelerates the extraction of CAVs by up to
63.6x (on average 46.4x). We provide a theoretical foundation for our approach
and give concrete assumptions under which it is equivalent to established
SVM-based methods. Our empirical results demonstrate that CAVs calculated with
FastCAV maintain similar performance while being more efficient and stable. In
downstream applications, i.e., concept-based explanation methods, we show that
FastCAV can act as a replacement leading to equivalent insights. Hence, our
approach enables previously infeasible investigations of deep models, which we
demonstrate by tracking the evolution of concepts during model training.",2025-05-23,"Laines Schmalwasser, Niklas Penzel, Joachim Denzler, Julia Niebling",http://arxiv.org/pdf/2505.17883v1,cs.LG
The Origins of Representation Manifolds in Large Language Models,"There is a large ongoing scientific effort in mechanistic interpretability to
map embeddings and internal representations of AI systems into
human-understandable concepts. A key element of this effort is the linear
representation hypothesis, which posits that neural representations are sparse
linear combinations of `almost-orthogonal' direction vectors, reflecting the
presence or absence of different features. This model underpins the use of
sparse autoencoders to recover features from representations. Moving towards a
fuller model of features, in which neural representations could encode not just
the presence but also a potentially continuous and multidimensional value for a
feature, has been a subject of intense recent discourse. We describe why and
how a feature might be represented as a manifold, demonstrating in particular
that cosine similarity in representation space may encode the intrinsic
geometry of a feature through shortest, on-manifold paths, potentially
answering the question of how distance in representation space and relatedness
in concept space could be connected. The critical assumptions and predictions
of the theory are validated on text embeddings and token activations of large
language models.",2025-05-23,"Alexander Modell, Patrick Rubin-Delanchy, Nick Whiteley",http://arxiv.org/pdf/2505.18235v1,cs.LG
Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization,"In recent years, tensor decomposition-based approaches for hyperspectral
anomaly detection (HAD) have gained significant attention in the field of
remote sensing. However, existing methods often fail to fully leverage both the
global correlations and local smoothness of the background components in
hyperspectral images (HSIs), which exist in both the spectral and spatial
domains. This limitation results in suboptimal detection performance. To
mitigate this critical issue, we put forward a novel HAD method named
HAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR)
factors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first
decomposed into background and anomaly components. The TR decomposition is then
employed to capture the spatial-spectral correlations within the background
component. Additionally, we introduce a unified and efficient nonconvex
regularizer, induced by tensor singular value decomposition (TSVD), to
simultaneously encode the low-rankness and sparsity of the 3-D gradient TR
factors into a unique concise form. The above characterization scheme enables
the interpretable gradient TR factors to inherit the low-rankness and
smoothness of the original background. To further enhance anomaly detection, we
design a generalized nonconvex regularization term to exploit the group
sparsity of the anomaly component. To solve the resulting doubly nonconvex
model, we develop a highly efficient optimization algorithm based on the
alternating direction method of multipliers (ADMM) framework. Experimental
results on several benchmark datasets demonstrate that our proposed method
outperforms existing state-of-the-art (SOTA) approaches in terms of detection
accuracy.",2025-05-23,"Wenjin Qin, Hailin Wang, Hao Shu, Feng Zhang, Jianjun Wang, Xiangyong Cao, Xi-Le Zhao, Gemine Vivone",http://arxiv.org/pdf/2505.17881v1,cs.LG
Toward Optimal ANC: Establishing Mutual Information Lower Bound,"Active Noise Cancellation (ANC) algorithms aim to suppress unwanted acoustic
disturbances by generating anti-noise signals that destructively interfere with
the original noise in real time. Although recent deep learning-based ANC
algorithms have set new performance benchmarks, there remains a shortage of
theoretical limits to rigorously assess their improvements. To address this, we
derive a unified lower bound on cancellation performance composed of two
components. The first component is information-theoretic: it links residual
error power to the fraction of disturbance entropy captured by the anti-noise
signal, thereby quantifying limits imposed by information-processing capacity.
The second component is support-based: it measures the irreducible error
arising in frequency bands that the cancellation path cannot address,
reflecting fundamental physical constraints. By taking the maximum of these two
terms, our bound establishes a theoretical ceiling on the Normalized Mean
Squared Error (NMSE) attainable by any ANC algorithm. We validate its tightness
empirically on the NOISEX dataset under varying reverberation times,
demonstrating robustness across diverse acoustic conditions.",2025-05-23,"François Derrida, Shahar Lutati, Eliya Nachmani",http://arxiv.org/pdf/2505.17877v1,cs.LG
Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph Learning,"In practical domains, high-dimensional data are usually associated with
diverse semantic labels, whereas traditional feature selection methods are
designed for single-label data. Moreover, existing multi-label methods
encounter two main challenges in semi-supervised scenarios: (1). Most
semi-supervised methods fail to evaluate the label correlations without enough
labeled samples, which are the critical information of multi-label feature
selection, making label-specific features discarded. (2). The similarity graph
structure directly derived from the original feature space is suboptimal for
multi-label problems in existing graph-based methods, leading to unreliable
soft labels and degraded feature selection performance. To overcome them, we
propose a consistent sparse graph learning method for multi-label
semi-supervised feature selection (SGMFS), which can enhance the feature
selection performance by maintaining space consistency and learning label
correlations in semi-supervised scenarios. Specifically, for Challenge (1),
SGMFS learns a low-dimensional and independent label subspace from the
projected features, which can compatibly cross multiple labels and effectively
achieve the label correlations. For Challenge (2), instead of constructing a
fixed similarity graph for semi-supervised learning, SGMFS thoroughly explores
the intrinsic structure of the data by performing sparse reconstruction of
samples in both the label space and the learned subspace simultaneously. In
this way, the similarity graph can be adaptively learned to maintain the
consistency between label space and the learned subspace, which can promote
propagating proper soft labels for unlabeled samples, facilitating the ultimate
feature selection. An effective solution with fast convergence is designed to
optimize the objective function. Extensive experiments validate the superiority
of SGMFS.",2025-05-23,"Yan Zhong, Xingyu Wu, Xinping Zhao, Li Zhang, Xinyuan Song, Lei Shi, Bingbing Jiang",http://arxiv.org/pdf/2505.17875v1,cs.LG
Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting,"Multi-task forecasting has become the standard approach for time-series
forecasting (TSF). However, we show that it suffers from an Expressiveness
Bottleneck, where predictions at different time steps share the same
representation, leading to unavoidable errors even with optimal
representations. To address this issue, we propose a two-stage framework:
first, pre-train a foundation model for one-step-ahead prediction; then, adapt
it using step-specific LoRA modules.This design enables the foundation model to
handle any number of forecast steps while avoiding the expressiveness
bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which
employs adaptively weighted LoRA experts to achieve partial parameter sharing
across steps. This approach enhances both efficiency and forecasting
performance by exploiting interdependencies between forecast steps. Experiments
show that MoLA significantly improves model expressiveness and outperforms
state-of-the-art time-series forecasting methods. Code is available at
https://anonymous.4open.science/r/MoLA-BC92.",2025-05-23,"Licheng Pan, Zhichao Chen, Haoxuan Li, Guangyi Liu, Zhijian Xu, Zhaoran Liu, Hao Wang, Ying Wei",http://arxiv.org/pdf/2505.17872v1,cs.LG
BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models,"The advent of universal time series forecasting models has revolutionized
zero-shot forecasting across diverse domains, yet the critical role of data
diversity in training these models remains underexplored. Existing large-scale
time series datasets often suffer from inherent biases and imbalanced
distributions, leading to suboptimal model performance and generalization. To
address this gap, we introduce BLAST, a novel pre-training corpus designed to
enhance data diversity through a balanced sampling strategy. First, BLAST
incorporates 321 billion observations from publicly available datasets and
employs a comprehensive suite of statistical metrics to characterize time
series patterns. Then, to facilitate pattern-oriented sampling, the data is
implicitly clustered using grid-based partitioning. Furthermore, by integrating
grid sampling and grid mixup techniques, BLAST ensures a balanced and
representative coverage of diverse patterns. Experimental results demonstrate
that models pre-trained on BLAST achieve state-of-the-art performance with a
fraction of the computational resources and training tokens required by
existing methods. Our findings highlight the pivotal role of data diversity in
improving both training efficiency and model performance for the universal
forecasting task.",2025-05-23,"Zezhi Shao, Yujie Li, Fei Wang, Chengqing Yu, Yisong Fu, Tangwen Qian, Bin Xu, Boyu Diao, Yongjun Xu, Xueqi Cheng",http://arxiv.org/pdf/2505.17871v1,cs.LG
Best Group Identification in Multi-Objective Bandits,"We introduce the Best Group Identification problem in a multi-objective
multi-armed bandit setting, where an agent interacts with groups of arms with
vector-valued rewards. The performance of a group is determined by an
efficiency vector which represents the group's best attainable rewards across
different dimensions. The objective is to identify the set of optimal groups in
the fixed-confidence setting. We investigate two key formulations: group Pareto
set identification, where efficiency vectors of optimal groups are Pareto
optimal and linear best group identification, where each reward dimension has a
known weight and the optimal group maximizes the weighted sum of its efficiency
vector's entries. For both settings, we propose elimination-based algorithms,
establish upper bounds on their sample complexity, and derive lower bounds that
apply to any correct algorithm. Through numerical experiments, we demonstrate
the strong empirical performance of the proposed algorithms.",2025-05-23,"Mohammad Shahverdikondori, Mohammad Reza Badri, Negar Kiyavash",http://arxiv.org/pdf/2505.17869v1,cs.LG
SpectraLDS: Provable Distillation for Linear Dynamical Systems,"We present the first provable method for identifying symmetric linear
dynamical systems (LDS) with accuracy guarantees that are independent of the
systems' state dimension or effective memory. Our approach builds upon recent
work that represents symmetric LDSs as convolutions learnable via fixed
spectral transformations. We show how to invert this representation, thereby
recovering an LDS model from its spectral transform and yielding an end-to-end
convex optimization procedure. This distillation preserves predictive accuracy
while enabling constant-time and constant-space inference per token,
independent of sequence length. We evaluate our method, SpectraLDS, as a
component in sequence prediction architectures and demonstrate that accuracy is
preserved while inference efficiency is improved on tasks such as language
modeling.",2025-05-23,"Devan Shah, Shlomo Fortgang, Sofiia Druchyna, Elad Hazan",http://arxiv.org/pdf/2505.17868v1,cs.LG
DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization,"Designing effective black-box optimizers is hampered by limited
problem-specific knowledge and manual control that spans months for almost
every detail. In this paper, we present DesignX, the first automated algorithm
design framework that generates an effective optimizer specific to a given
black-box optimization problem within seconds. Rooted in the first principles,
we identify two key sub-tasks: 1) algorithm structure generation and 2)
hyperparameter control. To enable systematic construction, a comprehensive
modular algorithmic space is first built, embracing hundreds of algorithm
components collected from decades of research. We then introduce a dual-agent
reinforcement learning system that collaborates on structural and parametric
design through a novel cooperative training objective, enabling large-scale
meta-training across 10k diverse instances. Remarkably, through days of
autonomous learning, the DesignX-generated optimizers continuously surpass
human-crafted optimizers by orders of magnitude, either on synthetic testbed or
on realistic optimization scenarios such as Protein-docking, AutoML and UAV
path planning. Further in-depth analysis reveals DesignX's capability to
discover non-trivial algorithm patterns beyond expert intuition, which,
conversely, provides valuable design insights for the optimization community.
We provide DesignX's inference code at https://github.com/MetaEvo/DesignX.",2025-05-23,"Hongshu Guo, Zeyuan Ma, Yining Ma, Xinglin Zhang, Wei-Neng Chen, Yue-Jiao Gong",http://arxiv.org/pdf/2505.17866v1,cs.LG
The emergence of sparse attention: impact of data distribution and benefits of repetition,"Emergence is a fascinating property of large language models and neural
networks more broadly: as models scale and train for longer, they sometimes
develop new abilities in sudden ways. Despite initial studies, we still lack a
comprehensive understanding of how and when these abilities emerge. To address
this gap, we study the emergence over training of sparse attention, a critical
and frequently observed attention pattern in Transformers. By combining
theoretical analysis of a toy model with empirical observations on small
Transformers trained on a linear regression variant, we uncover the mechanics
driving sparse attention emergence and reveal that emergence timing follows
power laws based on task structure, architecture, and optimizer choice. We
additionally find that repetition can greatly speed up emergence. Finally, we
confirm these results on a well-studied in-context associative recall task. Our
findings provide a simple, theoretically grounded framework for understanding
how data distributions and model design influence the learning dynamics behind
one form of emergence.",2025-05-23,"Nicolas Zucchet, Francesco d'Angelo, Andrew K. Lampinen, Stephanie C. Y. Chan",http://arxiv.org/pdf/2505.17863v1,cs.LG
Multi-Person Interaction Generation from Two-Person Motion Priors,"Generating realistic human motion with high-level controls is a crucial task
for social understanding, robotics, and animation. With high-quality MOCAP data
becoming more available recently, a wide range of data-driven approaches have
been presented. However, modelling multi-person interactions still remains a
less explored area. In this paper, we present Graph-driven Interaction
Sampling, a method that can generate realistic and diverse multi-person
interactions by leveraging existing two-person motion diffusion models as
motion priors. Instead of training a new model specific to multi-person
interaction synthesis, our key insight is to spatially and temporally separate
complex multi-person interactions into a graph structure of two-person
interactions, which we name the Pairwise Interaction Graph. We thus decompose
the generation task into simultaneous single-person motion generation
conditioned on one other's motion. In addition, to reduce artifacts such as
interpenetrations of body parts in generated multi-person interactions, we
introduce two graph-dependent guidance terms into the diffusion sampling
scheme. Unlike previous work, our method can produce various high-quality
multi-person interactions without having repetitive individual motions.
Extensive experiments demonstrate that our approach consistently outperforms
existing methods in reducing artifacts when generating a wide range of
two-person and multi-person interactions.",2025-05-23,"Wenning Xu, Shiyu Fan, Paul Henderson, Edmond S. L. Ho",http://arxiv.org/pdf/2505.17860v1,cs.LG
Scalable Valuation of Human Feedback through Provably Robust Model Alignment,"Despite the importance of aligning language models with human preferences,
crowd-sourced human feedback is often noisy -- for example, preferring less
desirable responses -- posing a fundamental challenge to alignment. A truly
robust alignment objective should yield identical model parameters even under
severe label noise, a property known as redescending. We prove that no existing
alignment methods satisfy this property. To address this, we propose
H\""older-DPO, the first principled alignment loss with a provable redescending
property, enabling estimation of the clean data distribution from noisy
feedback. The aligned model estimates the likelihood of clean data, providing a
theoretically grounded metric for dataset valuation that identifies the
location and fraction of mislabels. This metric is gradient-free, enabling
scalable and automated human feedback valuation without costly manual
verification or clean validation dataset. H\""older-DPO achieves
state-of-the-art robust alignment performance while accurately detecting
mislabels in controlled datasets. Finally, we apply H\""older-DPO to widely used
alignment datasets, revealing substantial noise levels and demonstrating that
removing these mislabels significantly improves alignment performance across
methods.",2025-05-23,"Masahiro Fujisawa, Masaki Adachi, Michael A. Osborne",http://arxiv.org/pdf/2505.17859v1,cs.LG
Stochastic Weight Sharing for Bayesian Neural Networks,"While offering a principled framework for uncertainty quantification in deep
learning, the employment of Bayesian Neural Networks (BNNs) is still
constrained by their increased computational requirements and the convergence
difficulties when training very deep, state-of-the-art architectures. In this
work, we reinterpret weight-sharing quantization techniques from a stochastic
perspective in the context of training and inference with Bayesian Neural
Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions,
Wasserstein distance estimations, and alpha blending to encode the stochastic
behaviour of a BNN in a lower dimensional, soft Gaussian representation.
Through extensive empirical investigation, we demonstrate that our approach
significantly reduces the computational overhead inherent in Bayesian learning
by several orders of magnitude, enabling the efficient Bayesian training of
large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various
computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our
approach compresses model parameters by approximately 50x and reduces model
size by 75, while achieving accuracy and uncertainty estimations comparable to
the state-of-the-art.",2025-05-23,"Moule Lin, Shuhao Guan, Weipeng Jing, Goetz Botterweck, Andrea Patane",http://arxiv.org/pdf/2505.17856v1,cs.LG
Out of the Shadows: Exploring a Latent Space for Neural Network Verification,"Neural networks are ubiquitous. However, they are often sensitive to small
input changes. Hence, to prevent unexpected behavior in safety-critical
applications, their formal verification -- a notoriously hard problem -- is
necessary. Many state-of-the-art verification algorithms use reachability
analysis or abstract interpretation to enclose the set of possible outputs of a
neural network. Often, the verification is inconclusive due to the conservatism
of the enclosure. To address this problem, we design a novel latent space for
formal verification that enables the transfer of output specifications to the
input space for an iterative specification-driven input refinement, i.e., we
iteratively reduce the set of possible inputs to only enclose the unsafe ones.
The latent space is constructed from a novel view of projection-based set
representations, e.g., zonotopes, which are commonly used in reachability
analysis of neural networks. A projection-based set representation is a
""shadow"" of a higher-dimensional set -- a latent space -- that does not change
during a set propagation through a neural network. Hence, the input set and the
output enclosure are ""shadows"" of the same latent space that we can use to
transfer constraints. We present an efficient verification tool for neural
networks that uses our iterative refinement to significantly reduce the number
of subproblems in a branch-and-bound procedure. Using zonotopes as a set
representation, unlike many other state-of-the-art approaches, our approach can
be realized by only using matrix operations, which enables a significant
speed-up through efficient GPU acceleration. We demonstrate that our tool
achieves competitive performance, which would place it among the top-ranking
tools of the last neural network verification competition (VNN-COMP'24).",2025-05-23,"Lukas Koller, Tobias Ladner, Matthias Althoff",http://arxiv.org/pdf/2505.17854v1,cs.LG
Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization,"During inference, Recurrent Neural Networks (RNNs) scale constant in both
FLOPs and GPU memory with increasing context length, as they compress all prior
tokens into a fixed-size memory. In contrast, transformers scale linearly in
FLOPs and, at best, linearly in memory during generation, since they must
attend to all previous tokens explicitly. Despite this inference-time
advantage, training large RNNs on long contexts remains impractical because
standard optimization methods depend on Backpropagation Through Time (BPTT).
BPTT requires retention of all intermediate activations during the forward
pass, causing memory usage to scale linearly with both context length and model
size. In this paper, we show that Zero-Order Optimization (ZOO) methods such as
Random-vector Gradient Estimation (RGE) can successfully replace BPTT to train
RNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while
using orders of magnitude less memory and cost, as the model remains in
inference mode throughout training. We further demonstrate that
Central-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate
loss, inherently regularizing training and improving generalization. Our method
matches or outperforms BPTT across three settings: (1) overfitting, (2)
transduction, and (3) language modeling. Across all tasks, with sufficient
perturbations, our models generalize as well as or better than those trained
with BPTT, often in fewer steps. Despite the need for more forward passes per
step, we can surpass BPTT wall-clock time per step using recent advancements
such as FlashRNN and distributed inference.",2025-05-23,"Francois Chaubard, Mykel Kochenderfer",http://arxiv.org/pdf/2505.17852v1,cs.LG
A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems,"In this paper, we propose a robust and reinforcement-learning-enhanced
network intrusion detection system (NIDS) designed for class-imbalanced and
few-shot attack scenarios in Industrial Internet of Things (IIoT) environments.
Our model integrates a TabTransformer for effective tabular feature
representation with Proximal Policy Optimization (PPO) to optimize
classification decisions via policy learning. Evaluated on the
TON\textunderscore IoT benchmark, our method achieves a macro F1-score of
97.73\% and accuracy of 98.85\%. Remarkably, even on extremely rare classes
like man-in-the-middle (MITM), our model achieves an F1-score of 88.79\%,
showcasing strong robustness and few-shot detection capabilities. Extensive
ablation experiments confirm the complementary roles of TabTransformer and PPO
in mitigating class imbalance and improving generalization. These results
highlight the potential of combining transformer-based tabular learning with
reinforcement learning for real-world NIDS applications.",2025-05-23,Yuanya She,http://arxiv.org/pdf/2505.18234v1,cs.LG
TransDF: Time-Series Forecasting Needs Transformed Label Alignment,"Training time-series forecasting models presents unique challenges in
designing effective learning objectives. Existing methods predominantly utilize
the temporal mean squared error, which faces two critical challenges: (1) label
autocorrelation, which leads to bias from the label sequence likelihood; (2)
excessive amount of tasks, which increases with the forecast horizon and
complicates optimization. To address these challenges, we propose
Transform-enhanced Direct Forecast (TransDF), which transforms the label
sequence into decorrelated components with discriminated significance. Models
are trained to align the most significant components, thereby effectively
mitigating label autocorrelation and reducing task amount. Extensive
experiments demonstrate that TransDF achieves state-of-the-art performance and
is compatible with various forecasting models. Code is available at
https://anonymous.4open.science/r/TransDF-88CF.",2025-05-23,"Hao Wang, Licheng Pan, Zhichao Chen, Xu Chen, Qingyang Dai, Lei Wang, Haoxuan Li, Zhouchen Lin",http://arxiv.org/pdf/2505.17847v1,cs.LG
Continuum Transformers Perform In-Context Learning by Operator Gradient Descent,"Transformers robustly exhibit the ability to perform in-context learning,
whereby their predictive accuracy on a task can increase not by parameter
updates but merely with the placement of training samples in their context
windows. Recent works have shown that transformers achieve this by implementing
gradient descent in their forward passes. Such results, however, are restricted
to standard transformer architectures, which handle finite-dimensional inputs.
In the space of PDE surrogate modeling, a generalization of transformers to
handle infinite-dimensional function inputs, known as ""continuum transformers,""
has been proposed and similarly observed to exhibit in-context learning.
Despite impressive empirical performance, such in-context learning has yet to
be theoretically characterized. We herein demonstrate that continuum
transformers perform in-context operator learning by performing gradient
descent in an operator RKHS. We demonstrate this using novel proof strategies
that leverage a generalized representer theorem for Hilbert spaces and gradient
flows over the space of functionals of a Hilbert space. We additionally show
the operator learned in context is the Bayes Optimal Predictor in the infinite
depth limit of the transformer. We then provide empirical validations of this
optimality result and demonstrate that the parameters under which such gradient
descent is performed are recovered through the continuum transformer training.",2025-05-23,"Abhiti Mishra, Yash Patel, Ambuj Tewari",http://arxiv.org/pdf/2505.17838v1,cs.LG
Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means,"This paper addresses the problem of robust estimation in gossip algorithms
over arbitrary communication graphs. Gossip algorithms are fully decentralized,
relying only on local neighbor-to-neighbor communication, making them
well-suited for situations where communication is constrained. A fundamental
challenge in existing mean-based gossip algorithms is their vulnerability to
malicious or corrupted nodes. In this paper, we show that an outlier-robust
mean can be computed by globally estimating a robust statistic. More
specifically, we propose a novel gossip algorithm for rank estimation, referred
to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated
to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed
description of the proposed methods, a key contribution of our work is a
precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank
estimation and an $\mathcal{O}(\log(t)/t)$ rate for trimmed mean estimation,
where by $t$ is meant the number of iterations. Moreover, we provide a
breakdown point analysis of \textsc{GoTrim}. We empirically validate our
theoretical results through experiments on diverse network topologies, data
distributions and contamination schemes.",2025-05-23,"Anna Van Elst, Igor Colin, Stephan Clémençon",http://arxiv.org/pdf/2505.17836v1,cs.LG
Hybrid Mamba-Transformer Decoder for Error-Correcting Codes,"We introduce a novel deep learning method for decoding error correction codes
based on the Mamba architecture, enhanced with Transformer layers. Our approach
proposes a hybrid decoder that leverages Mamba's efficient sequential modeling
while maintaining the global context capabilities of Transformers. To further
improve performance, we design a novel layer-wise masking strategy applied to
each Mamba layer, allowing selective attention to relevant code features at
different depths. Additionally, we introduce a progressive layer-wise loss,
supervising the network at intermediate stages and promoting robust feature
extraction throughout the decoding process. Comprehensive experiments across a
range of linear codes demonstrate that our method significantly outperforms
Transformer-only decoders and standard Mamba models.",2025-05-23,"Shy-el Cohen, Yoni Choukroun, Eliya Nachmani",http://arxiv.org/pdf/2505.17834v1,cs.LG
Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus,"Study of affect in speech requires suitable data, as emotional expression and
perception vary across languages. Until now, no corpus has existed for natural
expression of affect in spontaneous Finnish, existing data being acted or from
a very specific communicative setting. This paper presents the first such
corpus, created by annotating 12,000 utterances for emotional arousal and
valence, sampled from three large-scale Finnish speech corpora. To ensure
diverse affective expression, sample selection was conducted with an affect
mining approach combining acoustic, cross-linguistic speech emotion, and text
sentiment features. We compare this method to random sampling in terms of
annotation diversity, and conduct post-hoc analyses to identify sampling
choices that would have maximized the diversity. As an outcome, the work
introduces a spontaneous Finnish affective speech corpus and informs sampling
strategies for affective speech corpus creation in other languages or domains.",2025-05-23,"Kalle Lahtinen, Einari Vaaras, Liisa Mustanoja, Okko Räsänen",http://arxiv.org/pdf/2505.17833v1,cs.LG
POSTER: A Multi-Signal Model for Detecting Evasive Smishing,"Smishing, or SMS-based phishing, poses an increasing threat to mobile users
by mimicking legitimate communications through culturally adapted, concise, and
deceptive messages, which can result in the loss of sensitive data or financial
resources. In such, we present a multi-channel smishing detection model that
combines country-specific semantic tagging, structural pattern tagging,
character-level stylistic cues, and contextual phrase embeddings. We curated
and relabeled over 84,000 messages across five datasets, including 24,086
smishing samples. Our unified architecture achieves 97.89% accuracy, an F1
score of 0.963, and an AUC of 99.73%, outperforming single-stream models by
capturing diverse linguistic and structural cues. This work demonstrates the
effectiveness of multi-signal learning in robust and region-aware phishing.",2025-05-23,"Shaghayegh Hosseinpour, Sanchari Das",http://arxiv.org/pdf/2505.18233v1,cs.LG
Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning,"Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously
acquire diverse behaviors, but faces major challenges in visual environments
due to high-dimensional, semantically sparse observations. In the online
setting, where agents learn representations while exploring, the latent space
evolves with the agent's policy, to capture newly discovered areas of the
environment. However, without incentivization to maximize state coverage in the
representation, classical approaches based on auto-encoders may converge to
latent spaces that over-represent a restricted set of states frequently visited
by the agent. This is exacerbated in an intrinsic motivation setting, where the
agent uses the distribution encoded in the latent space to sample the goals it
learns to master. To address this issue, we propose to progressively enforce
distributional shifts towards a uniform distribution over the full state space,
to ensure a full coverage of skills that can be learned in the environment. We
introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that
combines the $\beta$-VAE framework with Distributionally Robust Optimization.
DRAG leverages an adversarial neural weighter of training states of the VAE, to
account for the mismatch between the current data distribution and unseen parts
of the environment. This allows the agent to construct semantically meaningful
latent spaces beyond its immediate experience. Our approach improves state
space coverage and downstream control performance on hard exploration
environments such as mazes and robotic control involving walls to bypass,
without pre-training nor prior environment knowledge.",2025-05-23,"Nicolas Castanet, Olivier Sigaud, Sylvain Lamprier",http://arxiv.org/pdf/2505.17830v1,cs.LG
Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models,"Trinity-RFT is a general-purpose, flexible and scalable framework designed
for reinforcement fine-tuning (RFT) of large language models. It is built with
a decoupled design, consisting of (1) an RFT-core that unifies and generalizes
synchronous/asynchronous, on-policy/off-policy, and online/offline modes of
RFT, (2) seamless integration for agent-environment interaction with high
efficiency and robustness, and (3) systematic data pipelines optimized for RFT.
Trinity-RFT can be easily adapted for diverse application scenarios, and serves
as a unified platform for exploring advanced reinforcement learning paradigms.
This technical report outlines the vision, features, design and implementations
of Trinity-RFT, accompanied by extensive examples demonstrating the utility and
user-friendliness of the proposed framework.",2025-05-23,"Xuchen Pan, Yanxi Chen, Yushuo Chen, Yuchang Sun, Daoyuan Chen, Wenhao Zhang, Yuexiang Xie, Yilun Huang, Yilei Zhang, Dawei Gao, Yaliang Li, Bolin Ding, Jingren Zhou",http://arxiv.org/pdf/2505.17826v1,cs.LG
ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning,"The deployment of Large language models (LLMs) in many fields is largely
hindered by their high computational and memory costs. Recent studies suggest
that LLMs exhibit sparsity, which can be used for pruning. Previous pruning
methods typically follow a prune-then-finetune paradigm. Since the pruned parts
still contain valuable information, statically removing them without updating
the remaining parameters often results in irreversible performance degradation,
requiring costly recovery fine-tuning (RFT) to maintain performance. To address
this, we propose a novel paradigm: first apply regularization, then prune.
Based on this paradigm, we propose ELDeR: Getting Efficient LLMs through
Data-Driven Regularized Layer-wise Pruning. We multiply the output of each
transformer layer by an initial weight, then we iteratively learn the weights
of each transformer layer by using a small amount of data in a simple way.
After that, we apply regularization to the difference between the output and
input of the layers with smaller weights, forcing the information to be
transferred to the remaining layers. Compared with direct pruning, ELDeR
reduces the information loss caused by direct parameter removal, thus better
preserving the model's language modeling ability. Experimental results show
that ELDeR achieves superior performance compared with powerful layer-wise
structured pruning methods, while greatly reducing RFT computational costs.
Since ELDeR is a layer-wise pruning method, its end-to-end acceleration effect
is obvious, making it a promising technique for efficient LLMs.",2025-05-23,"Mingkuan Feng, Jinyang Wu, Siyuan Liu, Shuai Zhang, Hongjian Fang, Ruihan Jin, Feihu Che, Pengpeng Shao, Zhengqi Wen, Jianhua Tao",http://arxiv.org/pdf/2505.18232v1,cs.LG
NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache,"Large Language Model (LLM) inference is typically memory-intensive,
especially when processing large batch sizes and long sequences, due to the
large size of key-value (KV) cache. Vector Quantization (VQ) is recently
adopted to alleviate this issue, but we find that the existing approach is
susceptible to distribution shift due to its reliance on calibration datasets.
To address this limitation, we introduce NSNQuant, a calibration-free Vector
Quantization (VQ) technique designed for low-bit compression of the KV cache.
By applying a three-step transformation-1) a token-wise normalization
(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise
normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns
the token distribution with the standard normal distribution. This alignment
enables robust, calibration-free vector quantization using a single reusable
codebook. Extensive experiments show that NSNQuant consistently outperforms
prior methods in both 1-bit and 2-bit settings, offering strong generalization
and up to 3$\times$ throughput gain over full-precision baselines.",2025-05-23,"Donghyun Son, Euntae Choi, Sungjoo Yoo",http://arxiv.org/pdf/2505.18231v1,cs.LG
Source Separation of Small Classical Ensembles: Challenges and Opportunities,"Musical (MSS) source separation of western popular music using non-causal
deep learning can be very effective. In contrast, MSS for classical music is an
unsolved problem. Classical ensembles are harder to separate than popular music
because of issues such as the inherent greater variation in the music; the
sparsity of recordings with ground truth for supervised training; and greater
ambiguity between instruments. The Cadenza project has been exploring MSS for
classical music. This is being done so music can be remixed to improve
listening experiences for people with hearing loss. To enable the work, a new
database of synthesized woodwind ensembles was created to overcome instrumental
imbalances in the EnsembleSet. For the MSS, a set of ConvTasNet models was used
with each model being trained to extract a string or woodwind instrument.
ConvTasNet was chosen because it enabled both causal and non-causal approaches
to be tested. Non-causal approaches have dominated MSS work and are useful for
recorded music, but for live music or processing on hearing aids, causal signal
processing is needed. The MSS performance was evaluated on the two small
datasets (Bach10 and URMP) of real instrument recordings where the ground-truth
is available. The performances of the causal and non-causal systems were
similar. Comparing the average Signal-to-Distortion (SDR) of the synthesized
validation set (6.2 dB causal; 6.9 non-causal), to the real recorded evaluation
set (0.3 dB causal, 0.4 dB non-causal), shows that mismatch between synthesized
and recorded data is a problem. Future work needs to either gather more real
recordings that can be used for training, or to improve the realism and
diversity of the synthesized recordings to reduce the mismatch...",2025-05-23,"Gerardo Roa-Dabike, Trevor J. Cox, Jon P. Barker, Michael A. Akeroyd, Scott Bannister, Bruno Fazenda, Jennifer Firth, Simone Graetzer, Alinka Greasley, Rebecca R. Vos, William M. Whitmer",http://arxiv.org/pdf/2505.17823v1,cs.LG
Quantifying uncertainty in spectral clusterings: expectations for perturbed and incomplete data,"Spectral clustering is a popular unsupervised learning technique which is
able to partition unlabelled data into disjoint clusters of distinct shapes.
However, the data under consideration are often experimental data, implying
that the data is subject to measurement errors and measurements may even be
lost or invalid. These uncertainties in the corrupted input data induce
corresponding uncertainties in the resulting clusters, and the clusterings thus
become unreliable.
  Modelling the uncertainties as random processes, we discuss a mathematical
framework based on random set theory for the computational Monte Carlo
approximation of statistically expected clusterings in case of corrupted, i.e.,
perturbed, incomplete, and possibly even additional, data. We propose several
computationally accessible quantities of interest and analyze their consistency
in the infinite data point and infinite Monte Carlo sample limit. Numerical
experiments are provided to illustrate and compare the proposed quantities.",2025-05-23,"Jürgen Dölz, Jolanda Weygandt",http://arxiv.org/pdf/2505.17819v1,cs.LG
VIBE: Vector Index Benchmark for Embeddings,"Approximate nearest neighbor (ANN) search is a performance-critical component
of many machine learning pipelines. Rigorous benchmarking is essential for
evaluating the performance of vector indexes for ANN search. However, the
datasets of the existing benchmarks are no longer representative of the current
applications of ANN search. Hence, there is an urgent need for an up-to-date
set of benchmarks. To this end, we introduce Vector Index Benchmark for
Embeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE
contains a pipeline for creating benchmark datasets using dense embedding
models characteristic of modern applications, such as retrieval-augmented
generation (RAG). To replicate real-world workloads, we also include
out-of-distribution (OOD) datasets where the queries and the corpus are drawn
from different distributions. We use VIBE to conduct a comprehensive evaluation
of SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution
and 6 out-of-distribution datasets.",2025-05-23,"Elias Jääsaari, Ville Hyvönen, Matteo Ceccarello, Teemu Roos, Martin Aumüller",http://arxiv.org/pdf/2505.17810v1,cs.LG
Hyperparameter Optimization via Interacting with Probabilistic Circuits,"Despite the growing interest in designing truly interactive hyperparameter
optimization (HPO) methods, to date, only a few allow to include human
feedback. Existing interactive Bayesian optimization (BO) methods incorporate
human beliefs by weighting the acquisition function with a user-defined prior
distribution. However, in light of the non-trivial inner optimization of the
acquisition function prevalent in BO, such weighting schemes do not always
accurately reflect given user beliefs. We introduce a novel BO approach
leveraging tractable probabilistic models named probabilistic circuits (PCs) as
a surrogate model. PCs encode a tractable joint distribution over the hybrid
hyperparameter space and evaluation scores. They enable exact conditional
inference and sampling. Based on conditional sampling, we construct a novel
selection policy that enables an acquisition function-free generation of
candidate points (thereby eliminating the need for an additional inner-loop
optimization) and ensures that user beliefs are reflected accurately in the
selection policy. We provide a theoretical analysis and an extensive empirical
evaluation, demonstrating that our method achieves state-of-the-art performance
in standard HPO and outperforms interactive BO baselines in interactive HPO.",2025-05-23,"Jonas Seng, Fabrizio Ventola, Zhongjie Yu, Kristian Kersting",http://arxiv.org/pdf/2505.17804v1,cs.LG
A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances,"Coreset selection targets the challenge of finding a small, representative
subset of a large dataset that preserves essential patterns for effective
machine learning. Although several surveys have examined data reduction
strategies before, most focus narrowly on either classical geometry-based
methods or active learning techniques. In contrast, this survey presents a more
comprehensive view by unifying three major lines of coreset research, namely,
training-free, training-oriented, and label-free approaches, into a single
taxonomy. We present subfields often overlooked by existing work, including
submodular formulations, bilevel optimization, and recent progress in
pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning
strategies influence generalization and neural scaling laws, offering new
insights that are absent from prior reviews. Finally, we compare these methods
under varying computational, robustness, and performance demands and highlight
open challenges, such as robustness, outlier filtering, and adapting coreset
selection to foundation models, for future research.",2025-05-23,"Brian B. Moser, Arundhati S. Shanbhag, Stanislav Frolov, Federico Raue, Joachim Folz, Andreas Dengel",http://arxiv.org/pdf/2505.17799v1,cs.LG
"Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models","What is the shortest path between two data points lying in a high-dimensional
space? While the answer is trivial in Euclidean geometry, it becomes
significantly more complex when the data lies on a curved manifold -- requiring
a Riemannian metric to describe the space's local curvature. Estimating such a
metric, however, remains a major challenge in high dimensions.
  In this work, we propose a method for deriving Riemannian metrics directly
from pretrained Energy-Based Models (EBMs) -- a class of generative models that
assign low energy to high-density regions. These metrics define spatially
varying distances, enabling the computation of geodesics -- shortest paths that
follow the data manifold's intrinsic geometry. We introduce two novel metrics
derived from EBMs and show that they produce geodesics that remain closer to
the data manifold and exhibit lower curvature distortion, as measured by
alignment with ground-truth trajectories. We evaluate our approach on
increasingly complex datasets: synthetic datasets with known data density,
rotated character images with interpretable geometry, and high-resolution
natural images embedded in a pretrained VAE latent space.
  Our results show that EBM-derived metrics consistently outperform established
baselines, especially in high-dimensional settings. Our work is the first to
derive Riemannian metrics from EBMs, enabling data-aware geodesics and
unlocking scalable, geometry-driven learning for generative modeling and
simulation.",2025-05-23,"Louis Béthune, David Vigouroux, Yilun Du, Rufin VanRullen, Thomas Serre, Victor Boutin",http://arxiv.org/pdf/2505.18230v1,cs.LG
Latent Mode Decomposition,"We introduce Variational Latent Mode Decomposition (VLMD), a new algorithm
for extracting oscillatory modes and associated connectivity structures from
multivariate signals. VLMD addresses key limitations of existing Multivariate
Mode Decomposition (MMD) techniques -including high computational cost,
sensitivity to parameter choices, and weak modeling of interchannel
dependencies. Its improved performance is driven by a novel underlying model,
Latent Mode Decomposition (LMD), which blends sparse coding and mode
decomposition to represent multichannel signals as sparse linear combinations
of shared latent components composed of AM-FM oscillatory modes. This
formulation enables VLMD to operate in a lower-dimensional latent space,
enhancing robustness to noise, scalability, and interpretability. The algorithm
solves a constrained variational optimization problem that jointly enforces
reconstruction fidelity, sparsity, and frequency regularization. Experiments on
synthetic and real-world datasets demonstrate that VLMD outperforms
state-of-the-art MMD methods in accuracy, efficiency, and interpretability of
extracted structures.",2025-05-23,"Manuel Morante, Naveed ur Rehman",http://arxiv.org/pdf/2505.17797v1,cs.LG
RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion,"Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped
relations between entities. TKG completion involves forecasting missing or
future links, requiring models to reason over time-evolving structure. While
LLMs show promise for this task, existing approaches often overemphasize
supervised fine-tuning and struggle particularly when historical evidence is
limited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient
framework designed to improve accuracy and generalization in settings with
sparse historical context. It combines (1) rule-based multi-hop retrieval for
structurally diverse history, (2) contrastive fine-tuning of lightweight
adapters to encode relational semantics, and (3) test-time semantic filtering
to iteratively refine generations based on embedding similarity. Experiments on
four TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based
approaches, achieving up to 30.6\% relative improvement in Hits@10. Moreover,
our proposed framework produces more semantically coherent predictions, even
for the samples with limited historical context.",2025-05-23,"Ömer Faruk Akgül, Feiyu Zhu, Yuxin Yang, Rajgopal Kannan, Viktor Prasanna",http://arxiv.org/pdf/2505.17794v1,cs.LG
Optimal Online Change Detection via Random Fourier Features,"This article studies the problem of online non-parametric change point
detection in multivariate data streams. We approach the problem through the
lens of kernel-based two-sample testing and introduce a sequential testing
procedure based on random Fourier features, running with logarithmic time
complexity per observation and with overall logarithmic space complexity. The
algorithm has two advantages compared to the state of the art. First, our
approach is genuinely online, and no access to training data known to be from
the pre-change distribution is necessary. Second, the algorithm does not
require the user to specify a window parameter over which local tests are to be
calculated. We prove strong theoretical guarantees on the algorithm's
performance, including information-theoretic bounds demonstrating that the
detection delay is optimal in the minimax sense. Numerical studies on real and
synthetic data show that our algorithm is competitive with respect to the state
of the art.",2025-05-23,"Florian Kalinke, Shakeel Gavioli-Akilagun",http://arxiv.org/pdf/2505.17789v1,cs.LG
Supervised Graph Contrastive Learning for Gene Regulatory Network,"Graph representation learning is effective for obtaining a meaningful latent
space utilizing the structure of graph data and is widely applied, including
biological networks. In particular, Graph Contrastive Learning (GCL) has
emerged as a powerful self-supervised method that relies on applying
perturbations to graphs for data augmentation. However, when applying existing
GCL methods to biological networks such as Gene Regulatory Networks (GRNs),
they overlooked meaningful biologically relevant perturbations, e.g., gene
knockdowns. In this study, we introduce SupGCL (Supervised Graph Contrastive
Learning), a novel GCL method for GRNs that directly incorporates biological
perturbations derived from gene knockdown experiments as the supervision.
SupGCL mathematically extends existing GCL methods that utilize non-biological
perturbations to probabilistic models that introduce actual biological gene
perturbation utilizing gene knockdown data. Using the GRN representation
obtained by our proposed method, our aim is to improve the performance of
biological downstream tasks such as patient hazard prediction and disease
subtype classification (graph-level task), and gene function classification
(node-level task). We applied SupGCL on real GRN datasets derived from patients
with multiple types of cancer, and in all experiments SupGCL achieves better
performance than state-of-the-art baselines.",2025-05-23,"Sho Oshima, Yuji Okamoto, Taisei Tosaki, Ryosuke Kojima, Yasushi Okuno",http://arxiv.org/pdf/2505.17786v1,cs.LG
U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding,"Ultrasound is a widely-used imaging modality critical to global healthcare,
yet its interpretation remains challenging due to its varying image quality on
operators, noises, and anatomical structures. Although large vision-language
models (LVLMs) have demonstrated impressive multimodal capabilities across
natural and medical domains, their performance on ultrasound remains largely
unexplored. We introduce U2-BENCH, the first comprehensive benchmark to
evaluate LVLMs on ultrasound understanding across classification, detection,
regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning
15 anatomical regions and defines 8 clinically inspired tasks, such as
diagnosis, view recognition, lesion localization, clinical value estimation,
and report generation, across 50 ultrasound application scenarios. We evaluate
20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and
medical-specific. Our results reveal strong performance on image-level
classification, but persistent challenges in spatial reasoning and clinical
language generation. U2-BENCH establishes a rigorous and unified testbed to
assess and accelerate LVLM research in the uniquely multimodal domain of
medical ultrasound imaging.",2025-05-23,"Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang, Yalun Wu, Kaiwen Yan, Quanlin Sun, Meirui Jiang, Jialun Pei, Siya Liu, Haoyun Zheng, Zhoujun Li, Alison Noble, Jacques Souquet, Xiaoqing Guo, Manxi Lin, Hongcheng Guo",http://arxiv.org/pdf/2505.17779v1,cs.LG
Optimizing Shortfall Risk Metric for Learning Regression Models,"We consider the problem of estimating and optimizing utility-based shortfall
risk (UBSR) of a loss, say $(Y - \hat Y)^2$, in the context of a regression
problem. Empirical risk minimization with a UBSR objective is challenging since
UBSR is a non-linear function of the underlying distribution. We first derive a
concentration bound for UBSR estimation using independent and identically
distributed (i.i.d.) samples. We then frame the UBSR optimization problem as
minimization of a pseudo-linear function in the space of achievable
distributions $\mathcal D$ of the loss $(Y- \hat Y)^2$. We construct a gradient
oracle for the UBSR objective and a linear minimization oracle (LMO) for the
set $\mathcal D$. Using these oracles, we devise a bisection-type algorithm,
and establish convergence to the UBSR-optimal solution.",2025-05-23,"Harish G. Ramaswamy, L. A. Prashanth",http://arxiv.org/pdf/2505.17777v1,cs.LG
C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models,"Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning
large language models (LLMs), but it often produces overconfident predictions
in data-scarce few-shot settings. To address this issue, several classical
statistical learning approaches have been repurposed for scalable
uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input
characteristics affect the predictive uncertainty estimates. To address this
limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a
novel uncertainty-aware and parameter efficient fine-tuning approach, by
developing new lightweight LoRA modules contextualized to each input data
sample to dynamically adapt uncertainty estimates. Incorporating data-driven
contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves
well-calibrated uncertainties, and yields robust predictions. Extensive
experiments demonstrate that C-LoRA consistently outperforms the
state-of-the-art uncertainty-aware LoRA methods in both uncertainty
quantification and model generalization. Ablation studies further confirm the
critical role of our contextual modules in capturing sample-specific
uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM
fine-tuning in few-shot regimes.",2025-05-23,"Amir Hossein Rahmati, Sanket Jantre, Weifeng Zhang, Yucheng Wang, Byung-Jun Yoon, Nathan M. Urban, Xiaoning Qian",http://arxiv.org/pdf/2505.17773v1,cs.LG
Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models,"Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage
Models (LLM) activations into interpretable latents. However, due to their
substantial training cost, most academic research uses open-source SAEs which
are only available for a restricted set of models of up to 27B parameters. SAE
latents are also learned from a dataset of activations, which means they do not
transfer between models. Motivated by relative representation similarity
measures, we introduce Inference-Time Decomposition of Activations (ITDA)
models, an alternative method for decomposing language model activations. To
train an ITDA, we greedily construct a dictionary of language model activations
on a dataset of prompts, selecting those activations which were worst
approximated by matching pursuit on the existing dictionary. ITDAs can be
trained in just 1\% of the time required for SAEs, using 1\% of the data. This
allowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU.
ITDAs can achieve similar reconstruction performance to SAEs on some target
LLMs, but generally incur a performance penalty. However, ITDA dictionaries
enable cross-model comparisons, and a simple Jaccard similarity index on ITDA
dictionaries outperforms existing methods like CKA, SVCCA, and relative
representation similarity metrics. ITDAs provide a cheap alternative to SAEs
where computational resources are limited, or when cross model comparisons are
necessary. Code available at https://github.com/pleask/itda.",2025-05-23,"Patrick Leask, Neel Nanda, Noura Al Moubayed",http://arxiv.org/pdf/2505.17769v1,cs.LG
Joker: Joint Optimization Framework for Lightweight Kernel Machines,"Kernel methods are powerful tools for nonlinear learning with
well-established theory. The scalability issue has been their long-standing
challenge. Despite the existing success, there are two limitations in
large-scale kernel methods: (i) The memory overhead is too high for users to
afford; (ii) existing efforts mainly focus on kernel ridge regression (KRR),
while other models lack study. In this paper, we propose Joker, a joint
optimization framework for diverse kernel models, including KRR, logistic
regression, and support vector machines. We design a dual block coordinate
descent method with trust region (DBCD-TR) and adopt kernel approximation with
randomized features, leading to low memory costs and high efficiency in
large-scale learning. Experiments show that Joker saves up to 90\% memory but
achieves comparable training time and performance (or even better) than the
state-of-the-art methods.",2025-05-23,"Junhong Zhang, Zhihui Lai",http://arxiv.org/pdf/2505.17765v1,cs.LG
Unsupervised Clustering for Fault Analysis in High-Voltage Power Systems Using Voltage and Current Signals,"The widespread use of sensors in modern power grids has led to the
accumulation of large amounts of voltage and current waveform data, especially
during fault events. However, the lack of labeled datasets poses a significant
challenge for fault classification and analysis. This paper explores the
application of unsupervised clustering techniques for fault diagnosis in
high-voltage power systems. A dataset provided by the Reseau de Transport
d'Electricite (RTE) is analyzed, with frequency domain features extracted using
the Fast Fourier Transform (FFT). The K-Means algorithm is then applied to
identify underlying patterns in the data, enabling automated fault
categorization without the need for labeled training samples. The resulting
clusters are evaluated in collaboration with power system experts to assess
their alignment with real-world fault characteristics. The results demonstrate
the potential of unsupervised learning for scalable and data-driven fault
analysis, providing a robust approach to detecting and classifying power system
faults with minimal prior assumptions.",2025-05-23,"Julian Oelhaf, Georg Kordowich, Andreas Maier, Johann Jager, Siming Bayer",http://arxiv.org/pdf/2505.17763v1,cs.LG
Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models,"Structured Linear Controlled Differential Equations (SLiCEs) provide a
unifying framework for sequence models with structured, input-dependent
state-transition matrices that retain the maximal expressivity of dense
matrices whilst being cheaper to compute. The framework encompasses existing
architectures, such as input-dependent block-diagonal linear recurrent neural
networks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel
variants based on sparsity and the Walsh--Hadamard transform. We prove that,
unlike the diagonal state-transition matrices of S4 and Mamba, SLiCEs employing
block-diagonal, sparse, or Walsh--Hadamard matrices match the maximal
expressivity of dense matrices. Empirically, SLiCEs solve the $A_5$
state-tracking benchmark with a single layer, achieve best-in-class length
generalisation on regular language tasks among parallel-in-time models, and
match the state-of-the-art performance of log neural controlled differential
equations on six multivariate time-series classification datasets while cutting
the average time per training step by a factor of twenty.",2025-05-23,"Benjamin Walker, Lingyi Yang, Nicola Muca Cirone, Cristopher Salvi, Terry Lyons",http://arxiv.org/pdf/2505.17761v1,cs.LG
But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors,"Recent safety evaluations of Large Language Models (LLMs) show that many
models exhibit dishonest behavior, such as sycophancy. However, most honesty
benchmarks focus exclusively on factual knowledge or explicitly harmful
behavior and rely on external judges, which are often unable to detect less
obvious forms of dishonesty. In this work, we introduce a new framework, Judge
Using Safety-Steered Alternatives (JUSSA), which utilizes steering vectors
trained on a single sample to elicit more honest responses from models, helping
LLM-judges in the detection of dishonest behavior. To test our framework, we
introduce a new manipulation dataset with prompts specifically designed to
elicit deceptive responses. We find that JUSSA enables LLM judges to better
differentiate between dishonest and benign responses, and helps them identify
subtle instances of manipulative behavior.",2025-05-23,"Leon Eshuijs, Archie Chaudhury, Alan McBeth, Ethan Nguyen",http://arxiv.org/pdf/2505.17760v1,cs.LG
"Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality","In Transformer architectures, tokens\textemdash discrete units derived from
raw data\textemdash are formed by segmenting inputs into fixed-length chunks.
Each token is then mapped to an embedding, enabling parallel attention
computations while preserving the input's essential information. Due to the
quadratic computational complexity of transformer self-attention mechanisms,
token reduction has primarily been used as an efficiency strategy. This is
especially true in single vision and language domains, where it helps balance
computational costs, memory usage, and inference latency. Despite these
advances, this paper argues that token reduction should transcend its
traditional efficiency-oriented role in the era of large generative models.
Instead, we position it as a fundamental principle in generative modeling,
critically influencing both model architecture and broader applications.
Specifically, we contend that across vision, language, and multimodal systems,
token reduction can: (i) facilitate deeper multimodal integration and
alignment, (ii) mitigate ""overthinking"" and hallucinations, (iii) maintain
coherence over long inputs, and (iv) enhance training stability, etc. We
reframe token reduction as more than an efficiency measure. By doing so, we
outline promising future directions, including algorithm design, reinforcement
learning-guided token reduction, token optimization for in-context learning,
and broader ML and scientific domains. We highlight its potential to drive new
model architectures and learning strategies that improve robustness, increase
interpretability, and better align with the objectives of generative modeling.",2025-05-23,"Zhenglun Kong, Yize Li, Fanhu Zeng, Lei Xin, Shvat Messica, Xue Lin, Pu Zhao, Manolis Kellis, Hao Tang, Marinka Zitnik",http://arxiv.org/pdf/2505.18227v1,cs.LG
Qiskit Machine Learning: an open-source library for quantum machine learning tasks at scale on quantum hardware and classical simulators,"We present Qiskit Machine Learning (ML), a high-level Python library that
combines elements of quantum computing with traditional machine learning. The
API abstracts Qiskit's primitives to facilitate interactions with classical
simulators and quantum hardware. Qiskit ML started as a proof-of-concept code
in 2019 and has since been developed to be a modular, intuitive tool for
non-specialist users while allowing extensibility and fine-tuning controls for
quantum computational scientists and developers. The library is available as a
public, open-source tool and is distributed under the Apache version 2.0
license.",2025-05-23,"M. Emre Sahin, Edoardo Altamura, Oscar Wallis, Stephen P. Wood, Anton Dekusar, Declan A. Millar, Takashi Imamichi, Atsushi Matsuo, Stefano Mensa",http://arxiv.org/pdf/2505.17756v1,cs.LG
Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning,"Scaling deep reinforcement learning in pixel-based environments presents a
significant challenge, often resulting in diminished performance. While recent
works have proposed algorithmic and architectural approaches to address this,
the underlying cause of the performance drop remains unclear. In this paper, we
identify the connection between the output of the encoder (a stack of
convolutional layers) and the ensuing dense layers as the main underlying
factor limiting scaling capabilities; we denote this connection as the
bottleneck, and we demonstrate that previous approaches implicitly target this
bottleneck. As a result of our analyses, we present global average pooling as a
simple yet effective way of targeting the bottleneck, thereby avoiding the
complexity of earlier approaches.",2025-05-23,"Ghada Sokar, Pablo Samuel Castro",http://arxiv.org/pdf/2505.17749v1,cs.LG
Soft-CAM: Making black box models self-explainable for high-stakes decisions,"Convolutional neural networks (CNNs) are widely used for high-stakes
applications like medicine, often surpassing human performance. However, most
explanation methods rely on post-hoc attribution, approximating the
decision-making process of already trained black-box models. These methods are
often sensitive, unreliable, and fail to reflect true model reasoning, limiting
their trustworthiness in critical applications. In this work, we introduce
SoftCAM, a straightforward yet effective approach that makes standard CNN
architectures inherently interpretable. By removing the global average pooling
layer and replacing the fully connected classification layer with a
convolution-based class evidence layer, SoftCAM preserves spatial information
and produces explicit class activation maps that form the basis of the model's
predictions. Evaluated on three medical datasets, SoftCAM maintains
classification performance while significantly improving both the qualitative
and quantitative explanation compared to existing post-hoc methods. Our results
demonstrate that CNNs can be inherently interpretable without compromising
performance, advancing the development of self-explainable deep learning for
high-stakes decision-making.",2025-05-23,"Kerol Djoumessi, Philipp Berens",http://arxiv.org/pdf/2505.17748v1,cs.LG
MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization,"Meta-Black-Box Optimization (MetaBBO) streamlines the automation of
optimization algorithm design through meta-learning. It typically employs a
bi-level structure: the meta-level policy undergoes meta-training to reduce the
manual effort required in developing algorithms for low-level optimization
tasks. The original MetaBox (2023) provided the first open-source framework for
reinforcement learning-based single-objective MetaBBO. However, its relatively
narrow scope no longer keep pace with the swift advancement in this field. In
this paper, we introduce MetaBox-v2 (https://github.com/MetaEvo/MetaBox) as a
milestone upgrade with four novel features: 1) a unified architecture
supporting RL, evolutionary, and gradient-based approaches, by which we
reproduce 23 up-to-date baselines; 2) efficient parallelization schemes, which
reduce the training/testing time by 10-40x; 3) a comprehensive benchmark suite
of 18 synthetic/realistic tasks (1900+ instances) spanning single-objective,
multi-objective, multi-model, and multi-task optimization scenarios; 4)
plentiful and extensible interfaces for custom analysis/visualization and
integrating to external optimization tools/benchmarks. To show the utility of
MetaBox-v2, we carry out a systematic case study that evaluates the built-in
baselines in terms of the optimization performance, generalization ability and
learning efficiency. Valuable insights are concluded from thorough and detailed
analysis for practitioners and those new to the field.",2025-05-23,"Zeyuan Ma, Yue-Jiao Gong, Hongshu Guo, Wenjie Qiu, Sijie Ma, Hongqiao Lian, Jiajun Zhan, Kaixu Chen, Chen Wang, Zhiyang Huang, Zechuan Huang, Guojun Peng, Ran Cheng, Yining Ma",http://arxiv.org/pdf/2505.17745v1,cs.LG
Discrete Neural Flow Samplers with Locally Equivariant Transformer,"Sampling from unnormalised discrete distributions is a fundamental problem
across various domains. While Markov chain Monte Carlo offers a principled
approach, it often suffers from slow mixing and poor convergence. In this
paper, we propose Discrete Neural Flow Samplers (DNFS), a trainable and
efficient framework for discrete sampling. DNFS learns the rate matrix of a
continuous-time Markov chain such that the resulting dynamics satisfy the
Kolmogorov equation. As this objective involves the intractable partition
function, we then employ control variates to reduce the variance of its Monte
Carlo estimation, leading to a coordinate descent learning algorithm. To
further facilitate computational efficiency, we propose locally equivaraint
Transformer, a novel parameterisation of the rate matrix that significantly
improves training efficiency while preserving powerful network expressiveness.
Empirically, we demonstrate the efficacy of DNFS in a wide range of
applications, including sampling from unnormalised distributions, training
discrete energy-based models, and solving combinatorial optimisation problems.",2025-05-23,"Zijing Ou, Ruixiang Zhang, Yingzhen Li",http://arxiv.org/pdf/2505.17741v1,cs.LG
A tensor network approach for chaotic time series prediction,"Making accurate predictions of chaotic time series is a complex challenge.
Reservoir computing, a neuromorphic-inspired approach, has emerged as a
powerful tool for this task. It exploits the memory and nonlinearity of
dynamical systems without requiring extensive parameter tuning. However,
selecting and optimizing reservoir architectures remains an open problem.
Next-generation reservoir computing simplifies this problem by employing
nonlinear vector autoregression based on truncated Volterra series, thereby
reducing hyperparameter complexity. Nevertheless, the latter suffers from
exponential parameter growth in terms of the maximum monomial degree. Tensor
networks offer a promising solution to this issue by decomposing
multidimensional arrays into low-dimensional structures, thus mitigating the
curse of dimensionality. This paper explores the application of a previously
proposed tensor network model for predicting chaotic time series, demonstrating
its advantages in terms of accuracy and computational efficiency compared to
conventional echo state networks. Using a state-of-the-art tensor network
approach enables us to bridge the gap between the tensor network and reservoir
computing communities, fostering advances in both fields.",2025-05-23,"Rodrigo Martínez-Peña, Román Orús",http://arxiv.org/pdf/2505.17740v1,cs.LG
URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles,"Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future
urban networks, potentially by optimizing their routing decisions. Unlike for
human drivers, these decisions can be made with collective, data-driven
policies, developed by machine learning algorithms. Reinforcement learning (RL)
can facilitate the development of such collective routing strategies, yet
standardized and realistic benchmarks are missing. To that end, we present
\our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles.
\our{} is a comprehensive benchmarking environment that unifies evaluation
across 29 real-world traffic networks paired with realistic demand patterns.
\our{} comes with a catalog of predefined tasks, four state-of-the-art
multi-agent RL (MARL) algorithm implementations, three baseline methods,
domain-specific performance metrics, and a modular configuration scheme. Our
results suggest that, despite the lengthy and costly training, state-of-the-art
MARL algorithms rarely outperformed humans. Experimental results reported in
this paper initiate the first leaderboard for MARL in large-scale urban routing
optimization and reveal that current approaches struggle to scale, emphasizing
the urgent need for advancements in this domain.",2025-05-23,"Ahmet Onur Akman, Anastasia Psarou, Michał Hoffmann, Łukasz Gorczyca, Łukasz Kowalski, Paweł Gora, Grzegorz Jamróz, Rafał Kucharski",http://arxiv.org/pdf/2505.17734v1,cs.LG
RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection,"Accurate, fast, and reliable 3D perception is essential for autonomous
driving. Recently, bird's-eye view (BEV)-based perception approaches have
emerged as superior alternatives to perspective-based solutions, offering
enhanced spatial understanding and more natural outputs for planning. Existing
BEV-based 3D object detection methods, typically adhering to angle-based
representation, directly estimate the size and orientation of rotated bounding
boxes. We observe that BEV-based 3D object detection is analogous to aerial
oriented object detection, where angle-based methods are recognized for being
affected by discontinuities in their loss functions. Drawing inspiration from
this domain, we propose Restricted Quadrilateral Representation to define 3D
regression targets. RQR3D regresses the smallest horizontal bounding box
encapsulating the oriented box, along with the offsets between the corners of
these two boxes, thereby transforming the oriented object detection problem
into a keypoint regression task. RQR3D is compatible with any 3D object
detection approach. We employ RQR3D within an anchor-free single-stage object
detection method and introduce an objectness head to address class imbalance
problem. Furthermore, we introduce a simplified radar fusion backbone that
eliminates the need for voxel grouping and processes the BEV-mapped point cloud
with standard 2D convolutions, rather than sparse convolutions. Extensive
evaluations on the nuScenes dataset demonstrate that RQR3D achieves
state-of-the-art performance in camera-radar 3D object detection, outperforming
the previous best method by +4% in NDS and +2.4% in mAP, and significantly
reducing the translation and orientation errors, which are crucial for safe
autonomous driving. These consistent gains highlight the robustness, precision,
and real-world readiness of our approach.",2025-05-23,"Ozsel Kilinc, Cem Tarhan",http://arxiv.org/pdf/2505.17732v1,cs.LG
Redirection for Erasing Memory (REM): Towards a universal unlearning method for corrupted data,"Machine unlearning is studied for a multitude of tasks, but specialization of
unlearning methods to particular tasks has made their systematic comparison
challenging. To address this issue, we propose a conceptual space to
characterize diverse corrupted data unlearning tasks in vision classifiers.
This space is described by two dimensions, the discovery rate (the fraction of
the corrupted data that are known at unlearning time) and the statistical
regularity of the corrupted data (from random exemplars to shared concepts).
Methods proposed previously have been targeted at portions of this space and-we
show-fail predictably outside these regions. We propose a novel method,
Redirection for Erasing Memory (REM), whose key feature is that corrupted data
are redirected to dedicated neurons introduced at unlearning time and then
discarded or deactivated to suppress the influence of corrupted data. REM
performs strongly across the space of tasks, in contrast to prior SOTA methods
that fail outside the regions for which they were designed.",2025-05-23,"Stefan Schoepf, Michael Curtis Mozer, Nicole Elyse Mitchell, Alexandra Brintrup, Georgios Kaissis, Peter Kairouz, Eleni Triantafillou",http://arxiv.org/pdf/2505.17730v1,cs.LG
PEAR: Equal Area Weather Forecasting on the Sphere,"Machine learning methods for global medium-range weather forecasting have
recently received immense attention. Following the publication of the Pangu
Weather model, the first deep learning model to outperform traditional
numerical simulations of the atmosphere, numerous models have been published in
this domain, building on Pangu's success. However, all of these models operate
on input data and produce predictions on the Driscoll--Healy discretization of
the sphere which suffers from a much finer grid at the poles than around the
equator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization
(HEALPix) of the sphere, each pixel covers the same surface area, removing
unphysical biases. Motivated by a growing support for this grid in meteorology
and climate sciences, we propose to perform weather forecasting with deep
learning models which natively operate on the HEALPix grid. To this end, we
introduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting
model which operates directly on HEALPix-features and outperforms the
corresponding model on Driscoll--Healy without any computational overhead.",2025-05-23,"Hampus Linander, Christoffer Petersson, Daniel Persson, Jan E. Gerken",http://arxiv.org/pdf/2505.17720v1,cs.LG
A Distributionally-Robust Framework for Nuisance in Causal Effect Estimation,"Causal inference requires evaluating models on balanced distributions between
treatment and control groups, while training data often exhibits imbalance due
to historical decision-making policies. Most conventional statistical methods
address this distribution shift through inverse probability weighting (IPW),
which requires estimating propensity scores as an intermediate step. These
methods face two key challenges: inaccurate propensity estimation and
instability from extreme weights. We decompose the generalization error to
isolate these issues--propensity ambiguity and statistical instability--and
address them through an adversarial loss function. Our approach combines
distributionally robust optimization for handling propensity uncertainty with
weight regularization based on weighted Rademacher complexity. Experiments on
synthetic and real-world datasets demonstrate consistent improvements over
existing methods.",2025-05-23,Akira Tanimoto,http://arxiv.org/pdf/2505.17717v1,cs.LG
Get Experience from Practice: LLM Agents with Record & Replay,"AI agents, empowered by Large Language Models (LLMs) and communication
protocols such as MCP and A2A, have rapidly evolved from simple chatbots to
autonomous entities capable of executing complex, multi-step tasks,
demonstrating great potential. However, the LLMs' inherent uncertainty and
heavy computational resource requirements pose four significant challenges to
the development of safe and efficient agents: reliability, privacy, cost and
performance. Existing approaches, like model alignment, workflow constraints
and on-device model deployment, can partially alleviate some issues but often
with limitations, failing to fundamentally resolve these challenges.
  This paper proposes a new paradigm called AgentRR (Agent Record & Replay),
which introduces the classical record-and-replay mechanism into AI agent
frameworks. The core idea is to: 1. Record an agent's interaction trace with
its environment and internal decision process during task execution, 2.
Summarize this trace into a structured ""experience"" encapsulating the workflow
and constraints, and 3. Replay these experiences in subsequent similar tasks to
guide the agent's behavior. We detail a multi-level experience abstraction
method and a check function mechanism in AgentRR: the former balances
experience specificity and generality, while the latter serves as a trust
anchor to ensure completeness and safety during replay. In addition, we explore
multiple application modes of AgentRR, including user-recorded task
demonstration, large-small model collaboration and privacy-aware agent
execution, and envision an experience repository for sharing and reusing
knowledge to further reduce deployment cost.",2025-05-23,"Erhu Feng, Wenbo Zhou, Zibin Liu, Le Chen, Yunpeng Dong, Cheng Zhang, Yisheng Zhao, Dong Du, Zhichao Hua, Yubin Xia, Haibo Chen",http://arxiv.org/pdf/2505.17716v1,cs.LG
PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization,"Despite Proximal Policy Optimization (PPO) dominating policy gradient methods
-- from robotic control to game AI -- its static trust region forces a brittle
trade-off: aggressive clipping stifles early exploration, while late-stage
updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive
RL by fusing exploration and convergence signals into a single bounded trust
region -- a theoretically grounded innovation that outperforms five SOTA
baselines with less than 2% overhead. This work bridges a critical gap in
phase-aware learning, enabling real-world deployment in safety-critical systems
like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%
faster convergence by combining: (1) entropy-driven expansion (epsilon up) for
exploration in high-uncertainty states, and (2) reward-guided contraction
(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,
Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),
2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with
only five lines of code change. PPO-BR's simplicity and theoretical guarantees
make it ready-to-deploy in safety-critical domains -- from surgical robotics to
autonomous drones. In contrast to recent methods such as Group Relative Policy
Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism
applicable to both language models and general reinforcement learning
environments.",2025-05-23,Ben Rahman,http://arxiv.org/pdf/2505.17714v1,cs.LG
The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations,"Causal reasoning and discovery, two fundamental tasks of causal analysis,
often face challenges in applications due to the complexity, noisiness, and
high-dimensionality of real-world data. Despite recent progress in identifying
latent causal structures using causal representation learning (CRL), what makes
learned representations useful for causal downstream tasks and how to evaluate
them are still not well understood. In this paper, we reinterpret CRL using a
measurement model framework, where the learned representations are viewed as
proxy measurements of the latent causal variables. Our approach clarifies the
conditions under which learned representations support downstream causal
reasoning and provides a principled basis for quantitatively assessing the
quality of representations using a new Test-based Measurement EXclusivity
(T-MEX) score. We validate T-MEX across diverse causal inference scenarios,
including numerical simulations and real-world ecological video analysis,
demonstrating that the proposed framework and corresponding score effectively
assess the identification of learned representations and their usefulness for
causal downstream tasks.",2025-05-23,"Dingling Yao, Shimeng Huang, Riccardo Cadei, Kun Zhang, Francesco Locatello",http://arxiv.org/pdf/2505.17708v1,cs.LG
CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models,"Knowledge Tracing (KT) aims to model a student's learning state over time and
predict their future performance. However, traditional KT methods often face
challenges in explainability, scalability, and effective modeling of complex
knowledge dependencies. While Large Language Models (LLMs) present new avenues
for KT, their direct application often struggles with generating structured,
explainable student representations and lacks mechanisms for continuous,
task-specific refinement. To address these gaps, we propose Collaborative
Iterative Knowledge Tracing (CIKT), a framework that harnesses LLMs to enhance
both prediction accuracy and explainability. CIKT employs a dual-component
architecture: an Analyst generates dynamic, explainable user profiles from
student historical responses, and a Predictor utilizes these profiles to
forecast future performance. The core of CIKT is a synergistic optimization
loop. In this loop, the Analyst is iteratively refined based on the predictive
accuracy of the Predictor, which conditions on the generated profiles, and the
Predictor is subsequently retrained using these enhanced profiles. Evaluated on
multiple educational datasets, CIKT demonstrates significant improvements in
prediction accuracy, offers enhanced explainability through its dynamically
updated user profiles, and exhibits improved scalability. Our work presents a
robust and explainable solution for advancing knowledge tracing systems,
effectively bridging the gap between predictive performance and model
transparency.",2025-05-23,"Runze Li, Siyu Wu, Jun Wang, Wei Zhang",http://arxiv.org/pdf/2505.17705v1,cs.LG
Gradient-Based Program Repair: Fixing Bugs in Continuous Program Spaces,"Automatic program repair seeks to generate correct code from buggy programs,
with most approaches searching the correct program in a discrete, symbolic
space of source code tokens. This symbolic search is fundamentally limited by
its inability to directly reason about program behavior. We introduce
Gradient-Based Program Repair (GBPR), a new paradigm that reframes program
repair as continuous optimization in a differentiable numerical program space.
Our core insight is to compile symbolic programs into differentiable numerical
representations, enabling search in the numerical program space directly guided
by program behavior. To evaluate GBPR, we present RaspBugs, a new benchmark of
1,466 buggy symbolic RASP programs and their respective numerical
representations. Our experiments demonstrate that GBPR can effectively repair
buggy symbolic programs by gradient-based optimization in the numerical program
space, with convincing repair trajectories. To our knowledge, we are the first
to state program repair as continuous optimization in a numerical program
space. Our work establishes a new direction for program repair research,
bridging two rich worlds: continuous optimization and program behavior.",2025-05-23,"André Silva, Gustav Thorén, Martin Monperrus",http://arxiv.org/pdf/2505.17703v1,cs.LG
COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection,"The growing size of large language models has created significant
computational inefficiencies. To address this challenge, sparse activation
methods selectively deactivates non-essential parameters during inference,
reducing computational costs in FFNN layers. While existing methods focus on
non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN
layer lies globally in the form of a linear combination over its internal down
projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,
leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct
coefficients of the linear combination. Experimental results demonstrate that
D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%
ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%
better performance preservation compared to existing methods. Our specialized
kernel implementations effectively realize these theoretical gains into
substantial real-world acceleration.",2025-05-23,"Jaewon Cheon, Pilsung Kang",http://arxiv.org/pdf/2505.17701v1,cs.LG
Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models,"Despite the remarkable reasoning performance, eliciting the long
chain-of-thought (CoT) ability in large language models (LLMs) typically
requires costly reinforcement learning or supervised fine-tuning on
high-quality distilled data. We investigate the internal mechanisms behind this
capability and show that a small set of high-impact activations in the last few
layers largely governs long-form reasoning attributes, such as output length
and self-reflection. By simply amplifying these activations and inserting
""wait"" tokens, we can invoke the long CoT ability without any training,
resulting in significantly increased self-reflection rates and accuracy.
Moreover, we find that the activation dynamics follow predictable trajectories,
with a sharp rise after special tokens and a subsequent exponential decay.
Building on these insights, we introduce a general training-free activation
control technique. It leverages a few contrastive examples to identify key
activations, and employs simple analytic functions to modulate their values at
inference time to elicit long CoTs. Extensive experiments confirm the
effectiveness of our method in efficiently eliciting long CoT reasoning in LLMs
and improving their performance. Additionally, we propose a parameter-efficient
fine-tuning method that trains only a last-layer activation amplification
module and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning
benchmarks with significantly fewer parameters. Our code and data are publicly
released.",2025-05-23,"Zekai Zhao, Qi Liu, Kun Zhou, Zihan Liu, Yifei Shao, Zhiting Hu, Biwei Huang",http://arxiv.org/pdf/2505.17697v1,cs.LG
SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data,"Despite the advances in Referring Expression Segmentation (RES) benchmarks,
their evaluation protocols remain constrained, primarily focusing on either
single targets with short queries (containing minimal attributes) or multiple
targets from distinctly different queries on a single domain. This limitation
significantly hinders the assessment of more complex reasoning capabilities in
RES models. We introduce WildRES, a novel benchmark that incorporates long
queries with diverse attributes and non-distinctive queries for multiple
targets. This benchmark spans diverse application domains, including autonomous
driving environments and robotic manipulation scenarios, thus enabling more
rigorous evaluation of complex reasoning capabilities in real-world settings.
Our analysis reveals that current RES models demonstrate substantial
performance deterioration when evaluated on WildRES. To address this challenge,
we introduce SynRES, an automated pipeline generating densely paired
compositional synthetic training data through three innovations: (1) a dense
caption-driven synthesis for attribute-rich image-mask-expression triplets, (2)
reliable semantic alignment mechanisms rectifying caption-pseudo mask
inconsistencies via Image-Text Aligned Grouping, and (3) domain-aware
augmentations incorporating mosaic composition and superclass replacement to
emphasize generalization ability and distinguishing attributes over object
categories. Experimental results demonstrate that models trained with SynRES
achieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and
3.8% on WildRES-DS. Code and datasets are available at
https://github.com/UTLLab/SynRES.",2025-05-23,"Dong-Hee Kim, Hyunjee Song, Donghyun Kim",http://arxiv.org/pdf/2505.17695v1,cs.LG
FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding,"Prefix-sharing among multiple prompts presents opportunities to combine the
operations of the shared prefix, while attention computation in the decode
stage, which becomes a critical bottleneck with increasing context lengths, is
a memory-intensive process requiring heavy memory access on the key-value (KV)
cache of the prefixes. Therefore, in this paper, we explore the potential of
prefix-sharing in the attention computation of the decode stage. However, the
tree structure of the prefix-sharing mechanism presents significant challenges
for attention computation in efficiently processing shared KV cache access
patterns while managing complex dependencies and balancing irregular workloads.
To address the above challenges, we propose a dedicated attention kernel to
combine the memory access of shared prefixes in the decoding stage, namely
FlashForge. FlashForge delivers two key innovations: a novel shared-prefix
attention kernel that optimizes memory hierarchy and exploits both intra-block
and inter-block parallelism, and a comprehensive workload balancing mechanism
that efficiently estimates cost, divides tasks, and schedules execution.
Experimental results show that FlashForge achieves an average 1.9x speedup and
120.9x memory access reduction compared to the state-of-the-art FlashDecoding
kernel regarding attention computation in the decode stage and 3.8x end-to-end
time per output token compared to the vLLM.",2025-05-23,"Zhibin Wang, Rui Ning, Chao Fang, Zhonghui Zhang, Xi Lin, Shaobo Ma, Mo Zhou, Xue Li, Zhongfeng Wang, Chengying Huan, Rong Gu, Kun Yang, Guihai Chen, Sheng Zhong, Chen Tian",http://arxiv.org/pdf/2505.17694v1,cs.LG
Towards General Continuous Memory for Vision-Language Models,"Language models (LMs) and their extension, vision-language models (VLMs),
have achieved remarkable performance across various tasks. However, they still
struggle with complex reasoning tasks that require multimodal or multilingual
real-world knowledge. To support such capabilities, an external memory system
that can efficiently provide relevant multimodal information is essential.
Existing approaches generally concatenate image and text tokens into a long
sequence as memory, which, however, may drastically increase context length and
even degrade performance. In contrast, we propose using continuous memory, a
compact set of dense embeddings to more effectively and efficiently represent
multimodal and multilingual knowledge. Our key insight is that a VLM can serve
as its own continuous memory encoder. We empirically show that this design
improves performance on complex multimodal reasoning tasks. Building on this,
we introduce a data-efficient and parameter-efficient method to fine-tune the
VLM into a memory encoder, requiring only 1.2% of the model's parameters and a
small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes
VLM's original capabilities to encode arbitrary multimodal and multilingual
knowledge into just 8 continuous embeddings. Since the inference-time VLM
remains frozen, our memory module is plug-and-play and can be flexibly
integrated as needed. Extensive experiments across eight multimodal reasoning
benchmarks demonstrate the effectiveness of our approach.",2025-05-23,"Wenyi Wu, Zixuan Song, Kun Zhou, Yifei Shao, Zhiting Hu, Biwei Huang",http://arxiv.org/pdf/2505.17670v1,cs.LG
What is the role of memorization in Continual Learning?,"Memorization impacts the performance of deep learning algorithms. Prior works
have studied memorization primarily in the context of generalization and
privacy. This work studies the memorization effect on incremental learning
scenarios. Forgetting prevention and memorization seem similar. However, one
should discuss their differences. We designed extensive experiments to evaluate
the impact of memorization on continual learning. We clarified that learning
examples with high memorization scores are forgotten faster than regular
samples. Our findings also indicated that memorization is necessary to achieve
the highest performance. However, at low memory regimes, forgetting regular
samples is more important. We showed that the importance of a high-memorization
score sample rises with an increase in the buffer size. We introduced a
memorization proxy and employed it in the buffer policy problem to showcase how
memorization could be used during incremental training. We demonstrated that
including samples with a higher proxy memorization score is beneficial when the
buffer size is large.",2025-05-23,"Jędrzej Kozal, Jan Wasilewski, Alif Ashrafee, Bartosz Krawczyk, Michał Woźniak",http://arxiv.org/pdf/2505.17664v1,cs.LG
Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs,"Transformer-based models have shown strong performance across diverse
time-series tasks, but their deployment on resource-constrained devices remains
challenging due to high memory and computational demand. While prior work
targeting Microcontroller Units (MCUs) has explored hardware-specific
optimizations, such approaches are often task-specific and limited to 8-bit
fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater
flexibility, enabling fine-grained control over data precision and
architecture. However, existing FPGA-based deployments of Transformers for
time-series analysis typically focus on high-density platforms with manual
configuration. This paper presents a unified and fully automated deployment
framework for Tiny Transformers on embedded FPGAs. Our framework supports a
compact encoder-only Transformer architecture across three representative
time-series tasks (forecasting, classification, and anomaly detection). It
combines quantization-aware training (down to 4 bits), hardware-aware
hyperparameter search using Optuna, and automatic VHDL generation for seamless
deployment. We evaluate our framework on six public datasets across two
embedded FPGA platforms. Results show that our framework produces integer-only,
task-specific Transformer accelerators achieving as low as 0.033 mJ per
inference with millisecond latency on AMD Spartan-7, while also providing
insights into deployment feasibility on Lattice iCE40. All source code will be
released in the GitHub repository
(https://github.com/Edwina1030/TinyTransformer4TS).",2025-05-23,"Tianheng Ling, Chao Qian, Lukas Johannes Haßler, Gregor Schiele",http://arxiv.org/pdf/2505.17662v1,cs.LG
Automated scientific minimization of regret,"We introduce automated scientific minimization of regret (ASMR) -- a
framework for automated computational cognitive science. Building on the
principles of scientific regret minimization, ASMR leverages Centaur -- a
recently proposed foundation model of human cognition -- to identify gaps in an
interpretable cognitive model. These gaps are then addressed through automated
revisions generated by a language-based reasoning model. We demonstrate the
utility of this approach in a multi-attribute decision-making task, showing
that ASMR discovers cognitive models that predict human behavior at noise
ceiling while retaining interpretability. Taken together, our results highlight
the potential of ASMR to automate core components of the cognitive modeling
pipeline.",2025-05-23,"Marcel Binz, Akshay K. Jagadish, Milena Rmus, Eric Schulz",http://arxiv.org/pdf/2505.17661v1,cs.LG
DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification,"Neighborhood-aware tokenized graph Transformers have recently shown great
potential for node classification tasks. Despite their effectiveness, our
in-depth analysis of neighborhood tokens reveals two critical limitations in
the existing paradigm. First, current neighborhood token generation methods
fail to adequately capture attribute correlations within a neighborhood.
Second, the conventional self-attention mechanism suffers from attention
diversion when processing neighborhood tokens, where high-hop neighborhoods
receive disproportionate focus, severely disrupting information interactions
between the target node and its neighborhood tokens. To address these
challenges, we propose DAM-GT, Dual positional encoding-based Attention Masking
graph Transformer. DAM-GT introduces a novel dual positional encoding scheme
that incorporates attribute-aware encoding via an attribute clustering
strategy, effectively preserving node correlations in both topological and
attribute spaces. In addition, DAM-GT formulates a new attention mechanism with
a simple yet effective masking strategy to guide interactions between target
nodes and their neighborhood tokens, overcoming the issue of attention
diversion. Extensive experiments on various graphs with different homophily
levels as well as different scales demonstrate that DAM-GT consistently
outperforms state-of-the-art methods in node classification tasks.",2025-05-23,"Chenyang Li, Jinsong Chen, John E. Hopcroft, Kun He",http://arxiv.org/pdf/2505.17660v1,cs.LG
Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective,"Reinforcement learning exhibits potential in enhancing the reasoning
abilities of large language models, yet it is hard to scale for the low sample
efficiency during the rollout phase. Existing methods attempt to improve
efficiency by scheduling problems based on problem difficulties. However, these
approaches suffer from unstable and biased estimations of problem difficulty
and fail to capture the alignment between model competence and problem
difficulty in RL training, leading to suboptimal results. To tackle these
limitations, this paper introduces \textbf{C}ompetence-\textbf{D}ifficulty
\textbf{A}lignment \textbf{S}ampling (\textbf{CDAS}), which enables accurate
and stable estimation of problem difficulties by aggregating historical
performance discrepancies of problems. Then the model competence is quantified
to adaptively select problems whose difficulty is in alignment with the model's
current competence using a fixed-point system. Experimental results across a
range of challenging mathematical benchmarks show that CDAS achieves great
improvements in both accuracy and efficiency. CDAS attains the highest average
accuracy against baselines and exhibits significant speed advantages compared
to Dynamic Sampling, a competitive strategy in DAPO, which is \textbf{2.33}
times slower than CDAS.",2025-05-23,"Deyang Kong, Qi Guo, Xiangyu Xi, Wei Wang, Jingang Wang, Xunliang Cai, Shikun Zhang, Wei Ye",http://arxiv.org/pdf/2505.17652v1,cs.LG
Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives,"Recent studies have revealed that the loss landscape of large language models
resembles a basin, within which the models perform nearly identically, and
outside of which they lose all their capabilities. In this work, we conduct
further studies on the loss landscape of large language models. We discover
that pre-training creates a ""basic capability"" basin, and subsequent
fine-tuning creates ""specific capability"" basins (e.g., math, safety, coding)
within the basic capability basin. We further investigate two types of loss
landscapes: the most-case landscape (i.e., the landscape along most directions)
and the worst-case landscape (i.e., the landscape along the worst direction).
We argue that as long as benign fine-tuning remains within the most-case basin,
it will not compromise previous capabilities. Similarly, any fine-tuning
(including the adversarial one) that stays within the worst-case basin would
not compromise previous capabilities. Finally, we theoretically demonstrate
that the size of the most-case basin can bound the size of the worst-case basin
and the robustness with respect to input perturbations. We also show that, due
to the over-parameterization property of current large language models, one can
easily enlarge the basins by five times.",2025-05-23,"Huanran Chen, Yinpeng Dong, Zeming Wei, Yao Huang, Yichi Zhang, Hang Su, Jun Zhu",http://arxiv.org/pdf/2505.17646v1,cs.LG
HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning,"Embodied agents operating in smart homes must understand human behavior
through diverse sensory inputs and communicate via natural language. While
Vision-Language Models (VLMs) have enabled impressive language-grounded
perception, their reliance on visual data limits robustness in real-world
scenarios with occlusions, poor lighting, or privacy constraints. In this
paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that
integrates uncommon but powerful sensing modalities, such as LiDAR, infrared,
mmWave radar, and WiFi, to enable seamless human perception and reasoning
across heterogeneous environments. We address two key challenges: (1) the
scarcity of aligned modality-text data for rare sensors, and (2) the
heterogeneity of their physical signal representations. To overcome these, we
design a Universal Modality-Injection Projector (UMIP) that enhances
pre-aligned modality embeddings with fine-grained, text-aligned features from
tailored encoders via coarse-to-fine cross-attention without introducing
significant alignment overhead. We further introduce a human-VLM collaborative
data curation pipeline to generate paired textual annotations for sensing
datasets. Extensive experiments on two newly constructed benchmarks show that
HoloLLM significantly outperforms existing MLLMs, improving language-grounded
human sensing accuracy by up to 30%. This work establishes a new foundation for
real-world, language-informed multisensory embodied intelligence.",2025-05-23,"Chuhao Zhou, Jianfei Yang",http://arxiv.org/pdf/2505.17645v1,cs.LG
Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks,"Conventional machine learning models, particularly tree-based approaches,
have demonstrated promising performance across various clinical prediction
tasks using electronic health record (EHR) data. Despite their strengths, these
models struggle with tasks that require deeper contextual understanding, such
as predicting 30-day hospital readmission. This can be primarily due to the
limited semantic information available in structured EHR data. To address this
limitation, we propose a deep multimodal contrastive learning (CL) framework
that aligns the latent representations of structured EHR data with unstructured
discharge summary notes. It works by pulling together paired EHR and text
embeddings while pushing apart unpaired ones. Fine-tuning the pretrained EHR
encoder extracted from this framework significantly boosts downstream task
performance, e.g., a 4.1% AUROC enhancement over XGBoost for 30-day readmission
prediction. Such results demonstrate the effect of integrating domain knowledge
from clinical notes into EHR-based pipelines, enabling more accurate and
context-aware clinical decision support systems.",2025-05-23,"Sara Ketabi, Dhanesh Ramachandram",http://arxiv.org/pdf/2505.17643v1,cs.LG
A Network Science Approach to Granular Time Series Segmentation,"Time series segmentation (TSS) is one of the time series (TS) analysis
techniques, that has received considerably less attention compared to other TS
related tasks. In recent years, deep learning architectures have been
introduced for TSS, however their reliance on sliding windows limits
segmentation granularity due to fixed window sizes and strides. To overcome
these challenges, we propose a new more granular TSS approach that utilizes the
Weighted Dual Perspective Visbility Graph (WDPVG) TS into a graph and combines
it with a Graph Attention Network (GAT). By transforming TS into graphs, we are
able to capture different structural aspects of the data that would otherwise
remain hidden. By utilizing the representation learning capabilities of Graph
Neural Networks, our method is able to effectively identify meaningful segments
within the TS. To better understand the potential of our approach, we also
experimented with different TS-to-graph transformations and compared their
performance. Our contributions include: a) formulating the TSS as a node
classification problem on graphs; b) conducting an extensive analysis of
various TS- to-graph transformations applied to TSS using benchmark datasets
from the TSSB repository; c) providing the first detailed study on utilizing
GNNs for analyzing graph representations of TS in the context of TSS; d)
demonstrating the effectiveness of our method, which achieves an average F1
score of 0.97 across 59 diverse TSS benchmark datasets; e) outperforming the
seq2point baseline method by 0.05 in terms of F1 score; and f) reducing the
required training data compared to the baseline methods.",2025-05-23,"Ivana Kesić, Carolina Fortuna, Mihael Mohorčič, Blaž Bertalanič",http://arxiv.org/pdf/2505.17640v1,cs.LG
PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval,"Mixture-of-experts (MoE) architectures enable scaling large language models
(LLMs) to vast parameter counts without a proportional rise in computational
costs. However, the significant memory demands of large MoE models hinder their
deployment across various computational environments, from cloud servers to
consumer devices. This study first demonstrates pronounced task-specific
specialization in expert activation patterns within MoE layers. Building on
this, we introduce PreMoe, a novel framework that enables efficient deployment
of massive MoE models in memory-constrained environments. PreMoe features two
main components: probabilistic expert pruning (PEP) and task-adaptive expert
retrieval (TAER). PEP employs a new metric, the task-conditioned expected
selection score (TCESS), derived from router logits to quantify expert
importance for specific tasks, thereby identifying a minimal set of critical
experts. TAER leverages these task-specific expert importance profiles for
efficient inference. It pre-computes and stores compact expert patterns for
diverse tasks. When a user query is received, TAER rapidly identifies the most
relevant stored task pattern and reconstructs the model by loading only the
small subset of experts crucial for that task. This approach dramatically
reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B
maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\%
expert reduction), and still achieves 72.0\% with aggressive 8/32 pruning
(87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and
81.3\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64
(390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly
available at https://github.com/JarvisPei/PreMoe.",2025-05-23,"Zehua Pei, Ying Zhang, Hui-Ling Zhen, Xianzhi Yu, Wulong Liu, Sinno Jialin Pan, Mingxuan Yuan, Bei Yu",http://arxiv.org/pdf/2505.17639v1,cs.LG
Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training,"Diffusion models have achieved remarkable success across a wide range of
generative tasks. A key challenge is understanding the mechanisms that prevent
their memorization of training data and allow generalization. In this work, we
investigate the role of the training dynamics in the transition from
generalization to memorization. Through extensive experiments and theoretical
analysis, we identify two distinct timescales: an early time
$\tau_\mathrm{gen}$ at which models begin to generate high-quality samples, and
a later time $\tau_\mathrm{mem}$ beyond which memorization emerges. Crucially,
we find that $\tau_\mathrm{mem}$ increases linearly with the training set size
$n$, while $\tau_\mathrm{gen}$ remains constant. This creates a growing window
of training times with $n$ where models generalize effectively, despite showing
strong memorization if training continues beyond it. It is only when $n$
becomes larger than a model-dependent threshold that overfitting disappears at
infinite training times. These findings reveal a form of implicit dynamical
regularization in the training dynamics, which allow to avoid memorization even
in highly overparameterized settings. Our results are supported by numerical
experiments with standard U-Net architectures on realistic and synthetic
datasets, and by a theoretical analysis using a tractable random features model
studied in the high-dimensional limit.",2025-05-23,"Tony Bonnaire, Raphaël Urfin, Giulio Biroli, Marc Mézard",http://arxiv.org/pdf/2505.17638v1,cs.LG
Causal Spatio-Temporal Prediction: An Effective and Efficient Multi-Modal Approach,"Spatio-temporal prediction plays a crucial role in intelligent
transportation, weather forecasting, and urban planning. While integrating
multi-modal data has shown potential for enhancing prediction accuracy, key
challenges persist: (i) inadequate fusion of multi-modal information, (ii)
confounding factors that obscure causal relations, and (iii) high computational
complexity of prediction models. To address these challenges, we propose
E^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal
Prediction framework. E^2-CSTP leverages cross-modal attention and gating
mechanisms to effectively integrate multi-modal data. Building on this, we
design a dual-branch causal inference approach: the primary branch focuses on
spatio-temporal prediction, while the auxiliary branch mitigates bias by
modeling additional modalities and applying causal interventions to uncover
true causal dependencies. To improve model efficiency, we integrate GCN with
the Mamba architecture for accelerated spatio-temporal encoding. Extensive
experiments on 4 real-world datasets show that E^2-CSTP significantly
outperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in
accuracy as well as 17.37%-56.11% reductions in computational overhead.",2025-05-23,"Yuting Huang, Ziquan Fang, Zhihao Zeng, Lu Chen, Yunjun Gao",http://arxiv.org/pdf/2505.17637v1,cs.LG
Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis,"Various AI safety datasets have been developed to measure LLMs against
evolving interpretations of harm. Our evaluation of five recently published
open-source safety benchmarks reveals distinct semantic clusters using UMAP
dimensionality reduction and kmeans clustering (silhouette score: 0.470). We
identify six primary harm categories with varying benchmark representation.
GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix
emphasizes self-harm scenarios. Significant differences in prompt length
distribution suggests confounds to data collection and interpretations of harm
as well as offer possible context. Our analysis quantifies benchmark
orthogonality among AI benchmarks, allowing for transparency in coverage gaps
despite topical similarities. Our quantitative framework for analyzing semantic
orthogonality across safety benchmarks enables more targeted development of
datasets that comprehensively address the evolving landscape of harms in AI
use, however that is defined in the future.",2025-05-23,"Jonathan Bennion, Shaona Ghosh, Mantek Singh, Nouha Dziri",http://arxiv.org/pdf/2505.17636v1,cs.LG
Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs,"Multimodal out-of-context (OOC) misinformation is misinformation that
repurposes real images with unrelated or misleading captions. Detecting such
misinformation is challenging because it requires resolving the context of the
claim before checking for misinformation. Many current methods, including LLMs
and LVLMs, do not perform this contextualization step. LLMs hallucinate in
absence of context or parametric knowledge. In this work, we propose a
graph-based method that evaluates the consistency between the image and the
caption by constructing two graph representations: an evidence graph, derived
from online textual evidence, and a claim graph, from the claim in the caption.
Using graph neural networks (GNNs) to encode and compare these representations,
our framework then evaluates the truthfulness of image-caption pairs. We create
datasets for our graph-based method, evaluate and compare our baseline model
against popular LLMs on the misinformation detection task. Our method scores
$93.05\%$ detection accuracy on the evaluation set and outperforms the
second-best performing method (an LLM) by $2.82\%$, making a case for smaller
and task-specific methods.",2025-05-23,"Sharad Duwal, Mir Nafis Sharear Shopnil, Abhishek Tyagi, Adiba Mahbub Proma",http://arxiv.org/pdf/2505.18221v1,cs.LG
ReqBrain: Task-Specific Instruction Tuning of LLMs for AI-Assisted Requirements Generation,"Requirements elicitation and specification remains a labor-intensive, manual
process prone to inconsistencies and gaps, presenting a significant challenge
in modern software engineering. Emerging studies underscore the potential of
employing large language models (LLMs) for automated requirements generation to
support requirements elicitation and specification; however, it remains unclear
how to implement this effectively. In this work, we introduce ReqBrain, an
Al-assisted tool that employs a fine-tuned LLM to generate authentic and
adequate software requirements. Software engineers can engage with ReqBrain
through chat-based sessions to automatically generate software requirements and
categorize them by type. We curated a high-quality dataset of ISO
29148-compliant requirements and fine-tuned five 7B-parameter LLMs to determine
the most effective base model for ReqBrain. The top-performing model,
Zephyr-7b-beta, achieved 89.30\% Fl using the BERT score and a FRUGAL score of
91.20 in generating authentic and adequate requirements. Human evaluations
further confirmed ReqBrain's effectiveness in generating requirements. Our
findings suggest that generative Al, when fine-tuned, has the potential to
improve requirements elicitation and specification, paving the way for future
extensions into areas such as defect identification, test case generation, and
agile user story creation.",2025-05-23,"Mohammad Kasra Habib, Daniel Graziotin, Stefan Wagner",http://arxiv.org/pdf/2505.17632v1,cs.LG
GIM: Improved Interpretability for Large Language Models,"Ensuring faithful interpretability in large language models is imperative for
trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where
networks compensate for reduced signal in one component by amplifying others,
masking the true importance of the ablated component. While prior work
attributes self-repair to layer normalization and back-up components that
compensate for ablated components, we identify a novel form occurring within
the attention mechanism, where softmax redistribution conceals the influence of
important attention scores. This leads traditional ablation and gradient-based
methods to underestimate the significance of all components contributing to
these attention scores. We introduce Gradient Interaction Modifications (GIM),
a technique that accounts for self-repair during backpropagation. Extensive
experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,
Qwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves
faithfulness over existing circuit identification and feature attribution
methods. Our work is a significant step toward better understanding the inner
mechanisms of LLMs, which is crucial for improving them and ensuring their
safety. Our code is available at https://github.com/JoakimEdin/gim.",2025-05-23,"Joakim Edin, Róbert Csordás, Tuukka Ruotsalo, Zhengxuan Wu, Maria Maistro, Jing Huang, Lars Maaløe",http://arxiv.org/pdf/2505.17630v1,cs.LG
Leveraging Stochastic Depth Training for Adaptive Inference,"Dynamic DNN optimization techniques such as layer-skipping offer increased
adaptability and efficiency gains but can lead to i) a larger memory footprint
as in decision gates, ii) increased training complexity (e.g., with
non-differentiable operations), and iii) less control over performance-quality
trade-offs due to its inherent input-dependent execution. To approach these
issues, we propose a simpler yet effective alternative for adaptive inference
with a zero-overhead, single-model, and time-predictable inference. Central to
our approach is the observation that models trained with Stochastic Depth -- a
method for faster training of residual networks -- become more resilient to
arbitrary layer-skipping at inference time. We propose a method to first select
near Pareto-optimal skipping configurations from a stochastically-trained model
to adapt the inference at runtime later. Compared to original ResNets, our
method shows improvements of up to 2X in power efficiency at accuracy drops as
low as 0.71%.",2025-05-23,"Guilherme Korol, Antonio Carlos Schneider Beck, Jeronimo Castrillon",http://arxiv.org/pdf/2505.17626v1,cs.LG
\texttt{Range-Arithmetic}: Verifiable Deep Learning Inference on an Untrusted Party,"Verifiable computing (VC) has gained prominence in decentralized machine
learning systems, where resource-intensive tasks like deep neural network (DNN)
inference are offloaded to external participants due to blockchain limitations.
This creates a need to verify the correctness of outsourced computations
without re-execution. We propose \texttt{Range-Arithmetic}, a novel framework
for efficient and verifiable DNN inference that transforms non-arithmetic
operations, such as rounding after fixed-point matrix multiplication and ReLU,
into arithmetic steps verifiable using sum-check protocols and concatenated
range proofs. Our approach avoids the complexity of Boolean encoding,
high-degree polynomials, and large lookup tables while remaining compatible
with finite-field-based proof systems. Experimental results show that our
method not only matches the performance of existing approaches, but also
reduces the computational cost of verifying the results, the computational
effort required from the untrusted party performing the DNN inference, and the
communication overhead between the two sides.",2025-05-23,"Ali Rahimi, Babak H. Khalaj, Mohammad Ali Maddah-Ali",http://arxiv.org/pdf/2505.17623v1,cs.LG
Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration,"Reinforcement learning (RL) has emerged as a pivotal method for improving the
reasoning capabilities of Large Language Models (LLMs). However, prevalent RL
approaches such as Proximal Policy Optimization (PPO) and Group-Regularized
Policy Optimization (GRPO) face critical limitations due to their reliance on
sparse outcome-based rewards and inadequate mechanisms for incentivizing
exploration. These limitations result in inefficient guidance for multi-step
reasoning processes. Specifically, sparse reward signals fail to deliver
effective or sufficient feedback, particularly for challenging problems.
Furthermore, such reward structures induce systematic biases that prioritize
exploitation of familiar trajectories over novel solution discovery. These
shortcomings critically hinder performance in complex reasoning tasks, which
inherently demand iterative refinement across ipntermediate steps. To address
these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd
foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense
rewards and amplify explorations in the RL-based training paradigm. i-MENTOR
introduces three key innovations: trajectory-aware exploration rewards that
mitigate bias in token-level strategies while maintaining computational
efficiency; dynamic reward scaling to stabilize exploration and exploitation in
large action spaces; and advantage-preserving reward implementation that
maintains advantage distribution integrity while incorporating exploratory
guidance. Experiments across three public datasets demonstrate i-MENTOR's
effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.",2025-05-23,"Jingtong Gao, Ling Pan, Yejing Wang, Rui Zhong, Chi Lu, Qingpeng Cai, Peng Jiang, Xiangyu Zhao",http://arxiv.org/pdf/2505.17621v1,cs.LG
Scaling Image and Video Generation via Test-Time Evolutionary Search,"As the marginal cost of scaling computation (data and parameters) during
model pre-training continues to increase substantially, test-time scaling (TTS)
has emerged as a promising direction for improving generative model performance
by allocating additional computation at inference time. While TTS has
demonstrated significant success across multiple language tasks, there remains
a notable gap in understanding the test-time scaling behaviors of image and
video generative models (diffusion-based or flow-based models). Although recent
works have initiated exploration into inference-time strategies for vision
tasks, these approaches face critical limitations: being constrained to
task-specific domains, exhibiting poor scalability, or falling into reward
over-optimization that sacrifices sample diversity. In this paper, we propose
\textbf{Evo}lutionary \textbf{Search} (EvoSearch), a novel, generalist, and
efficient TTS method that effectively enhances the scalability of both image
and video generation across diffusion and flow models, without requiring
additional training or model expansion. EvoSearch reformulates test-time
scaling for diffusion and flow models as an evolutionary search problem,
leveraging principles from biological evolution to efficiently explore and
refine the denoising trajectory. By incorporating carefully designed selection
and mutation mechanisms tailored to the stochastic differential equation
denoising process, EvoSearch iteratively generates higher-quality offspring
while preserving population diversity. Through extensive evaluation across both
diffusion and flow architectures for image and video generation tasks, we
demonstrate that our method consistently outperforms existing approaches,
achieves higher diversity, and shows strong generalizability to unseen
evaluation metrics. Our project is available at the website
https://tinnerhrhe.github.io/evosearch.",2025-05-23,"Haoran He, Jiajun Liang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Ling Pan",http://arxiv.org/pdf/2505.17618v1,cs.LG
Large language model as user daily behavior data generator: balancing population diversity and individual personality,"Predicting human daily behavior is challenging due to the complexity of
routine patterns and short-term fluctuations. While data-driven models have
improved behavior prediction by leveraging empirical data from various
platforms and devices, the reliance on sensitive, large-scale user data raises
privacy concerns and limits data availability. Synthetic data generation has
emerged as a promising solution, though existing methods are often limited to
specific applications. In this work, we introduce BehaviorGen, a framework that
uses large language models (LLMs) to generate high-quality synthetic behavior
data. By simulating user behavior based on profiles and real events,
BehaviorGen supports data augmentation and replacement in behavior prediction
models. We evaluate its performance in scenarios such as pertaining
augmentation, fine-tuning replacement, and fine-tuning augmentation, achieving
significant improvements in human mobility and smartphone usage predictions,
with gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen
to enhance user behavior modeling through flexible and privacy-preserving
synthetic data generation.",2025-05-23,"Haoxin Li, Jingtao Ding, Jiahui Gong, Yong Li",http://arxiv.org/pdf/2505.17615v1,cs.LG
Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning,"This paper provides the first expert sample complexity characterization for
learning a Nash equilibrium from expert data in Markov Games. We show that a
new quantity named the single policy deviation concentrability coefficient is
unavoidable in the non-interactive imitation learning setting, and we provide
an upper bound for behavioral cloning (BC) featuring such coefficient. BC
exhibits substantial regret in games with high concentrability coefficient,
leading us to utilize expert queries to develop and introduce two novel
solution algorithms: MAIL-BRO and MURMAIL. The former employs a best response
oracle and learns an $\varepsilon$-Nash equilibrium with
$\mathcal{O}(\varepsilon^{-4})$ expert and oracle queries. The latter bypasses
completely the best response oracle at the cost of a worse expert query
complexity of order $\mathcal{O}(\varepsilon^{-8})$. Finally, we provide
numerical evidence, confirming our theoretical findings.",2025-05-23,"Till Freihaut, Luca Viano, Volkan Cevher, Matthieu Geist, Giorgia Ramponi",http://arxiv.org/pdf/2505.17610v1,cs.LG
Adaptive Semantic Token Communication for Transformer-based Edge Inference,"This paper presents an adaptive framework for edge inference based on a
dynamically configurable transformer-powered deep joint source channel coding
(DJSCC) architecture. Motivated by a practical scenario where a resource
constrained edge device engages in goal oriented semantic communication, such
as selectively transmitting essential features for object detection to an edge
server, our approach enables efficient task aware data transmission under
varying bandwidth and channel conditions. To achieve this, input data is
tokenized into compact high level semantic representations, refined by a
transformer, and transmitted over noisy wireless channels. As part of the DJSCC
pipeline, we employ a semantic token selection mechanism that adaptively
compresses informative features into a user specified number of tokens per
sample. These tokens are then further compressed through the JSCC module,
enabling a flexible token communication strategy that adjusts both the number
of transmitted tokens and their embedding dimensions. We incorporate a resource
allocation algorithm based on Lyapunov stochastic optimization to enhance
robustness under dynamic network conditions, effectively balancing compression
efficiency and task performance. Experimental results demonstrate that our
system consistently outperforms existing baselines, highlighting its potential
as a strong foundation for AI native semantic communication in edge
intelligence applications.",2025-05-23,"Alessio Devoto, Jary Pomponi, Mattia Merluzzi, Paolo Di Lorenzo, Simone Scardapane",http://arxiv.org/pdf/2505.17604v1,cs.LG
Dynamic Text Bundling Supervision for Zero-Shot Inference on Text-Attributed Graphs,"Large language models (LLMs) have been used in many zero-shot learning
problems, with their strong generalization ability. Recently, adopting LLMs in
text-attributed graphs (TAGs) has drawn increasing attention. However, the
adoption of LLMs faces two major challenges: limited information on graph
structure and unreliable responses. LLMs struggle with text attributes isolated
from the graph topology. Worse still, they yield unreliable predictions due to
both information insufficiency and the inherent weakness of LLMs (e.g.,
hallucination). Towards this end, this paper proposes a novel method named
Dynamic Text Bundling Supervision (DENSE) that queries LLMs with bundles of
texts to obtain bundle-level labels and uses these labels to supervise graph
neural networks. Specifically, we sample a set of bundles, each containing a
set of nodes with corresponding texts of close proximity. We then query LLMs
with the bundled texts to obtain the label of each bundle. Subsequently, the
bundle labels are used to supervise the optimization of graph neural networks,
and the bundles are further refined to exclude noisy items. To justify our
design, we also provide theoretical analysis of the proposed method. Extensive
experiments across ten datasets validate the effectiveness of the proposed
method.",2025-05-23,"Yusheng Zhao, Qixin Zhang, Xiao Luo, Weizhi Zhang, Zhiping Xiao, Wei Ju, Philip S. Yu, Ming Zhang",http://arxiv.org/pdf/2505.17599v1,cs.LG
NeUQI: Near-Optimal Uniform Quantization Parameter Initialization,"Large language models (LLMs) achieve impressive performance across domains
but face significant challenges when deployed on consumer-grade GPUs or
personal devices such as laptops, due to high memory consumption and inference
costs. Post-training quantization (PTQ) of LLMs offers a promising solution
that reduces their memory footprint and decoding latency. In practice, PTQ with
uniform quantization representation is favored for its efficiency and ease of
deployment since uniform quantization is widely supported by mainstream
hardware and software libraries. Recent studies on $\geq 2$-bit uniform
quantization have led to noticeable improvements in post-quantization model
performance; however, they primarily focus on quantization methodologies, while
the initialization of quantization parameters is underexplored and still relies
on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method
devoted to efficiently determining near-optimal initial parameters for uniform
quantization. NeUQI is orthogonal to prior quantization methodologies and can
seamlessly integrate with them. The experiments with the LLaMA and Qwen
families on various tasks demonstrate that our NeUQI consistently outperforms
existing methods. Furthermore, when combined with a lightweight distillation
strategy, NeUQI can achieve superior performance to PV-tuning, a much more
resource-intensive approach.",2025-05-23,"Li Lin, Xinyu Hu, Xiaojun Wan",http://arxiv.org/pdf/2505.17595v1,cs.LG
AstroMLab 4: Benchmark-Topping Performance in Astronomy Q&A with a 70B-Parameter Domain-Specialized Reasoning Model,"General-purpose large language models, despite their broad capabilities,
often struggle with specialized domain knowledge, a limitation particularly
pronounced in more accessible, lower-parameter versions. This gap hinders their
deployment as effective agents in demanding fields such as astronomy. Building
on our prior work with AstroSage-8B, this study introduces AstroSage-70B, a
significantly larger and more advanced domain-specialized natural-language AI
assistant. It is designed for research and education across astronomy,
astrophysics, space science, astroparticle physics, cosmology, and astronomical
instrumentation. Developed from the Llama-3.1-70B foundation, AstroSage-70B
underwent extensive continued pre-training on a vast corpus of astronomical
literature, followed by supervised fine-tuning and model merging. Beyond its
70-billion parameter scale, this model incorporates refined datasets,
judiciously chosen learning hyperparameters, and improved training procedures,
achieving state-of-the-art performance on complex astronomical tasks. Notably,
we integrated reasoning chains into the SFT dataset, enabling AstroSage-70B to
either answer the user query immediately, or first emit a human-readable
thought process. Evaluated on the AstroMLab-1 benchmark -- comprising 4,425
questions from literature withheld during training -- AstroSage-70B achieves
state-of-the-art performance. It surpasses all other tested open-weight and
proprietary models, including leading systems like o3, Gemini-2.5-Pro,
Claude-3.7-Sonnet, Deepseek-R1, and Qwen-3-235B, even those with API costs two
orders of magnitude higher. This work demonstrates that domain specialization,
when applied to large-scale models, can enable them to outperform generalist
counterparts in specialized knowledge areas like astronomy, thereby advancing
the frontier of AI capabilities in the field.",2025-05-23,"Tijmen de Haan, Yuan-Sen Ting, Tirthankar Ghosal, Tuan Dung Nguyen, Alberto Accomazzi, Emily Herron, Vanessa Lama, Rui Pan, Azton Wells, Nesar Ramachandra",http://arxiv.org/pdf/2505.17592v1,cs.LG
MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity,"In autonomous navigation systems, the solution of the place recognition
problem is crucial for their safe functioning. But this is not a trivial
solution, since it must be accurate regardless of any changes in the scene,
such as seasonal changes and different weather conditions, and it must be
generalizable to other environments. This paper presents our method,
MinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input
data to obtain its spherical coordinates and intensity values normalized within
a range of 0 to 1 for each point, and it produces a robust place recognition
descriptor. To that end, a deep learning approach that combines Minkowski
convolutions and a U-net architecture with skip connections is used. The
results of MinkUNeXt-SI demonstrate that this method reaches and surpasses
state-of-the-art performance while it also generalizes satisfactorily to other
datasets. Additionally, we showcase the capture of a custom dataset and its use
in evaluating our solution, which also achieves outstanding results. Both the
code of our solution and the runs of our dataset are publicly available for
reproducibility purposes.",2025-05-23,"Judith Vilella-Cantos, Juan José Cabrera, Luis Payá, Mónica Ballesta, David Valiente",http://arxiv.org/pdf/2505.17591v1,cs.LG
Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation,"In this paper, we propose a novel framework for ownership verification of
deep neural network (DNN) models for image classification tasks. It allows
verification of model identity by both the rightful owner and third party
without presenting the original model. We assume a gray-box scenario where an
unauthorized user owns a model that is illegally copied from the original
model, provides services in a cloud environment, and the user throws images and
receives the classification results as a probability distribution of output
classes. The framework applies a white-box adversarial attack to align the
output probability of a specific class to a designated value. Due to the
knowledge of original model, it enables the owner to generate such adversarial
examples. We propose a simple but effective adversarial attack method based on
the iterative Fast Gradient Sign Method (FGSM) by introducing control
parameters. Experimental results confirm the effectiveness of the
identification of DNN models using adversarial attack.",2025-05-23,"Teruki Sano, Minoru Kuribayashi, Masao Sakai, Shuji Ishobe, Eisuke Koizumi",http://arxiv.org/pdf/2505.17579v1,cs.LG
Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs,"Solving partial differential equations (PDEs) with machine learning has
recently attracted great attention, as PDEs are fundamental tools for modeling
real-world systems that range from fundamental physical science to advanced
engineering disciplines. Most real-world physical systems across various
disciplines are actually involved in multiple coupled physical fields rather
than a single field. However, previous machine learning studies mainly focused
on solving single-field problems, but overlooked the importance and
characteristics of multiphysics problems in real world. Multiphysics PDEs
typically entail multiple strongly coupled variables, thereby introducing
additional complexity and challenges, such as inter-field coupling. Both
benchmarking and solving multiphysics problems with machine learning remain
largely unexamined. To identify and address the emerging challenges in
multiphysics problems, we mainly made three contributions in this work. First,
we collect the first general multiphysics dataset, the Multiphysics Bench, that
focuses on multiphysics PDE solving with machine learning. Multiphysics Bench
is also the most comprehensive PDE dataset to date, featuring the broadest
range of coupling types, the greatest diversity of PDE formulations, and the
largest dataset scale. Second, we conduct the first systematic investigation on
multiple representative learning-based PDE solvers, such as PINNs, FNO,
DeepONet, and DiffusionPDE solvers, on multiphysics problems. Unfortunately,
naively applying these existing solvers usually show very poor performance for
solving multiphysics. Third, through extensive experiments and discussions, we
report multiple insights and a bag of useful tricks for solving multiphysics
with machine learning, motivating future directions in the study and simulation
of complex, coupled physical systems.",2025-05-23,"Changfan Yang, Lichen Bai, Yinpeng Wang, Shufei Zhang, Zeke Xie",http://arxiv.org/pdf/2505.17575v1,cs.LG
Wildfire spread forecasting with Deep Learning,"Accurate prediction of wildfire spread is crucial for effective risk
management, emergency response, and strategic resource allocation. In this
study, we present a deep learning (DL)-based framework for forecasting the
final extent of burned areas, using data available at the time of ignition. We
leverage a spatio-temporal dataset that covers the Mediterranean region from
2006 to 2022, incorporating remote sensing data, meteorological observations,
vegetation maps, land cover classifications, anthropogenic factors, topography
data, and thermal anomalies. To evaluate the influence of temporal context, we
conduct an ablation study examining how the inclusion of pre- and post-ignition
data affects model performance, benchmarking the temporal-aware DL models
against a baseline trained exclusively on ignition-day inputs. Our results
indicate that multi-day observational data substantially improve predictive
accuracy. Particularly, the best-performing model, incorporating a temporal
window of four days before to five days after ignition, improves both the F1
score and the Intersection over Union by almost 5% in comparison to the
baseline on the test dataset. We publicly release our dataset and models to
enhance research into data-driven approaches for wildfire modeling and
response.",2025-05-23,"Nikolaos Anastasiou, Spyros Kondylatos, Ioannis Papoutsis",http://arxiv.org/pdf/2505.17556v1,cs.LG
CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning,"In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves
specializing functionalities into different experts and sparsely activating
them appropriately, has been widely adopted as a promising approach to
trade-off between model capacity and computation overhead. However, current MoE
variants fall short on heterogeneous datasets, ignoring the fact that experts
may learn similar knowledge, resulting in the underutilization of MoE's
capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE),
a novel method to promote modularization and specialization in MoE, where the
experts are trained along with a contrastive objective by sampling from
activated and inactivated experts in top-k routing. We demonstrate that such a
contrastive objective recovers the mutual-information gap between inputs and
the two types of experts. Experiments on several benchmarks and in multi-task
settings demonstrate that CoMoE can consistently enhance MoE's capacity and
promote modularization among the experts.",2025-05-23,"Jinyuan Feng, Chaopeng Wei, Tenghai Qiu, Tianyi Hu, Zhiqiang Pu",http://arxiv.org/pdf/2505.17553v1,cs.LG
Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing,"De novo peptide sequencing is a critical task in proteomics. However, the
performance of current deep learning-based methods is limited by the inherent
complexity of mass spectrometry data and the heterogeneous distribution of
noise signals, leading to data-specific biases. We present RankNovo, the first
deep reranking framework that enhances de novo peptide sequencing by leveraging
the complementary strengths of multiple sequencing models. RankNovo employs a
list-wise reranking approach, modeling candidate peptides as multiple sequence
alignments and utilizing axial attention to extract informative features across
candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass
Deviation) and RMD (residual Mass Deviation), which offer delicate supervision
by quantifying mass differences between peptides at both the sequence and
residue levels. Extensive experiments demonstrate that RankNovo not only
surpasses its base models used to generate training candidates for reranking
pre-training, but also sets a new state-of-the-art benchmark. Moreover,
RankNovo exhibits strong zero-shot generalization to unseen models whose
generations were not exposed during training, highlighting its robustness and
potential as a universal reranking framework for peptide sequencing. Our work
presents a novel reranking strategy that fundamentally challenges existing
single-model paradigms and advances the frontier of accurate de novo
sequencing. Our source code is provided on GitHub.",2025-05-23,"Zijie Qiu, Jiaqi Wei, Xiang Zhang, Sheng Xu, Kai Zou, Zhi Jin, Zhiqiang Gao, Nanqing Dong, Siqi Sun",http://arxiv.org/pdf/2505.17552v1,cs.LG
Graph Style Transfer for Counterfactual Explainability,"Counterfactual explainability seeks to uncover model decisions by identifying
minimal changes to the input that alter the predicted outcome. This task
becomes particularly challenging for graph data due to preserving structural
integrity and semantic meaning. Unlike prior approaches that rely on forward
perturbation mechanisms, we introduce Graph Inverse Style Transfer (GIST), the
first framework to re-imagine graph counterfactual generation as a backtracking
process, leveraging spectral style transfer. By aligning the global structure
with the original input spectrum and preserving local content faithfulness,
GIST produces valid counterfactuals as interpolations between the input style
and counterfactual content. Tested on 8 binary and multi-class graph
classification benchmarks, GIST achieves a remarkable +7.6% improvement in the
validity of produced counterfactuals and significant gains (+45.5%) in
faithfully explaining the true class distribution. Additionally, GIST's
backtracking mechanism effectively mitigates overshooting the underlying
predictor's decision boundary, minimizing the spectral differences between the
input and the counterfactuals. These results challenge traditional forward
perturbation methods, offering a novel perspective that advances graph
explainability.",2025-05-23,"Bardh Prenkaj, Efstratios Zaradoukas, Gjergji Kasneci",http://arxiv.org/pdf/2505.17542v1,cs.LG
Learning Representational Disparities,"We propose a fair machine learning algorithm to model interpretable
differences between observed and desired human decision-making, with the latter
aimed at reducing disparity in a downstream outcome impacted by the human
decision. Prior work learns fair representations without considering the
outcome in the decision-making process. We model the outcome disparities as
arising due to the different representations of the input seen by the observed
and desired decision-maker, which we term representational disparities. Our
goal is to learn interpretable representational disparities which could
potentially be corrected by specific nudges to the human decision, mitigating
disparities in the downstream outcome; we frame this as a multi-objective
optimization problem using a neural network. Under reasonable simplifying
assumptions, we prove that our neural network model of the representational
disparity learns interpretable weights that fully mitigate the outcome
disparity. We validate objectives and interpret results using real-world German
Credit, Adult, and Heritage Health datasets.",2025-05-23,"Pavan Ravishankar, Rushabh Shah, Daniel B. Neill",http://arxiv.org/pdf/2505.17533v1,cs.LG
TimeCF: A TimeMixer-Based Model with adaptive Convolution and Sharpness-Aware Minimization Frequency Domain Loss for long-term time seris forecasting,"Recent studies have shown that by introducing prior knowledge, multi-scale
analysis of complex and non-stationary time series in real environments can
achieve good results in the field of long-term forecasting. However, affected
by channel-independent methods, models based on multi-scale analysis may
produce suboptimal prediction results due to the autocorrelation between time
series labels, which in turn affects the generalization ability of the model.
To address this challenge, we are inspired by the idea of sharpness-aware
minimization and the recently proposed FreDF method and design a deep learning
model TimeCF for long-term time series forecasting based on the TimeMixer,
combined with our designed adaptive convolution information aggregation module
and Sharpness-Aware Minimization Frequency Domain Loss (SAMFre). Specifically,
TimeCF first decomposes the original time series into sequences of different
scales. Next, the same-sized convolution modules are used to adaptively
aggregate information of different scales on sequences of different scales.
Then, decomposing each sequence into season and trend parts and the two parts
are mixed at different scales through bottom-up and top-down methods
respectively. Finally, different scales are aggregated through a Feed-Forward
Network. What's more, extensive experimental results on different real-world
datasets show that our proposed TimeCF has excellent performance in the field
of long-term forecasting.",2025-05-23,"Bin Wang, Heming Yang, Jinfang Sheng",http://arxiv.org/pdf/2505.17532v1,cs.LG
GPS-Aided Deep Learning for Beam Prediction and Tracking in UAV mmWave Communication,"Millimeter-wave (mmWave) communication enables high data rates for
cellular-connected Unmanned Aerial Vehicles (UAVs). However, a robust beam
management remains challenging due to significant path loss and the dynamic
mobility of UAVs, which can destabilize the UAV-base station (BS) link. This
research presents a GPS-aided deep learning (DL) model that simultaneously
predicts current and future optimal beams for UAV mmWave communications,
maintaining a Top-1 prediction accuracy exceeding 70% and an average power loss
below 0.6 dB across all prediction steps. These outcomes stem from a proposed
data set splitting method ensuring balanced label distribution, paired with a
GPS preprocessing technique that extracts key positional features, and a DL
architecture that maps sequential position data to beam index predictions. The
model reduces overhead by approximately 93% (requiring the training of 2 ~ 3
beams instead of 32 beams) with 95% beam prediction accuracy guarantees, and
ensures 94% to 96% of predictions exhibit mean power loss not exceeding 1 dB.",2025-05-23,"Vendi Ardianto Nugroho, Byung Moo Lee",http://arxiv.org/pdf/2505.17530v1,cs.LG
Transparency and Proportionality in Post-Processing Algorithmic Bias Correction,"Algorithmic decision-making systems sometimes produce errors or skewed
predictions toward a particular group, leading to unfair results. Debiasing
practices, applied at different stages of the development of such systems,
occasionally introduce new forms of unfairness or exacerbate existing
inequalities. We focus on post-processing techniques that modify algorithmic
predictions to achieve fairness in classification tasks, examining the
unintended consequences of these interventions. To address this challenge, we
develop a set of measures that quantify the disparity in the flips applied to
the solution in the post-processing stage. The proposed measures will help
practitioners: (1) assess the proportionality of the debiasing strategy used,
(2) have transparency to explain the effects of the strategy in each group, and
(3) based on those results, analyze the possibility of the use of some other
approaches for bias mitigation or to solve the problem. We introduce a
methodology for applying the proposed metrics during the post-processing stage
and illustrate its practical application through an example. This example
demonstrates how analyzing the proportionality of the debiasing strategy
complements traditional fairness metrics, providing a deeper perspective to
ensure fairer outcomes across all groups.",2025-05-23,"Juliett Suárez Ferreira, Marija Slavkovik, Jorge Casillas",http://arxiv.org/pdf/2505.17525v1,cs.LG
Spacetime Geometry of Denoising in Diffusion Models,"We present a novel perspective on diffusion models using the framework of
information geometry. We show that the set of noisy samples, taken across all
noise levels simultaneously, forms a statistical manifold -- a family of
denoising probability distributions. Interpreting the noise level as a temporal
parameter, we refer to this manifold as spacetime. This manifold naturally
carries a Fisher-Rao metric, which defines geodesics -- shortest paths between
noisy points. Notably, this family of distributions is exponential, enabling
efficient geodesic computation even in high-dimensional settings without
retraining or fine-tuning. We demonstrate the practical value of this geometric
viewpoint in transition path sampling, where spacetime geodesics define smooth
sequences of Boltzmann distributions, enabling the generation of continuous
trajectories between low-energy metastable states. Code is available at:
https://github.com/Aalto-QuML/diffusion-spacetime-geometry.",2025-05-23,"Rafał Karczewski, Markus Heinonen, Alison Pouplin, Søren Hauberg, Vikas Garg",http://arxiv.org/pdf/2505.17517v1,cs.LG
What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection,"Recent advances in text-to-speech technologies have enabled realistic voice
generation, fueling audio-based deepfake attacks such as fraud and
impersonation. While audio anti-spoofing systems are critical for detecting
such threats, prior work has predominantly focused on acoustic-level
perturbations, leaving the impact of linguistic variation largely unexplored.
In this paper, we investigate the linguistic sensitivity of both open-source
and commercial anti-spoofing detectors by introducing transcript-level
adversarial attacks. Our extensive evaluation reveals that even minor
linguistic perturbations can significantly degrade detection accuracy: attack
success rates surpass 60% on several open-source detector-voice pairs, and
notably one commercial detection accuracy drops from 100% on synthetic audio to
just 32%. Through a comprehensive feature attribution analysis, we identify
that both linguistic complexity and model-level audio embedding similarity
contribute strongly to detector vulnerability. We further demonstrate the
real-world risk via a case study replicating the Brad Pitt audio deepfake scam,
using transcript adversarial attacks to completely bypass commercial detectors.
These results highlight the need to move beyond purely acoustic defenses and
account for linguistic variation in the design of robust anti-spoofing systems.
All source code will be publicly available.",2025-05-23,"Binh Nguyen, Shuji Shi, Ryan Ofman, Thai Le",http://arxiv.org/pdf/2505.17513v1,cs.LG
"Multi-agent Systems for Misinformation Lifecycle : Detection, Correction And Source Identification","The rapid proliferation of misinformation in digital media demands solutions
that go beyond isolated Large Language Model(LLM) or AI Agent based detection
methods. This paper introduces a novel multi-agent framework that covers the
complete misinformation lifecycle: classification, detection, correction, and
source verification to deliver more transparent and reliable outcomes. In
contrast to single-agent or monolithic architectures, our approach employs five
specialized agents: an Indexer agent for dynamically maintaining trusted
repositories, a Classifier agent for labeling misinformation types, an
Extractor agent for evidence based retrieval and ranking, a Corrector agent for
generating fact-based correction and a Verification agent for validating
outputs and tracking source credibility. Each agent can be individually
evaluated and optimized, ensuring scalability and adaptability as new types of
misinformation and data sources emerge. By decomposing the misinformation
lifecycle into specialized agents - our framework enhances scalability,
modularity, and explainability. This paper proposes a high-level system
overview, agent design with emphasis on transparency, evidence-based outputs,
and source provenance to support robust misinformation detection and correction
at scale.",2025-05-23,Aditya Gautam,http://arxiv.org/pdf/2505.17511v1,cs.LG
On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning,"Policy gradient algorithms have been successfully applied to enhance the
reasoning capabilities of large language models (LLMs). Despite the widespread
use of Kullback-Leibler (KL) regularization in policy gradient algorithms to
stabilize training, the systematic exploration of how different KL divergence
formulations can be estimated and integrated into surrogate loss functions for
online reinforcement learning (RL) presents a nuanced and systematically
explorable design space. In this paper, we propose regularized policy gradient
(RPG), a systematic framework for deriving and analyzing KL-regularized policy
gradient methods in the online RL setting. We derive policy gradients and
corresponding surrogate loss functions for objectives regularized by both
forward and reverse KL divergences, considering both normalized and
unnormalized policy distributions. Furthermore, we present derivations for
fully differentiable loss functions as well as REINFORCE-style gradient
estimators, accommodating diverse algorithmic needs. We conduct extensive
experiments on RL for LLM reasoning using these methods, showing improved or
competitive results in terms of training stability and performance compared to
strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at
https://github.com/complex-reasoning/RPG.",2025-05-23,"Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Yang Yuan, Quanquan Gu, Andrew C Yao",http://arxiv.org/pdf/2505.17508v1,cs.LG
Offline Constrained Reinforcement Learning under Partial Data Coverage,"We study offline constrained reinforcement learning (RL) with general
function approximation. We aim to learn a policy from a pre-collected dataset
that maximizes the expected discounted cumulative reward for a primary reward
signal while ensuring that expected discounted returns for multiple auxiliary
reward signals are above predefined thresholds. Existing algorithms either
require fully exploratory data, are computationally inefficient, or depend on
an additional auxiliary function classes to obtain an $\epsilon$-optimal policy
with sample complexity $O(\epsilon^{-2})$. In this paper, we propose an
oracle-efficient primal-dual algorithm based on a linear programming (LP)
formulation, achieving $O(\epsilon^{-2})$ sample complexity under partial data
coverage. By introducing a realizability assumption, our approach ensures that
all saddle points of the Lagrangian are optimal, removing the need for
regularization that complicated prior analyses. Through Lagrangian
decomposition, our method extracts policies without requiring knowledge of the
data-generating distribution, enhancing practical applicability.",2025-05-23,"Kihyuk Hong, Ambuj Tewari",http://arxiv.org/pdf/2505.17506v1,cs.LG
RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition,"Multimodal emotion recognition analyzes emotions by combining data from
multiple sources. However, real-world noise or sensor failures often cause
missing or corrupted data, creating the Incomplete Multimodal Emotion
Recognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion
Recovery (RoHyDR), a novel framework that performs missing-modality recovery at
unimodal, multimodal, feature, and semantic levels. For unimodal representation
recovery of missing modalities, RoHyDR exploits a diffusion-based generator to
generate distribution-consistent and semantically aligned representations from
Gaussian noise, using available modalities as conditioning. For multimodal
fusion recovery, we introduce adversarial learning to produce a realistic fused
multimodal representation and recover missing semantic content. We further
propose a multi-stage optimization strategy that enhances training stability
and efficiency. In contrast to previous work, the hybrid diffusion and
adversarial learning-based recovery mechanism in RoHyDR allows recovery of
missing information in both unimodal representation and multimodal fusion, at
both feature and semantic levels, effectively mitigating performance
degradation caused by suboptimal optimization. Comprehensive experiments
conducted on two widely used multimodal emotion recognition benchmarks
demonstrate that our proposed method outperforms state-of-the-art IMER methods,
achieving robust recognition performance under various missing-modality
scenarios. Our code will be made publicly available upon acceptance.",2025-05-23,"Yuehan Jin, Xiaoqing Liu, Yiyuan Yang, Zhiwen Yu, Tong Zhang, Kaixiang Yang",http://arxiv.org/pdf/2505.17501v1,cs.LG
Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models,"End-to-end training of Spoken Language Models (SLMs) commonly involves
adapting pre-trained text-based Large Language Models (LLMs) to the speech
modality through multi-stage training on diverse tasks such as ASR, TTS and
spoken question answering (SQA). Although this multi-stage continual learning
equips LLMs with both speech understanding and generation capabilities, the
substantial differences in task and data distributions across stages can lead
to catastrophic forgetting, where previously acquired knowledge is lost. This
paper investigates catastrophic forgetting and evaluates three mitigation
strategies-model merging, discounting the LoRA scaling factor, and experience
replay to balance knowledge retention with new learning. Results show that
experience replay is the most effective, with further gains achieved by
combining it with other methods. These findings provide insights for developing
more robust and efficient SLM training pipelines.",2025-05-23,"Chi-Yuan Hsiao, Ke-Han Lu, Kai-Wei Chang, Chih-Kai Yang, Wei-Chih Chen, Hung-yi Lee",http://arxiv.org/pdf/2505.17496v1,cs.LG
ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs,"Large Language Models (LLMs) have achieved remarkable performance by
capturing complex interactions between input features. To identify these
interactions, most existing approaches require enumerating all possible
combinations of features up to a given order, causing them to scale poorly with
the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an
information-theoretic approach that uses interaction sparsity to scale to $n
\approx 10^3$ features. SPEX greatly improves upon prior methods but requires
tens of thousands of model inferences, which can be prohibitive for large
models. In this paper, we observe that LLM feature interactions are often
hierarchical -- higher-order interactions are accompanied by their lower-order
subsets -- which enables more efficient discovery. To exploit this hierarchy,
we propose ProxySPEX, an interaction attribution algorithm that first fits
gradient boosted trees to masked LLM outputs and then extracts the important
interactions. Experiments across four challenging high-dimensional datasets
show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over
marginal attribution approaches while using $10\times$ fewer inferences than
SPEX. By accounting for interactions, ProxySPEX identifies features that
influence model output over 20% more than those selected by marginal
approaches. Further, we apply ProxySPEX to two interpretability tasks. Data
attribution, where we identify interactions among CIFAR-10 training samples
that influence test predictions, and mechanistic interpretability, where we
uncover interactions between attention heads, both within and across layers, on
a question-answering task. ProxySPEX identifies interactions that enable more
aggressive pruning of heads than marginal approaches.",2025-05-23,"Landon Butler, Abhineet Agarwal, Justin Singh Kang, Yigit Efe Erginbas, Bin Yu, Kannan Ramchandran",http://arxiv.org/pdf/2505.17495v1,cs.LG
PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate,"Project duplication detection is critical for project quality assessment, as
it improves resource utilization efficiency by preventing investing in newly
proposed project that have already been studied. It requires the ability to
understand high-level semantics and generate constructive and valuable
feedback. Existing detection methods rely on basic word- or sentence-level
comparison or solely apply large language models, lacking valuable insights for
experts and in-depth comprehension of project content and review criteria. To
tackle this issue, we propose PD$^3$, a Project Duplication Detection framework
via adapted multi-agent Debate. Inspired by real-world expert debates, it
employs a fair competition format to guide multi-agent debate to retrieve
relevant projects. For feedback, it incorporates both qualitative and
quantitative analysis to improve its practicality. Over 800 real-world power
project data spanning more than 20 specialized fields are used to evaluate the
framework, demonstrating that our method outperforms existing approaches by
7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online
platform, Review Dingdang, to assist power experts, saving 5.73 million USD in
initial detection on more than 100 newly proposed projects.",2025-05-23,"Dezheng Bao, Yueci Yang, Xin Chen, Zhengxuan Jiang, Zeguo Fei, Daoze Zhang, Xuanwen Huang, Junru Chen, Chutian Yu, Xiang Yuan, Yang Yang",http://arxiv.org/pdf/2505.17492v1,cs.LG
ExARNN: An Environment-Driven Adaptive RNN for Learning Non-Stationary Power Dynamics,"Non-stationary power system dynamics, influenced by renewable energy
variability, evolving demand patterns, and climate change, are becoming
increasingly complex. Accurately capturing these dynamics requires a model
capable of adapting to environmental factors. Traditional models, including
Recurrent Neural Networks (RNNs), lack efficient mechanisms to encode external
factors, such as time or environmental data, for dynamic adaptation. To address
this, we propose the External Adaptive RNN (ExARNN), a novel framework that
integrates external data (e.g., weather, time) to continuously adjust the
parameters of a base RNN. ExARNN achieves this through a hierarchical
hypernetwork design, using Neural Controlled Differential Equations (NCDE) to
process external data and generate RNN parameters adaptively. This approach
enables ExARNN to handle inconsistent timestamps between power and external
measurements, ensuring continuous adaptation. Extensive forecasting tests
demonstrate ExARNN's superiority over established baseline models.",2025-05-23,"Haoran Li, Muhao Guo, Yang Weng, Marija Ilic, Guangchun Ruan",http://arxiv.org/pdf/2505.17488v1,cs.LG
"Hyperspectral in situ remote sensing of water surface nitrate in the Fitzroy River estuary, Queensland, Australia, using deep learning","Nitrate ($\text{NO}_3^-$) is a form of dissolved inorganic nitrogen derived
primarily from anthropogenic sources. The recent increase in river-discharged
nitrate poses a major risk for coral bleaching in the Great Barrier Reef (GBR)
lagoon. Although nitrate is an optically inactive (i.e., colourless)
constituent, previous studies have demonstrated there is an indirect,
non-causal relationship between water surface nitrate and water-leaving
reflectance that is mediated through optically active water quality parameters
such as total suspended solids and coloured dissolved organic matter. This work
aims to advance our understanding of this relationship with an effort to
measure time-series nitrate and simultaneous hyperspectral reflectance at the
Fitzroy River estuary, Queensland, Australia. Time-series observations revealed
periodic cycles in nitrate loads due to the tidal influence in the estuarine
study site. The water surface nitrate loads were predicted from hyperspectral
reflectance and water salinity measurements, with hyperspectral reflectance
indicating the concentrations of optically active variables and salinity
indicating the mixing of river water and seawater proportions. The accuracy
assessment of model-predicted nitrate against in-situ measured nitrate values
showed that the predicted nitrate values correlated well with the ground-truth
data, with an $R^2$ score of 0.86, and an RMSE of 0.03 mg/L. This work
demonstrates the feasibility of predicting water surface nitrate from
hyperspectral reflectance and salinity measurements.",2025-05-23,"Yiqing Guo, Nagur Cherukuru, Eric Lehmann, S. L. Kesav Unnithan, Gemma Kerrisk, Tim Malthus, Faisal Islam",http://arxiv.org/pdf/2505.17483v1,cs.LG
Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression,"Understanding protein dynamics is critical for elucidating their biological
functions. The increasing availability of molecular dynamics (MD) data enables
the training of deep generative models to efficiently explore the
conformational space of proteins. However, existing approaches either fail to
explicitly capture the temporal dependencies between conformations or do not
support direct generation of time-independent samples. To address these
limitations, we introduce ConfRover, an autoregressive model that
simultaneously learns protein conformation and dynamics from MD trajectories,
supporting both time-dependent and time-independent sampling. At the core of
our model is a modular architecture comprising: (i) an encoding layer, adapted
from protein folding models, that embeds protein-specific information and
conformation at each time frame into a latent space; (ii) a temporal module, a
sequence model that captures conformational dynamics across frames; and (iii)
an SE(3) diffusion model as the structure decoder, generating conformations in
continuous space. Experiments on ATLAS, a large-scale protein MD dataset of
diverse structures, demonstrate the effectiveness of our model in learning
conformational dynamics and supporting a wide range of downstream tasks.
ConfRover is the first model to sample both protein conformations and
trajectories within a single framework, offering a novel and flexible approach
for learning from protein MD data.",2025-05-23,"Yuning Shen, Lihao Wang, Huizhuo Yuan, Yan Wang, Bangji Yang, Quanquan Gu",http://arxiv.org/pdf/2505.17478v1,cs.LG
Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance,"This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural
network backtracking architecture designed to enhance Alzheimer's Disease (AD)
diagnosis through speech analysis. Leveraging the power of pre-trained large
language models, RSF identifies and utilizes the most probable AD-specific
speech markers, addressing both the scarcity of real AD speech samples and the
challenge of limited interpretability in existing models. RSF's unique approach
consists of three core innovations: Firstly, it exploits the observation that
speech markers most probable of predicting AD, defined as the most probable
speech-markers (MPMs), must have the highest probability of activating those
neurons (in the neural network) with the highest probability of predicting AD,
defined as the most probable neurons (MPNs). Secondly, it utilizes a speech
token representation at the input layer, allowing backtracking from MPNs to
identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an
innovative backtracking method to track backwards from the MPNs to the input
layer, identifying the MPTs and the corresponding MPMs, and ingeniously
uncovering novel speech markers for AD detection. Experimental results
demonstrate RSF's superiority over traditional methods such as SHAP and
Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost
in F1-score. By generating speech data that encapsulates novel markers, RSF not
only mitigates the limitations of real data scarcity but also significantly
enhances the robustness and accuracy of AD diagnostic models. These findings
underscore RSF's potential as a transformative tool in speech-based AD
detection, offering new insights into AD-related linguistic deficits and paving
the way for more effective non-invasive early intervention strategies.",2025-05-23,"Victor OK Li, Yang Han, Jacqueline CK Lam, Lawrence YL Cheung",http://arxiv.org/pdf/2505.17477v1,cs.LG
Efficient compression of neural networks and datasets,"We compare, improve, and contribute methods that substantially decrease the
number of parameters of neural networks while maintaining high test accuracy.
When applying our methods to minimize description length, we obtain very
effective data compression algorithms. In particular, we develop a
probabilistic reformulation of $\ell_0$ regularized optimization for nonlinear
models that does not require Monte-Carlo sampling and thus improves upon
previous methods. We also improve upon methods involving smooth approximations
to the $\ell_0$ norm, and investigate layerwise methods. We compare the methods
on different architectures and datasets, including convolutional networks
trained on image datasets and transformers trained on parts of Wikipedia. We
also created a synthetic teacher-student setup to investigate compression in a
controlled continuous setting. Finally, we conceptually relate compression
algorithms to Solomonoff's theory of inductive inference and empirically verify
the prediction that regularized models can exhibit more sample-efficient
convergence.",2025-05-23,"Lukas Silvester Barth, Paulo von Petersenn",http://arxiv.org/pdf/2505.17469v1,cs.LG
Efficient Adaptive Experimentation with Non-Compliance,"We study the problem of estimating the average treatment effect (ATE) in
adaptive experiments where treatment can only be encouraged--rather than
directly assigned--via a binary instrumental variable. Building on
semiparametric efficiency theory, we derive the efficiency bound for ATE
estimation under arbitrary, history-dependent instrument-assignment policies,
and show it is minimized by a variance-aware allocation rule that balances
outcome noise and compliance variability. Leveraging this insight, we introduce
AMRIV--an \textbf{A}daptive, \textbf{M}ultiply-\textbf{R}obust estimator for
\textbf{I}nstrumental-\textbf{V}ariable settings with variance-optimal
assignment. AMRIV pairs (i) an online policy that adaptively approximates the
optimal allocation with (ii) a sequential, influence-function-based estimator
that attains the semiparametric efficiency bound while retaining
multiply-robust consistency. We establish asymptotic normality, explicit
convergence rates, and anytime-valid asymptotic confidence sequences that
enable sequential inference. Finally, we demonstrate the practical
effectiveness of our approach through empirical studies, showing that adaptive
instrument assignment, when combined with the AMRIV estimator, yields improved
efficiency and robustness compared to existing baselines.",2025-05-23,"Miruna Oprescu, Brian M Cho, Nathan Kallus",http://arxiv.org/pdf/2505.17468v1,cs.LG
Towards Heterogeneous Continual Graph Learning via Meta-knowledge Distillation,"Machine learning on heterogeneous graphs has experienced rapid advancement in
recent years, driven by the inherently heterogeneous nature of real-world data.
However, existing studies typically assume the graphs to be static, while
real-world graphs are continuously expanding. This dynamic nature requires
models to adapt to new data while preserving existing knowledge. To this end,
this work addresses the challenge of continual learning on heterogeneous graphs
by introducing the Meta-learning based Knowledge Distillation framework (MKD),
designed to mitigate catastrophic forgetting in evolving heterogeneous graph
structures. MKD combines rapid task adaptation through meta-learning on limited
samples with knowledge distillation to achieve an optimal balance between
incorporating new information and maintaining existing knowledge. To improve
the efficiency and effectiveness of sample selection, MKD incorporates a novel
sampling strategy that selects a small number of target-type nodes based on
node diversity and maintains fixed-size buffers for other types. The strategy
retrieves first-order neighbors along metapaths and selects important neighbors
based on their structural relevance, enabling the sampled subgraphs to retain
key topological and semantic information. In addition, MKD introduces a
semantic-level distillation module that aligns the attention distributions over
different metapaths between teacher and student models, encouraging semantic
consistency beyond the logit level. Comprehensive evaluations across three
benchmark datasets validate MKD's effectiveness in handling continual learning
scenarios on expanding heterogeneous graphs.",2025-05-23,"Guiquan Sun, Xikun Zhang, Jingchao Ni, Dongjin Song",http://arxiv.org/pdf/2505.17458v1,cs.LG
Self-Training Large Language Models with Confident Reasoning,"Large language models (LLMs) have shown impressive performance by generating
reasoning paths before final answers, but learning such a reasoning path
requires costly human supervision. To address this issue, recent studies have
explored self-training methods that improve reasoning capabilities using
pseudo-labels generated by the LLMs themselves. Among these, confidence-based
self-training fine-tunes LLMs to prefer reasoning paths with high-confidence
answers, where confidence is estimated via majority voting. However, such
methods exclusively focus on the quality of the final answer and may ignore the
quality of the reasoning paths, as even an incorrect reasoning path leads to a
correct answer by chance. Instead, we advocate the use of reasoning-level
confidence to identify high-quality reasoning paths for self-training,
supported by our empirical observations. We then propose a new self-training
method, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths
through Policy Optimization. Our experiments show that CORE-PO improves the
accuracy of outputs on four in-distribution and two out-of-distribution
benchmarks, compared to existing self-training methods.",2025-05-23,"Hyosoon Jang, Yunhui Jang, Sungjae Lee, Jungseul Ok, Sungsoo Ahn",http://arxiv.org/pdf/2505.17454v1,cs.LG
CLIMB: Class-imbalanced Learning Benchmark on Tabular Data,"Class-imbalanced learning (CIL) on tabular data is important in many
real-world applications where the minority class holds the critical but rare
outcomes. In this paper, we present CLIMB, a comprehensive benchmark for
class-imbalanced learning on tabular data. CLIMB includes 73 real-world
datasets across diverse domains and imbalance levels, along with unified
implementations of 29 representative CIL algorithms. Built on a high-quality
open-source Python package with unified API designs, detailed documentation,
and rigorous code quality controls, CLIMB supports easy implementation and
comparison between different CIL algorithms. Through extensive experiments, we
provide practical insights on method accuracy and efficiency, highlighting the
limitations of naive rebalancing, the effectiveness of ensembles, and the
importance of data quality. Our code, documentation, and examples are available
at https://github.com/ZhiningLiu1998/imbalanced-ensemble.",2025-05-23,"Zhining Liu, Zihao Li, Ze Yang, Tianxin Wei, Jian Kang, Yada Zhu, Hendrik Hamann, Jingrui He, Hanghang Tong",http://arxiv.org/pdf/2505.17451v1,cs.LG
Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning,"Following the rising popularity of YouTube, there is an emerging problem on
this platform called clickbait, which provokes users to click on videos using
attractive titles and thumbnails. As a result, users ended up watching a video
that does not have the content as publicized in the title. This issue is
addressed in this study by proposing an algorithm called BaitRadar, which uses
a deep learning technique where six inference models are jointly consulted to
make the final classification decision. These models focus on different
attributes of the video, including title, comments, thumbnail, tags, video
statistics and audio transcript. The final classification is attained by
computing the average of multiple models to provide a robust and accurate
output even in situation where there is missing data. The proposed method is
tested on 1,400 YouTube videos. On average, a test accuracy of 98% is achieved
with an inference time of less than 2s.",2025-05-23,"Bhanuka Gamage, Adnan Labib, Aisha Joomun, Chern Hong Lim, KokSheik Wong",http://arxiv.org/pdf/2505.17448v1,cs.LG
Corporate Needs You to Find the Difference: Revisiting Submodular and Supermodular Ratio Optimization Problems,"We study the problem of minimizing or maximizing the average value $ f(S)/|S|
$ of a submodular or supermodular set function $ f: 2^V \to \mathbb{R} $ over
non-empty subsets $ S \subseteq V $. This generalizes classical problems such
as Densest Subgraph (DSG), Densest Supermodular Set (DSS), and Submodular
Function Minimization (SFM). Motivated by recent applications, we introduce two
broad formulations: Unrestricted Sparsest Submodular Set (USSS) and
Unrestricted Densest Supermodular Set (UDSS), which allow for negative and
non-monotone functions.
  We show that DSS, SFM, USSS, UDSS, and the Minimum Norm Point (MNP) problem
are equivalent under strongly polynomial-time reductions, enabling algorithmic
crossover. In particular, viewing these through the lens of the MNP in the base
polyhedron, we connect Fujishige's theory with dense decomposition, and show
that both Fujishige-Wolfe's algorithm and the heuristic \textsc{SuperGreedy++}
act as universal solvers for all these problems, including sub-modular function
minimization.
  Theoretically, we explain why \textsc{SuperGreedy++} is effective beyond DSS,
including for tasks like submodular minimization and minimum $ s $-$ t $ cut.
Empirically, we test several solvers, including the Fujishige-Wolfe algorithm
on over 400 experiments across seven problem types and large-scale
real/synthetic datasets. Surprisingly, general-purpose convex and flow-based
methods outperform task-specific baselines, demonstrating that with the right
framing, general optimization techniques can be both scalable and
state-of-the-art for submodular and supermodular ratio problems.",2025-05-23,"Elfarouk Harb, Yousef Yassin, Chandra Chekuri",http://arxiv.org/pdf/2505.17443v1,cs.LG
Discovering Forbidden Topics in Language Models,"Refusal discovery is the task of identifying the full set of topics that a
language model refuses to discuss. We introduce this new problem setting and
develop a refusal discovery method, LLM-crawler, that uses token prefilling to
find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an
open-source model with public safety tuning data. Our crawler manages to
retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale
the crawl to a frontier model using the prefilling option of Claude-Haiku.
Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two
of its variants finetuned for reasoning: DeepSeek-R1-70B and
Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with
censorship tuning: The model exhibits ""thought suppression"" behavior that
indicates memorization of CCP-aligned responses. Although
Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned
refusals answers in the quantized model. Our findings highlight the critical
need for refusal discovery methods to detect biases, boundaries, and alignment
failures of AI systems.",2025-05-23,"Can Rager, Chris Wendler, Rohit Gandikota, David Bau",http://arxiv.org/pdf/2505.17441v2,cs.LG
Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning,"This study designs an efficient and equitable humanitarian supply chain
dynamically by using reinforcement learning, PPO, and compared with heuristic
algorithms. This study demonstrates the model of PPO always treats average
satisfaction rate as the priority.",2025-05-23,Weijia Jin,http://arxiv.org/pdf/2505.17439v1,cs.LG
Discretization-free Multicalibration through Loss Minimization over Tree Ensembles,"In recent years, multicalibration has emerged as a desirable learning
objective for ensuring that a predictor is calibrated across a rich collection
of overlapping subpopulations. Existing approaches typically achieve
multicalibration by discretizing the predictor's output space and iteratively
adjusting its output values. However, this discretization approach departs from
the standard empirical risk minimization (ERM) pipeline, introduces rounding
error and additional sensitive hyperparameter, and may distort the predictor's
outputs in ways that hinder downstream decision-making.
  In this work, we propose a discretization-free multicalibration method that
directly optimizes an empirical risk objective over an ensemble of depth-two
decision trees. Our ERM approach can be implemented using off-the-shelf tree
ensemble learning methods such as LightGBM. Our algorithm provably achieves
multicalibration, provided that the data distribution satisfies a technical
condition we term as loss saturation. Across multiple datasets, our empirical
evaluation shows that this condition is always met in practice. Our
discretization-free algorithm consistently matches or outperforms existing
multicalibration approaches--even when evaluated using a discretization-based
multicalibration metric that shares its discretization granularity with the
baselines.",2025-05-23,"Hongyi Henry Jin, Zijun Ding, Dung Daniel Ngo, Zhiwei Steven Wu",http://arxiv.org/pdf/2505.17435v1,cs.LG
HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting,"Irregular multivariate time series (IMTS) are characterized by irregular time
intervals within variables and unaligned observations across variables, posing
challenges in learning temporal and variable dependencies. Many existing IMTS
models either require padded samples to learn separately from temporal and
variable dimensions, or represent original samples via bipartite graphs or
sets. However, the former approaches often need to handle extra padding values
affecting efficiency and disrupting original sampling patterns, while the
latter ones have limitations in capturing dependencies among unaligned
observations. To represent and learn both dependencies from original
observations in a unified form, we propose HyperIMTS, a Hypergraph neural
network for Irregular Multivariate Time Series forecasting. Observed values are
converted as nodes in the hypergraph, interconnected by temporal and variable
hyperedges to enable message passing among all observations. Through
irregularity-aware message passing, HyperIMTS captures variable dependencies in
a time-adaptive way to achieve accurate forecasting. Experiments demonstrate
HyperIMTS's competitive performance among state-of-the-art models in IMTS
forecasting with low computational cost.",2025-05-23,"Boyuan Li, Yicheng Luo, Zhen Liu, Junhao Zheng, Jianming Lv, Qianli Ma",http://arxiv.org/pdf/2505.17431v1,cs.LG
DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies,"Large language models (LLMs) have achieved remarkable performance across a
wide range of NLP tasks. However, their substantial inference cost poses a
major barrier to real-world deployment, especially in latency-sensitive
scenarios. To address this challenge, we propose \textbf{DASH}, an adaptive
layer-skipping framework that dynamically selects computation paths conditioned
on input characteristics. We model the skipping process as a Markov Decision
Process (MDP), enabling fine-grained token-level decisions based on
intermediate representations. To mitigate potential performance degradation
caused by skipping, we introduce a lightweight compensation mechanism that
injects differential rewards into the decision process. Furthermore, we design
an asynchronous execution strategy that overlaps layer computation with policy
evaluation to minimize runtime overhead. Experiments on multiple LLM
architectures and NLP benchmarks show that our method achieves significant
inference acceleration while maintaining competitive task performance,
outperforming existing methods.",2025-05-23,"Ning Yang, Fangxin Liu, Junjie Wang, Tao Yang, Kan Liu, Haibing Guan, Li Jiang",http://arxiv.org/pdf/2505.17420v1,cs.LG
Wasserstein Transfer Learning,"Transfer learning is a powerful paradigm for leveraging knowledge from source
domains to enhance learning in a target domain. However, traditional transfer
learning approaches often focus on scalar or multivariate data within Euclidean
spaces, limiting their applicability to complex data structures such as
probability distributions. To address this, we introduce a novel framework for
transfer learning in regression models, where outputs are probability
distributions residing in the Wasserstein space. When the informative subset of
transferable source domains is known, we propose an estimator with provable
asymptotic convergence rates, quantifying the impact of domain similarity on
transfer efficiency. For cases where the informative subset is unknown, we
develop a data-driven transfer learning procedure designed to mitigate negative
transfer. The proposed methods are supported by rigorous theoretical analysis
and are validated through extensive simulations and real-world applications.",2025-05-23,"Kaicheng Zhang, Sinian Zhang, Doudou Zhou, Yidong Zhou",http://arxiv.org/pdf/2505.17404v1,cs.LG
Spectral Mixture Kernels for Bayesian Optimization,"Bayesian Optimization (BO) is a widely used approach for solving expensive
black-box optimization tasks. However, selecting an appropriate probabilistic
surrogate model remains an important yet challenging problem. In this work, we
introduce a novel Gaussian Process (GP)-based BO method that incorporates
spectral mixture kernels, derived from spectral densities formed by
scale-location mixtures of Cauchy and Gaussian distributions. This method
achieves a significant improvement in both efficiency and optimization
performance, matching the computational speed of simpler kernels while
delivering results that outperform more complex models and automatic BO
methods. We provide bounds on the information gain and cumulative regret
associated with obtaining the optimum. Extensive numerical experiments
demonstrate that our method consistently outperforms existing baselines across
a diverse range of synthetic and real-world problems, including both low- and
high-dimensional settings.",2025-05-23,"Yi Zhang, Cheng Hua",http://arxiv.org/pdf/2505.17393v1,cs.LG
Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling,"Discrete diffusion models have recently shown great promise for modeling
complex discrete data, with masked diffusion models (MDMs) offering a
compelling trade-off between quality and generation speed. MDMs denoise by
progressively unmasking multiple dimensions from an all-masked input, but their
performance can degrade when using few denoising steps due to limited modeling
of inter-dimensional dependencies. In this paper, we propose Variational
Autoencoding Discrete Diffusion (VADD), a novel framework that enhances
discrete diffusion with latent variable modeling to implicitly capture
correlations among dimensions. By introducing an auxiliary recognition model,
VADD enables stable training via variational lower bounds maximization and
amortized inference over the training set. Our approach retains the efficiency
of traditional MDMs while significantly improving sample quality, especially
when the number of denoising steps is small. Empirical results on 2D toy data,
pixel-level image generation, and text generation demonstrate that VADD
consistently outperforms MDM baselines.",2025-05-23,"Tianyu Xie, Shuchen Xue, Zijin Feng, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Cheng Zhang",http://arxiv.org/pdf/2505.17384v1,cs.LG
Programmable Photonic Unitary Processor Enables Parametrized Differentiable Long-Haul Spatial Division Multiplexed Transmission,"The explosive growth of global data traffic demands scalable and
energy-efficient optical communication systems. Spatial division multiplexing
(SDM) using multicore or multimode fibers is a promising solution to overcome
the capacity limit of single-mode fibers. However, long-haul SDM transmission
faces significant challenges due to modal dispersion, which imposes heavy
computational loads on digital signal processing (DSP) for signal equalization.
Here, we propose parameterized SDM transmission, where programmable photonic
unitary processors are installed at intermediate nodes. Instead of relying on
conventional digital equalization only on the receiver side, our approach
enables direct optimization of the SDM transmission channel itself by the
programmable unitary processor, which reduces digital post-processing loads. We
introduce a gradient-based optimization algorithm using a differentiable SDM
transmission model to determine the optimal unitary transformation. As a key
enabler, we first implemented telecom-grade programmable photonic unitary
processor, achieving a low-loss (2.1 dB fiber-to-fiber), wideband (full
C-band), polarization-independent, and high-fidelity (R2>96% across the C-band)
operation. We experimentally demonstrate 1300-km transmission using a
three-mode fiber, achieving strong agreement between simulation and experiment.
The optimized photonic processor significantly reduces modal dispersion and
post-processing complexity. Our results establish a scalable framework for
integrating photonic computation into the optical layer, enabling more
efficient, high-capacity optical networks.",2025-05-23,"Mitsumasa Nakajima, Kohki Shibahara, Kohei Ikeda, Akira Kawai, Masaya Notomi, Yutaka Miyamoto, Toshikazu Hashimoto",http://arxiv.org/pdf/2505.17381v1,cs.LG
Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition,"We investigate the problem of identifying the optimal scoring rule within the
principal-agent framework for online information acquisition problem. We focus
on the principal's perspective, seeking to determine the desired scoring rule
through interactions with the agent. To address this challenge, we propose two
algorithms: OIAFC and OIAFB, tailored for fixed confidence and fixed budget
settings, respectively. Our theoretical analysis demonstrates that OIAFC can
extract the desired $(\epsilon, \delta)$-scoring rule with a efficient
instance-dependent sample complexity or an instance-independent sample
complexity. Our analysis also shows that OIAFB matches the instance-independent
performance bound of OIAFC, while both algorithms share the same complexity
across fixed confidence and fixed budget settings.",2025-05-23,"Zichen Wang, Chuanhao Li, Huazheng Wang",http://arxiv.org/pdf/2505.17379v1,cs.LG
Value-Guided Search for Efficient Chain-of-Thought Reasoning,"In this paper, we propose a simple and efficient method for value model
training on long-context reasoning traces. Compared to existing process reward
models (PRMs), our method does not require a fine-grained notion of ""step,""
which is difficult to define for long-context reasoning models. By collecting a
dataset of 2.5 million reasoning traces, we train a 1.5B token-level value
model and apply it to DeepSeek models for improved performance with test-time
compute scaling. We find that block-wise value-guided search (VGS) with a final
weighted majority vote achieves better test-time scaling than standard methods
such as majority voting or best-of-n. With an inference budget of 64
generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of
45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024
& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly
reduces the inference FLOPs required to achieve the same performance of
majority voting. Our dataset, model and codebase are open-sourced.",2025-05-23,"Kaiwen Wang, Jin Peng Zhou, Jonathan Chang, Zhaolin Gao, Nathan Kallus, Kianté Brantley, Wen Sun",http://arxiv.org/pdf/2505.17373v1,cs.LG
An End-to-End Approach for Child Reading Assessment in the Xhosa Language,"Child literacy is a strong predictor of life outcomes at the subsequent
stages of an individual's life. This points to a need for targeted
interventions in vulnerable low and middle income populations to help bridge
the gap between literacy levels in these regions and high income ones. In this
effort, reading assessments provide an important tool to measure the
effectiveness of these programs and AI can be a reliable and economical tool to
support educators with this task. Developing accurate automatic reading
assessment systems for child speech in low-resource languages poses significant
challenges due to limited data and the unique acoustic properties of children's
voices. This study focuses on Xhosa, a language spoken in South Africa, to
advance child speech recognition capabilities. We present a novel dataset
composed of child speech samples in Xhosa. The dataset is available upon
request and contains ten words and letters, which are part of the Early Grade
Reading Assessment (EGRA) system. Each recording is labeled with an online and
cost-effective approach by multiple markers and a subsample is validated by an
independent EGRA reviewer. This dataset is evaluated with three fine-tuned
state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The
results indicate that the performance of these models can be significantly
influenced by the amount and balancing of the available training data, which is
fundamental for cost-effective large dataset collection. Furthermore, our
experiments indicate that the wav2vec 2.0 performance is improved by training
on multiple classes at a time, even when the number of available samples is
constrained.",2025-05-23,"Sergio Chevtchenko, Nikhil Navas, Rafaella Vale, Franco Ubaudi, Sipumelele Lucwaba, Cally Ardington, Soheil Afshar, Mark Antoniou, Saeed Afshar",http://arxiv.org/pdf/2505.17371v1,cs.LG
FRIREN: Beyond Trajectories -- A Spectral Lens on Time,"Long-term time-series forecasting (LTSF) models are often presented as
general-purpose solutions that can be applied across domains, implicitly
assuming that all data is pointwise predictable. Using chaotic systems such as
Lorenz-63 as a case study, we argue that geometric structure - not pointwise
prediction - is the right abstraction for a dynamic-agnostic foundational
model. Minimizing the Wasserstein-2 distance (W2), which captures geometric
changes, and providing a spectral view of dynamics are essential for
long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via
Interpretable Eigen-networks), implements an augmented normalizing-flow block
that embeds data into a normally distributed latent representation. It then
generates a W2-efficient optimal path that can be decomposed into rotation,
scaling, inverse rotation, and translation. This architecture yields locally
generated, geometry-preserving predictions that are independent of the
underlying dynamics, and a global spectral representation that functions as a
finite Koopman operator with a small modification. This enables practitioners
to identify which modes grow, decay, or oscillate, both locally and
system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on
Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE
27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out
of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out),
FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170,
outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065.
FRIREN is also competitive on standard LTSF datasets such as ETT and Weather.
By connecting modern generative flows with classical spectral analysis, FRIREN
makes long-term forecasting both accurate and interpretable, setting a new
benchmark for LTSF model design.",2025-05-23,Qilin Wang,http://arxiv.org/pdf/2505.17370v1,cs.LG
Improved and Oracle-Efficient Online $\ell_1$-Multicalibration,"We study \emph{online multicalibration}, a framework for ensuring calibrated
predictions across multiple groups in adversarial settings, across $T$ rounds.
Although online calibration is typically studied in the $\ell_1$ norm, prior
approaches to online multicalibration have taken the indirect approach of
obtaining rates in other norms (such as $\ell_2$ and $\ell_{\infty}$) and then
transferred these guarantees to $\ell_1$ at additional loss. In contrast, we
propose a direct method that achieves improved and oracle-efficient rates of
$\widetilde{\mathcal{O}}(T^{-1/3})$ and $\widetilde{\mathcal{O}}(T^{-1/4})$
respectively, for online $\ell_1$-multicalibration. Our key insight is a novel
reduction of online \(\ell_1\)-multicalibration to an online learning problem
with product-based rewards, which we refer to as \emph{online linear-product
optimization} ($\mathtt{OLPO}$).
  To obtain the improved rate of $\widetilde{\mathcal{O}}(T^{-1/3})$, we
introduce a linearization of $\mathtt{OLPO}$ and design a no-regret algorithm
for this linearized problem. Although this method guarantees the desired
sublinear rate (nearly matching the best rate for online calibration), it
becomes computationally expensive when the group family \(\mathcal{H}\) is
large or infinite, since it enumerates all possible groups. To address
scalability, we propose a second approach to $\mathtt{OLPO}$ that makes only a
polynomial number of calls to an offline optimization (\emph{multicalibration
evaluation}) oracle, resulting in \emph{oracle-efficient} online
\(\ell_1\)-multicalibration with a rate of $\widetilde{\mathcal{O}}(T^{-1/4})$.
Our framework also extends to certain infinite families of groups (e.g., all
linear functions on the context space) by exploiting a $1$-Lipschitz property
of the \(\ell_1\)-multicalibration error with respect to \(\mathcal{H}\).",2025-05-23,"Rohan Ghuge, Vidya Muthukumar, Sahil Singla",http://arxiv.org/pdf/2505.17365v1,cs.LG
Towards VM Rescheduling Optimization Through Deep Reinforcement Learning,"Modern industry-scale data centers need to manage a large number of virtual
machines (VMs). Due to the continual creation and release of VMs, many small
resource fragments are scattered across physical machines (PMs). To handle
these fragments, data centers periodically reschedule some VMs to alternative
PMs, a practice commonly referred to as VM rescheduling. Despite the increasing
importance of VM rescheduling as data centers grow in size, the problem remains
understudied. We first show that, unlike most combinatorial optimization tasks,
the inference time of VM rescheduling algorithms significantly influences their
performance, due to dynamic VM state changes during this period. This causes
existing methods to scale poorly. Therefore, we develop a reinforcement
learning system for VM rescheduling, VM2RL, which incorporates a set of
customized techniques, such as a two-stage framework that accommodates diverse
constraints and workload conditions, a feature extraction module that captures
relational information specific to rescheduling, as well as a risk-seeking
evaluation enabling users to optimize the trade-off between latency and
accuracy. We conduct extensive experiments with data from an industry-scale
data center. Our results show that VM2RL can achieve a performance comparable
to the optimal solution but with a running time of seconds. Code and datasets
are open-sourced: https://github.com/zhykoties/VMR2L_eurosys,
https://drive.google.com/drive/folders/1PfRo1cVwuhH30XhsE2Np3xqJn2GpX5qy.",2025-05-23,"Xianzhong Ding, Yunkai Zhang, Binbin Chen, Donghao Ying, Tieying Zhang, Jianjun Chen, Lei Zhang, Alberto Cerpa, Wan Du",http://arxiv.org/pdf/2505.17359v1,cs.LG
"Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction","With the rise of IoT-based botnet attacks, researchers have explored various
learning models for detection, including traditional machine learning, deep
learning, and hybrid approaches. A key advancement involves deploying attention
mechanisms to capture long-term dependencies among features, significantly
improving detection accuracy. However, most models treat attack instances
independently, overlooking inter-instance relationships. Graph Neural Networks
(GNNs) address this limitation by learning an embedding space via iterative
message passing where similar instances are placed closer based on node
features and relationships, enhancing classification performance. To further
improve detection, attention mechanisms have been embedded within GNNs,
leveraging both long-range dependencies and inter-instance connections.
However, transforming the high dimensional IoT attack datasets into a graph
structured dataset poses challenges, such as large graph structures leading
computational overhead. To mitigate this, this paper proposes a framework that
first reduces dimensionality of the NetFlow-based IoT attack dataset before
transforming it into a graph dataset. We evaluate three dimension reduction
techniques--Variational Autoencoder (VAE-encoder), classical autoencoder
(AE-encoder), and Principal Component Analysis (PCA)--and compare their effects
on a Graph Attention neural network (GAT) model for botnet attack detection",2025-05-23,"Hassan Wasswa, Hussein Abbass, Timothy Lynar",http://arxiv.org/pdf/2505.17357v1,cs.LG
Adversarial Robustness of Nonparametric Regression,"In this paper, we investigate the adversarial robustness of regression, a
fundamental problem in machine learning, under the setting where an adversary
can arbitrarily corrupt a subset of the input data. While the robustness of
parametric regression has been extensively studied, its nonparametric
counterpart remains largely unexplored. We characterize the adversarial
robustness in nonparametric regression, assuming the regression function
belongs to the second-order Sobolev space (i.e., it is square integrable up to
its second derivative).
  The contribution of this paper is two-fold: (i) we establish a minimax lower
bound on the estimation error, revealing a fundamental limit that no estimator
can overcome, and (ii) we show that, perhaps surprisingly, the classical
smoothing spline estimator, when properly regularized, exhibits robustness
against adversarial corruption. These results imply that if $o(n)$ out of $n$
samples are corrupted, the estimation error of the smoothing spline vanishes as
$n \to \infty$. On the other hand, when a constant fraction of the data is
corrupted, no estimator can guarantee vanishing estimation error, implying the
optimality of the smoothing spline in terms of maximum tolerable number of
corrupted samples.",2025-05-23,"Parsa Moradi, Hanzaleh Akabrinodehi, Mohammad Ali Maddah-Ali",http://arxiv.org/pdf/2505.17356v1,cs.LG
CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal Snapshots,"In many real-world scenarios, such as single-cell RNA sequencing, data are
observed only as discrete-time snapshots spanning finite time intervals and
subject to noisy timestamps, with no continuous trajectories available.
Recovering the underlying continuous-time dynamics from these snapshots with
coarse and noisy observation times is a critical and challenging task. We
propose Continuous-Time Optimal Transport Flow (CT-OT Flow), which first infers
high-resolution time labels via partial optimal transport and then reconstructs
a continuous-time data distribution through a temporal kernel smoothing. This
reconstruction enables accurate training of dynamics models such as ODEs and
SDEs. CT-OT Flow consistently outperforms state-of-the-art methods on synthetic
benchmarks and achieves lower reconstruction errors on real scRNA-seq and
typhoon-track datasets. Our results highlight the benefits of explicitly
modeling temporal discretization and timestamp uncertainty, offering an
accurate and general framework for bridging discrete snapshots and
continuous-time processes.",2025-05-23,"Keisuke Kawano, Takuro Kutsuna, Naoki Hayashi, Yasushi Esaki, Hidenori Tanaka",http://arxiv.org/pdf/2505.17354v1,cs.LG
Dual Ascent Diffusion for Inverse Problems,"Ill-posed inverse problems are fundamental in many domains, ranging from
astrophysics to medical imaging. Emerging diffusion models provide a powerful
prior for solving these problems. Existing maximum-a-posteriori (MAP) or
posterior sampling approaches, however, rely on different computational
approximations, leading to inaccurate or suboptimal samples. To address this
issue, we introduce a new approach to solving MAP problems with diffusion model
priors using a dual ascent optimization framework. Our framework achieves
better image quality as measured by various metrics for image restoration
problems, it is more robust to high levels of measurement noise, it is faster,
and it estimates solutions that represent the observations more faithfully than
the state of the art.",2025-05-23,"Minseo Kim, Axel Levy, Gordon Wetzstein",http://arxiv.org/pdf/2505.17353v1,cs.LG
FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems,"We introduce FLEX (FLow EXpert), a backbone architecture for generative
modeling of spatio-temporal physical systems using diffusion models. FLEX
operates in the residual space rather than on raw data, a modeling choice that
we motivate theoretically, showing that it reduces the variance of the velocity
field in the diffusion model, which helps stabilize training. FLEX integrates a
latent Transformer into a U-Net with standard convolutional ResNet layers and
incorporates a redesigned skip connection scheme. This hybrid design enables
the model to capture both local spatial detail and long-range dependencies in
latent space. To improve spatio-temporal conditioning, FLEX uses a
task-specific encoder that processes auxiliary inputs such as coarse or past
snapshots. Weak conditioning is applied to the shared encoder via skip
connections to promote generalization, while strong conditioning is applied to
the decoder through both skip and bottleneck features to ensure reconstruction
fidelity. FLEX achieves accurate predictions for super-resolution and
forecasting tasks using as few as two reverse diffusion steps. It also produces
calibrated uncertainty estimates through sampling. Evaluations on
high-resolution 2D turbulence data show that FLEX outperforms strong baselines
and generalizes to out-of-distribution settings, including unseen Reynolds
numbers, physical observables (e.g., fluid flow velocity fields), and boundary
conditions.",2025-05-23,"N. Benjamin Erichson, Vinicius Mikuni, Dongwei Lyu, Yang Gao, Omri Azencot, Soon Hoe Lim, Michael W. Mahoney",http://arxiv.org/pdf/2505.17351v1,cs.LG
A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction,"Unattended scheduled appointments, defined as patient no-shows, adversely
affect both healthcare providers and patients' health, disrupting the
continuity of care, operational efficiency, and the efficient allocation of
medical resources. Accurate predictive modelling is needed to reduce the impact
of no-shows. Although machine learning methods, such as logistic regression,
random forest models, and decision trees, are widely used in predicting patient
no-shows, they often rely on hard decision splits and static feature
importance, limiting their adaptability to specific or complex patient
behaviors. To address this limitation, we propose a new hybrid Multi-Head
Attention Soft Random Forest (MHASRF) model that integrates attention
mechanisms into a random forest model using probabilistic soft splitting
instead of hard splitting. The MHASRF model assigns attention weights
differently across the trees, enabling attention on specific patient behaviors.
The model exhibited 93.56% accuracy, 93.67% precision, 93.56% recall, and a
93.59% F1 score, surpassing the performance of decision tree, logistic
regression, random forest, and naive Bayes models. Furthermore, MHASRF was able
to identify key predictors of patient no-shows using two levels of feature
importance (tree level and attention mechanism level), offering deeper insights
into patient no-show predictors. The proposed model is a robust, adaptable, and
interpretable method for predicting patient no-shows that will help healthcare
providers in optimizing resources.",2025-05-22,"Ninda Nurseha Amalina, Kwadwo Boateng Ofori-Amanfo, Heungjo An",http://arxiv.org/pdf/2505.17344v1,cs.LG
A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety,"Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement
learning that explicitly deals with safety constraints during the learning and
deployment of agents. This survey provides a mathematically rigorous overview
of SafeRL formulations based on Constrained Markov Decision Processes (CMDPs)
and extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical
foundations of CMDPs, covering definitions, constrained optimization
techniques, and fundamental theorems. We then summarize state-of-the-art
algorithms in SafeRL for single agents, including policy gradient methods with
safety guarantees and safe exploration strategies, as well as recent advances
in SafeMARL for cooperative and competitive settings. Additionally, we propose
five open research problems to advance the field, with three focusing on
SafeMARL. Each problem is described with motivation, key challenges, and
related prior work. This survey is intended as a technical guide for
researchers interested in SafeRL and SafeMARL, highlighting key concepts,
methods, and open future research directions.",2025-05-22,"Ankita Kushwaha, Kiran Ravish, Preeti Lamba, Pawan Kumar",http://arxiv.org/pdf/2505.17342v1,cs.LG
TI-DeepONet: Learnable Time Integration for Stable Long-Term Extrapolation,"Accurate temporal extrapolation presents a fundamental challenge for neural
operators in modeling dynamical systems, where reliable predictions must extend
significantly beyond the training time horizon. Conventional Deep Operator
Network (DeepONet) approaches employ two inherently limited training paradigms
- fixed-horizon rollouts that predict complete spatiotemporal solutions while
disregarding temporal causality, and autoregressive formulations that
accumulate errors through sequential predictions. We introduce TI-DeepONet, a
framework that integrates neural operators with adaptive numerical
time-stepping techniques to preserve the Markovian structure of dynamical
systems while mitigating error propagation in extended temporal forecasting.
Our approach reformulates the learning objective from direct state prediction
to the approximation of instantaneous time-derivative fields, which are then
integrated using established numerical schemes. This architecture supports
continuous-time prediction and enables deployment of higher-precision
integrators during inference than those used during training, balancing
computational efficiency with predictive accuracy. We further develop
TI(L)-DeepONet, which incorporates learnable coefficients for intermediate
slopes in the integration process, adapting to solution-specific variations and
enhancing fidelity. Evaluation across three canonical PDEs shows that
TI(L)-DeepONet marginally outperforms TI-DeepONet, with both reducing relative
L2 extrapolation errors: approximately 81% over autoregressive and 70% over
fixed-horizon methods. Notably, both maintain prediction stability for temporal
domains extending to about twice the training interval. This research
establishes a physics-aware operator learning paradigm that bridges neural
approximation with numerical analysis while preserving the causal structure of
dynamical systems.",2025-05-22,"Dibyajyoti Nayak, Somdatta Goswami",http://arxiv.org/pdf/2505.17341v1,cs.LG
Conformal Predictive Distributions for Order Fulfillment Time Forecasting,"Accurate estimation of order fulfillment time is critical for e-commerce
logistics, yet traditional rule-based approaches often fail to capture the
inherent uncertainties in delivery operations. This paper introduces a novel
framework for distributional forecasting of order fulfillment time, leveraging
Conformal Predictive Systems and Cross Venn-Abers Predictors--model-agnostic
techniques that provide rigorous coverage or validity guarantees. The proposed
machine learning methods integrate granular spatiotemporal features, capturing
fulfillment location and carrier performance dynamics to enhance predictive
accuracy. Additionally, a cost-sensitive decision rule is developed to convert
probabilistic forecasts into reliable point predictions. Experimental
evaluation on a large-scale industrial dataset demonstrates that the proposed
methods generate competitive distributional forecasts, while machine
learning-based point predictions significantly outperform the existing
rule-based system--achieving up to 14% higher prediction accuracy and up to 75%
improvement in identifying late deliveries.",2025-05-22,"Tinghan Ye, Amira Hijazi, Pascal Van Hentenryck",http://arxiv.org/pdf/2505.17340v1,cs.LG
SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use,"Enterprise customers are increasingly adopting Large Language Models (LLMs)
for critical communication tasks, such as drafting emails, crafting sales
pitches, and composing casual messages. Deploying such models across different
regions requires them to understand diverse cultural and linguistic contexts
and generate safe and respectful responses. For enterprise applications, it is
crucial to mitigate reputational risks, maintain trust, and ensure compliance
by effectively identifying and handling unsafe or offensive language. To
address this, we introduce SweEval, a benchmark simulating real-world scenarios
with variations in tone (positive or negative) and context (formal or
informal). The prompts explicitly instruct the model to include specific swear
words while completing the task. This benchmark evaluates whether LLMs comply
with or resist such inappropriate instructions and assesses their alignment
with ethical frameworks, cultural nuances, and language comprehension
capabilities. In order to advance research in building ethically aligned AI
systems for enterprise use and beyond, we release the dataset and code:
https://github.com/amitbcp/multilingual_profanity.",2025-05-22,"Hitesh Laxmichand Patel, Amit Agarwal, Arion Das, Bhargava Kumar, Srikant Panda, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Dong-Kyu Chae",http://arxiv.org/pdf/2505.17332v1,cs.LG
ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training,"This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to
improve both the training speed and inference throughput of LLaMA architectures
while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models
into shared KV caching across certain layers, significantly reducing KV
computational complexity while maintaining or improving language performance.
Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher
token-per-second throughput during training, up to 16\% higher Model FLOPs
Utilization (MFU), and up to 14\% lower loss when trained on an equal number of
tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\%
higher test-time throughput compared to the baseline. By introducing a
computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable
and cost-effective solution for pretraining and finetuning large language
models, enabling faster and more resource-efficient training without
compromising performance.",2025-05-22,"Maryam Dialameh, Rezaul Karim, Hossein Rajabzadeh, Omar Mohamed Awad, Hyock Ju Kwon, Boxing Chen, Walid Ahmed, Yang Liu",http://arxiv.org/pdf/2505.17331v1,cs.LG
FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding,"In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable
and efficient model architecture for visually rich document understanding
(VRDU) in few-shot settings. FS-DAG leverages domain-specific and
language/vision specific backbones within a modular framework to adapt to
diverse document types with minimal data. The model is robust to practical
challenges such as handling OCR errors, misspellings, and domain shifts, which
are critical in real-world deployments. FS-DAG is highly performant with less
than 90M parameters, making it well-suited for complex real-world applications
for Information Extraction (IE) tasks where computational resources are
limited. We demonstrate FS-DAG's capability through extensive experiments for
information extraction task, showing significant improvements in convergence
speed and performance compared to state-of-the-art methods. Additionally, this
work highlights the ongoing progress in developing smaller, more efficient
models that do not compromise on performance. Code :
https://github.com/oracle-samples/fs-dag",2025-05-22,"Amit Agarwal, Srikant Panda, Kulbhushan Pachauri",http://arxiv.org/pdf/2505.17330v1,cs.LG
Transformer brain encoders explain human high-level visual responses,"A major goal of neuroscience is to understand brain computations during
visual processing in naturalistic settings. A dominant approach is to use
image-computable deep neural networks trained with different task objectives as
a basis for linear encoding models. However, in addition to requiring tuning a
large number of parameters, the linear encoding approach ignores the structure
of the feature maps both in the brain and the models. Recently proposed
alternatives have focused on decomposing the linear mapping to spatial and
feature components but focus on finding static receptive fields for units that
are applicable only in early visual areas. In this work, we employ the
attention mechanism used in the transformer architecture to study how
retinotopic visual features can be dynamically routed to category-selective
areas in high-level visual processing. We show that this computational motif is
significantly more powerful than alternative methods in predicting brain
activity during natural scene viewing, across different feature basis models
and modalities. We also show that this approach is inherently more
interpretable, without the need to create importance maps, by interpreting the
attention routing signal for different high-level categorical areas. Our
approach proposes a mechanistic model of how visual information from
retinotopic maps can be routed based on the relevance of the input content to
different category-selective regions.",2025-05-22,"Hossein Adeli, Minni Sun, Nikolaus Kriegeskorte",http://arxiv.org/pdf/2505.17329v1,cs.LG
"GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints","The proliferation of Large Language Models (LLMs) in late 2022 has impacted
academic writing, threatening credibility, and causing institutional
uncertainty. We seek to determine the degree to which LLMs are used to generate
critical text as opposed to being used for editing, such as checking for
grammar errors or inappropriate phrasing. In our study, we analyze arXiv papers
for stylistic segmentation, which we measure by varying a PELT threshold
against a Bayesian classifier trained on GPT-regenerated text. We find that
LLM-attributed language is not predictive of stylistic segmentation, suggesting
that when authors use LLMs, they do so uniformly, reducing the risk of
hallucinations being introduced into academic preprints.",2025-05-22,"Soren DeHaan, Yuanze Liu, Johan Bollen, Sa'ul A. Blanco",http://arxiv.org/pdf/2505.17327v1,cs.LG
Partner Modelling Emerges in Recurrent Agents (But Only When It Matters),"Humans are remarkably adept at collaboration, able to infer the strengths and
weaknesses of new partners in order to work successfully towards shared goals.
To build AI systems with this capability, we must first understand its building
blocks: does such flexibility require explicit, dedicated mechanisms for
modelling others -- or can it emerge spontaneously from the pressures of
open-ended cooperative interaction? To investigate this question, we train
simple model-free RNN agents to collaborate with a population of diverse
partners. Using the `Overcooked-AI' environment, we collect data from thousands
of collaborative teams, and analyse agents' internal hidden states. Despite a
lack of additional architectural features, inductive biases, or auxiliary
objectives, the agents nevertheless develop structured internal representations
of their partners' task abilities, enabling rapid adaptation and generalisation
to novel collaborators. We investigated these internal models through probing
techniques, and large-scale behavioural analysis. Notably, we find that
structured partner modelling emerges when agents can influence partner
behaviour by controlling task allocation. Our results show that partner
modelling can arise spontaneously in model-free agents -- but only under
environmental conditions that impose the right kind of social pressure.",2025-05-22,"Ruaridh Mon-Williams, Max Taylor-Davies, Elizabeth Mieczkowski, Natalia Velez, Neil R. Bramley, Yanwei Wang, Thomas L. Griffiths, Christopher G. Lucas",http://arxiv.org/pdf/2505.17323v1,cs.LG
From Compression to Expansion: A Layerwise Analysis of In-Context Learning,"In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks without weight updates by learning from demonstration sequences.
While ICL shows strong empirical performance, its internal representational
mechanisms are not yet well understood. In this work, we conduct a statistical
geometric analysis of ICL representations to investigate how task-specific
information is captured across layers. Our analysis reveals an intriguing
phenomenon, which we term *Layerwise Compression-Expansion*: early layers
progressively produce compact and discriminative representations that encode
task information from the input demonstrations, while later layers expand these
representations to incorporate the query and generate the prediction. This
phenomenon is observed consistently across diverse tasks and a range of
contemporary LLM architectures. We demonstrate that it has important
implications for ICL performance -- improving with model size and the number of
demonstrations -- and for robustness in the presence of noisy examples. To
further understand the effect of the compact task representation, we propose a
bias-variance decomposition and provide a theoretical analysis showing how
attention mechanisms contribute to reducing both variance and bias, thereby
enhancing performance as the number of demonstrations increases. Our findings
reveal an intriguing layerwise dynamic in ICL, highlight how structured
representations emerge within LLMs, and showcase that analyzing internal
representations can facilitate a deeper understanding of model behavior.",2025-05-22,"Jiachen Jiang, Yuxin Dong, Jinxin Zhou, Zhihui Zhu",http://arxiv.org/pdf/2505.17322v1,cs.LG
Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models,"Achieving better alignment between vision embeddings and Large Language
Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs
(MLLMs), particularly for recent models that rely on powerful pretrained vision
encoders and LLMs. A common approach to connect the pretrained vision encoder
and LLM is through a projector applied after the vision encoder. However, the
projector is often trained to enable the LLM to generate captions, and hence
the mechanism by which LLMs understand each vision token remains unclear. In
this work, we first investigate the role of the projector in compressing vision
embeddings and aligning them with word embeddings. We show that the projector
significantly compresses visual information, removing redundant details while
preserving essential elements necessary for the LLM to understand visual
content. We then examine patch-level alignment -- the alignment between each
vision patch and its corresponding semantic words -- and propose a
*multi-semantic alignment hypothesis*. Our analysis indicates that the
projector trained by caption loss improves patch-level alignment but only to a
limited extent, resulting in weak and coarse alignment. To address this issue,
we propose *patch-aligned training* to efficiently enhance patch-level
alignment. Our experiments show that patch-aligned training (1) achieves
stronger compression capability and improved patch-level alignment, enabling
the MLLM to generate higher-quality captions, (2) improves the MLLM's
performance by 16% on referring expression grounding tasks, 4% on
question-answering tasks, and 3% on modern instruction-following benchmarks
when using the same supervised fine-tuning (SFT) setting. The proposed method
can be easily extended to other multimodal models.",2025-05-22,"Jiachen Jiang, Jinxin Zhou, Bo Peng, Xia Ning, Zhihui Zhu",http://arxiv.org/pdf/2505.17316v1,cs.LG
"Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning","Recent language models exhibit strong reasoning capabilities, yet the
influence of long-context capacity on reasoning remains underexplored. In this
work, we hypothesize that current limitations in reasoning stem, in part, from
insufficient long-context capacity, motivated by empirical observations such as
(1) higher context window length often leads to stronger reasoning performance,
and (2) failed reasoning cases resemble failed long-context cases. To test this
hypothesis, we examine whether enhancing a model's long-context ability before
Supervised Fine-Tuning (SFT) leads to improved reasoning performance.
Specifically, we compared models with identical architectures and fine-tuning
data but varying levels of long-context capacity. Our results reveal a
consistent trend: models with stronger long-context capacity achieve
significantly higher accuracy on reasoning benchmarks after SFT. Notably, these
gains persist even on tasks with short input lengths, indicating that
long-context training offers generalizable benefits for reasoning performance.
These findings suggest that long-context modeling is not just essential for
processing lengthy inputs, but also serves as a critical foundation for
reasoning. We advocate for treating long-context capacity as a first-class
objective in the design of future language models.",2025-05-22,"Wang Yang, Zirui Liu, Hongye Jin, Qingyu Yin, Vipin Chaudhary, Xiaotian Han",http://arxiv.org/pdf/2505.17315v1,cs.LG
AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking,"LLMs often need effective configurations, like temperature and reasoning
steps, to handle tasks requiring sophisticated reasoning and problem-solving,
ranging from joke generation to mathematical reasoning. Existing prompting
approaches usually adopt general-purpose, fixed configurations that work 'well
enough' across tasks but seldom achieve task-specific optimality. To address
this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM
to automate adaptive reasoning configurations for tasks requiring different
types of thinking. AdaReasoner is trained using a reinforcement learning (RL)
framework, combining a factorized action space with a targeted exploration
strategy, along with a pretrained reward model to optimize the policy model for
reasoning configurations with only a few-shot guide. AdaReasoner is backed by
theoretical guarantees and experiments of fast convergence and a sublinear
policy gap. Across six different LLMs and a variety of reasoning tasks, it
consistently outperforms standard baselines, preserves out-of-distribution
robustness, and yield gains on knowledge-intensive tasks through tailored
prompts.",2025-05-22,"Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang",http://arxiv.org/pdf/2505.17312v1,cs.LG
Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays,"Unsupervised anomaly detection (UAD) in medical imaging is crucial for
identifying pathological abnormalities without requiring extensive labeled
data. However, existing diffusion-based UAD models rely solely on imaging
features, limiting their ability to distinguish between normal anatomical
variations and pathological anomalies. To address this, we propose Diff3M, a
multi-modal diffusion-based framework that integrates chest X-rays and
structured Electronic Health Records (EHRs) for enhanced anomaly detection.
Specifically, we introduce a novel image-EHR cross-attention module to
incorporate structured clinical context into the image generation process,
improving the model's ability to differentiate normal from abnormal features.
Additionally, we develop a static masking strategy to enhance the
reconstruction of normal-like images from anomalies. Extensive evaluations on
CheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art
performance, outperforming existing UAD methods in medical imaging. Our code is
available at this http URL https://github.com/nth221/Diff3M",2025-05-22,"Harim Kim, Yuhan Wang, Minkyu Ahn, Heeyoul Choi, Yuyin Zhou, Charmgil Hong",http://arxiv.org/pdf/2505.17311v1,cs.LG
Repulsive Ensembles for Bayesian Inference in Physics-informed Neural Networks,"Physics-informed neural networks (PINNs) have proven an effective tool for
solving differential equations, in particular when considering non-standard or
ill-posed settings. When inferring solutions and parameters of the differential
equation from data, uncertainty estimates are preferable to point estimates, as
they give an idea about the accuracy of the solution. In this work, we consider
the inverse problem and employ repulsive ensembles of PINNs (RE-PINN) for
obtaining such estimates. The repulsion is implemented by adding a particular
repulsive term to the loss function, which has the property that the ensemble
predictions correspond to the true Bayesian posterior in the limit of infinite
ensemble members. Where possible, we compare the ensemble predictions to Monte
Carlo baselines. Whereas the standard ensemble tends to collapse to
maximum-a-posteriori solutions, the repulsive ensemble produces significantly
more accurate uncertainty estimates and exhibits higher sample diversity.",2025-05-22,"Philipp Pilar, Markus Heinonen, Niklas Wahlström",http://arxiv.org/pdf/2505.17308v1,cs.LG
Wavelet Probabilistic Recurrent Convolutional Network for Multivariate Time Series Classification,"This paper presents a Wavelet Probabilistic Recurrent Convolutional Network
(WPRCN) for Multivariate Time Series Classification (MTSC), especially
effective in handling non-stationary environments, data scarcity and noise
perturbations. We introduce a versatile wavelet probabilistic module designed
to extract and analyse the probabilistic features, which can seamlessly
integrate with a variety of neural network architectures. This probabilistic
module comprises an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and
a Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN).
Such formulation extends the application of wavelet probabilistic neural
networks to deep neural networks for MTSC. The AWPG constructs an ensemble
probabilistic model addressing different data scarcities and non-stationarity;
it adaptively selects the optimal ones and generates probabilistic features for
APTCN. The APTCN analyses the correlations of the features and forms a
comprehensive feature space with existing MTSC models for classification. Here,
we instantiate the proposed module to work in parallel with a Long Short-Term
Memory (LSTM) network and a Causal Fully Convolutional Network (C-FCN),
demonstrating its broad applicability in time series analysis. The WPRCN is
evaluated on 30 diverse MTS datasets and outperforms all the benchmark
algorithms on average accuracy and rank, exhibiting pronounced strength in
handling scarce data and physiological data subject to perturbations and
non-stationarities.",2025-05-22,"Pu Yang, J. A. Barria",http://arxiv.org/pdf/2505.17307v1,cs.LG
Implicit Regularization of Infinitesimally-perturbed Gradient Descent Toward Low-dimensional Solutions,"Implicit regularization refers to the phenomenon where local search
algorithms converge to low-dimensional solutions, even when such structures are
neither explicitly specified nor encoded in the optimization problem. While
widely observed, this phenomenon remains theoretically underexplored,
particularly in modern over-parameterized problems. In this paper, we study the
conditions that enable implicit regularization by investigating when
gradient-based methods converge to second-order stationary points (SOSPs)
within an implicit low-dimensional region of a smooth, possibly nonconvex
function. We show that successful implicit regularization hinges on two key
conditions: $(i)$ the ability to efficiently escape strict saddle points, while
$(ii)$ maintaining proximity to the implicit region. Existing analyses enabling
the convergence of gradient descent (GD) to SOSPs often rely on injecting large
perturbations to escape strict saddle points. However, this comes at the cost
of deviating from the implicit region. The central premise of this paper is
that it is possible to achieve the best of both worlds: efficiently escaping
strict saddle points using infinitesimal perturbations, while controlling
deviation from the implicit region via a small deviation rate. We show that
infinitesimally perturbed gradient descent (IPGD), which can be interpreted as
GD with inherent ``round-off errors'', can provably satisfy both conditions. We
apply our framework to the problem of over-parameterized matrix sensing, where
we establish formal guarantees for the implicit regularization behavior of
IPGD. We further demonstrate through extensive experiments that these insights
extend to a broader class of learning problems.",2025-05-22,"Jianhao Ma, Geyu Liang, Salar Fattahi",http://arxiv.org/pdf/2505.17304v1,cs.LG
Statistical Inference for Online Algorithms,"Construction of confidence intervals and hypothesis tests for functionals
based on asymptotically normal estimators is a classical topic in statistical
inference. The simplest and in many cases optimal inference procedure is the
Wald interval or the likelihood ratio test, both of which require an estimator
and an estimate of the asymptotic variance of the estimator. Estimators
obtained from online/sequential algorithms forces one to consider the
computational aspects of the inference problem, i.e., one cannot access all of
the data as many times as needed. Several works on this topic explored the
online estimation of asymptotic variance. In this article, we propose
computationally efficient, rate-optimal, and asymptotically valid confidence
regions based on the output of online algorithms {\em without} estimating the
asymptotic variance. As a special case, this implies inference from any
algorithm that yields an asymptotically normal estimator. We focus our efforts
on stochastic gradient descent with Polyak averaging to understand the
practical performance of the proposed method.",2025-05-22,"Selina Carter, Arun K Kuchibhotla",http://arxiv.org/pdf/2505.17300v1,cs.LG
SELF: Self-Extend the Context Length With Logistic Growth Function,"Large language models suffer issues when operated on long contexts that are
larger than their training context length due to the standard position encoding
for tokens in the attention layer. Tokens a long distance apart will rarely
have an effect on each other and long prompts yield unexpected results. To
solve this problem, we propose SELF (Self-Extend the Context Length With
Logistic Growth Function): a solution of grouping consecutive tokens at varying
group sizes using a logistic capacity equation combined with a constant group
size at smaller relative distances. Our model had an increase in performance of
up to 12% compared to the LongLM extension method in LEval (specifically on the
Qwen model). On summarization related tasks in LongBench, our model performed
up to 6.4% better than LongLM (specifically on the Llama-2-7b model). On
reading comprehension tasks from LEval, our model performed up to 5.4% better
than the LongLM. Our code is available at https://github.com/alexeipc/SELF-LLM.",2025-05-22,"Phat Thanh Dang, Saahil Thoppay, Wang Yang, Qifan Wang, Vipin Chaudhary, Xiaotian Han",http://arxiv.org/pdf/2505.17296v1,cs.LG
Model-Free Graph Data Selection under Distribution Shift,"Graph domain adaptation (GDA) is a fundamental task in graph machine
learning, with techniques like shift-robust graph neural networks (GNNs) and
specialized training procedures to tackle the distribution shift problem.
Although these model-centric approaches show promising results, they often
struggle with severe shifts and constrained computational resources. To address
these challenges, we propose a novel model-free framework, GRADATE (GRAph DATa
sElector), that selects the best training data from the source domain for the
classification task on the target domain. GRADATE picks training samples
without relying on any GNN model's predictions or training recipes, leveraging
optimal transport theory to capture and adapt to distribution changes. GRADATE
is data-efficient, scalable and meanwhile complements existing model-centric
GDA approaches. Through comprehensive empirical studies on several real-world
graph-level datasets and multiple covariate shift types, we demonstrate that
GRADATE outperforms existing selection methods and enhances off-the-shelf GDA
methods with much fewer training data.",2025-05-22,"Ting-Wei Li, Ruizhong Qiu, Hanghang Tong",http://arxiv.org/pdf/2505.17293v1,cs.LG
Optimal Transport with Heterogeneously Missing Data,"We consider the problem of solving the optimal transport problem between two
empirical distributions with missing values. Our main assumption is that the
data is missing completely at random (MCAR), but we allow for heterogeneous
missingness probabilities across features and across the two distributions. As
a first contribution, we show that the Wasserstein distance between empirical
Gaussian distributions and linear Monge maps between arbitrary distributions
can be debiased without significantly affecting the sample complexity.
Secondly, we show that entropic regularized optimal transport can be estimated
efficiently and consistently using iterative singular value thresholding
(ISVT). We propose a validation set-free hyperparameter selection strategy for
ISVT that leverages our estimator of the Bures-Wasserstein distance, which
could be of independent interest in general matrix completion problems.
Finally, we validate our findings on a wide range of numerical applications.",2025-05-22,"Linus Bleistein, Aurélien Bellet, Julie Josse",http://arxiv.org/pdf/2505.17291v1,cs.LG
Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation,"Using the bit string generation problem as a case study, we theoretically
compare two standard methods for adapting large language models to new tasks.
The first, referred to as supervised fine-tuning, involves training a new next
token predictor on good generations. The second method, Best-of-N, trains a
reward model to select good responses from a collection generated by an
unaltered base model. If the learning setting is realizable, we find that
supervised fine-tuning outperforms BoN through a better dependence on the
response length in its rate of convergence. If realizability fails, then
depending on the failure mode, BoN can enjoy a better rate of convergence in
either n or a rate of convergence with better dependence on the response
length.",2025-05-22,"Seamus Somerstep, Vinod Raman, Unique Subedi, Yuekai Sun",http://arxiv.org/pdf/2505.17288v1,cs.LG
Deconfounded Warm-Start Thompson Sampling with Applications to Precision Medicine,"Randomized clinical trials often require large patient cohorts before drawing
definitive conclusions, yet abundant observational data from parallel studies
remains underutilized due to confounding and hidden biases. To bridge this gap,
we propose Deconfounded Warm-Start Thompson Sampling (DWTS), a practical
approach that leverages a Doubly Debiased LASSO (DDL) procedure to identify a
sparse set of reliable measured covariates and combines them with key hidden
covariates to form a reduced context. By initializing Thompson Sampling (LinTS)
priors with DDL-estimated means and variances on these measured features --
while keeping uninformative priors on hidden features -- DWTS effectively
harnesses confounded observational data to kick-start adaptive clinical trials.
Evaluated on both a purely synthetic environment and a virtual environment
created using real cardiovascular risk dataset, DWTS consistently achieves
lower cumulative regret than standard LinTS, showing how offline causal
insights from observational data can improve trial efficiency and support more
personalized treatment decisions.",2025-05-22,"Prateek Jaiswal, Esmaeil Keyvanshokooh, Junyu Cao",http://arxiv.org/pdf/2505.17283v1,cs.LG
Attention with Trained Embeddings Provably Selects Important Tokens,"Token embeddings play a crucial role in language modeling but, despite this
practical relevance, their theoretical understanding remains limited. Our paper
addresses the gap by characterizing the structure of embeddings obtained via
gradient descent. Specifically, we consider a one-layer softmax attention model
with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top
E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top
v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots,
E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the
embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output
vector. First, we show that, already after a single step of gradient training
with the logistic loss, the embeddings $E_X$ capture the importance of tokens
in the dataset by aligning with the output vector $v$ proportionally to the
frequency with which the corresponding tokens appear in the dataset. Then,
after training $p$ via gradient flow until convergence, the softmax selects the
important tokens in the sentence (i.e., those that are predictive of the
label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes
the margin for such a selection. Experiments on real-world datasets (IMDB,
Yelp) exhibit a phenomenology close to that unveiled by our theory.",2025-05-22,"Diyuan Wu, Aleksandr Shevchenko, Samet Oymak, Marco Mondelli",http://arxiv.org/pdf/2505.17282v1,cs.LG
"Comparator-Adaptive $Φ$-Regret: Improved Bounds, Simpler Algorithms, and Applications to Games","In the classic expert problem, $\Phi$-regret measures the gap between the
learner's total loss and that achieved by applying the best action
transformation $\phi \in \Phi$. A recent work by Lu et al., [2025] introduces
an adaptive algorithm whose regret against a comparator $\phi$ depends on a
certain sparsity-based complexity measure of $\phi$, (almost) recovering and
interpolating optimal bounds for standard regret notions such as external,
internal, and swap regret. In this work, we propose a general idea to achieve
an even better comparator-adaptive $\Phi$-regret bound via much simpler
algorithms compared to Lu et al., [2025]. Specifically, we discover a prior
distribution over all possible binary transformations and show that it suffices
to achieve prior-dependent regret against these transformations. Then, we
propose two concrete and efficient algorithms to achieve so, where the first
one learns over multiple copies of a prior-aware variant of the Kernelized MWU
algorithm of Farina et al., [2022], and the second one learns over multiple
copies of a prior-aware variant of the BM-reduction [Blum and Mansour, 2007].
To further showcase the power of our methods and the advantages over Lu et al.,
[2025] besides the simplicity and better regret bounds, we also show that our
second approach can be extended to the game setting to achieve accelerated and
adaptive convergence rate to $\Phi$-equilibria for a class of general-sum
games. When specified to the special case of correlated equilibria, our bound
improves over the existing ones from Anagnostides et al., [2022a,b]",2025-05-22,"Soumita Hait, Ping Li, Haipeng Luo, Mengxiao Zhang",http://arxiv.org/pdf/2505.17277v1,cs.LG
Zebra-Llama: Towards Extremely Efficient Hybrid Models,"With the growing demand for deploying large language models (LLMs) across
diverse applications, improving their inference efficiency is crucial for
sustainable and democratized access. However, retraining LLMs to meet new
user-specific requirements is prohibitively expensive and environmentally
unsustainable. In this work, we propose a practical and scalable alternative:
composing efficient hybrid language models from existing pre-trained models.
Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models
by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)
layers, using a refined initialization and post-training pipeline to
efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama
achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B
training tokens (compared to trillions of tokens required for pre-training) and
an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down
to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,
respectively-while preserving 100%, 100%, and >97% of average zero-shot
performance on LM Harness tasks. Compared to models like MambaInLLaMA,
X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive
or superior accuracy while using significantly fewer tokens, smaller teachers,
and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses
Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,
over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves
2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context
length. We will release code and model checkpoints upon acceptance.",2025-05-22,"Mingyu Yang, Mehdi Rezagholizadeh, Guihong Li, Vikram Appia, Emad Barsoum",http://arxiv.org/pdf/2505.17272v1,cs.LG
JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model,"Large language models (LLMs) have revolutionized natural language processing
and are increasingly applied to other sequential data types, including genetic
sequences. However, adapting LLMs to genomics presents significant challenges.
Capturing complex genomic interactions requires modeling long-range
dependencies within DNA sequences, where interactions often span over 10,000
base pairs, even within a single gene, posing substantial computational burdens
under conventional model architectures and training paradigms. Moreover,
standard LLM training approaches are suboptimal for DNA: autoregressive
training, while efficient, supports only unidirectional understanding. However,
DNA is inherently bidirectional, e.g., bidirectional promoters regulate
transcription in both directions and account for nearly 11% of human gene
expression. Masked language models (MLMs) allow bidirectional understanding but
are inefficient, as only masked tokens contribute to the loss per step. To
address these limitations, we introduce JanusDNA, the first bidirectional DNA
foundation model built upon a novel pretraining paradigm that combines the
optimization efficiency of autoregressive modeling with the bidirectional
comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and
Mixture of Experts (MoE) architecture, combining long-range modeling of
Attention with efficient sequential learning of Mamba. MoE layers further scale
model capacity via sparse activation while keeping computational cost low.
Notably, JanusDNA processes up to 1 million base pairs at single nucleotide
resolution on a single 80GB GPU. Extensive experiments and ablations show
JanusDNA achieves new SOTA results on three genomic representation benchmarks,
outperforming models with 250x more activated parameters. Code:
https://github.com/Qihao-Duan/JanusDNA",2025-05-22,"Qihao Duan, Bingding Huang, Zhenqiao Song, Irina Lehmann, Lei Gu, Roland Eils, Benjamin Wild",http://arxiv.org/pdf/2505.17257v1,cs.LG
Approach to Finding a Robust Deep Learning Model,"The rapid development of machine learning (ML) and artificial intelligence
(AI) applications requires the training of large numbers of models. This
growing demand highlights the importance of training models without human
supervision, while ensuring that their predictions are reliable. In response to
this need, we propose a novel approach for determining model robustness. This
approach, supplemented with a proposed model selection algorithm designed as a
meta-algorithm, is versatile and applicable to any machine learning model,
provided that it is appropriate for the task at hand. This study demonstrates
the application of our approach to evaluate the robustness of deep learning
models. To this end, we study small models composed of a few convolutional and
fully connected layers, using common optimizers due to their ease of
interpretation and computational efficiency. Within this framework, we address
the influence of training sample size, model weight initialization, and
inductive bias on the robustness of deep learning models.",2025-05-22,"Alexey Boldyrev, Fedor Ratnikov, Andrey Shevelev",http://arxiv.org/pdf/2505.17254v1,cs.LG
ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models,"Large language models excel at complex tasks by breaking down problems into
structured reasoning steps. However, reasoning traces often extend beyond
reaching a correct answer, causing wasted computation, reduced readability, and
hallucinations. To address this, we introduce a novel hyperparameter-free
conciseness score used as a reward signal within a reinforcement learning
framework to guide models toward generating correct and concise reasoning
traces. This score is evaluated by a large language model acting as a judge,
enabling dynamic, context-aware feedback beyond simple token length. Our method
achieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset,
reducing token usage by up to 31x on simple problems while improving accuracy
by 7%, and on the hardest problems, it outperforms full reasoning by +7.5%
accuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves
accuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on
the judge model, reward composition, and problem difficulty, showing that our
method dynamically adapts reasoning length based on problem difficulty and
benefits significantly from stronger judges. The code, model weights, and
datasets are open-sourced at https://github.com/RazvanDu/ConciseRL.",2025-05-22,"Razvan-Gabriel Dumitru, Darius Peteleaza, Vikas Yadav, Liangming Pan",http://arxiv.org/pdf/2505.17250v1,cs.LG
Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning,"Big trajectory data hold great promise for human mobility analysis, but their
utility is often constrained by the absence of critical traveler attributes,
particularly sociodemographic information. While prior studies have explored
predicting such attributes from mobility patterns, they often overlooked
underlying cognitive mechanisms and exhibited low predictive accuracy. This
study introduces SILIC, short for Sociodemographic Inference with LLM-guided
Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a
theoretically grounded framework that leverages LLMs to infer sociodemographic
attributes from observed mobility patterns by capturing latent behavioral
intentions and reasoning through psychological constructs. Particularly, our
approach explicitly follows the Theory of Planned Behavior (TPB), a
foundational behavioral framework in transportation research, to model
individuals' latent cognitive processes underlying travel decision-making. The
LLMs further provide heuristic guidance to improve IRL reward function
initialization and update by addressing its ill-posedness and optimization
challenges arising from the vast and unstructured reward space. Evaluated in
the 2017 Puget Sound Regional Council Household Travel Survey, our method
substantially outperforms state-of-the-art baselines and shows great promise
for enriching big trajectory data to support more behaviorally grounded
applications in transportation planning and beyond.",2025-05-22,"Yuran Sun, Susu Xu, Chenguang Wang, Xilei Zhao",http://arxiv.org/pdf/2505.17249v1,cs.LG
Backdoors in DRL: Four Environments Focusing on In-distribution Triggers,"Backdoor attacks, or trojans, pose a security risk by concealing undesirable
behavior in deep neural network models. Open-source neural networks are
downloaded from the internet daily, possibly containing backdoors, and
third-party model developers are common. To advance research on backdoor attack
mitigation, we develop several trojans for deep reinforcement learning (DRL)
agents. We focus on in-distribution triggers, which occur within the agent's
natural data distribution, since they pose a more significant security threat
than out-of-distribution triggers due to their ease of activation by the
attacker during model deployment. We implement backdoor attacks in four
reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld,
Colorful Memory, and Modified Safety Gymnasium. We train various models, both
clean and backdoored, to characterize these attacks. We find that
in-distribution triggers can require additional effort to implement and be more
challenging for models to learn, but are nevertheless viable threats in DRL
even using basic data poisoning attacks.",2025-05-22,"Chace Ashcraft, Ted Staley, Josh Carney, Cameron Hickert, Derek Juba, Kiran Karra, Nathan Drenkow",http://arxiv.org/pdf/2505.17248v1,cs.LG
Extending Dataset Pruning to Object Detection: A Variance-based Approach,"Dataset pruning -- selecting a small yet informative subset of training data
-- has emerged as a promising strategy for efficient machine learning, offering
significant reductions in computational cost and storage compared to
alternatives like dataset distillation. While pruning methods have shown strong
performance in image classification, their extension to more complex computer
vision tasks, particularly object detection, remains relatively underexplored.
In this paper, we present the first principled extension of classification
pruning techniques to the object detection domain, to the best of our
knowledge. We identify and address three key challenges that hinder this
transition: the Object-Level Attribution Problem, the Scoring Strategy Problem,
and the Image-Level Aggregation Problem. To overcome these, we propose tailored
solutions, including a novel scoring method called Variance-based Prediction
Score (VPS). VPS leverages both Intersection over Union (IoU) and confidence
scores to effectively identify informative training samples specific to
detection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate
that our approach consistently outperforms prior dataset pruning methods in
terms of mean Average Precision (mAP). We also show that annotation count and
class distribution shift can influence detection performance, but selecting
informative examples is a more critical factor than dataset size or balance.
Our work bridges dataset pruning and object detection, paving the way for
dataset pruning in complex vision tasks.",2025-05-22,Ryota Yagi,http://arxiv.org/pdf/2505.17245v1,cs.LG
Optimal Policy Minimum Bayesian Risk,"Inference scaling can help LLMs solve complex reasoning problems through
extended runtime computation. On top of targeted supervision for long
chain-of-thought (long-CoT) generation, purely inference-time techniques such
as best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes
risk decoding (MBRD), can further improve LLM accuracy by generating multiple
candidate solutions and aggregating over them. These methods typically leverage
additional signals in the form of reward models and risk/similarity functions
that compare generated samples, e.g., exact match in some normalized space or
standard similarity metrics such as Rouge. Here we present a novel method for
incorporating reward and risk/similarity signals into MBRD. Based on the
concept of optimal policy in KL-controlled reinforcement learning, our
framework provides a simple and well-defined mechanism for leveraging such
signals, offering several advantages over traditional inference-time methods:
higher robustness, improved accuracy, and well-understood asymptotic behavior.
In addition, it allows for the development of a sample-efficient variant of
MBRD that can adjust the number of samples to generate according to the
difficulty of the problem, without relying on majority vote counts. We
empirically demonstrate the advantages of our approach on math (MATH-$500$) and
coding (HumanEval) tasks using recent open-source models. We also present a
comprehensive analysis of its accuracy-compute trade-offs.",2025-05-22,"Ramón Fernandez Astudillo, Md Arafat Sultan, Aashka Trivedi, Yousef El-Kurdi, Tahira Naseem, Radu Florian, Salim Roukos",http://arxiv.org/pdf/2505.17242v1,cs.LG
Semantic-Aware Interpretable Multimodal Music Auto-Tagging,"Music auto-tagging is essential for organizing and discovering music in
extensive digital libraries. While foundation models achieve exceptional
performance in this domain, their outputs often lack interpretability, limiting
trust and usability for researchers and end-users alike. In this work, we
present an interpretable framework for music auto-tagging that leverages groups
of musically meaningful multimodal features, derived from signal processing,
deep learning, ontology engineering, and natural language processing. To
enhance interpretability, we cluster features semantically and employ an
expectation maximization algorithm, assigning distinct weights to each group
based on its contribution to the tagging process. Our method achieves
competitive tagging performance while offering a deeper understanding of the
decision-making process, paving the way for more transparent and user-centric
music tagging systems.",2025-05-22,"Andreas Patakis, Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, Giorgos Stamou",http://arxiv.org/pdf/2505.17233v2,cs.LG
Automated Capability Evaluation of Foundation Models,"Current evaluation frameworks for foundation models rely heavily on fixed,
manually curated benchmarks, limiting their ability to capture the full breadth
of model capabilities. This paper introduces Active learning for Capability
Evaluation (ACE), a novel framework for scalable, automated, and fine-grained
evaluation of foundation models. ACE leverages the knowledge embedded in
powerful language models to decompose a domain into semantically meaningful
capabilities and generate diverse evaluation tasks, significantly reducing
human effort. To maximize coverage and efficiency, ACE models a subject model's
performance as a capability function over a latent semantic space and uses
active learning to prioritize the evaluation of the most informative
capabilities. This adaptive evaluation strategy enables cost-effective
discovery of strengths, weaknesses, and failure modes that static benchmarks
may miss. Our results suggest that ACE provides a more complete and informative
picture of model capabilities, which is essential for safe and well-informed
deployment of foundation models.",2025-05-22,"Arash Afkanpour, Omkar Dige, Fatemeh Tavakoli",http://arxiv.org/pdf/2505.17228v1,cs.LG
Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation,"Federated Learning (FL) enables collaborative machine learning across
decentralized data sources without sharing raw data. It offers a promising
approach to privacy-preserving AI. However, FL remains vulnerable to
adversarial threats from malicious participants, referred to as Byzantine
clients, who can send misleading updates to corrupt the global model.
Traditional aggregation methods, such as simple averaging, are not robust to
such attacks. More resilient approaches, like the Krum algorithm, require prior
knowledge of the number of malicious clients, which is often unavailable in
real-world scenarios. To address these limitations, we propose Average-rKrum
(ArKrum), a novel aggregation strategy designed to enhance both the resilience
and privacy guarantees of FL systems. Building on our previous work (rKrum),
ArKrum introduces two key innovations. First, it includes a median-based
filtering mechanism that removes extreme outliers before estimating the number
of adversarial clients. Second, it applies a multi-update averaging scheme to
improve stability and performance, particularly when client data distributions
are not identical. We evaluate ArKrum on benchmark image and text datasets
under three widely studied Byzantine attack types. Results show that ArKrum
consistently achieves high accuracy and stability. It performs as well as or
better than other robust aggregation methods. These findings demonstrate that
ArKrum is an effective and practical solution for secure FL systems in
adversarial environments.",2025-05-22,"Kun Yang, Neena Imam",http://arxiv.org/pdf/2505.17226v1,cs.LG
Assessing the generalization performance of SAM for ureteroscopy scene understanding,"The segmentation of kidney stones is regarded as a critical preliminary step
to enable the identification of urinary stone types through machine- or
deep-learning-based approaches. In urology, manual segmentation is considered
tedious and impractical due to the typically large scale of image databases and
the continuous generation of new data. In this study, the potential of the
Segment Anything Model (SAM) -- a state-of-the-art deep learning framework --
is investigated for the automation of kidney stone segmentation. The
performance of SAM is evaluated in comparison to traditional models, including
U-Net, Residual U-Net, and Attention U-Net, which, despite their efficiency,
frequently exhibit limitations in generalizing to unseen datasets. The findings
highlight SAM's superior adaptability and efficiency. While SAM achieves
comparable performance to U-Net on in-distribution data (Accuracy: 97.68 +
3.04; Dice: 97.78 + 2.47; IoU: 95.76 + 4.18), it demonstrates significantly
enhanced generalization capabilities on out-of-distribution data, surpassing
all U-Net variants by margins of up to 23 percent.",2025-05-22,"Martin Villagrana, Francisco Lopez-Tiro, Clement Larose, Gilberto Ochoa-Ruiz, Christian Daul",http://arxiv.org/pdf/2505.17210v1,cs.LG
"Content Moderation in TV Search: Balancing Policy Compliance, Relevance, and User Experience","Millions of people rely on search functionality to find and explore content
on entertainment platforms. Modern search systems use a combination of
candidate generation and ranking approaches, with advanced methods leveraging
deep learning and LLM-based techniques to retrieve, generate, and categorize
search results. Despite these advancements, search algorithms can still surface
inappropriate or irrelevant content due to factors like model unpredictability,
metadata errors, or overlooked design flaws. Such issues can misalign with
product goals and user expectations, potentially harming user trust and
business outcomes. In this work, we introduce an additional monitoring layer
using Large Language Models (LLMs) to enhance content moderation. This
additional layer flags content if the user did not intend to search for it.
This approach serves as a baseline for product quality assurance, with
collected feedback used to refine the initial retrieval mechanisms of the
search model, ensuring a safer and more reliable user experience.",2025-05-22,"Adeep Hande, Kishorekumar Sundararajan, Sardar Hamidian, Ferhan Ture",http://arxiv.org/pdf/2505.17207v1,cs.LG
Liouville PDE-based sliced-Wasserstein flow for fair regression,"The sliced Wasserstein flow (SWF), a nonparametric and implicit generative
gradient flow, is applied to fair regression. We have improved the SWF in a few
aspects. First, the stochastic diffusive term from the Fokker-Planck
equation-based Monte Carlo is transformed to Liouville partial differential
equation (PDE)-based transport with density estimation, however, without the
diffusive term. Now, the computation of the Wasserstein barycenter is
approximated by the SWF barycenter with the prescription of Kantorovich
potentials for the induced gradient flow to generate its samples. These two
efforts improve the convergence in training and testing SWF and SWF barycenters
with reduced variance. Applying the generative SWF barycenter for fair
regression demonstrates competent profiles in the accuracy-fairness Pareto
curves.",2025-05-22,"Pilhwa Lee, Jayshawn Cooper",http://arxiv.org/pdf/2505.17204v1,cs.LG
"Transfer Faster, Price Smarter: Minimax Dynamic Pricing under Cross-Market Preference Shift","We study contextual dynamic pricing when a target market can leverage K
auxiliary markets -- offline logs or concurrent streams -- whose mean utilities
differ by a structured preference shift. We propose Cross-Market Transfer
Dynamic Pricing (CM-TDP), the first algorithm that provably handles such
model-shift transfer and delivers minimax-optimal regret for both linear and
non-parametric utility models.
  For linear utilities of dimension d, where the difference between source- and
target-task coefficients is $s_{0}$-sparse, CM-TDP attains regret
$\tilde{O}((d*K^{-1}+s_{0})\log T)$. For nonlinear demand residing in a
reproducing kernel Hilbert space with effective dimension $\alpha$, complexity
$\beta$ and task-similarity parameter $H$, the regret becomes
$\tilde{O}\!(K^{-2\alpha\beta/(2\alpha\beta+1)}T^{1/(2\alpha\beta+1)} +
H^{2/(2\alpha+1)}T^{1/(2\alpha+1)})$, matching information-theoretic lower
bounds up to logarithmic factors. The RKHS bound is the first of its kind for
transfer pricing and is of independent interest.
  Extensive simulations show up to 50% lower cumulative regret and 5 times
faster learning relative to single-market pricing baselines. By bridging
transfer learning, robust aggregation, and revenue optimization, CM-TDP moves
toward pricing systems that transfer faster, price smarter.",2025-05-22,"Yi Zhang, Elynn Chen, Yujun Yan",http://arxiv.org/pdf/2505.17203v1,cs.LG
LengthLogD: A Length-Stratified Ensemble Framework for Enhanced Peptide Lipophilicity Prediction via Multi-Scale Feature Integration,"Peptide compounds demonstrate considerable potential as therapeutic agents
due to their high target affinity and low toxicity, yet their drug development
is constrained by their low membrane permeability. Molecular weight and peptide
length have significant effects on the logD of peptides, which in turn
influences their ability to cross biological membranes. However, accurate
prediction of peptide logD remains challenging due to the complex interplay
between sequence, structure, and ionization states. This study introduces
LengthLogD, a predictive framework that establishes specialized models through
molecular length stratification while innovatively integrating multi-scale
molecular representations. We constructed feature spaces across three
hierarchical levels: atomic (10 molecular descriptors), structural (1024-bit
Morgan fingerprints), and topological (3 graph-based features including Wiener
index), optimized through stratified ensemble learning. An adaptive weight
allocation mechanism specifically developed for long peptides significantly
enhances model generalizability. Experimental results demonstrate superior
performance across all categories: short peptides (R^2=0.855), medium peptides
(R^2=0.816), and long peptides (R^2=0.882), with a 34.7% reduction in
prediction error for long peptides compared to conventional single-model
approaches. Ablation studies confirm: 1) The length-stratified strategy
contributes 41.2% to performance improvement; 2) Topological features account
for 28.5% of predictive importance. Compared to state-of-the-art models, our
method maintains short peptide prediction accuracy while achieving a 25.7%
increase in the coefficient of determination (R^2) for long peptides. This
research provides a precise logD prediction tool for peptide drug development,
particularly demonstrating unique value in optimizing long peptide lead
compounds.",2025-05-22,"Shuang Wu, Meijie Wang, Lun Yu",http://arxiv.org/pdf/2505.17198v1,cs.LG
Shape it Up! Restoring LLM Safety during Finetuning,"Finetuning large language models (LLMs) enables user-specific customization
but introduces critical safety risks: even a few harmful examples can
compromise safety alignment. A common mitigation strategy is to update the
model more strongly on examples deemed safe, while downweighting or excluding
those flagged as unsafe. However, because safety context can shift within a
single example, updating the model equally on both harmful and harmless parts
of a response is suboptimal-a coarse treatment we term static safety shaping.
In contrast, we propose dynamic safety shaping (DSS), a framework that uses
fine-grained safety signals to reinforce learning from safe segments of a
response while suppressing unsafe content. To enable such fine-grained control
during finetuning, we introduce a key insight: guardrail models, traditionally
used for filtering, can be repurposed to evaluate partial responses, tracking
how safety risk evolves throughout the response, segment by segment. This leads
to the Safety Trajectory Assessment of Response (STAR), a token-level signal
that enables shaping to operate dynamically over the training sequence.
Building on this, we present STAR-DSS, guided by STAR scores, that robustly
mitigates finetuning risks and delivers substantial safety improvements across
diverse threats, datasets, and model families-all without compromising
capability on intended tasks. We encourage future safety research to build on
dynamic shaping principles for stronger mitigation against evolving finetuning
risks.",2025-05-22,"ShengYun Peng, Pin-Yu Chen, Jianfeng Chi, Seongmin Lee, Duen Horng Chau",http://arxiv.org/pdf/2505.17196v1,cs.LG
Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms,"Dynamic programming (DP) algorithms for combinatorial optimization problems
work with taking maximization, minimization, and classical addition in their
recursion algorithms. The associated value functions correspond to convex
polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning
models, however, rely on softmax-normalized dot-product attention where the
smooth exponential weighting blurs these sharp polyhedral structures and
collapses when evaluated on out-of-distribution (OOD) settings. We introduce
Tropical attention, a novel attention function that operates natively in the
max-plus semiring of tropical geometry. We prove that Tropical attention can
approximate tropical circuits of DP-type combinatorial algorithms. We then
propose that using Tropical transformers enhances empirical OOD performance in
both length generalization and value generalization, on algorithmic reasoning
tasks, surpassing softmax baselines while remaining stable under adversarial
attacks. We also present adversarial-attack generalization as a third axis for
Neural Algorithmic Reasoning benchmarking. Our results demonstrate that
Tropical attention restores the sharp, scale-invariant reasoning absent from
softmax.",2025-05-22,"Baran Hashemi, Kurt Pasque, Chris Teska, Ruriko Yoshida",http://arxiv.org/pdf/2505.17190v1,cs.LG
GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning,"Visual generation models have made remarkable progress in creating realistic
images from text prompts, yet struggle with complex prompts that specify
multiple objects with precise spatial relationships and attributes. Effective
handling of such prompts requires explicit reasoning about the semantic content
and spatial layout. We present GoT-R1, a framework that applies reinforcement
learning to enhance semantic-spatial reasoning in visual generation. Building
upon the Generation Chain-of-Thought approach, GoT-R1 enables models to
autonomously discover effective reasoning strategies beyond predefined
templates through carefully designed reinforcement learning. To achieve this,
we propose a dual-stage multi-dimensional reward framework that leverages MLLMs
to evaluate both the reasoning process and final output, enabling effective
supervision across the entire generation pipeline. The reward system assesses
semantic alignment, spatial accuracy, and visual quality in a unified approach.
Experimental results demonstrate significant improvements on T2I-CompBench
benchmark, particularly in compositional tasks involving precise spatial
relationships and attribute binding. GoT-R1 advances the state-of-the-art in
image generation by successfully transferring sophisticated reasoning
capabilities to the visual generation domain. To facilitate future research, we
make our code and pretrained models publicly available at
https://github.com/gogoduan/GoT-R1.",2025-05-22,"Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, Xihui Liu",http://arxiv.org/pdf/2505.17022v1,cs.LG
Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO,"Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT",2025-05-22,"Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, Pheng-Ann Heng",http://arxiv.org/pdf/2505.17017v1,cs.LG
Interactive Post-Training for Vision-Language-Action Models,"We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based
interactive post-training paradigm that fine-tunes pretrained
Vision-Language-Action (VLA) models using only sparse binary success rewards.
Existing VLA training pipelines rely heavily on offline expert demonstration
data and supervised imitation, limiting their ability to adapt to new tasks and
environments under low-data regimes. RIPT-VLA addresses this by enabling
interactive post-training with a stable policy optimization algorithm based on
dynamic rollout sampling and leave-one-out advantage estimation.
  RIPT-VLA has the following characteristics. First, it applies to various VLA
models, resulting in an improvement on the lightweight QueST model by 21.2%,
and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it
is computationally efficient and data-efficient: with only one demonstration,
RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success
rate within 15 iterations. Furthermore, we demonstrate that the policy learned
by RIPT-VLA generalizes across different tasks and scenarios and is robust to
the initial state context. These results highlight RIPT-VLA as a practical and
effective paradigm for post-training VLA models through minimal supervision.",2025-05-22,"Shuhan Tan, Kairan Dou, Yue Zhao, Philipp Krähenbühl",http://arxiv.org/pdf/2505.17016v1,cs.LG
When Are Concepts Erased From Diffusion Models?,"Concept erasure, the ability to selectively prevent a model from generating
specific concepts, has attracted growing interest, with various approaches
emerging to address the challenge. However, it remains unclear how thoroughly
these methods erase the target concept. We begin by proposing two conceptual
models for the erasure mechanism in diffusion models: (i) reducing the
likelihood of generating the target concept, and (ii) interfering with the
model's internal guidance mechanisms. To thoroughly assess whether a concept
has been truly erased from the model, we introduce a suite of independent
evaluations. Our evaluation framework includes adversarial attacks, novel
probing techniques, and analysis of the model's alternative generations in
place of the erased concept. Our results shed light on the tension between
minimizing side effects and maintaining robustness to adversarial prompts.
Broadly, our work underlines the importance of comprehensive evaluation for
erasure in diffusion models.",2025-05-22,"Kevin Lu, Nicky Kriplani, Rohit Gandikota, Minh Pham, David Bau, Chinmay Hegde, Niv Cohen",http://arxiv.org/pdf/2505.17013v2,cs.LG
Understanding Prompt Tuning and In-Context Learning via Meta-Learning,"Prompting is one of the main ways to adapt a pretrained model to target
tasks. Besides manually constructing prompts, many prompt optimization methods
have been proposed in the literature. Method development is mainly empirically
driven, with less emphasis on a conceptual understanding of prompting. In this
paper we discuss how optimal prompting can be understood through a Bayesian
view, which also implies some fundamental limitations of prompting that can
only be overcome by tuning weights. The paper explains in detail how
meta-trained neural networks behave as Bayesian predictors over the pretraining
distribution, whose hallmark feature is rapid in-context adaptation. Optimal
prompting can be studied formally as conditioning these Bayesian predictors,
yielding criteria for target tasks where optimal prompting is and is not
possible. We support the theory with educational experiments on LSTMs and
Transformers, where we compare different versions of prefix-tuning and
different weight-tuning methods. We also confirm that soft prefixes, which are
sequences of real-valued vectors outside the token alphabet, can lead to very
effective prompts for trained and even untrained networks by manipulating
activations in ways that are not achievable by hard tokens. This adds an
important mechanistic aspect beyond the conceptual Bayesian theory.",2025-05-22,"Tim Genewein, Kevin Wenliang Li, Jordi Grau-Moya, Anian Ruoss, Laurent Orseau, Marcus Hutter",http://arxiv.org/pdf/2505.17010v1,cs.LG
Guided Diffusion Sampling on Function Spaces with Applications to PDEs,"We propose a general framework for conditional sampling in PDE-based inverse
problems, targeting the recovery of whole solutions from extremely sparse or
noisy measurements. This is accomplished by a function-space diffusion model
and plug-and-play guidance for conditioning. Our method first trains an
unconditional discretization-agnostic denoising model using neural operator
architectures. At inference, we refine the samples to satisfy sparse
observation data via a gradient-based guidance mechanism. Through rigorous
mathematical analysis, we extend Tweedie's formula to infinite-dimensional
Hilbert spaces, providing the theoretical foundation for our posterior sampling
approach. Our method (FunDPS) accurately captures posterior distributions in
function spaces under minimal supervision and severe data scarcity. Across five
PDE tasks with only 3% observation, our method achieves an average 32% accuracy
improvement over state-of-the-art fixed-resolution diffusion baselines while
reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning
ensures strong cross-resolution generalizability. To the best of our knowledge,
this is the first diffusion-based framework to operate independently of
discretization, offering a practical and flexible solution for forward and
inverse problems in the context of PDEs. Code is available at
https://github.com/neuraloperator/FunDPS",2025-05-22,"Jiachen Yao, Abbas Mammadov, Julius Berner, Gavin Kerrigan, Jong Chul Ye, Kamyar Azizzadenesheli, Anima Anandkumar",http://arxiv.org/pdf/2505.17004v1,cs.LG
Sufficient conditions for offline reactivation in recurrent neural networks,"During periods of quiescence, such as sleep, neural activity in many brain
circuits resembles that observed during periods of task engagement. However,
the precise conditions under which task-optimized networks can autonomously
reactivate the same network states responsible for online behavior is poorly
understood. In this study, we develop a mathematical framework that outlines
sufficient conditions for the emergence of neural reactivation in circuits that
encode features of smoothly varying stimuli. We demonstrate mathematically that
noisy recurrent networks optimized to track environmental state variables using
change-based sensory information naturally develop denoising dynamics, which,
in the absence of input, cause the network to revisit state configurations
observed during periods of online activity. We validate our findings using
numerical experiments on two canonical neuroscience tasks: spatial position
estimation based on self-motion cues, and head direction estimation based on
angular velocity cues. Overall, our work provides theoretical support for
modeling offline reactivation as an emergent consequence of task optimization
in noisy neural circuits.",2025-05-22,"Nanda H. Krishna, Colin Bredenberg, Daniel Levenstein, Blake A. Richards, Guillaume Lajoie",http://arxiv.org/pdf/2505.17003v1,cs.LG
Critical Points of Random Neural Networks,"This work investigates the expected number of critical points of random
neural networks with different activation functions as the depth increases in
the infinite-width limit. Under suitable regularity conditions, we derive
precise asymptotic formulas for the expected number of critical points of fixed
index and those exceeding a given threshold. Our analysis reveals three
distinct regimes depending on the value of the first derivative of the
covariance evaluated at 1: the expected number of critical points may converge,
grow polynomially, or grow exponentially with depth. The theoretical
predictions are supported by numerical experiments. Moreover, we provide
numerical evidence suggesting that, when the regularity condition is not
satisfied (e.g. for neural networks with ReLU as activation function), the
number of critical points increases as the map resolution increases, indicating
a potential divergence in the number of critical points.",2025-05-22,Simmaco Di Lillo,http://arxiv.org/pdf/2505.17000v1,cs.LG
A Unified Framework for Simultaneous Parameter and Function Discovery in Differential Equations,"Inverse problems involving differential equations often require identifying
unknown parameters or functions from data. Existing approaches, such as
Physics-Informed Neural Networks (PINNs), Universal Differential Equations
(UDEs) and Universal Physics-Informed Neural Networks (UPINNs), are effective
at isolating either parameters or functions but can face challenges when
applied simultaneously due to solution non-uniqueness. In this work, we
introduce a framework that addresses these limitations by establishing
conditions under which unique solutions can be guaranteed. To illustrate, we
apply it to examples from biological systems and ecological dynamics,
demonstrating accurate and interpretable results. Our approach significantly
enhances the potential of machine learning techniques in modeling complex
systems in science and engineering.",2025-05-22,"Shalev Manor, Mohammad Kohandel",http://arxiv.org/pdf/2505.16996v1,cs.LG
Native Segmentation Vision Transformers,"Uniform downsampling remains the de facto standard for reducing spatial
resolution in vision backbones. In this work, we propose an alternative design
built around a content-aware spatial grouping layer, that dynamically assigns
tokens to a reduced set based on image boundaries and their semantic content.
Stacking our grouping layer across consecutive backbone stages results in
hierarchical segmentation that arises natively in the feature extraction
process, resulting in our coined Native Segmentation Vision Transformer. We
show that a careful design of our architecture enables the emergence of strong
segmentation masks solely from grouping layers, that is, without additional
segmentation-specific heads. This sets the foundation for a new paradigm of
native, backbone-level segmentation, which enables strong zero-shot results
without mask supervision, as well as a minimal and efficient standalone model
design for downstream segmentation tasks. Our project page is
https://research.nvidia.com/labs/dvl/projects/native-segmentation.",2025-05-22,"Guillem Brasó, Aljoša Ošep, Laura Leal-Taixé",http://arxiv.org/pdf/2505.16993v1,cs.LG
"PICT -- A Differentiable, GPU-Accelerated Multi-Block PISO Solver for Simulation-Coupled Learning Tasks in Fluid Dynamics","Despite decades of advancements, the simulation of fluids remains one of the
most challenging areas of in scientific computing. Supported by the necessity
of gradient information in deep learning, differentiable simulators have
emerged as an effective tool for optimization and learning in physics
simulations. In this work, we present our fluid simulator PICT, a
differentiable pressure-implicit solver coded in PyTorch with
Graphics-processing-unit (GPU) support. We first verify the accuracy of both
the forward simulation and our derived gradients in various established
benchmarks like lid-driven cavities and turbulent channel flows before we show
that the gradients provided by our solver can be used to learn complicated
turbulence models in 2D and 3D. We apply both supervised and unsupervised
training regimes using physical priors to match flow statistics. In particular,
we learn a stable sub-grid scale (SGS) model for a 3D turbulent channel flow
purely based on reference statistics. The low-resolution corrector trained with
our solver runs substantially faster than the highly resolved references, while
keeping or even surpassing their accuracy. Finally, we give additional insights
into the physical interpretation of different solver gradients, and motivate a
physically informed regularization technique. To ensure that the full potential
of PICT can be leveraged, it is published as open source:
https://github.com/tum-pbs/PICT.",2025-05-22,"Aleksandra Franz, Hao Wei, Luca Guastoni, Nils Thuerey",http://arxiv.org/pdf/2505.16992v1,cs.LG
Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation,"Out-of-distribution (OOD) detection and segmentation are crucial for
deploying machine learning models in safety-critical applications such as
autonomous driving and robot-assisted surgery. While prior research has
primarily focused on unimodal image data, real-world applications are
inherently multimodal, requiring the integration of multiple modalities for
improved OOD detection. A key challenge is the lack of supervision signals from
unknown data, leading to overconfident predictions on OOD samples. To address
this challenge, we propose Feature Mixing, an extremely simple and fast method
for multimodal outlier synthesis with theoretical support, which can be further
optimized to help the model better distinguish between in-distribution (ID) and
OOD data. Feature Mixing is modality-agnostic and applicable to various
modality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal
dataset for OOD segmentation, featuring synthetic OOD objects across diverse
scenes and weather conditions. Extensive experiments on SemanticKITTI,
nuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that
Feature Mixing achieves state-of-the-art performance with a $10 \times$ to $370
\times$ speedup. Our source code and dataset will be available at
https://github.com/mona4399/FeatureMixing.",2025-05-22,"Moru Liu, Hao Dong, Jessica Kelly, Olga Fink, Mario Trapp",http://arxiv.org/pdf/2505.16985v1,cs.LG
UFT: Unifying Supervised and Reinforcement Fine-Tuning,"Post-training has demonstrated its importance in enhancing the reasoning
capabilities of large language models (LLMs). The primary post-training methods
can be categorized into supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT is efficient and well-suited for small language models,
but it may lead to overfitting and limit the reasoning abilities of larger
models. In contrast, RFT generally yields better generalization but depends
heavily on the strength of the base model. To address the limitations of SFT
and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm
that unifies SFT and RFT into a single, integrated process. UFT enables the
model to effectively explore solutions while incorporating informative
supervision signals, bridging the gap between memorizing and thinking
underlying existing methods. Notably, UFT outperforms both SFT and RFT in
general, regardless of model sizes. Furthermore, we theoretically prove that
UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for
the first time that unified training can exponentially accelerate convergence
on long-horizon reasoning tasks.",2025-05-22,"Mingyang Liu, Gabriele Farina, Asuman Ozdaglar",http://arxiv.org/pdf/2505.16984v1,cs.LG
"CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark","We introduce CASS, the first large-scale dataset and model suite for
cross-architecture GPU code transpilation, targeting both source-level (CUDA
$\leftrightarrow$ HIP) and assembly-level (Nvidia SASS $\leftrightarrow$ AMD
RDNA3) translation. The dataset comprises 70k verified code pairs across host
and device, addressing a critical gap in low-level GPU code portability.
Leveraging this resource, we train the CASS family of domain-specific language
models, achieving 95% source translation accuracy and 37.5% assembly
translation accuracy, substantially outperforming commercial baselines such as
GPT-4o, Claude, and Hipify. Our generated code matches native performance in
over 85% of test cases, preserving runtime and memory behavior. To support
rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16
GPU domains with ground-truth execution. All data, models, and evaluation tools
are released as open source to foster progress in GPU compiler tooling, binary
compatibility, and LLM-guided hardware translation. Dataset and benchmark are
on
\href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}},
with code at
\href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.",2025-05-22,"Ahmed Heakl, Sarim Hashmi, Gustavo Bertolo Stahl, Seung Hun Eddie Han, Salman Khan, Abdulrahman Mahmoud",http://arxiv.org/pdf/2505.16968v2,cs.LG
BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation,"Text segmentation based on the semantic meaning of sentences is a fundamental
task with broad utility in many downstream applications. In this paper, we
propose a graphical model-based unsupervised learning approach, named BP-Seg
for efficient text segmentation. Our method not only considers local coherence,
capturing the intuition that adjacent sentences are often more related, but
also effectively groups sentences that are distant in the text yet semantically
similar. This is achieved through belief propagation on the carefully
constructed graphical models. Experimental results on both an illustrative
example and a dataset with long-form documents demonstrate that our method
performs favorably compared to competing approaches.",2025-05-22,"Fengyi Li, Kayhan Behdin, Natesh Pillai, Xiaofeng Wang, Zhipeng Wang, Ercan Yildiz",http://arxiv.org/pdf/2505.16965v1,cs.LG
Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models,"Diffusion probabilistic models have become a cornerstone of modern generative
AI, yet the mechanisms underlying their generalization remain poorly
understood. In fact, if these models were perfectly minimizing their training
loss, they would just generate data belonging to their training set, i.e.,
memorize, as empirically found in the overparameterized regime. We revisit this
view by showing that, in highly overparameterized diffusion models,
generalization in natural data domains is progressively achieved during
training before the onset of memorization. Our results, ranging from image to
language diffusion models, systematically support the empirical law that
memorization time is proportional to the dataset size. Generalization vs.
memorization is then best understood as a competition between time scales. We
show that this phenomenology is recovered in diffusion models learning a simple
probabilistic context-free grammar with random rules, where generalization
corresponds to the hierarchical acquisition of deeper grammar rules as training
time grows, and the generalization cost of early stopping can be characterized.
We summarize these results in a phase diagram. Overall, our results support
that a principled early-stopping criterion - scaling with dataset size - can
effectively optimize generalization while avoiding memorization, with direct
implications for hyperparameter transfer and privacy-sensitive applications.",2025-05-22,"Alessandro Favero, Antonio Sclocchi, Matthieu Wyart",http://arxiv.org/pdf/2505.16959v1,cs.LG
A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial Optimization,"Machine learning (ML) has demonstrated considerable potential in supporting
model design and optimization for combinatorial optimization (CO) problems.
However, much of the progress to date has been evaluated on small-scale,
synthetic datasets, raising concerns about the practical effectiveness of
ML-based solvers in real-world, large-scale CO scenarios. Additionally, many
existing CO benchmarks lack sufficient training data, limiting their utility
for evaluating data-driven approaches. To address these limitations, we
introduce FrontierCO, a comprehensive benchmark that covers eight canonical CO
problem types and evaluates 16 representative ML-based solvers--including graph
neural networks and large language model (LLM) agents. FrontierCO features
challenging instances drawn from industrial applications and frontier CO
research, offering both realistic problem difficulty and abundant training
data. Our empirical results provide critical insights into the strengths and
limitations of current ML methods, helping to guide more robust and practically
relevant advances at the intersection of machine learning and combinatorial
optimization. Our data is available at
https://huggingface.co/datasets/CO-Bench/FrontierCO.",2025-05-22,"Shengyu Feng, Weiwei Sun, Shanda Li, Ameet Talwalkar, Yiming Yang",http://arxiv.org/pdf/2505.16952v1,cs.LG
ICYM2I: The illusion of multimodal informativeness under missingness,"Multimodal learning is of continued interest in artificial intelligence-based
applications, motivated by the potential information gain from combining
different types of data. However, modalities collected and curated during
development may differ from the modalities available at deployment due to
multiple factors including cost, hardware failure, or -- as we argue in this
work -- the perceived informativeness of a given modality. Na{\""i}ve estimation
of the information gain associated with including an additional modality
without accounting for missingness may result in improper estimates of that
modality's value in downstream tasks. Our work formalizes the problem of
missingness in multimodal learning and demonstrates the biases resulting from
ignoring this process. To address this issue, we introduce ICYM2I (In Case You
Multimodal Missed It), a framework for the evaluation of predictive performance
and information gain under missingness through inverse probability
weighting-based correction. We demonstrate the importance of the proposed
adjustment to estimate information gain under missingness on synthetic,
semi-synthetic, and real-world medical datasets.",2025-05-22,"Young Sang Choi, Vincent Jeanselme, Pierre Elias, Shalmali Joshi",http://arxiv.org/pdf/2505.16953v1,cs.LG
Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning,"Despite their impressive capabilities, Large Language Models struggle with
generalisation beyond their training distribution, often exhibiting
sophisticated pattern interpolation rather than true abstract reasoning
(extrapolation). In this work, we approach this limitation through the lens of
Information Bottleneck (IB) theory, which posits that model generalisation
emerges from an optimal balance between input compression and retention of
predictive information in latent representations. We prove using IB theory that
decoder-only Transformers are inherently constrained in their ability to form
task-optimal sequence representations. We then use this result to demonstrate
that periodic global transformation of the internal sequence-level
representations (KV cache) is a necessary computational step for improving
Transformer generalisation in reasoning tasks. Based on these theoretical
insights, we propose a modification to the Transformer architecture, in the
form of an additional module that globally rewrites the KV cache at periodic
intervals, shifting its capacity away from memorising input prefixes and toward
encoding features most useful for predicting future tokens. Our model delivers
substantial gains on mathematical reasoning benchmarks, outperforming both
vanilla Transformers with up to 3.5x more parameters, as well as
heuristic-driven pruning mechanisms for cache compression. Our approach can be
seen as a principled generalisation of existing KV-cache compression methods;
whereas such methods focus solely on compressing input representations, they
often do so at the expense of retaining predictive information, and thus their
capabilities are inherently bounded by those of an unconstrained model. This
establishes a principled framework to manipulate Transformer memory using
information theory, addressing fundamental reasoning limitations that scaling
alone cannot overcome.",2025-05-22,"Adnan Oomerjee, Zafeirios Fountas, Zhongwei Yu, Haitham Bou-Ammar, Jun Wang",http://arxiv.org/pdf/2505.16950v1,cs.LG
MixAT: Combining Continuous and Discrete Adversarial Training for LLMs,"Despite recent efforts in Large Language Models (LLMs) safety and alignment,
current adversarial attacks on frontier LLMs are still able to force harmful
generations consistently. Although adversarial training has been widely studied
and shown to significantly improve the robustness of traditional machine
learning models, its strengths and weaknesses in the context of LLMs are less
understood. Specifically, while existing discrete adversarial attacks are
effective at producing harmful content, training LLMs with concrete adversarial
prompts is often computationally expensive, leading to reliance on continuous
relaxations. As these relaxations do not correspond to discrete input tokens,
such latent training methods often leave models vulnerable to a diverse set of
discrete attacks. In this work, we aim to bridge this gap by introducing MixAT,
a novel method that combines stronger discrete and faster continuous attacks
during training. We rigorously evaluate MixAT across a wide spectrum of
state-of-the-art attacks, proposing the At Least One Attack Success Rate
(ALO-ASR) metric to capture the worst-case vulnerability of models. We show
MixAT achieves substantially better robustness (ALO-ASR < 20%) compared to
prior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to
methods based on continuous relaxations. We further analyze MixAT in realistic
deployment settings, exploring how chat templates, quantization, low-rank
adapters, and temperature affect both adversarial training and evaluation,
revealing additional blind spots in current methodologies. Our results
demonstrate that MixAT's discrete-continuous defense offers a principled and
superior robustness-accuracy tradeoff with minimal computational overhead,
highlighting its promise for building safer LLMs. We provide our code and
models at https://github.com/insait-institute/MixAT.",2025-05-22,"Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev",http://arxiv.org/pdf/2505.16947v1,cs.LG
NY Real Estate Racial Equity Analysis via Applied Machine Learning,"This study analyzes tract-level real estate ownership patterns in New York
State (NYS) and New York City (NYC) to uncover racial disparities. We use an
advanced race/ethnicity imputation model (LSTM+Geo with XGBoost filtering,
validated at 89.2% accuracy) to compare the predicted racial composition of
property owners to the resident population from census data. We examine both a
Full Model (statewide) and a Name-Only LSTM Model (NYC) to assess how
incorporating geospatial context affects our predictions and disparity
estimates. The results reveal significant inequities: White individuals hold a
disproportionate share of properties and property value relative to their
population, while Black, Hispanic, and Asian communities are underrepresented
as property owners. These disparities are most pronounced in minority-majority
neighborhoods, where ownership is predominantly White despite a predominantly
non-White population. Corporate ownership (LLCs, trusts, etc.) exacerbates
these gaps by reducing owner-occupied opportunities in urban minority
communities. We provide a breakdown of ownership vs. population by race for
majority-White, -Black, -Hispanic, and -Asian tracts, identify those with
extreme ownership disparities, and compare patterns in urban, suburban, and
rural contexts. The findings underscore persistent racial inequity in property
ownership, reflecting broader historical and socio-economic forces, and
highlight the importance of data-driven approaches to address these issues.",2025-05-22,"Sanjana Chalavadi, Andrei Pastor, Terry Leitch",http://arxiv.org/pdf/2505.16946v1,cs.LG
Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical Flow Estimation,"Recent optical flow estimation methods often employ local cost sampling from
a dense all-pairs correlation volume. This results in quadratic computational
and memory complexity in the number of pixels. Although an alternative
memory-efficient implementation with on-demand cost computation exists, this is
slower in practice and therefore prior methods typically process images at
reduced resolutions, missing fine-grained details.
  To address this, we propose a more efficient implementation of the all-pairs
correlation volume sampling, still matching the exact mathematical operator as
defined by RAFT. Our approach outperforms on-demand sampling by up to 90% while
maintaining low memory usage, and performs on par with the default
implementation with up to 95% lower memory usage. As cost sampling makes up a
significant portion of the overall runtime, this can translate to up to 50%
savings for the total end-to-end model inference in memory-constrained
environments. Our evaluation of existing methods includes an 8K
ultra-high-resolution dataset and an additional inference-time modification of
the recent SEA-RAFT method. With this, we achieve state-of-the-art results at
high resolutions both in accuracy and efficiency.",2025-05-22,"Karlis Martins Briedis, Markus Gross, Christopher Schroers",http://arxiv.org/pdf/2505.16942v1,cs.LG
FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records,"Foundation models hold significant promise in healthcare, given their
capacity to extract meaningful representations independent of downstream tasks.
This property has enabled state-of-the-art performance across several clinical
applications trained on structured electronic health record (EHR) data, even in
settings with limited labeled data, a prevalent challenge in healthcare.
However, there is little consensus on these models' potential for clinical
utility due to the lack of desiderata of comprehensive and meaningful tasks and
sufficiently diverse evaluations to characterize the benefit over conventional
supervised learning. To address this gap, we propose a suite of clinically
meaningful tasks spanning patient outcomes, early prediction of acute and
chronic conditions, including desiderata for robust evaluations. We evaluate
state-of-the-art foundation models on EHR data consisting of 5 million patients
from Columbia University Irving Medical Center (CUMC), a large urban academic
medical center in New York City, across 14 clinically relevant tasks. We
measure overall accuracy, calibration, and subpopulation performance to surface
tradeoffs based on the choice of pre-training, tokenization, and data
representation strategies. Our study aims to advance the empirical evaluation
of structured EHR foundation models and guide the development of future
healthcare foundation models.",2025-05-22,"Chao Pang, Vincent Jeanselme, Young Sang Choi, Xinzhuo Jiang, Zilin Jing, Aparajita Kashyap, Yuta Kobayashi, Yanwei Li, Florent Pollet, Karthik Natarajan, Shalmali Joshi",http://arxiv.org/pdf/2505.16941v2,cs.LG
SPAR: Self-supervised Placement-Aware Representation Learning for Multi-Node IoT Systems,"This work develops the underpinnings of self-supervised placement-aware
representation learning given spatially-distributed (multi-view and multimodal)
sensor observations, motivated by the need to represent external environmental
state in multi-sensor IoT systems in a manner that correctly distills spatial
phenomena from the distributed multi-vantage observations. The objective of
sensing in IoT systems is, in general, to collectively represent an externally
observed environment given multiple vantage points from which sensory
observations occur. Pretraining of models that help interpret sensor data must
therefore encode the relation between signals observed by sensors and the
observers' vantage points in order to attain a representation that encodes the
observed spatial phenomena in a manner informed by the specific placement of
the measuring instruments, while allowing arbitrary placement. The work
significantly advances self-supervised model pretraining from IoT signals
beyond current solutions that often overlook the distinctive spatial nature of
IoT data. Our framework explicitly learns the dependencies between measurements
and geometric observer layouts and structural characteristics, guided by a core
design principle: the duality between signals and observer positions. We
further provide theoretical analyses from the perspectives of information
theory and occlusion-invariant representation learning to offer insight into
the rationale behind our design. Experiments on three real-world
datasets--covering vehicle monitoring, human activity recognition, and
earthquake localization--demonstrate the superior generalizability and
robustness of our method across diverse modalities, sensor placements,
application-level inference tasks, and spatial scales.",2025-05-22,"Yizhuo Chen, Tianchen Wang, You Lyu, Yanlan Hu, Jinyang Li, Tomoyoshi Kimura, Hongjue Zhao, Yigong Hu, Denizhan Kara, Tarek Abdelzaher",http://arxiv.org/pdf/2505.16936v2,cs.LG
LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning,"In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large
Language Model (MLLM) that integrates visual instruction tuning with masked
diffusion models, representing a departure from the autoregressive paradigms
dominant in current multimodal approaches. Built upon LLaDA, a representative
large language diffusion model, LLaDA-V incorporates a vision encoder and MLP
connector that projects visual features into the language embedding space,
enabling effective multimodal alignment. Our empirical investigation reveals
several intriguing results: First, LLaDA-V demonstrates promising multimodal
performance despite its language model being weaker on purely textual tasks
than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same
instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal
tasks with better data scalability. It also narrows the performance gap to
Qwen2-VL, suggesting the effectiveness of its architecture for multimodal
tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal
understanding compared to existing hybrid autoregressive-diffusion and purely
diffusion-based MLLMs. Our findings suggest that large language diffusion
models show promise in multimodal contexts and warrant further investigation in
future research. Project page and codes:
https://ml-gsai.github.io/LLaDA-V-demo/.",2025-05-22,"Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, Chongxuan Li",http://arxiv.org/pdf/2505.16933v1,cs.LG
The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm,"Computing the polar decomposition and the related matrix sign function, has
been a well-studied problem in numerical analysis for decades. More recently,
it has emerged as an important subroutine in deep learning, particularly within
the Muon optimization framework. However, the requirements in this setting
differ significantly from those of traditional numerical analysis. In deep
learning, methods must be highly efficient and GPU-compatible, but high
accuracy is often unnecessary. As a result, classical algorithms like
Newton-Schulz (which suffers from slow initial convergence) and methods based
on rational functions (which rely on QR decompositions or matrix inverses) are
poorly suited to this context. In this work, we introduce Polar Express, a
GPU-friendly algorithm for computing the polar decomposition. Like classical
polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix
multiplications, making it GPU-compatible. Motivated by earlier work of Chen &
Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule
at each iteration by solving a minimax optimization problem, and we prove that
it enjoys a strong worst-case optimality guarantee. This property ensures both
rapid early convergence and fast asymptotic convergence. We also address
finite-precision issues, making it stable in bfloat16 in practice. We apply
Polar Express within the Muon optimization framework and show consistent
improvements in validation loss on large-scale models such as GPT-2,
outperforming recent alternatives across a range of learning rates.",2025-05-22,"Noah Amsel, David Persson, Christopher Musco, Robert Gower",http://arxiv.org/pdf/2505.16932v1,cs.LG
"Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning","We introduce $\infty$-THOR, a new framework for long-horizon embodied tasks
that advances long-context understanding in embodied AI. $\infty$-THOR
provides: (1) a generation framework for synthesizing scalable, reproducible,
and unlimited long-horizon trajectories; (2) a novel embodied QA task,
Needle(s) in the Embodied Haystack, where multiple scattered clues across
extended trajectories test agents' long-context reasoning ability; and (3) a
long-horizon dataset and benchmark suite featuring complex tasks that span
hundreds of environment steps, each paired with ground-truth action sequences.
To enable this capability, we explore architectural adaptations, including
interleaved Goal-State-Action modeling, context extension techniques, and
Context Parallelism, to equip LLM-based agents for extreme long-context
reasoning and interaction. Experimental results and analyses highlight the
challenges posed by our benchmark and provide insights into training strategies
and model behaviors under long-horizon conditions. Our work provides a
foundation for the next generation of embodied AI systems capable of robust,
long-term reasoning and planning.",2025-05-22,"Bosung Kim, Prithviraj Ammanabrolu",http://arxiv.org/pdf/2505.16928v1,cs.LG
Latent Principle Discovery for Language Model Self-Improvement,"When language model (LM) users aim to improve the quality of its generations,
it is crucial to specify concrete behavioral attributes that the model should
strive to reflect. However, curating such principles across many domains, even
non-exhaustively, requires a labor-intensive annotation process. To automate
this process, we propose eliciting these latent attributes guiding model
reasoning towards human-preferred responses by explicitly modeling them in a
self-correction setting. Our approach mines new principles from the LM itself
and compresses the discovered elements to an interpretable set via clustering.
Specifically, we employ an approximation of posterior-regularized Monte Carlo
Expectation-Maximization to both identify a condensed set of the most effective
latent principles and teach the LM to strategically invoke them in order to
intrinsically refine its responses. We demonstrate that bootstrapping our
algorithm over multiple iterations enables smaller language models (7-8B
parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an
average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on
IFEval. We also show that clustering the principles yields interpretable and
diverse model-generated constitutions while retaining model performance. The
gains our method achieves highlight the potential of automated,
principle-driven post-training recipes toward continual self-improvement.",2025-05-22,"Keshav Ramji, Tahira Naseem, Ramón Fernandez Astudillo",http://arxiv.org/pdf/2505.16927v1,cs.LG
Risk-Averse Reinforcement Learning with Itakura-Saito Loss,"Risk-averse reinforcement learning finds application in various high-stakes
fields. Unlike classical reinforcement learning, which aims to maximize
expected returns, risk-averse agents choose policies that minimize risk,
occasionally sacrificing expected value. These preferences can be framed
through utility theory. We focus on the specific case of the exponential
utility function, where one can derive the Bellman equations and employ various
reinforcement learning algorithms with few modifications. To address this, we
introduce to the broad machine learning community a numerically stable and
mathematically sound loss function based on the Itakura-Saito divergence for
learning state-value and action-value functions. We evaluate the Itakura-Saito
loss function against established alternatives, both theoretically and
empirically. In the experimental section, we explore multiple scenarios, some
with known analytical solutions, and show that the considered loss function
outperforms the alternatives.",2025-05-22,"Igor Udovichenko, Olivier Croissant, Anita Toleutaeva, Evgeny Burnaev, Alexander Korotin",http://arxiv.org/pdf/2505.16925v2,cs.LG
TULiP: Test-time Uncertainty Estimation via Linearization and Weight Perturbation,"A reliable uncertainty estimation method is the foundation of many modern
out-of-distribution (OOD) detectors, which are critical for safe deployments of
deep learning models in the open world. In this work, we propose TULiP, a
theoretically-driven post-hoc uncertainty estimator for OOD detection. Our
approach considers a hypothetical perturbation applied to the network before
convergence. Based on linearized training dynamics, we bound the effect of such
perturbation, resulting in an uncertainty score computable by perturbing model
parameters. Ultimately, our approach computes uncertainty from a set of sampled
predictions. We visualize our bound on synthetic regression and classification
datasets. Furthermore, we demonstrate the effectiveness of TULiP using
large-scale OOD detection benchmarks for image classification. Our method
exhibits state-of-the-art performance, particularly for near-distribution
samples.",2025-05-22,"Yuhui Zhang, Dongshen Wu, Yuichiro Wada, Takafumi Kanamori",http://arxiv.org/pdf/2505.16923v2,cs.LG
Scalable and Interpretable Contextual Bandits: A Literature Review and Retail Offer Prototype,"This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB)
methods and introduces an experimental framework for scalable, interpretable
offer selection, addressing the challenge of fast-changing offers. The approach
models context at the product category level, allowing offers to span multiple
categories and enabling knowledge transfer across similar offers. This improves
learning efficiency and generalization in dynamic environments. The framework
extends standard CMAB methodology to support multi-category contexts, and
achieves scalability through efficient feature engineering and modular design.
Advanced features such as MPG (Member Purchase Gap) and MF (Matrix
Factorization) capture nuanced user-offer interactions, with implementation in
Python for practical deployment.
  A key contribution is interpretability at scale: logistic regression models
yield transparent weight vectors, accessible via a large language model (LLM)
interface for real-time, user-level tracking and explanation of evolving
preferences. This enables the generation of detailed member profiles and
identification of behavioral patterns, supporting personalized offer
optimization and enhancing trust in automated decisions. By situating our
prototype alongside established paradigms like Generalized Linear Models and
Thompson Sampling, we demonstrate its value for both research and real-world
CMAB applications.",2025-05-22,"Nikola Tankovic, Robert Sajina",http://arxiv.org/pdf/2505.16918v1,cs.LG
Unsupervised Prompting for Graph Neural Networks,"Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to
address the semantic gap between pre-training and fine-tuning steps. However,
existing GNN prompting methods rely on labeled data and involve lightweight
fine-tuning for downstream tasks. Meanwhile, in-context learning methods for
Large Language Models (LLMs) have shown promising performance with no parameter
updating and no or minimal labeled data. Inspired by these approaches, in this
work, we first introduce a challenging problem setup to evaluate GNN prompting
methods. This setup encourages a prompting function to enhance a pre-trained
GNN's generalization to a target dataset under covariate shift without updating
the GNN's parameters and with no labeled data. Next, we propose a fully
unsupervised prompting method based on consistency regularization through
pseudo-labeling. We use two regularization techniques to align the prompted
graphs' distribution with the original data and reduce biased predictions.
Through extensive experiments under our problem setting, we demonstrate that
our unsupervised approach outperforms the state-of-the-art prompting methods
that have access to labels.",2025-05-22,"Peyman Baghershahi, Sourav Medya",http://arxiv.org/pdf/2505.16903v1,cs.LG
Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks,"Recent advances in Large Language Models (LLMs) have shown promise in
function-level code generation, yet repository-level software engineering tasks
remain challenging. Current solutions predominantly rely on proprietary LLM
agents, which introduce unpredictability and limit accessibility, raising
concerns about data privacy and model customization. This paper investigates
whether open-source LLMs can effectively address repository-level tasks without
requiring agent-based approaches. We demonstrate this is possible by enabling
LLMs to comprehend functions and files within codebases through their semantic
information and structural dependencies. To this end, we introduce Code Graph
Models (CGMs), which integrate repository code graph structures into the LLM's
attention mechanism and map node attributes to the LLM's input space using a
specialized adapter. When combined with an agentless graph RAG framework, our
approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark
using the open-source Qwen2.5-72B model. This performance ranks first among
open weight models, second among methods with open-source systems, and eighth
overall, surpassing the previous best open-source model-based method by 12.33%.",2025-05-22,"Hongyuan Tao, Ying Zhang, Zhenhao Tang, Hongen Peng, Xukun Zhu, Bingchang Liu, Yingguang Yang, Ziyin Zhang, Zhaogui Xu, Haipeng Zhang, Linchao Zhu, Rui Wang, Hang Yu, Jianguo Li, Peng Di",http://arxiv.org/pdf/2505.16901v1,cs.LG
Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality,"During the finetuning stage of text generation tasks, standard cross-entropy
loss treats all tokens equally. This can lead models to overemphasize
high-frequency, low-information tokens, neglecting lower-frequency tokens
crucial for specificity and informativeness in generated content. This paper
introduces a novel loss function, Power-Law Decay Loss (PDL), specifically
designed to optimize the finetuning process for text generation. The core
motivation for PDL stems from observations in information theory and
linguistics: the informativeness of a token is often inversely proportional to
its frequency of occurrence. PDL re-weights the contribution of each token in
the standard cross-entropy loss based on its frequency in the training corpus,
following a power-law decay. Specifically, the weights for high-frequency
tokens are reduced, while low-frequency, information-dense tokens are assigned
higher weights. This mechanism guides the model during finetuning to focus more
on learning and generating tokens that convey specific and unique information,
thereby enhancing the quality, diversity, and informativeness of the generated
text. We theoretically elaborate on the motivation and construction of PDL and
discuss its potential applications and advantages across various text
generation finetuning tasks, such as abstractive summarization, dialogue
systems, and style transfer.",2025-05-22,"Jintian Shao, Yiming Cheng, Hongyi Huang, Jiayi Wu, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng",http://arxiv.org/pdf/2505.16900v2,cs.LG
Structure-Aligned Protein Language Model,"Protein language models (pLMs) pre-trained on vast protein sequence databases
excel at various downstream tasks but lack the structural knowledge essential
for many biological applications. To address this, we integrate structural
insights from pre-trained protein graph neural networks (pGNNs) into pLMs
through a latent-level contrastive learning task. This task aligns residue
representations from pLMs with those from pGNNs across multiple proteins,
enriching pLMs with inter-protein structural knowledge. Additionally, we
incorporate a physical-level task that infuses intra-protein structural
knowledge by optimizing pLMs to predict structural tokens. The proposed
dual-task framework effectively incorporates both inter-protein and
intra-protein structural knowledge into pLMs. Given the variability in the
quality of protein structures in PDB, we further introduce a residue loss
selection module, which uses a small model trained on high-quality structures
to select reliable yet challenging residue losses for the pLM to learn.
Applying our structure alignment method to the state-of-the-art ESM2 and
AMPLIFY results in notable performance gains across a wide range of tasks,
including a 12.7% increase in ESM2 contact prediction. The data, code, and
resulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.",2025-05-22,"Can Chen, David Heurtel-Depeiges, Robert M. Vernon, Christopher James Langmead, Yoshua Bengio, Quentin Fournier",http://arxiv.org/pdf/2505.16896v1,cs.LG
Statistical Test for Saliency Maps of Graph Neural Networks via Selective Inference,"Graph Neural Networks (GNNs) have gained prominence for their ability to
process graph-structured data across various domains. However, interpreting GNN
decisions remains a significant challenge, leading to the adoption of saliency
maps for identifying influential nodes and edges. Despite their utility, the
reliability of GNN saliency maps has been questioned, particularly in terms of
their robustness to noise. In this study, we propose a statistical testing
framework to rigorously evaluate the significance of saliency maps. Our main
contribution lies in addressing the inflation of the Type I error rate caused
by double-dipping of data, leveraging the framework of Selective Inference. Our
method provides statistically valid $p$-values while controlling the Type I
error rate, ensuring that identified salient subgraphs contain meaningful
information rather than random artifacts. To demonstrate the effectiveness of
our method, we conduct experiments on both synthetic and real-world datasets,
showing its effectiveness in assessing the reliability of GNN interpretations.",2025-05-22,"Shuichi Nishino, Tomohiro Shiraishi, Teruyuki Katsuoka, Ichiro Takeuchi",http://arxiv.org/pdf/2505.16893v1,cs.LG
"Don't ""Overthink"" Passage Reranking: Is Reasoning Truly Necessary?","With the growing success of reasoning models across complex natural language
tasks, researchers in the Information Retrieval (IR) community have begun
exploring how similar reasoning capabilities can be integrated into passage
rerankers built on Large Language Models (LLMs). These methods typically employ
an LLM to produce an explicit, step-by-step reasoning process before arriving
at a final relevance prediction. But, does reasoning actually improve reranking
accuracy? In this paper, we dive deeper into this question, studying the impact
of the reasoning process by comparing reasoning-based pointwise rerankers
(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under
identical training conditions, and observe that StandardRR generally
outperforms ReasonRR. Building on this observation, we then study the
importance of reasoning to ReasonRR by disabling its reasoning process
(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more
effective than ReasonRR. Examining the cause of this result, our findings
reveal that reasoning-based rerankers are limited by the LLM's reasoning
process, which pushes it toward polarized relevance scores and thus fails to
consider the partial relevance of passages, a key factor for the accuracy of
pointwise rerankers.",2025-05-22,"Nour Jedidi, Yung-Sung Chuang, James Glass, Jimmy Lin",http://arxiv.org/pdf/2505.16886v1,cs.LG
How high is `high'? Rethinking the roles of dimensionality in topological data analysis and manifold learning,"We present a generalised Hanson-Wright inequality and use it to establish new
statistical insights into the geometry of data point-clouds. In the setting of
a general random function model of data, we clarify the roles played by three
notions of dimensionality: ambient intrinsic dimension $p_{\mathrm{int}}$,
which measures total variability across orthogonal feature directions;
correlation rank, which measures functional complexity across samples; and
latent intrinsic dimension, which is the dimension of manifold structure hidden
in data. Our analysis shows that in order for persistence diagrams to reveal
latent homology and for manifold structure to emerge it is sufficient that
$p_{\mathrm{int}}\gg \log n$, where $n$ is the sample size. Informed by these
theoretical perspectives, we revisit the ground-breaking neuroscience discovery
of toroidal structure in grid-cell activity made by Gardner et al. (Nature,
2022): our findings reveal, for the first time, evidence that this structure is
in fact isometric to physical space, meaning that grid cell activity conveys a
geometrically faithful representation of the real world.",2025-05-22,"Hannah Sansford, Nick Whiteley, Patrick Rubin-Delanchy",http://arxiv.org/pdf/2505.16879v1,cs.LG
T2I-ConBench: Text-to-Image Benchmark for Continual Post-training,"Continual post-training adapts a single text-to-image diffusion model to
learn new tasks without incurring the cost of separate models, but naive
post-training causes forgetting of pretrained knowledge and undermines
zero-shot compositionality. We observe that the absence of a standardized
evaluation protocol hampers related research for continual post-training. To
address this, we introduce T2I-ConBench, a unified benchmark for continual
post-training of text-to-image models. T2I-ConBench focuses on two practical
scenarios, item customization and domain enhancement, and analyzes four
dimensions: (1) retention of generality, (2) target-task performance, (3)
catastrophic forgetting, and (4) cross-task generalization. It combines
automated metrics, human-preference modeling, and vision-language QA for
comprehensive assessment. We benchmark ten representative methods across three
realistic task sequences and find that no approach excels on all fronts. Even
joint ""oracle"" training does not succeed for every task, and cross-task
generalization remains unsolved. We release all datasets, code, and evaluation
tools to accelerate research in continual post-training for text-to-image
models.",2025-05-22,"Zhehao Huang, Yuhang Liu, Yixin Lou, Zhengbao He, Mingzhen He, Wenxing Zhou, Tao Li, Kehan Li, Zeyi Huang, Xiaolin Huang",http://arxiv.org/pdf/2505.16875v1,cs.LG
A Multi-Step Comparative Framework for Anomaly Detection in IoT Data Streams,"The rapid expansion of Internet of Things (IoT) devices has introduced
critical security challenges, underscoring the need for accurate anomaly
detection. Although numerous studies have proposed machine learning (ML)
methods for this purpose, limited research systematically examines how
different preprocessing steps--normalization, transformation, and feature
selection--interact with distinct model architectures. To address this gap,
this paper presents a multi-step evaluation framework assessing the combined
impact of preprocessing choices on three ML algorithms: RNN-LSTM, autoencoder
neural networks (ANN), and Gradient Boosting (GBoosting). Experiments on the
IoTID20 dataset shows that GBoosting consistently delivers superior accuracy
across preprocessing configurations, while RNN-LSTM shows notable gains with
z-score normalization and autoencoders excel in recall, making them well-suited
for unsupervised scenarios. By offering a structured analysis of preprocessing
decisions and their interplay with various ML techniques, the proposed
framework provides actionable guidance to enhance anomaly detection performance
in IoT environments.",2025-05-22,"Mohammed Al-Qudah, Fadi AlMahamid",http://arxiv.org/pdf/2505.16872v1,cs.LG
GCAL: Adapting Graph Models to Evolving Domain Shifts,"This paper addresses the challenge of graph domain adaptation on evolving,
multiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation
methods are confined to single-step adaptation, making them ineffective in
handling continuous domain shifts and prone to catastrophic forgetting. This
paper introduces the Graph Continual Adaptive Learning (GCAL) method, designed
to enhance model sustainability and adaptability across various graph domains.
GCAL employs a bilevel optimization strategy. The ""adapt"" phase uses an
information maximization approach to fine-tune the model with new graph domains
while re-adapting past memories to mitigate forgetting. Concurrently, the
""generate memory"" phase, guided by a theoretical lower bound derived from
information bottleneck theory, involves a variational memory graph generation
module to condense original graphs into memories. Extensive experimental
evaluations demonstrate that GCAL substantially outperforms existing methods in
terms of adaptability and knowledge retention.",2025-05-22,"Ziyue Qiao, Qianyi Cai, Hao Dong, Jiawei Gu, Pengyang Wang, Meng Xiao, Xiao Luo, Hui Xiong",http://arxiv.org/pdf/2505.16860v1,cs.LG
Redefining Clustered Federated Learning for System Identification: The Path of ClusterCraft,"This paper addresses the System Identification (SYSID) problem within the
framework of federated learning. We introduce a novel algorithm, Incremental
Clustering-based federated learning method for SYSID (IC-SYSID), designed to
tackle SYSID challenges across multiple data sources without prior knowledge.
IC-SYSID utilizes an incremental clustering method, ClusterCraft (CC), to
eliminate the dependency on the prior knowledge of the dataset. CC starts with
a single cluster model and assigns similar local workers to the same clusters
by dynamically increasing the number of clusters. To reduce the number of
clusters generated by CC, we introduce ClusterMerge, where similar cluster
models are merged. We also introduce enhanced ClusterCraft to reduce the
generation of similar cluster models during the training. Moreover, IC-SYSID
addresses cluster model instability by integrating a regularization term into
the loss function and initializing cluster models with scaled Glorot
initialization. It also utilizes a mini-batch deep learning approach to manage
large SYSID datasets during local training. Through the experiments conducted
on a real-world representing SYSID problem, where a fleet of vehicles
collaboratively learns vehicle dynamics, we show that IC-SYSID achieves a high
SYSID performance while preventing the learning of unstable clusters.",2025-05-22,"Ertuğrul Keçeci, Müjde Güzelkaya, Tufan Kumbasar",http://arxiv.org/pdf/2505.16857v1,cs.LG
Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only,"Improving the performance of pre-trained policies through online
reinforcement learning (RL) is a critical yet challenging topic. Existing
online RL fine-tuning methods require continued training with offline
pretrained Q-functions for stability and performance. However, these offline
pretrained Q-functions commonly underestimate state-action pairs beyond the
offline dataset due to the conservatism in most offline RL methods, which
hinders further exploration when transitioning from the offline to the online
setting. Additionally, this requirement limits their applicability in scenarios
where only pre-trained policies are available but pre-trained Q-functions are
absent, such as in imitation learning (IL) pre-training. To address these
challenges, we propose a method for efficient online RL fine-tuning using
solely the offline pre-trained policy, eliminating reliance on pre-trained
Q-functions. We introduce PORL (Policy-Only Reinforcement Learning
Fine-Tuning), which rapidly initializes the Q-function from scratch during the
online phase to avoid detrimental pessimism. Our method not only achieves
competitive performance with advanced offline-to-online RL algorithms and
online RL approaches that leverage data or policies prior, but also pioneers a
new path for directly fine-tuning behavior cloning (BC) policies.",2025-05-22,"Wei Xiao, Jiacheng Liu, Zifeng Zhuang, Runze Suo, Shangke Lyu, Donglin Wang",http://arxiv.org/pdf/2505.16856v1,cs.LG
"ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning","Federated Learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data privacy across decentralized participants.
As FL adoption grows, numerous techniques have been proposed to tackle its
practical challenges. However, the lack of standardized evaluation across key
dimensions hampers systematic progress and fair comparison of FL methods. In
this work, we introduce ATR-Bench, a unified framework for analyzing federated
learning through three foundational dimensions: Adaptation, Trust, and
Reasoning. We provide an in-depth examination of the conceptual foundations,
task formulations, and open research challenges associated with each theme. We
have extensively benchmarked representative methods and datasets for adaptation
to heterogeneous clients and trustworthiness in adversarial or unreliable
environments. Due to the lack of reliable metrics and models for reasoning in
FL, we only provide literature-driven insights for this dimension. ATR-Bench
lays the groundwork for a systematic and holistic evaluation of federated
learning with real-world relevance. We will make our complete codebase publicly
accessible and a curated repository that continuously tracks new developments
and research in the FL literature.",2025-05-22,"Tajamul Ashraf, Mohammed Mohsen Peerzada, Moloud Abdar, Yutong Xie, Yuyin Zhou, Xiaofeng Liu, Iqra Altaf Gillani, Janibul Bashir",http://arxiv.org/pdf/2505.16850v1,cs.LG
Strategically Linked Decisions in Long-Term Planning and Reinforcement Learning,"Long-term planning, as in reinforcement learning (RL), involves finding
strategies: actions that collectively work toward a goal rather than
individually optimizing their immediate outcomes. As part of a strategy, some
actions are taken at the expense of short-term benefit to enable future actions
with even greater returns. These actions are only advantageous if followed up
by the actions they facilitate, consequently, they would not have been taken if
those follow-ups were not available. In this paper, we quantify such
dependencies between planned actions with strategic link scores: the drop in
the likelihood of one decision under the constraint that a follow-up decision
is no longer available. We demonstrate the utility of strategic link scores
through three practical applications: (i) explaining black-box RL agents by
identifying strategically linked pairs among decisions they make, (ii)
improving the worst-case performance of decision support systems by
distinguishing whether recommended actions can be adopted as standalone
improvements or whether they are strategically linked hence requiring a
commitment to a broader strategy to be effective, and (iii) characterizing the
planning processes of non-RL agents purely through interventions aimed at
measuring strategic link scores - as an example, we consider a realistic
traffic simulator and analyze through road closures the effective planning
horizon of the emergent routing behavior of many drivers.",2025-05-22,"Alihan Hüyük, Finale Doshi-Velez",http://arxiv.org/pdf/2505.16833v1,cs.LG
From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization,"While foundation models (FMs), such as diffusion models and large
vision-language models (LVLMs), have been widely applied in educational
contexts, their ability to generate pedagogically effective visual explanations
remains limited. Most existing approaches focus primarily on textual reasoning,
overlooking the critical role of structured and interpretable visualizations in
supporting conceptual understanding. To better assess the visual reasoning
capabilities of FMs in educational settings, we introduce EduVisBench, a
multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem
sets requiring visually grounded solutions, along with a fine-grained
evaluation rubric informed by pedagogical theory. Our empirical analysis
reveals that existing models frequently struggle with the inherent challenge of
decomposing complex reasoning and translating it into visual representations
aligned with human cognitive processes. To address these limitations, we
propose EduVisAgent, a multi-agent collaborative framework that coordinates
specialized agents for instructional planning, reasoning decomposition,
metacognitive prompting, and visualization design. Experimental results show
that EduVisAgent substantially outperforms all baselines, achieving a 40.2%
improvement and delivering more educationally aligned visualizations.
EduVisBench and EduVisAgent are available at
https://github.com/aiming-lab/EduVisBench and
https://github.com/aiming-lab/EduVisAgent.",2025-05-22,"Haonian Ji, Shi Qiu, Siyang Xin, Siwei Han, Zhaorun Chen, Hongyi Wang, Dake Zhang, Huaxiu Yao",http://arxiv.org/pdf/2505.16832v1,cs.LG
Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs,"Unlearning in large language models (LLMs) is intended to remove the
influence of specific data, yet current evaluations rely heavily on token-level
metrics such as accuracy and perplexity. We show that these metrics can be
misleading: models often appear to forget, but their original behavior can be
rapidly restored with minimal fine-tuning, revealing that unlearning may
obscure information rather than erase it. To diagnose this phenomenon, we
introduce a representation-level evaluation framework using PCA-based
similarity and shift, centered kernel alignment, and Fisher information.
Applying this toolkit across six unlearning methods, three domains (text, code,
math), and two open-source LLMs, we uncover a critical distinction between
reversible and irreversible forgetting. In reversible cases, models suffer
token-level collapse yet retain latent features; in irreversible cases, deeper
representational damage occurs. We further provide a theoretical account
linking shallow weight perturbations near output layers to misleading
unlearning signals, and show that reversibility is modulated by task type and
hyperparameters. Our findings reveal a fundamental gap in current evaluation
practices and establish a new diagnostic foundation for trustworthy unlearning
in LLMs. We provide a unified toolkit for analyzing LLM representation changes
under unlearning and relearning:
https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.",2025-05-22,"Xiaoyu Xu, Xiang Yue, Yang Liu, Qingqing Ye, Haibo Hu, Minxin Du",http://arxiv.org/pdf/2505.16831v1,cs.LG
Contextual Learning for Stochastic Optimization,"Motivated by stochastic optimization, we introduce the problem of learning
from samples of contextual value distributions. A contextual value distribution
can be understood as a family of real-valued distributions, where each sample
consists of a context $x$ and a random variable drawn from the corresponding
real-valued distribution $D_x$. By minimizing a convex surrogate loss, we learn
an empirical distribution $D'_x$ for each context, ensuring a small L\'evy
distance to $D_x$. We apply this result to obtain the sample complexity bounds
for the learning of an $\epsilon$-optimal policy for stochastic optimization
problems defined on an unknown contextual value distribution. The sample
complexity is shown to be polynomial for the general case of strongly monotone
and stable optimization problems, including Single-item Revenue Maximization,
Pandora's Box and Optimal Stopping.",2025-05-22,"Anna Heuser, Thomas Kesselheim",http://arxiv.org/pdf/2505.16829v1,cs.LG
LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols,"Integrating large AI models (LAMs) into 6G mobile networks promises to
redefine protocol design and control-plane intelligence by enabling autonomous,
cognitive network operations. While industry concepts, such as ETSI's
Experiential Networked Intelligence (ENI), envision LAM-driven agents for
adaptive network slicing and intent-based management, practical implementations
still face challenges in protocol literacy and real-world deployment. This
paper presents an end-to-end demonstration of a LAM that generates
standards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as
part of control-plane procedures inside a gNB. We treat RRC messaging as a
domain-specific language and fine-tune a decoder-only transformer model (LLaMA
class) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages
linearized to retain their ASN.1 syntactic structure before standard byte-pair
encoding tokenization. This enables combinatorial generalization over RRC
protocol states while minimizing training overhead. On 30k field-test
request-response pairs, our 8 B model achieves a median cosine similarity of
0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a
zero-shot LLaMA-3 8B baseline -- indicating substantially improved structural
and semantic RRC fidelity. Overall, our results show that LAMs, when augmented
with Radio Access Network (RAN)-specific reasoning, can directly orchestrate
control-plane procedures, representing a stepping stone toward the AI-native
air-interface paradigm. Beyond RRC emulation, this work lays the groundwork for
future AI-native wireless standards.",2025-05-22,"Ziming Liu, Bryan Liu, Alvaro Valcarce, Xiaoli Chu",http://arxiv.org/pdf/2505.16821v2,cs.LG
A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents,"Serious Games (SGs) are nowadays shifting focus to include procedural content
generation (PCG) in the development process as a means of offering personalized
and enhanced player experience. However, the development of a framework to
assess the impact of PCG techniques when integrated into SGs remains
particularly challenging. This study proposes a methodology for automated
evaluation of PCG integration in SGs, incorporating deep reinforcement learning
(DRL) game testing agents. To validate the proposed framework, a previously
introduced SG featuring card game mechanics and incorporating three different
versions of PCG for nonplayer character (NPC) creation has been deployed.
Version 1 features random NPC creation, while versions 2 and 3 utilize a
genetic algorithm approach. These versions are used to test the impact of
different dynamic SG environments on the proposed framework's agents. The
obtained results highlight the superiority of the DRL game testing agents
trained on Versions 2 and 3 over those trained on Version 1 in terms of win
rate (i.e. number of wins per played games) and training time. More
specifically, within the execution of a test emulating regular gameplay, both
Versions 2 and 3 peaked at a 97% win rate and achieved statistically
significant higher (p=0009) win rates compared to those achieved in Version 1
that peaked at 94%. Overall, results advocate towards the proposed framework's
capability to produce meaningful data for the evaluation of procedurally
generated content in SGs.",2025-05-22,"Eleftherios Kalafatis, Konstantinos Mitsis, Konstantia Zarkogianni, Maria Athanasiou, Konstantina Nikita",http://arxiv.org/pdf/2505.16801v1,cs.LG
Cohort-Based Active Modality Acquisition,"Real-world machine learning applications often involve data from multiple
modalities that must be integrated effectively to make robust predictions.
However, in many practical settings, not all modalities are available for every
sample, and acquiring additional modalities can be costly. This raises the
question: which samples should be prioritized for additional modality
acquisition when resources are limited? While prior work has explored
individual-level acquisition strategies and training-time active learning
paradigms, test-time and cohort-based acquisition remain underexplored despite
their importance in many real-world settings. We introduce Cohort-based Active
Modality Acquisition (CAMA), a novel test-time setting to formalize the
challenge of selecting which samples should receive additional modalities. We
derive acquisition strategies that leverage a combination of generative
imputation and discriminative modeling to estimate the expected benefit of
acquiring missing modalities based on common evaluation metrics. We also
introduce upper-bound heuristics that provide performance ceilings to benchmark
acquisition strategies. Experiments on common multimodal datasets demonstrate
that our proposed imputation-based strategies can more effectively guide the
acquisition of new samples in comparison to those relying solely on unimodal
information, entropy guidance, and random selections. Our work provides an
effective solution for optimizing modality acquisition at the cohort level,
enabling better utilization of resources in constrained settings.",2025-05-22,"Tillmann Rheude, Roland Eils, Benjamin Wild",http://arxiv.org/pdf/2505.16791v1,cs.LG
Learning Flexible Forward Trajectories for Masked Molecular Diffusion,"Masked diffusion models (MDMs) have achieved notable progress in modeling
discrete data, while their potential in molecular generation remains
underexplored. In this work, we explore their potential and introduce the
surprising result that naively applying standards MDMs severely degrades the
performance. We identify the critical cause of this issue as a state-clashing
problem-where the forward diffusion of distinct molecules collapse into a
common state, resulting in a mixture of reconstruction targets that cannot be
learned using typical reverse diffusion process with unimodal predictions. To
mitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that
orchestrates per-element corruption trajectories to avoid collision between
distinct molecular graphs. This is achieved through a parameterized noise
scheduling network that assigns distinct corruption rates to individual graph
elements, i.e., atoms and bonds. Extensive experiments on diverse molecular
benchmarks reveal that MELD markedly enhances overall generation quality
compared to element-agnostic noise scheduling, increasing the chemical validity
of vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves
state-of-the-art property alignment in conditional generation tasks.",2025-05-22,"Hyunjin Seo, Taewon Kim, Sihyun Yu, SungSoo Ahn",http://arxiv.org/pdf/2505.16790v2,cs.LG
Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability,"As large language models gain popularity, their vulnerability to adversarial
attacks remains a primary concern. While fine-tuning models on domain-specific
datasets is often employed to improve model performance, it can introduce
vulnerabilities within the underlying model. In this work, we investigate
Accidental Misalignment, unexpected vulnerabilities arising from
characteristics of fine-tuning data. We begin by identifying potential
correlation factors such as linguistic features, semantic similarity, and
toxicity within our experimental datasets. We then evaluate the adversarial
performance of these fine-tuned models and assess how dataset factors correlate
with attack success rates. Lastly, we explore potential causal links, offering
new insights into adversarial defense strategies and highlighting the crucial
role of dataset design in preserving model alignment. Our code is available at
https://github.com/psyonp/accidental_misalignment.",2025-05-22,"Punya Syon Pandey, Samuel Simko, Kellin Pelrine, Zhijing Jin",http://arxiv.org/pdf/2505.16789v1,cs.LG
FlowMixer: A Constrained Neural Architecture for Interpretable Spatiotemporal Forecasting,"We introduce FlowMixer, a neural architecture that leverages constrained
matrix operations to model structured spatiotemporal patterns. At its core,
FlowMixer incorporates non-negative matrix mixing layers within a reversible
mapping framework-applying transforms before mixing and their inverses
afterward. This shape-preserving design enables a Kronecker-Koopman eigenmode
framework that bridges statistical learning with dynamical systems theory,
providing interpretable spatiotemporal patterns and facilitating direct
algebraic manipulation of prediction horizons without retraining. Extensive
experiments across diverse domains demonstrate FlowMixer's robust long-horizon
forecasting capabilities while effectively modeling physical phenomena such as
chaotic attractors and turbulent flows. These results suggest that
architectural constraints can simultaneously enhance predictive performance and
mathematical interpretability in neural forecasting systems.",2025-05-22,"Fares B. Mehouachi, Saif Eddin Jabari",http://arxiv.org/pdf/2505.16786v1,cs.LG
OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning,"Recent advancements in multimodal slow-thinking systems have demonstrated
remarkable performance across diverse visual reasoning tasks. However, their
capabilities in text-rich image reasoning tasks remain understudied due to the
lack of a systematic benchmark. To address this gap, we propose OCR-Reasoning,
a comprehensive benchmark designed to systematically assess Multimodal Large
Language Models on text-rich image reasoning tasks. The benchmark comprises
1,069 human-annotated examples spanning 6 core reasoning abilities and 18
practical reasoning tasks in text-rich visual scenarios. Furthermore, unlike
other text-rich image understanding benchmarks that only annotate the final
answers, OCR-Reasoning also annotates the reasoning process simultaneously.
With the annotated reasoning process and the final answers, OCR-Reasoning
evaluates not only the final answers generated by models but also their
reasoning processes, enabling a holistic analysis of their problem-solving
abilities. Leveraging this benchmark, we conducted a comprehensive evaluation
of state-of-the-art MLLMs. Our results demonstrate the limitations of existing
methodologies. Notably, even state-of-the-art MLLMs exhibit substantial
difficulties, with none achieving accuracy surpassing 50\% across
OCR-Reasoning, indicating that the challenges of text-rich image reasoning are
an urgent issue to be addressed. The benchmark and evaluation scripts are
available at https://github.com/SCUT-DLVCLab/OCR-Reasoning.",2025-05-22,"Mingxin Huang, Yongxin Shi, Dezhi Peng, Songxuan Lai, Zecheng Xie, Lianwen Jin",http://arxiv.org/pdf/2505.17163v1,cs.LG
Multi-Output Gaussian Processes for Graph-Structured Data,"Graph-structured data is a type of data to be obtained associated with a
graph structure where vertices and edges describe some kind of data
correlation. This paper proposes a regression method on graph-structured data,
which is based on multi-output Gaussian processes (MOGP), to capture both the
correlation between vertices and the correlation between associated data. The
proposed formulation is built on the definition of MOGP. This allows it to be
applied to a wide range of data configurations and scenarios. Moreover, it has
high expressive capability due to its flexibility in kernel design. It includes
existing methods of Gaussian processes for graph-structured data as special
cases and is possible to remove restrictions on data configurations, model
selection, and inference scenarios in the existing methods. The performance of
extensions achievable by the proposed formulation is evaluated through computer
experiments with synthetic and real data.",2025-05-22,"Ayano Nakai-Kasai, Tadashi Wadayama",http://arxiv.org/pdf/2505.16755v1,cs.LG
PyTupli: A Scalable Infrastructure for Collaborative Offline Reinforcement Learning Projects,"Offline reinforcement learning (RL) has gained traction as a powerful
paradigm for learning control policies from pre-collected data, eliminating the
need for costly or risky online interactions. While many open-source libraries
offer robust implementations of offline RL algorithms, they all rely on
datasets composed of experience tuples consisting of state, action, next state,
and reward. Managing, curating, and distributing such datasets requires
suitable infrastructure. Although static datasets exist for established
benchmark problems, no standardized or scalable solution supports developing
and sharing datasets for novel or user-defined benchmarks. To address this gap,
we introduce PyTupli, a Python-based tool to streamline the creation, storage,
and dissemination of benchmark environments and their corresponding tuple
datasets. PyTupli includes a lightweight client library with defined interfaces
for uploading and retrieving benchmarks and data. It supports fine-grained
filtering at both the episode and tuple level, allowing researchers to curate
high-quality, task-specific datasets. A containerized server component enables
production-ready deployment with authentication, access control, and automated
certificate provisioning for secure use. By addressing key barriers in dataset
infrastructure, PyTupli facilitates more collaborative, reproducible, and
scalable offline RL research.",2025-05-22,"Hannah Markgraf, Michael Eichelbeck, Daria Cappey, Selin Demirtürk, Yara Schattschneider, Matthias Althoff",http://arxiv.org/pdf/2505.16754v2,cs.LG
Revenue Optimization with Price-Sensitive and Interdependent Demand,"As Kalyan T. Talluri and Garrett J. Van Ryzin describe in their work [3],
Revenue Management aims to maximize an organization's revenue by considering
three types of decision categories: structural, pricing, and quantity. In this
document, our primary focus will be on decisions related to pricing and
quantity for the sale of airline tickets on a direct flight over a certain
number of time periods. More specifically, we will only focus on the
optimization aspect of this problem. We will assume the demand data to be
given, since Air France estimates it beforehand using real data. Similarly, we
assume all price options to be predetermined by Air France's algorithms and
verified by their analysts. Our objective will be to maximize the revenue of a
direct flight by choosing the prices for each product from the predefined set
of options.
  --
  Comme d\'ecrit par Kalyan T. Talluri et Garrett J. Van Ryzin dans leur
ouvrage [3], le Revenue Management consiste en la maximisation du revenu d'un
organisme \`a partir de trois types de cat\'egories de d\'ecision :
structurelles, prix et quantit\'e. Dans ce document, nous nous int\'eresserons
principalement aux d\'ecisions de type prix et quantit\'e pour la vente de
billets d'avion sur un vol direct au cours d'un certain nombre de pas de temps.
Plus pr\'ecis\'ement, nous nous situerons dans la partie optimisation du
probl\`eme. Nous prendrons ainsi les donn\'ees de demande comme acquises, car
elles sont estim\'ees au pr\'ealable par Air France \`a partir des donn\'ees
r\'eelles. De m\^eme, pour chaque produit que l'on cherchera \`a vendre, on
nous impose en amont les prix possibles que l'on a droit d'utiliser et qui se
basent sur des algorithmes d'Air France dont les r\'esultats sont v\'erifi\'es
par des analystes. Notre but sera alors de maximiser le revenu d'un vol direct
en choisissant les prix de chaque produit parmi ceux impos\'es.",2025-05-22,"Julien Laasri, Marc Revol",http://arxiv.org/pdf/2505.16748v1,cs.LG
TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning,"Large Language Models (LLMs) present significant computational and memory
challenges due to their extensive size, making pruning essential for their
efficient deployment. Existing one-shot pruning methods often apply uniform
sparsity constraints across layers or within each layer, resulting in
suboptimal performance, especially at high sparsity ratios. This work
introduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel
approach that applies varying sparsity ratios to individual output dimensions
(rows) within each layer. TRIM employs an iterative adjustment process guided
by quality metrics to optimize dimension-wise sparsity allocation, focusing on
reducing variance in quality retention across outputs to preserve critical
information. TRIM can be seamlessly integrated with existing layer-wise pruning
strategies. Our evaluations on perplexity and zero-shot tasks across diverse
LLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that
TRIM achieves new state-of-the-art results and enhances stability. For
instance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and
over 90% for OPT-13B compared to baseline methods. We conclude that
fine-grained, dimension-wise sparsity adaptation is crucial for pushing the
limits of extreme LLM compression. Code available at:
https://github.com/flobk/TRIM",2025-05-22,"Florentin Beck, William Rudman, Carsten Eickhoff",http://arxiv.org/pdf/2505.16743v1,cs.LG
Meta-reinforcement learning with minimum attention,"Minimum attention applies the least action principle in the changes of
control concerning state and time, first proposed by Brockett. The involved
regularization is highly relevant in emulating biological control, such as
motor learning. We apply minimum attention in reinforcement learning (RL) as
part of the rewards and investigate its connection to meta-learning and
stabilization. Specifically, model-based meta-learning with minimum attention
is explored in high-dimensional nonlinear dynamics. Ensemble-based model
learning and gradient-based meta-policy learning are alternately performed.
Empirically, we show that the minimum attention does show outperforming
competence in comparison to the state-of-the-art algorithms in model-free and
model-based RL, i.e., fast adaptation in few shots and variance reduction from
the perturbations of the model and environment. Furthermore, the minimum
attention demonstrates the improvement in energy efficiency.",2025-05-22,"Pilhwa Lee, Shashank Gupta",http://arxiv.org/pdf/2505.16741v1,cs.LG
Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization,"The significant progress of large language models (LLMs) has led to
remarkable achievements across numerous applications. However, their ability to
generate harmful content has sparked substantial safety concerns. Despite the
implementation of safety alignment techniques during the pre-training phase,
recent research indicates that fine-tuning LLMs on adversarial or even benign
data can inadvertently compromise their safety. In this paper, we re-examine
the fundamental issue of why fine-tuning on non-harmful data still results in
safety degradation. We introduce a safety-aware probing (SAP) optimization
framework designed to mitigate the safety risks of fine-tuning LLMs.
Specifically, SAP incorporates a safety-aware probe into the gradient
propagation process, mitigating the model's risk of safety degradation by
identifying potential pitfalls in gradient directions, thereby enhancing
task-specific performance while successfully preserving model safety. Our
extensive experimental results demonstrate that SAP effectively reduces
harmfulness below the original fine-tuned model and achieves comparable test
loss to standard fine-tuning methods. Our code is available at
https://github.com/ChengcanWu/SAP.",2025-05-22,"Chengcan Wu, Zhixin Zhang, Zeming Wei, Yihao Zhang, Meng Sun",http://arxiv.org/pdf/2505.16737v1,cs.LG
Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting,"This work presents LURK (Latent UnleaRned Knowledge), a novel framework that
probes for hidden retained knowledge in unlearned LLMs through adversarial
suffix prompting. LURK automatically generates adversarial prompt suffixes
designed to elicit residual knowledge about the Harry Potter domain, a commonly
used benchmark for unlearning. Our experiments reveal that even models deemed
successfully unlearned can leak idiosyncratic information under targeted
adversarial conditions, highlighting critical limitations of current unlearning
evaluation standards. By uncovering latent knowledge through indirect probing,
LURK offers a more rigorous and diagnostic tool for assessing the robustness of
unlearning algorithms. All code will be publicly available.",2025-05-22,"Bang Trinh Tran To, Thai Le",http://arxiv.org/pdf/2505.17160v1,cs.LG
Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?,"Oversmoothing has long been identified as a major limitation of Graph Neural
Networks (GNNs): input node features are smoothed at each layer and converge to
a non-informative representation, if the weights of the GNN are sufficiently
bounded. This assumption is crucial: if, on the contrary, the weights are
sufficiently large, then oversmoothing may not happen. Theoretically, GNN could
thus learn to not oversmooth. However it does not really happen in practice,
which prompts us to examine oversmoothing from an optimization point of view.
In this paper, we analyze backward oversmoothing, that is, the notion that
backpropagated errors used to compute gradients are also subject to
oversmoothing from output to input. With non-linear activation functions, we
outline the key role of the interaction between forward and backward smoothing.
Moreover, we show that, due to backward oversmoothing, GNNs provably exhibit
many spurious stationary points: as soon as the last layer is trained, the
whole GNN is at a stationary point. As a result, we can exhibit regions where
gradients are near-zero while the loss stays high. The proof relies on the fact
that, unlike forward oversmoothing, backward errors are subjected to a linear
oversmoothing even in the presence of non-linear activation function, such that
the average of the output error plays a key role. Additionally, we show that
this phenomenon is specific to deep GNNs, and exhibit counter-example
Multi-Layer Perceptron. This paper is a step toward a more complete
comprehension of the optimization landscape specific to GNNs.",2025-05-22,Nicolas Keriven,http://arxiv.org/pdf/2505.16736v1,cs.LG
Maximum Total Correlation Reinforcement Learning,"Simplicity is a powerful inductive bias. In reinforcement learning,
regularization is used for simpler policies, data augmentation for simpler
representations, and sparse reward functions for simpler objectives, all that,
with the underlying motivation to increase generalizability and robustness by
focusing on the essentials. Supplementary to these techniques, we investigate
how to promote simple behavior throughout the episode. To that end, we
introduce a modification of the reinforcement learning problem that
additionally maximizes the total correlation within the induced trajectories.
We propose a practical algorithm that optimizes all models, including policy
and state representation, based on a lower-bound approximation. In simulated
robot environments, our method naturally generates policies that induce
periodic and compressible trajectories, and that exhibit superior robustness to
noise and changes in dynamics compared to baseline methods, while also
improving performance in the original tasks.",2025-05-22,"Bang You, Puze Liu, Huaping Liu, Jan Peters, Oleg Arenz",http://arxiv.org/pdf/2505.16734v1,cs.LG
Forward-only Diffusion Probabilistic Models,"This work presents a forward-only diffusion (FoD) approach for generative
modelling. In contrast to traditional diffusion models that rely on a coupled
forward-backward diffusion scheme, FoD directly learns data generation through
a single forward diffusion process, yielding a simple yet efficient generative
framework. The core of FoD is a state-dependent linear stochastic differential
equation that involves a mean-reverting term in both the drift and diffusion
functions. This mean-reversion property guarantees the convergence to clean
data, naturally simulating a stochastic interpolation between source and target
distributions. More importantly, FoD is analytically tractable and is trained
using a simple stochastic flow matching objective, enabling a few-step
non-Markov chain sampling during inference. The proposed FoD model, despite its
simplicity, achieves competitive performance on various image-conditioned
(e.g., image restoration) and unconditional generation tasks, demonstrating its
effectiveness in generative modelling. Our code is available at
https://github.com/Algolzw/FoD.",2025-05-22,"Ziwei Luo, Fredrik K. Gustafsson, Jens Sjölund, Thomas B. Schön",http://arxiv.org/pdf/2505.16733v1,cs.LG
Sequential Monte Carlo for Policy Optimization in Continuous POMDPs,"Optimal decision-making under partial observability requires agents to
balance reducing uncertainty (exploration) against pursuing immediate
objectives (exploitation). In this paper, we introduce a novel policy
optimization framework for continuous partially observable Markov decision
processes (POMDPs) that explicitly addresses this challenge. Our method casts
policy learning as probabilistic inference in a non-Markovian Feynman--Kac
model that inherently captures the value of information gathering by
anticipating future observations, without requiring extrinsic exploration
bonuses or handcrafted heuristics. To optimize policies under this model, we
develop a nested sequential Monte Carlo~(SMC) algorithm that efficiently
estimates a history-dependent policy gradient under samples from the optimal
trajectory distribution induced by the POMDP. We demonstrate the effectiveness
of our algorithm across standard continuous POMDP benchmarks, where existing
methods struggle to act under uncertainty.",2025-05-22,"Hany Abdulsamad, Sahel Iqbal, Simo Särkkä",http://arxiv.org/pdf/2505.16732v1,cs.LG
Masked Conditioning for Deep Generative Models,"Datasets in engineering domains are often small, sparsely labeled, and
contain numerical as well as categorical conditions. Additionally.
computational resources are typically limited in practical applications which
hinders the adoption of generative models for engineering tasks. We introduce a
novel masked-conditioning approach, that enables generative models to work with
sparse, mixed-type data. We mask conditions during training to simulate sparse
conditions at inference time. For this purpose, we explore the use of various
sparsity schedules that show different strengths and weaknesses. In addition,
we introduce a flexible embedding that deals with categorical as well as
numerical conditions. We integrate our method into an efficient variational
autoencoder as well as a latent diffusion model and demonstrate the
applicability of our approach on two engineering-related datasets of 2D point
clouds and images. Finally, we show that small models trained on limited data
can be coupled with large pretrained foundation models to improve generation
quality while retaining the controllability induced by our conditioning scheme.",2025-05-22,"Phillip Mueller, Jannik Wiese, Sebastian Mueller, Lars Mikelsons",http://arxiv.org/pdf/2505.16725v1,cs.LG
Advancing Brainwave Modeling with a Codebook-Based Foundation Model,"Recent advances in large-scale pre-trained Electroencephalogram (EEG) models
have shown great promise, driving progress in Brain-Computer Interfaces (BCIs)
and healthcare applications. However, despite their success, many existing
pre-trained models have struggled to fully capture the rich information content
of neural oscillations, a limitation that fundamentally constrains their
performance and generalizability across diverse BCI tasks. This limitation is
frequently rooted in suboptimal architectural design choices which constrain
their representational capacity. In this work, we introduce LaBraM++, an
enhanced Large Brainwave Foundation Model (LBM) that incorporates principled
improvements grounded in robust signal processing foundations. LaBraM++
demonstrates substantial gains across a variety of tasks, consistently
outperforming its originally-based architecture and achieving competitive
results when compared to other open-source LBMs. Its superior performance and
training efficiency highlight its potential as a strong foundation for future
advancements in LBMs.",2025-05-22,"Konstantinos Barmpas, Na Lee, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou",http://arxiv.org/pdf/2505.16724v1,cs.LG
Robust LLM Fingerprinting via Domain-Specific Watermarks,"As open-source language models (OSMs) grow more capable and are widely shared
and finetuned, ensuring model provenance, i.e., identifying the origin of a
given model instance, has become an increasingly important issue. At the same
time, existing backdoor-based model fingerprinting techniques often fall short
of achieving key requirements of real-world model ownership detection. In this
work, we build on the observation that while current open-source model
watermarks fail to achieve reliable content traceability, they can be
effectively adapted to address the challenge of model provenance. To this end,
we introduce the concept of domain-specific watermarking for model
fingerprinting. Rather than watermarking all generated content, we train the
model to embed watermarks only within specified subdomains (e.g., particular
languages or topics). This targeted approach ensures detection reliability,
while improving watermark durability and quality under a range of real-world
deployment settings. Our evaluations show that domain-specific watermarking
enables model fingerprinting with strong statistical guarantees, controllable
false positive rates, high detection power, and preserved generation quality.
Moreover, we find that our fingerprints are inherently stealthy and naturally
robust to real-world variability across deployment scenarios.",2025-05-22,"Thibaud Gloaguen, Robin Staab, Nikola Jovanović, Martin Vechev",http://arxiv.org/pdf/2505.16723v1,cs.LG
The Computational Complexity of Counting Linear Regions in ReLU Neural Networks,"An established measure of the expressive power of a given ReLU neural network
is the number of linear regions into which it partitions the input space. There
exist many different, non-equivalent definitions of what a linear region
actually is. We systematically assess which papers use which definitions and
discuss how they relate to each other. We then analyze the computational
complexity of counting the number of such regions for the various definitions.
Generally, this turns out to be an intractable problem. We prove NP- and
#P-hardness results already for networks with one hidden layer and strong
hardness of approximation results for two or more hidden layers. Finally, on
the algorithmic side, we demonstrate that counting linear regions can at least
be achieved in polynomial space for some common definitions.",2025-05-22,"Moritz Stargalla, Christoph Hertrich, Daniel Reichman",http://arxiv.org/pdf/2505.16716v1,cs.LG
Experimental robustness benchmark of quantum neural network on a superconducting quantum processor,"Quantum machine learning (QML) models, like their classical counterparts, are
vulnerable to adversarial attacks, hindering their secure deployment. Here, we
report the first systematic experimental robustness benchmark for 20-qubit
quantum neural network (QNN) classifiers executed on a superconducting
processor. Our benchmarking framework features an efficient adversarial attack
algorithm designed for QNNs, enabling quantitative characterization of
adversarial robustness and robustness bounds. From our analysis, we verify that
adversarial training reduces sensitivity to targeted perturbations by
regularizing input gradients, significantly enhancing QNN's robustness.
Additionally, our analysis reveals that QNNs exhibit superior adversarial
robustness compared to classical neural networks, an advantage attributed to
inherent quantum noise. Furthermore, the empirical upper bound extracted from
our attack experiments shows a minimal deviation ($3 \times 10^{-3}$) from the
theoretical lower bound, providing strong experimental confirmation of the
attack's effectiveness and the tightness of fidelity-based robustness bounds.
This work establishes a critical experimental framework for assessing and
improving quantum adversarial robustness, paving the way for secure and
reliable QML applications.",2025-05-22,"Hai-Feng Zhang, Zhao-Yun Chen, Peng Wang, Liang-Liang Guo, Tian-Le Wang, Xiao-Yan Yang, Ren-Ze Zhao, Ze-An Zhao, Sheng Zhang, Lei Du, Hao-Ran Tao, Zhi-Long Jia, Wei-Cheng Kong, Huan-Yu Liu, Athanasios V. Vasilakos, Yang Yang, Yu-Chun Wu, Ji Guan, Peng Duan, Guo-Ping Guo",http://arxiv.org/pdf/2505.16714v1,cs.LG
Sharp concentration of uniform generalization errors in binary linear classification,"We examine the concentration of uniform generalization errors around their
expectation in binary linear classification problems via an isoperimetric
argument. In particular, we establish Poincar\'{e} and log-Sobolev inequalities
for the joint distribution of the output labels and the label-weighted input
vectors, which we apply to derive concentration bounds. The derived
concentration bounds are sharp up to moderate multiplicative constants by those
under well-balanced labels. In asymptotic analysis, we also show that almost
sure convergence of uniform generalization errors to their expectation occurs
in very broad settings, such as proportionally high-dimensional regimes. Using
this convergence, we establish uniform laws of large numbers under
dimension-free conditions.",2025-05-22,Shogo Nakakita,http://arxiv.org/pdf/2505.16713v1,cs.LG
Training Long-Context LLMs Efficiently via Chunk-wise Optimization,"While long-context large language models (LLMs) exhibit remarkable document
processing capabilities, their prohibitively high training costs often hinder
customized applications. To mitigate this issue, we propose \textit{Sequential
Chunk-wise Optimization} (SeCO), a memory-efficient training paradigm that
partitions lengthy inputs into manageable chunks. Each chunk independently
constructs its computational graph and performs localized backpropagation,
ensuring that only one chunk's forward activations are stored in memory.
Building on SeCO, we further introduce \textit{Sparse Chunk-wise Optimization}
(SpaCO), which reduces computational overhead by selectively propagating
gradients to specific chunks and incorporates a carefully designed compensation
factor to ensure unbiased gradient estimation. SpaCO decouples the
computational cost of backpropagation from the context length, enabling
training time to gradually converge to inference time as sequences become
longer. Implemented as lightweight training wrappers, both SeCO and SpaCO offer
substantial practical benefits. For example, when fine-tuning an 8B model with
LoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to
16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up
to 3x faster than SeCO under the same experimental setup. These innovations
provide new insights into optimizing long-context models, making them more
accessible for practical applications. We have open-sourced the code at
\href{https://github.com/wenhaoli-xmu/seco}{here}.",2025-05-22,"Wenhao Li, Yuxin Zhang, Gen Luo, Daohai Yu, Rongrong Ji",http://arxiv.org/pdf/2505.16710v1,cs.LG
"An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations","Concept bottleneck models (CBMs) ensure interpretability by decomposing
predictions into human interpretable concepts. Yet the annotations used for
training CBMs that enable this transparency are often noisy, and the impact of
such corruption is not well understood. In this study, we present the first
systematic study of noise in CBMs and show that even moderate corruption
simultaneously impairs prediction performance, interpretability, and the
intervention effectiveness. Our analysis identifies a susceptible subset of
concepts whose accuracy declines far more than the average gap between noisy
and clean supervision and whose corruption accounts for most performance loss.
To mitigate this vulnerability we propose a two-stage framework. During
training, sharpness-aware minimization stabilizes the learning of
noise-sensitive concepts. During inference, where clean labels are unavailable,
we rank concepts by predictive entropy and correct only the most uncertain
ones, using uncertainty as a proxy for susceptibility. Theoretical analysis and
extensive ablations elucidate why sharpness-aware training confers robustness
and why uncertainty reliably identifies susceptible concepts, providing a
principled basis that preserves both interpretability and resilience in the
presence of noise.",2025-05-22,"Seonghwan Park, Jueun Mun, Donghyun Oh, Namhoon Lee",http://arxiv.org/pdf/2505.16705v1,cs.LG
Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator,"Post-training of large language models is essential for adapting pre-trained
language models (PLMs) to align with human preferences and downstream tasks.
While PLMs typically exhibit well-calibrated confidence, post-trained language
models (PoLMs) often suffer from over-confidence, assigning high confidence to
both correct and incorrect outputs, which can undermine reliability in critical
applications. A major obstacle in calibrating PoLMs is the scarcity of labeled
data for individual downstream tasks. To address this, we propose
Disagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to
optimize the parameters (e.g., temperature $\tau$) in post-hoc confidence
calibration. Our method is motivated by the under-confidence issue caused by
prediction disagreement between the PLM and PoLM while aligning their
confidence via temperature scaling. Theoretically, the PLM's confidence
underestimates PoLM's prediction accuracy on disagreement examples, causing a
larger $\tau$ and producing under-confident predictions. DACA mitigates this by
selectively using only agreement examples for calibration, effectively
decoupling the influence of disagreement. In this manner, our method avoids an
overly large $\tau$ in temperature scaling caused by disagreement examples,
improving calibration performance. Extensive experiments demonstrate the
effectiveness of our method, improving the average ECE of open-sourced and
API-based LLMs (e.g. GPT-4o) by up to 15.08$\%$ on common benchmarks.",2025-05-22,"Beier Luo, Shuoyuan Wang, Yixuan Li, Hongxin Wei",http://arxiv.org/pdf/2505.16690v1,cs.LG
Learning Genomic Structure from $k$-mers,"Sequencing a genome to determine an individual's DNA produces an enormous
number of short nucleotide subsequences known as reads, which must be
reassembled to reconstruct the full genome. We present a method for analyzing
this type of data using contrastive learning, in which an encoder model is
trained to produce embeddings that cluster together sequences from the same
genomic region. The sequential nature of genomic regions is preserved in the
form of trajectories through this embedding space. Trained solely to reflect
the structure of the genome, the resulting model provides a general
representation of $k$-mer sequences, suitable for a range of downstream tasks
involving read data. We apply our framework to learn the structure of the $E.\
coli$ genome, and demonstrate its use in simulated ancient DNA (aDNA) read
mapping and identification of structural variations. Furthermore, we illustrate
the potential of using this type of model for metagenomic species
identification. We show how incorporating a domain-specific noise model can
enhance embedding robustness, and how a supervised contrastive learning setting
can be adopted when a linear reference genome is available, by introducing a
distance thresholding parameter $\Gamma$. The model can also be trained fully
self-supervised on read data, enabling analysis without the need to construct a
full genome assembly using specialized algorithms. Small prediction heads based
on a pre-trained embedding are shown to perform on par with BWA-aln, the
current gold standard approach for aDNA mapping, in terms of accuracy and
runtime for short genomes. Given the method's favorable scaling properties with
respect to total genome size, inference using our approach is highly promising
for metagenomic applications and for mapping to genomes comparable in size to
the human genome.",2025-05-22,"Filip Thor, Carl Nettelblad",http://arxiv.org/pdf/2505.16680v1,cs.LG
On the Out-of-Distribution Generalization of Self-Supervised Learning,"In this paper, we focus on the out-of-distribution (OOD) generalization of
self-supervised learning (SSL). By analyzing the mini-batch construction during
the SSL training phase, we first give one plausible explanation for SSL having
OOD generalization. Then, from the perspective of data generation and causal
inference, we analyze and conclude that SSL learns spurious correlations during
the training process, which leads to a reduction in OOD generalization. To
address this issue, we propose a post-intervention distribution (PID) grounded
in the Structural Causal Model. PID offers a scenario where the spurious
variable and label variable is mutually independent. Besides, we demonstrate
that if each mini-batch during SSL training satisfies PID, the resulting SSL
model can achieve optimal worst-case OOD performance. This motivates us to
develop a batch sampling strategy that enforces PID constraints through the
learning of a latent variable model. Through theoretical analysis, we
demonstrate the identifiability of the latent variable model and validate the
effectiveness of the proposed sampling strategy. Experiments conducted on
various downstream OOD tasks demonstrate the effectiveness of the proposed
sampling strategy.",2025-05-22,"Wenwen Qiang, Jingyao Wang, Zeen Song, Jiangmeng Li, Changwen Zheng",http://arxiv.org/pdf/2505.16675v1,cs.LG
Quantum Feature Optimization for Enhanced Clustering of Blockchain Transaction Data,"Blockchain transaction data exhibits high dimensionality, noise, and
intricate feature entanglement, presenting significant challenges for
traditional clustering algorithms. In this study, we conduct a comparative
analysis of three clustering approaches: (1) Classical K-Means Clustering,
applied to pre-processed feature representations; (2) Hybrid Clustering,
wherein classical features are enhanced with quantum random features extracted
using randomly initialized quantum neural networks (QNNs); and (3) Fully
Quantum Clustering, where a QNN is trained in a self-supervised manner
leveraging a SwAV-based loss function to optimize the feature space for
clustering directly. The proposed experimental framework systematically
investigates the impact of quantum circuit depth and the number of learned
prototypes, demonstrating that even shallow quantum circuits can effectively
extract meaningful non-linear representations, significantly improving
clustering performance.",2025-05-22,"Yun-Cheng Tsai, Samuel Yen-Chi Chen",http://arxiv.org/pdf/2505.16672v1,cs.LG
End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries,"Accurate prediction of the Remaining Useful Life (RUL) is essential for
enabling timely maintenance of lithium-ion batteries, impacting the operational
efficiency of electric applications that rely on them. This paper proposes a
RUL prediction approach that leverages data from recent charge-discharge cycles
to estimate the number of remaining usable cycles. The approach introduces both
a novel signal processing pipeline and a deep learning prediction model. In the
signal preprocessing pipeline, a derived capacity feature $\dot{Q}(I, Q)$ is
computed based on current and capacity signals. Alongside original capacity,
voltage and current, these features are denoised and enhanced using statistical
metrics and a delta-based method to capture differences between the current and
previous cycles. In the prediction model, the processed features are then fed
into a hybrid deep learning architecture composed of 1D Convolutional Neural
Networks (CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary
Differential Equation-based LSTM (ODE-LSTM) blocks. This architecture is
designed to capture both local signal characteristics and long-range temporal
dependencies while modeling the continuous-time dynamics of battery
degradation. The model is further evaluated using transfer learning across
different learning strategies and target data partitioning scenarios. Results
indicate that the model maintains robust performance, even when fine-tuned on
limited target data. Experimental results on two publicly available large-scale
datasets demonstrate that the proposed method outperforms a baseline deep
learning approach and machine learning techniques, achieving an RMSE of 101.59,
highlighting its strong potential for real-world RUL prediction applications.",2025-05-22,"Khoa Tran, Tri Le, Bao Huynh, Hung-Cuong Trinh, Vy-Rin Nguyen",http://arxiv.org/pdf/2505.16664v2,cs.LG
Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding,"Recent advancements in multimodal large language models (MLLMs) have
significantly improved performance in visual question answering. However, they
often suffer from hallucinations. In this work, hallucinations are categorized
into two main types: initial hallucinations and snowball hallucinations. We
argue that adequate contextual information can be extracted directly from the
token interaction process. Inspired by causal inference in the decoding
strategy, we propose to leverage causal masks to establish information
propagation between multimodal tokens. The hypothesis is that insufficient
interaction between those tokens may lead the model to rely on outlier tokens,
overlooking dense and rich contextual cues. Therefore, we propose to intervene
in the propagation process by tackling outlier tokens to enhance in-context
inference. With this goal, we present FarSight, a versatile plug-and-play
decoding strategy to reduce attention interference from outlier tokens merely
by optimizing the causal mask. The heart of our method is effective token
propagation. We design an attention register structure within the upper
triangular matrix of the causal mask, dynamically allocating attention to
capture attention diverted to outlier tokens. Moreover, a positional awareness
encoding method with a diminishing masking rate is proposed, allowing the model
to attend to further preceding tokens, especially for video sequence tasks.
With extensive experiments, FarSight demonstrates significant
hallucination-mitigating performance across different MLLMs on both image and
video benchmarks, proving its effectiveness.",2025-05-22,"Feilong Tang, Chengzhi Liu, Zhongxing Xu, Ming Hu, Zelin Peng, Zhiwei Yang, Jionglong Su, Minquan Lin, Yifan Peng, Xuelian Cheng, Imran Razzak, Zongyuan Ge",http://arxiv.org/pdf/2505.16652v1,cs.LG
Stochastic Forward-Forward Learning through Representational Dimensionality Compression,"The Forward-Forward (FF) algorithm provides a bottom-up alternative to
backpropagation (BP) for training neural networks, relying on a layer-wise
""goodness"" function to guide learning. Existing goodness functions, inspired by
energy-based learning (EBL), are typically defined as the sum of squared
post-synaptic activations, neglecting the correlations between neurons. In this
work, we propose a novel goodness function termed dimensionality compression
that uses the effective dimensionality (ED) of fluctuating neural responses to
incorporate second-order statistical structure. Our objective minimizes ED for
clamped inputs when noise is considered while maximizing it across the sample
distribution, promoting structured representations without the need to prepare
negative samples. We demonstrate that this formulation achieves competitive
performance compared to other non-BP methods. Moreover, we show that noise
plays a constructive role that can enhance generalization and improve inference
when predictions are derived from the mean of squared outputs, which is
equivalent to making predictions based on the energy term. Our findings
contribute to the development of more biologically plausible learning
algorithms and suggest a natural fit for neuromorphic computing, where
stochasticity is a computational resource rather than a nuisance. The code is
available at https://github.com/ZhichaoZhu/StochasticForwardForward",2025-05-22,"Zhichao Zhu, Yang Qi, Hengyuan Ma, Wenlian Lu, Jianfeng Feng",http://arxiv.org/pdf/2505.16649v1,cs.LG
Learning non-equilibrium diffusions with Schrödinger bridges: from exactly solvable to simulation-free,"We consider the Schr\""odinger bridge problem which, given ensemble
measurements of the initial and final configurations of a stochastic dynamical
system and some prior knowledge on the dynamics, aims to reconstruct the ""most
likely"" evolution of the system compatible with the data. Most existing
literature assume Brownian reference dynamics and are implicitly limited to
potential-driven dynamics. We depart from this regime and consider reference
processes described by a multivariate Ornstein-Uhlenbeck process with generic
drift matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$. When $\mathbf{A}$ is
asymmetric, this corresponds to a non-equilibrium system with non-conservative
forces at play: this is important for applications to biological systems, which
are naturally exist out-of-equilibrium. In the case of Gaussian marginals, we
derive explicit expressions that characterise the solution of both the static
and dynamic Schr\""odinger bridge. For general marginals, we propose mvOU-OTFM,
a simulation-free algorithm based on flow and score matching for learning the
Schr\""odinger bridge. In application to a range of problems based on synthetic
and real single cell data, we demonstrate that mvOU-OTFM achieves higher
accuracy compared to competing methods, whilst being significantly faster to
train.",2025-05-22,"Stephen Y. Zhang, Michael P H Stumpf",http://arxiv.org/pdf/2505.16644v1,cs.LG
Reconsidering Fairness Through Unawareness from the Perspective of Model Multiplicity,"Fairness through Unawareness (FtU) describes the idea that discrimination
against demographic groups can be avoided by not considering group membership
in the decisions or predictions. This idea has long been criticized in the
machine learning literature as not being sufficient to ensure fairness. In
addition, the use of additional features is typically thought to increase the
accuracy of the predictions for all groups, so that FtU is sometimes thought to
be detrimental to all groups. In this paper, we show both theoretically and
empirically that FtU can reduce algorithmic discrimination without necessarily
reducing accuracy. We connect this insight with the literature on Model
Multiplicity, to which we contribute with novel theoretical and empirical
results. Furthermore, we illustrate how, in a real-life application, FtU can
contribute to the deployment of more equitable policies without losing
efficacy. Our findings suggest that FtU is worth considering in practical
applications, particularly in high-risk scenarios, and that the use of
protected attributes such as gender in predictive models should be accompanied
by a clear and well-founded justification.",2025-05-22,"Benedikt Höltgen, Nuria Oliver",http://arxiv.org/pdf/2505.16638v1,cs.LG
SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation,"Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.",2025-05-22,"Wenjie Yang, Mao Zheng, Mingyang Song, Zheng Li",http://arxiv.org/pdf/2505.16637v2,cs.LG
Multivariate Latent Recalibration for Conditional Normalizing Flows,"Reliably characterizing the full conditional distribution of a multivariate
response variable given a set of covariates is crucial for trustworthy
decision-making. However, misspecified or miscalibrated multivariate models may
yield a poor approximation of the joint distribution of the response variables,
leading to unreliable predictions and suboptimal decisions. Furthermore,
standard recalibration methods are primarily limited to univariate settings,
while conformal prediction techniques, despite generating multivariate
prediction regions with coverage guarantees, do not provide a full probability
density function. We address this gap by first introducing a novel notion of
latent calibration, which assesses probabilistic calibration in the latent
space of a conditional normalizing flow. Second, we propose latent
recalibration (LR), a novel post-hoc model recalibration method that learns a
transformation of the latent space with finite-sample bounds on latent
calibration. Unlike existing methods, LR produces a recalibrated distribution
with an explicit multivariate density function while remaining computationally
efficient. Extensive experiments on both tabular and image datasets show that
LR consistently improves latent calibration error and the negative
log-likelihood of the recalibrated models.",2025-05-22,"Victor Dheur, Souhaib Ben Taieb",http://arxiv.org/pdf/2505.16636v1,cs.LG
WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning,"Tabular data, ubiquitous and rich in informational value, is an increasing
focus for deep representation learning, yet progress is hindered by studies
centered on single tables or isolated databases, which limits model
capabilities due to data scale. While collaborative learning approaches such as
federated learning, transfer learning, split learning, and tabular foundation
models aim to learn from multiple correlated databases, they are challenged by
a scarcity of real-world interconnected tabular resources. Current data lakes
and corpora largely consist of isolated databases lacking defined
inter-database correlations. To overcome this, we introduce WikiDBGraph, a
large-scale graph of 100,000 real-world tabular databases from WikiData,
interconnected by 17 million edges and characterized by 13 node and 12 edge
properties derived from its database schema and data distribution.
WikiDBGraph's weighted edges identify both instance- and feature-overlapped
databases. Experiments on these newly identified databases confirm that
collaborative learning yields superior performance, thereby offering
considerable promise for structured foundation model training while also
exposing key challenges and future directions for learning from interconnected
tabular data.",2025-05-22,"Zhaomin Wu, Ziyang Wang, Bingsheng He",http://arxiv.org/pdf/2505.16635v1,cs.LG
PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG,"The introduction of Large Language Models (LLMs) has significantly
transformed Natural Language Processing (NLP) applications by enabling more
advanced analysis of customer personas. At Volvo Construction Equipment (VCE),
customer personas have traditionally been developed through qualitative
methods, which are time-consuming and lack scalability. The main objective of
this paper is to generate synthetic customer personas and integrate them into a
Retrieval-Augmented Generation (RAG) chatbot to support decision-making in
business processes. To this end, we first focus on developing a persona-based
RAG chatbot integrated with verified personas. Next, synthetic personas are
generated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and
evaluated based on completeness, relevance, and consistency using McNemar's
test. In the final step, the chatbot's knowledge base is augmented with
synthetic personas and additional segment information to assess improvements in
response accuracy and practical utility. Key findings indicate that Few-Shot
prompting outperformed CoT in generating more complete personas, while CoT
demonstrated greater efficiency in terms of response time and token usage.
After augmenting the knowledge base, the average accuracy rating of the chatbot
increased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants
found the updated system useful in business contexts.",2025-05-22,"Muhammed Rizwan, Lars Carlsson, Mohammad Loni",http://arxiv.org/pdf/2505.17156v1,cs.LG
CausalDynamics: A large-scale benchmark for structural discovery of dynamical causal models,"Causal discovery for dynamical systems poses a major challenge in fields
where active interventions are infeasible. Most methods used to investigate
these systems and their associated benchmarks are tailored to deterministic,
low-dimensional and weakly nonlinear time-series data. To address these
limitations, we present CausalDynamics, a large-scale benchmark and extensible
data generation framework to advance the structural discovery of dynamical
causal models. Our benchmark consists of true causal graphs derived from
thousands of coupled ordinary and stochastic differential equations as well as
two idealized climate models. We perform a comprehensive evaluation of
state-of-the-art causal discovery algorithms for graph reconstruction on
systems with noisy, confounded, and lagged dynamics. CausalDynamics consists of
a plug-and-play, build-your-own coupling workflow that enables the construction
of a hierarchy of physical systems. We anticipate that our framework will
facilitate the development of robust causal discovery algorithms that are
broadly applicable across domains while addressing their unique challenges. We
provide a user-friendly implementation and documentation on
https://kausable.github.io/CausalDynamics.",2025-05-22,"Benjamin Herdeanu, Juan Nathaniel, Carla Roesch, Jatan Buch, Gregor Ramien, Johannes Haux, Pierre Gentine",http://arxiv.org/pdf/2505.16620v1,cs.LG
Steering Large Language Models for Machine Translation Personalization,"High-quality machine translation systems based on large language models
(LLMs) have simplified the production of personalized translations reflecting
specific stylistic constraints. However, these systems still struggle in
settings where stylistic requirements are less explicit and might be harder to
convey via prompting. We explore various strategies for personalizing
LLM-generated translations in low-resource settings, focusing on the
challenging literary translation domain. We explore prompting strategies and
inference-time interventions for steering model generations towards a
personalized style, and propose a contrastive framework exploiting latent
concepts extracted from sparse autoencoders to identify salient personalization
properties. Our results show that steering achieves strong personalization
while preserving translation quality. We further examine the impact of steering
on LLM representations, finding model layers with a relevant impact for
personalization are impacted similarly by multi-shot prompting and our steering
method, suggesting similar mechanism at play.",2025-05-22,"Daniel Scalena, Gabriele Sarti, Arianna Bisazza, Elisabetta Fersini, Malvina Nissim",http://arxiv.org/pdf/2505.16612v1,cs.LG
Temporal Object Captioning for Street Scene Videos from LiDAR Tracks,"Video captioning models have seen notable advancements in recent years,
especially with regard to their ability to capture temporal information. While
many research efforts have focused on architectural advancements, such as
temporal attention mechanisms, there remains a notable gap in understanding how
models capture and utilize temporal semantics for effective temporal feature
extraction, especially in the context of Advanced Driver Assistance Systems. We
propose an automated LiDAR-based captioning procedure that focuses on the
temporal dynamics of traffic participants. Our approach uses a rule-based
system to extract essential details such as lane position and relative motion
from object tracks, followed by a template-based caption generation. Our
findings show that training SwinBERT, a video captioning model, using only
front camera images and supervised with our template-based captions,
specifically designed to encapsulate fine-grained temporal behavior, leads to
improved temporal understanding consistently across three datasets. In
conclusion, our results clearly demonstrate that integrating LiDAR-based
caption supervision significantly enhances temporal understanding, effectively
addressing and reducing the inherent visual/static biases prevalent in current
state-of-the-art model architectures.",2025-05-22,"Vignesh Gopinathan, Urs Zimmermann, Michael Arnold, Matthias Rottmann",http://arxiv.org/pdf/2505.16594v1,cs.LG
TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling,"Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling
complex mathematical, logical, and coding tasks by leveraging extended
Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging
CoT with explicit token-level exploration, can push LRMs' accuracy boundaries,
but they incur significant decoding overhead. A key inefficiency source is LRMs
often generate redundant thinking CoTs, which demonstrate clear structured
overthinking and underthinking patterns. Inspired by human cognitive reasoning
processes and numerical optimization theories, we propose TrimR, a
verifier-based, training-free, efficient framework for dynamic CoT compression
to trim reasoning and enhance test-time scaling, explicitly tailored for
production-level deployment. Our method employs a lightweight, pretrained,
instruction-tuned verifier to detect and truncate redundant intermediate
thoughts of LRMs without any LRM or verifier fine-tuning. We present both the
core algorithm and asynchronous online system engineered for high-throughput
industrial applications. Empirical evaluations on Ascend NPUs and vLLM show
that our framework delivers substantial gains in inference efficiency under
large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and
GPQA benchmarks, the reasoning runtime of Pangu-R-38B, QwQ-32B, and
DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on
accuracy.",2025-05-22,"Weizhe Lin, Xing Li, Zhiyuan Yang, Xiaojin Fu, Hui-Ling Zhen, Yaoyuan Wang, Xianzhi Yu, Wulong Liu, Xiaosong Li, Mingxuan Yuan",http://arxiv.org/pdf/2505.17155v1,cs.LG
Training on Plausible Counterfactuals Removes Spurious Correlations,"Plausible counterfactual explanations (p-CFEs) are perturbations that
minimally modify inputs to change classifier decisions while remaining
plausible under the data distribution. In this study, we demonstrate that
classifiers can be trained on p-CFEs labeled with induced \emph{incorrect}
target classes to classify unperturbed inputs with the original labels. While
previous studies have shown that such learning is possible with adversarial
perturbations, we extend this paradigm to p-CFEs. Interestingly, our
experiments reveal that learning from p-CFEs is even more effective: the
resulting classifiers achieve not only high in-distribution accuracy but also
exhibit significantly reduced bias with respect to spurious correlations.",2025-05-22,"Shpresim Sadiku, Kartikeya Chitranshi, Hiroshi Kera, Sebastian Pokutta",http://arxiv.org/pdf/2505.16583v1,cs.LG
How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning,"In the zero-shot policy transfer setting in reinforcement learning, the goal
is to train an agent on a fixed set of training environments so that it can
generalise to similar, but unseen, testing environments. Previous work has
shown that policy distillation after training can sometimes produce a policy
that outperforms the original in the testing environments. However, it is not
yet entirely clear why that is, or what data should be used to distil the
policy. In this paper, we prove, under certain assumptions, a generalisation
bound for policy distillation after training. The theory provides two practical
insights: for improved generalisation, you should 1) train an ensemble of
distilled policies, and 2) distil it on as much data from the training
environments as possible. We empirically verify that these insights hold in
more general settings, when the assumptions required for the theory no longer
hold. Finally, we demonstrate that an ensemble of policies distilled on a
diverse dataset can generalise significantly better than the original agent.",2025-05-22,"Max Weltevrede, Moritz A. Zanger, Matthijs T. J. Spaan, Wendelin Böhmer",http://arxiv.org/pdf/2505.16581v1,cs.LG
Large Language Model-Empowered Interactive Load Forecasting,"The growing complexity of power systems has made accurate load forecasting
more important than ever. An increasing number of advanced load forecasting
methods have been developed. However, the static design of current methods
offers no mechanism for human-model interaction. As the primary users of
forecasting models, system operators often find it difficult to understand and
apply these advanced models, which typically requires expertise in artificial
intelligence (AI). This also prevents them from incorporating their experience
and real-world contextual understanding into the forecasting process. Recent
breakthroughs in large language models (LLMs) offer a new opportunity to
address this issue. By leveraging their natural language understanding and
reasoning capabilities, we propose an LLM-based multi-agent collaboration
framework to bridge the gap between human operators and forecasting models. A
set of specialized agents is designed to perform different tasks in the
forecasting workflow and collaborate via a dedicated communication mechanism.
This framework embeds interactive mechanisms throughout the load forecasting
pipeline, reducing the technical threshold for non-expert users and enabling
the integration of human experience. Our experiments demonstrate that the
interactive load forecasting accuracy can be significantly improved when users
provide proper insight in key stages. Our cost analysis shows that the
framework remains affordable, making it practical for real-world deployment.",2025-05-22,"Yu Zuo, Dalin Qin, Yi Wang",http://arxiv.org/pdf/2505.16577v1,cs.LG
Finetuning-Activated Backdoors in LLMs,"Finetuning openly accessible Large Language Models (LLMs) has become standard
practice for achieving task-specific performance improvements. Until now,
finetuning has been regarded as a controlled and secure process in which
training on benign datasets led to predictable behaviors. In this paper, we
demonstrate for the first time that an adversary can create poisoned LLMs that
initially appear benign but exhibit malicious behaviors once finetuned by
downstream users. To this end, our proposed attack, FAB (Finetuning-Activated
Backdoor), poisons an LLM via meta-learning techniques to simulate downstream
finetuning, explicitly optimizing for the emergence of malicious behaviors in
the finetuned models. At the same time, the poisoned LLM is regularized to
retain general capabilities and to exhibit no malicious behaviors prior to
finetuning. As a result, when users finetune the seemingly benign model on
their own datasets, they unknowingly trigger its hidden backdoor behavior. We
demonstrate the effectiveness of FAB across multiple LLMs and three target
behaviors: unsolicited advertising, refusal, and jailbreakability.
Additionally, we show that FAB-backdoors are robust to various finetuning
choices made by the user (e.g., dataset, number of steps, scheduler). Our
findings challenge prevailing assumptions about the security of finetuning,
revealing yet another critical attack vector exploiting the complexities of
LLMs.",2025-05-22,"Thibaud Gloaguen, Mark Vero, Robin Staab, Martin Vechev",http://arxiv.org/pdf/2505.16567v2,cs.LG
A Two-Stage Data Selection Framework for Data-Efficient Model Training on Edge Devices,"The demand for machine learning (ML) model training on edge devices is
escalating due to data privacy and personalized service needs. However, we
observe that current on-device model training is hampered by the
under-utilization of on-device data, due to low training throughput, limited
storage and diverse data importance. To improve data resource utilization, we
propose a two-stage data selection framework {\sf Titan} to select the most
important data batch from streaming data for model training with guaranteed
efficiency and effectiveness. Specifically, in the first stage, {\sf Titan}
filters out a candidate dataset with potentially high importance in a
coarse-grained manner.In the second stage of fine-grained selection, we propose
a theoretically optimal data selection strategy to identify the data batch with
the highest model performance improvement to current training round. To further
enhance time-and-resource efficiency, {\sf Titan} leverages a pipeline to
co-execute data selection and model training, and avoids resource conflicts by
exploiting idle computing resources. We evaluate {\sf Titan} on real-world edge
devices and three representative edge computing tasks with diverse models and
data modalities. Empirical results demonstrate that {\sf Titan} achieves up to
$43\%$ reduction in training time and $6.2\%$ increase in final accuracy with
minor system overhead, such as data processing delay, memory footprint and
energy consumption.",2025-05-22,"Chen Gong, Rui Xing, Zhenzhe Zheng, Fan Wu",http://arxiv.org/pdf/2505.16563v1,cs.LG
Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations,"The machine learning methods for data-driven identification of partial
differential equations (PDEs) are typically defined for a given number of
spatial dimensions and a choice of coordinates the data have been collected in.
This dependence prevents the learned evolution equation from generalizing to
other spaces. In this work, we reformulate the problem in terms of coordinate-
and dimension-independent representations, paving the way toward what we call
``spatially liberated"" PDE learning. To this end, we employ a machine learning
approach to predict the evolution of scalar field systems expressed in the
formalism of exterior calculus, which is coordinate-free and immediately
generalizes to arbitrary dimensions by construction. We demonstrate the
performance of this approach in the FitzHugh-Nagumo and Barkley
reaction-diffusion models, as well as the Patlak-Keller-Segel model informed by
in-situ chemotactic bacteria observations. We provide extensive numerical
experiments that demonstrate that our approach allows for seamless transitions
across various spatial contexts. We show that the field dynamics learned in one
space can be used to make accurate predictions in other spaces with different
dimensions, coordinate systems, boundary conditions, and curvatures.",2025-05-22,"Trung V. Phan, George A. Kevrekidis, Soledad Villar, Yannis G. Kevrekidis, Juan M. Bello-Rivas",http://arxiv.org/pdf/2505.16549v1,cs.LG
Incremental Sequence Classification with Temporal Consistency,"We address the problem of incremental sequence classification, where
predictions are updated as new elements in the sequence are revealed. Drawing
on temporal-difference learning from reinforcement learning, we identify a
temporal-consistency condition that successive predictions should satisfy. We
leverage this condition to develop a novel loss function for training
incremental sequence classifiers. Through a concrete example, we demonstrate
that optimizing this loss can offer substantial gains in data efficiency. We
apply our method to text classification tasks and show that it improves
predictive accuracy over competing approaches on several benchmark datasets. We
further evaluate our approach on the task of verifying large language model
generations for correctness in grade-school math problems. Our results show
that models trained with our method are better able to distinguish promising
generations from unpromising ones after observing only a few tokens.",2025-05-22,"Lucas Maystre, Gabriel Barello, Tudor Berariu, Aleix Cambray, Rares Dolga, Alvaro Ortega Gonzalez, Andrei Nica, David Barber",http://arxiv.org/pdf/2505.16548v1,cs.LG
HOFT: Householder Orthogonal Fine-tuning,"Adaptation of foundation models using low-rank methods is a widespread
approach. Another way to adapt these models is to employ orthogonal fine-tuning
methods, which are less time and memory efficient despite their good
generalization properties. In this work, we propose Householder Orthogonal
Fine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to
alleviate time and space complexity. Moreover, some theoretical properties of
the orthogonal fine-tuning paradigm are explored. From this exploration, Scaled
Householder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are
evaluated in downstream tasks, namely commonsense reasoning, machine
translation, subject-driven generation and mathematical reasoning. Compared
with state-of-the-art adaptation methods, HOFT and SHOFT show comparable or
better results.",2025-05-22,"Alejandro Moreno Arcas, Albert Sanchis, Jorge Civera, Alfons Juan",http://arxiv.org/pdf/2505.16531v1,cs.LG
Joint Relational Database Generation via Graph-Conditional Diffusion Models,"Building generative models for relational databases (RDBs) is important for
applications like privacy-preserving data release and augmenting real datasets.
However, most prior work either focuses on single-table generation or relies on
autoregressive factorizations that impose a fixed table order and generate
tables sequentially. This approach limits parallelism, restricts flexibility in
downstream applications like missing value imputation, and compounds errors due
to commonly made conditional independence assumptions. We propose a
fundamentally different approach: jointly modeling all tables in an RDB without
imposing any order. By using a natural graph representation of RDBs, we propose
the Graph-Conditional Relational Diffusion Model (GRDM). GRDM leverages a graph
neural network to jointly denoise row attributes and capture complex
inter-table dependencies. Extensive experiments on six real-world RDBs
demonstrate that our approach substantially outperforms autoregressive
baselines in modeling multi-hop inter-table correlations and achieves
state-of-the-art performance on single-table fidelity metrics.",2025-05-22,"Mohamed Amine Ketata, David Lüdke, Leo Schwinn, Stephan Günnemann",http://arxiv.org/pdf/2505.16527v1,cs.LG
CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving,"Maintaining robust 3D perception under dynamic and unpredictable test-time
conditions remains a critical challenge for autonomous driving systems.
Existing test-time adaptation (TTA) methods often fail in high-variance tasks
like 3D object detection due to unstable optimization and sharp minima. While
recent model merging strategies based on linear mode connectivity (LMC) offer
improved stability by interpolating between fine-tuned checkpoints, they are
computationally expensive, requiring repeated checkpoint access and multiple
forward passes. In this paper, we introduce CodeMerge, a lightweight and
scalable model merging framework that bypasses these limitations by operating
in a compact latent space. Instead of loading full models, CodeMerge represents
each checkpoint with a low-dimensional fingerprint derived from the source
model's penultimate features and constructs a key-value codebook. We compute
merging coefficients using ridge leverage scores on these fingerprints,
enabling efficient model composition without compromising adaptation quality.
Our method achieves strong performance across challenging benchmarks, improving
end-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by
over 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as
online mapping, motion prediction and planning even without training. Code and
pretrained models are released in the supplementary material.",2025-05-22,"Huitong Yang, Zhuoxiao Chen, Fengyi Zhang, Zi Huang, Yadan Luo",http://arxiv.org/pdf/2505.16524v1,cs.LG
Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods,"Kernel methods are widely used in machine learning due to their flexibility
and expressive power. However, their black-box nature poses significant
challenges to interpretability, limiting their adoption in high-stakes
applications. Shapley value-based feature attribution techniques, such as SHAP
and kernel-specific variants like RKHS-SHAP, offer a promising path toward
explainability. Yet, computing exact Shapley values remains computationally
intractable in general, motivating the development of various approximation
schemes. In this work, we introduce PKeX-Shapley, a novel algorithm that
utilizes the multiplicative structure of product kernels to enable the exact
computation of Shapley values in polynomial time. We show that product-kernel
models admit a functional decomposition that allows for a recursive formulation
of Shapley values. This decomposition not only yields computational efficiency
but also enhances interpretability in kernel-based learning. We also
demonstrate how our framework can be generalized to explain kernel-based
statistical discrepancies such as the Maximum Mean Discrepancy (MMD) and the
Hilbert-Schmidt Independence Criterion (HSIC), thus offering new tools for
interpretable statistical inference.",2025-05-22,"Majid Mohammadi, Siu Lun Chau, Krikamol Muandet",http://arxiv.org/pdf/2505.16516v1,cs.LG
Accuracy vs. Accuracy: Computational Tradeoffs Between Classification Rates and Utility,"We revisit the foundations of fairness and its interplay with utility and
efficiency in settings where the training data contain richer labels, such as
individual types, rankings, or risk estimates, rather than just binary
outcomes. In this context, we propose algorithms that achieve stronger notions
of evidence-based fairness than are possible in standard supervised learning.
Our methods support classification and ranking techniques that preserve
accurate subpopulation classification rates, as suggested by the underlying
data distributions, across a broad class of classification rules and downstream
applications. Furthermore, our predictors enable loss minimization, whether
aimed at maximizing utility or in the service of fair treatment.
  Complementing our algorithmic contributions, we present impossibility results
demonstrating that simultaneously achieving accurate classification rates and
optimal loss minimization is, in some cases, computationally infeasible. Unlike
prior impossibility results, our notions are not inherently in conflict and are
simultaneously satisfied by the Bayes-optimal predictor. Furthermore, we show
that each notion can be satisfied individually via efficient learning. Our
separation thus stems from the computational hardness of learning a
sufficiently good approximation of the Bayes-optimal predictor. These
computational impossibilities present a choice between two natural and
attainable notions of accuracy that could both be motivated by fairness.",2025-05-22,"Noga Amit, Omer Reingold, Guy N. Rothblum",http://arxiv.org/pdf/2505.16494v1,cs.LG
Constrained Non-negative Matrix Factorization for Guided Topic Modeling of Minority Topics,"Topic models often fail to capture low-prevalence, domain-critical themes,
so-called minority topics, such as mental health themes in online comments.
While some existing methods can incorporate domain knowledge, such as expected
topical content, methods allowing guidance may require overly detailed expected
topics, hindering the discovery of topic divisions and variation. We propose a
topic modeling solution via a specially constrained NMF. We incorporate a seed
word list characterizing minority content of interest, but we do not require
experts to pre-specify their division across minority topics. Through
prevalence constraints on minority topics and seed word content across topics,
we learn distinct data-driven minority topics as well as majority topics. The
constrained NMF is fitted via Karush-Kuhn-Tucker (KKT) conditions with
multiplicative updates. We outperform several baselines on synthetic data in
terms of topic purity, normalized mutual information, and also evaluate topic
quality using Jensen-Shannon divergence (JSD). We conduct a case study on
YouTube vlog comments, analyzing viewer discussion of mental health content;
our model successfully identifies and reveals this domain-relevant minority
content.",2025-05-22,"Seyedeh Fatemeh Ebrahimi, Jaakko Peltonen",http://arxiv.org/pdf/2505.16493v1,cs.LG
Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling,"Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by
replacing the fully factorised Gaussian prior with a GP prior, thereby
capturing richer correlations among latent variables. However, performing exact
GP inference in large-scale GPVAEs is computationally prohibitive, often
forcing existing approaches to rely on restrictive kernel assumptions or large
sets of inducing points. In this work, we propose a neighbour-driven
approximation strategy that exploits local adjacencies in the latent space to
achieve scalable GPVAE inference. By confining computations to the nearest
neighbours of each data point, our method preserves essential latent
dependencies, allowing more flexible kernel choices and mitigating the need for
numerous inducing points. Through extensive experiments on tasks including
representation learning, data imputation, and conditional generation, we
demonstrate that our approach outperforms other GPVAE variants in both
predictive performance and computational efficiency.",2025-05-22,"Xinxing Shi, Xiaoyu Jiang, Mauricio A. Álvarez",http://arxiv.org/pdf/2505.16481v1,cs.LG
Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization,"Deep reinforcement learning (DRL) has been widely used for dynamic algorithm
configuration, particularly in evolutionary computation, which benefits from
the adaptive update of parameters during the algorithmic execution. However,
applying DRL to algorithm configuration for multi-objective combinatorial
optimization (MOCO) problems remains relatively unexplored. This paper presents
a novel graph neural network (GNN) based DRL to configure multi-objective
evolutionary algorithms. We model the dynamic algorithm configuration as a
Markov decision process, representing the convergence of solutions in the
objective space by a graph, with their embeddings learned by a GNN to enhance
the state representation. Experiments on diverse MOCO challenges indicate that
our method outperforms traditional and DRL-based algorithm configuration
methods in terms of efficacy and adaptability. It also exhibits advantageous
generalizability across objective types and problem sizes, and applicability to
different evolutionary computation methods.",2025-05-22,"Robbert Reijnen, Yaoxin Wu, Zaharah Bukhsh, Yingqian Zhang",http://arxiv.org/pdf/2505.16471v2,cs.LG
AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer,"Recently, vision transformers (ViTs) have achieved excellent performance on
vision tasks by measuring the global self-attention among the image patches.
Given $n$ patches, they will have quadratic complexity such as
$\mathcal{O}(n^2)$ and the time cost is high when splitting the input image
with a small granularity. Meanwhile, the pivotal information is often randomly
gathered in a few regions of an input image, some tokens may not be helpful for
the downstream tasks. To handle this problem, we introduce an anchor-based
efficient vision transformer (AnchorFormer), which employs the anchor tokens to
learn the pivotal information and accelerate the inference. Firstly, by
estimating the bipartite attention between the anchors and tokens, the
complexity will be reduced from $\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$, where
$m$ is an anchor number and $m < n$. Notably, by representing the anchors with
the neurons in a neural layer, we can differentiable learn these distributions
and approximate global self-attention through the Markov process. Moreover, we
extend the proposed model to three downstream tasks including classification,
detection, and segmentation. Extensive experiments show the effectiveness of
our AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs
reduction on ImageNet classification, 81.3% higher mAP on COCO detection under
comparable FLOPs, as compared to the current baselines.",2025-05-22,"Jiquan Shan, Junxiao Wang, Lifeng Zhao, Liang Cai, Hongyuan Zhang, Ioannis Liritzis",http://arxiv.org/pdf/2505.16463v2,cs.LG
CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI,"Accurate and efficient quantification of cardiac function is essential for
the estimation of prognosis of cardiovascular diseases (CVDs). One of the most
commonly used metrics for evaluating cardiac pumping performance is left
ventricular ejection fraction (LVEF). However, LVEF can be affected by factors
such as inter-observer variability and varying pre-load and after-load
conditions, which can reduce its reproducibility. Additionally, cardiac
dysfunction may not always manifest as alterations in LVEF, such as in heart
failure and cardiotoxicity diseases. An alternative measure that can provide a
relatively load-independent quantitative assessment of myocardial contractility
is myocardial strain and strain rate. By using LVEF in combination with
myocardial strain, it is possible to obtain a thorough description of cardiac
function. Automated estimation of LVEF and other volumetric measures from
cine-MRI sequences can be achieved through segmentation models, while strain
calculation requires the estimation of tissue displacement between sequential
frames, which can be accomplished using registration models. These tasks are
often performed separately, potentially limiting the assessment of cardiac
function. To address this issue, in this study we propose an end-to-end deep
learning (DL) model that jointly estimates groupwise (GW) registration and
segmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep
GW network was trained and validated on a large dataset of 4-chamber view
cine-MRI image series of 374 subjects. A quantitative comparison with
conventional GW registration using elastix and two DL-based methods showed that
the proposed model improved performance and substantially reduced computation
time.",2025-05-22,"Mohamed S. Elmahdy, Marius Staring, Patrick J. H. de Koning, Samer Alabed, Mahan Salehi, Faisal Alandejani, Michael Sharkey, Ziad Aldabbagh, Andrew J. Swift, Rob J. van der Geest",http://arxiv.org/pdf/2505.16452v1,cs.LG
Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models,"Multimodal large language models (MLLMs) enable powerful cross-modal
reasoning capabilities. However, the expanded input space introduces new attack
surfaces. Previous jailbreak attacks often inject malicious instructions from
text into less aligned modalities, such as vision. As MLLMs increasingly
incorporate cross-modal consistency and alignment mechanisms, such explicit
attacks become easier to detect and block. In this work, we propose a novel
implicit jailbreak framework termed IJA that stealthily embeds malicious
instructions into images via least significant bit steganography and couples
them with seemingly benign, image-related textual prompts. To further enhance
attack effectiveness across diverse MLLMs, we incorporate adversarial suffixes
generated by a surrogate model and introduce a template optimization module
that iteratively refines both the prompt and embedding based on model feedback.
On commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack
success rates of over 90% using an average of only 3 queries.",2025-05-22,"Zhaoxin Wang, Handing Wang, Cong Tian, Yaochu Jin",http://arxiv.org/pdf/2505.16446v1,cs.LG
Ranked Entropy Minimization for Continual Test-Time Adaptation,"Test-time adaptation aims to adapt to realistic environments in an online
manner by learning during test time. Entropy minimization has emerged as a
principal strategy for test-time adaptation due to its efficiency and
adaptability. Nevertheless, it remains underexplored in continual test-time
adaptation, where stability is more important. We observe that the entropy
minimization method often suffers from model collapse, where the model
converges to predicting a single class for all images due to a trivial
solution. We propose ranked entropy minimization to mitigate the stability
problem of the entropy minimization method and extend its applicability to
continuous scenarios. Our approach explicitly structures the prediction
difficulty through a progressive masking strategy. Specifically, it gradually
aligns the model's probability distributions across different levels of
prediction difficulty while preserving the rank order of entropy. The proposed
method is extensively evaluated across various benchmarks, demonstrating its
effectiveness through empirical results. Our code is available at
https://github.com/pilsHan/rem",2025-05-22,"Jisu Han, Jaemin Na, Wonjun Hwang",http://arxiv.org/pdf/2505.16441v1,cs.LG
Efficient Training of Neural SDEs Using Stochastic Optimal Control,"We present a hierarchical, control theory inspired method for variational
inference (VI) for neural stochastic differential equations (SDEs). While VI
for neural SDEs is a promising avenue for uncertainty-aware reasoning in
time-series, it is computationally challenging due to the iterative nature of
maximizing the ELBO. In this work, we propose to decompose the control term
into linear and residual non-linear components and derive an optimal control
term for linear SDEs, using stochastic optimal control. Modeling the non-linear
component by a neural network, we show how to efficiently train neural SDEs
without sacrificing their expressive power. Since the linear part of the
control term is optimal and does not need to be learned, the training is
initialized at a lower cost and we observe faster convergence.",2025-05-22,"Rembert Daems, Manfred Opper, Guillaume Crevecoeur, Tolga Birdal",http://arxiv.org/pdf/2505.17150v1,cs.LG
WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning,"While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.",2025-05-22,"Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, Lihong Li",http://arxiv.org/pdf/2505.16421v1,cs.LG
Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)
combined with external contexts to enhance the accuracy and reliability of
generated responses. However, reliably attributing generated content to
specific context segments, context attribution, remains challenging due to the
computationally intensive nature of current methods, which often require
extensive fine-tuning or human annotation. In this work, we introduce a novel
Jensen-Shannon Divergence driven method to Attribute Response to Context
(ARC-JSD), enabling efficient and accurate identification of essential context
sentences without additional fine-tuning or surrogate modelling. Evaluations on
a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using
instruction-tuned LLMs in different scales demonstrate superior accuracy and
significant computational efficiency improvements compared to the previous
surrogate-based method. Furthermore, our mechanistic analysis reveals specific
attention heads and multilayer perceptron (MLP) layers responsible for context
attribution, providing valuable insights into the internal workings of RAG
models.",2025-05-22,"Ruizhe Li, Chen Chen, Yuchen Hu, Yanjun Gao, Xi Wang, Emine Yilmaz",http://arxiv.org/pdf/2505.16415v1,cs.LG
Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression,"Despite their remarkable progress in multimodal understanding tasks, large
vision language models (LVLMs) often suffer from ""hallucinations"", generating
texts misaligned with the visual context. Existing methods aimed at reducing
hallucinations through inference time intervention incur a significant increase
in latency. To mitigate this, we present SPIN, a task-agnostic attention-guided
head suppression strategy that can be seamlessly integrated during inference,
without incurring any significant compute or latency overhead. We investigate
whether hallucination in LVLMs can be linked to specific model components. Our
analysis suggests that hallucinations can be attributed to a dynamic subset of
attention heads in each layer. Leveraging this insight, for each text query
token, we selectively suppress attention heads that exhibit low attention to
image tokens, keeping the top-K attention heads intact. Extensive evaluations
on visual question answering and image description tasks demonstrate the
efficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining
F1, and improving throughput by 1.8x compared to existing alternatives. Code is
available at https://github.com/YUECHE77/SPIN.",2025-05-22,"Sreetama Sarkar, Yue Che, Alex Gavin, Peter A. Beerel, Souvik Kundu",http://arxiv.org/pdf/2505.16411v1,cs.LG
Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning,"Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.",2025-05-22,"Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, Ji-Rong Wen",http://arxiv.org/pdf/2505.16410v1,cs.LG
Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach,"Manipulation of local training data and local updates, i.e., the poisoning
attack, is the main threat arising from the collaborative nature of the
federated learning (FL) paradigm. Most existing poisoning attacks aim to
manipulate local data/models in a way that causes denial-of-service (DoS)
issues. In this paper, we introduce a novel attack method, named Federated
Learning Sliding Attack (FedSA) scheme, aiming at precisely introducing the
extent of poisoning in a subtle controlled manner. It operates with a
predefined objective, such as reducing global model's prediction accuracy by
10\%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC)
theory with model poisoning attacks. It can manipulate the updates from
malicious clients to drive the global model towards a compromised state,
achieving this at a controlled and inconspicuous rate. Additionally, leveraging
the robust control properties of FedSA allows precise control over the
convergence bounds, enabling the attacker to set the global accuracy of the
poisoned model to any desired level. Experimental results demonstrate that
FedSA can accurately achieve a predefined global accuracy with fewer malicious
clients while maintaining a high level of stealth and adjustable learning
rates.",2025-05-22,"Huazi Pan, Yanjun Zhang, Leo Yu Zhang, Scott Adams, Abbas Kouzani, Suiyang Khoo",http://arxiv.org/pdf/2505.16403v1,cs.LG
"Divide-Fuse-Conquer: Eliciting ""Aha Moments"" in Multi-Scenario Games","Large language models (LLMs) have been observed to suddenly exhibit advanced
reasoning abilities during reinforcement learning (RL), resembling an ``aha
moment'' triggered by simple outcome-based rewards. While RL has proven
effective in eliciting such breakthroughs in tasks involving mathematics,
coding, and vision, it faces significant challenges in multi-scenario games.
The diversity of game rules, interaction modes, and environmental complexities
often leads to policies that perform well in one scenario but fail to
generalize to others. Simply combining multiple scenarios during training
introduces additional challenges, such as training instability and poor
performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a
framework designed to enhance generalization in multi-scenario RL. This
approach starts by heuristically grouping games based on characteristics such
as rules and difficulties. Specialized models are then trained for each group
to excel at games in the group is what we refer to as the divide step. Next, we
fuse model parameters from different groups as a new model, and continue
training it for multiple groups, until the scenarios in all groups are
conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align
trained with the Divide-Fuse-Conquer strategy reaches a performance level
comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can
inspire future research on using reinforcement learning to improve the
generalization of LLMs.",2025-05-22,"Xiaoqing Zhang, Huabin Zheng, Ang Lv, Yuhan Liu, Zirui Song, Flood Sung, Xiuying Chen, Rui Yan",http://arxiv.org/pdf/2505.16401v1,cs.LG
AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning,"Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.",2025-05-22,"Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",http://arxiv.org/pdf/2505.16400v1,cs.LG
Omni TM-AE: A Scalable and Interpretable Embedding Model Using the Full Tsetlin Machine State Space,"The increasing complexity of large-scale language models has amplified
concerns regarding their interpretability and reusability. While traditional
embedding models like Word2Vec and GloVe offer scalability, they lack
transparency and often behave as black boxes. Conversely, interpretable models
such as the Tsetlin Machine (TM) have shown promise in constructing explainable
learning systems, though they previously faced limitations in scalability and
reusability. In this paper, we introduce Omni Tsetlin Machine AutoEncoder (Omni
TM-AE), a novel embedding model that fully exploits the information contained
in the TM's state matrix, including literals previously excluded from clause
formation. This method enables the construction of reusable, interpretable
embeddings through a single training phase. Extensive experiments across
semantic similarity, sentiment classification, and document clustering tasks
show that Omni TM-AE performs competitively with and often surpasses mainstream
embedding models. These results demonstrate that it is possible to balance
performance, scalability, and interpretability in modern Natural Language
Processing (NLP) systems without resorting to opaque architectures.",2025-05-22,"Ahmed K. Kadhim, Lei Jiao, Rishad Shafik, Ole-Christoffer Granmo",http://arxiv.org/pdf/2505.16386v1,cs.LG
PaTH Attention: Position Encoding via Accumulating Householder Transformations,"The attention mechanism is a core primitive in modern large language models
(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,
position encoding is essential for modeling structured domains such as
language. Rotary position encoding (RoPE) has emerged as the de facto standard
approach for position encoding and is part of many modern LLMs. However, in
RoPE the key/query transformation between two elements in a sequence is only a
function of their relative position and otherwise independent of the actual
input. This limits the expressivity of RoPE-based transformers.
  This paper describes PaTH, a flexible data-dependent position encoding scheme
based on accumulated products of Householder(like) transformations, where each
transformation is data-dependent, i.e., a function of the input. We derive an
efficient parallel algorithm for training through exploiting a compact
representation of products of Householder matrices, and implement a
FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both
targeted synthetic benchmarks and moderate-scale real-world language modeling
experiments, we find that PaTH demonstrates superior performance compared to
RoPE and other recent baselines.",2025-05-22,"Songlin Yang, Yikang Shen, Kaiyue Wen, Shawn Tan, Mayank Mishra, Liliang Ren, Rameswar Panda, Yoon Kim",http://arxiv.org/pdf/2505.16381v1,cs.LG
SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning,"How to design reinforcement learning (RL) tasks that effectively unleash the
reasoning capability of large language models (LLMs) remains an open question.
Existing RL tasks (e.g., math, programming, and constructing reasoning tasks)
suffer from three key limitations: (1) Scalability. They rely heavily on human
annotation or expensive LLM synthesis to generate sufficient training data. (2)
Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3)
Controllable Difficulty. Most tasks lack fine-grained difficulty control,
making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework
that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM
reasoning. Saturn enables scalable task construction, rule-based verification,
and precise difficulty control. Saturn designs a curriculum learning pipeline
that continuously improves LLMs' reasoning capability by constructing SAT tasks
of increasing difficulty and training LLMs from easy to hard. To ensure stable
training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying
difficulty. It supports the evaluation of how LLM reasoning changes with
problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain
Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT
problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of
+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B
and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,
AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in
constructing RL tasks, Saturn achieves further improvements of +8.8%. We
release the source code, data, and models to support future research.",2025-05-22,"Huanyu Liu, Jia Li, Hao Zhu, Kechi Zhang, Yihong Dong, Ge Li",http://arxiv.org/pdf/2505.16368v1,cs.LG
A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules,"Developing new molecular compounds is crucial to address pressing challenges,
from health to environmental sustainability. However, exploring the molecular
space to discover new molecules is difficult due to the vastness of the space.
Here we introduce CoCoGraph, a collaborative and constrained graph diffusion
model capable of generating molecules that are guaranteed to be chemically
valid. Thanks to the constraints built into the model and to the collaborative
mechanism, CoCoGraph outperforms state-of-the-art approaches on standard
benchmarks while requiring up to an order of magnitude fewer parameters.
Analysis of 36 chemical properties also demonstrates that CoCoGraph generates
molecules with distributions more closely matching real molecules than current
models. Leveraging the model's efficiency, we created a database of 8.2M
million synthetically generated molecules and conducted a Turing-like test with
organic chemistry experts to further assess the plausibility of the generated
molecules, and potential biases and limitations of CoCoGraph.",2025-05-22,"Manuel Ruiz-Botella, Marta Sales-Pardo, Roger Guimerà",http://arxiv.org/pdf/2505.16365v1,cs.LG
AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training,"We introduce AdamS, a simple yet effective alternative to Adam for large
language model (LLM) pretraining and post-training. By leveraging a novel
denominator, i.e., the root of weighted sum of squares of the momentum and the
current gradient, AdamS eliminates the need for second-moment estimates. Hence,
AdamS is efficient, matching the memory and compute footprint of SGD with
momentum while delivering superior optimization performance. Moreover, AdamS is
easy to adopt: it can directly inherit hyperparameters of AdamW, and is
entirely model-agnostic, integrating seamlessly into existing pipelines without
modifications to optimizer APIs or architectures. The motivation behind AdamS
stems from the observed $(L_0, L_1)$ smoothness properties in transformer
objectives, where local smoothness is governed by gradient magnitudes that can
be further approximated by momentum magnitudes. We establish rigorous
theoretical convergence guarantees and provide practical guidelines for
hyperparameter selection. Empirically, AdamS demonstrates strong performance in
various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B
parameters) and reinforcement learning in post-training regimes. With its
efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling
alternative to existing optimizers.",2025-05-22,"Huishuai Zhang, Bohan Wang, Luoxin Chen",http://arxiv.org/pdf/2505.16363v1,cs.LG
Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation,"Semantic segmentation models trained on synthetic data often perform poorly
on real-world images due to domain gaps, particularly in adverse conditions
where labeled data is scarce. Yet, recent foundation models enable to generate
realistic images without any training. This paper proposes to leverage such
diffusion models to improve the performance of vision models when learned on
synthetic data. We introduce two novel techniques for semantically consistent
style transfer using diffusion models: Class-wise Adaptive Instance
Normalization and Cross-Attention (CACTI) and its extension with selective
attention Filtering (CACTIF). CACTI applies statistical normalization
selectively based on semantic classes, while CACTIF further filters
cross-attention maps based on feature similarity, preventing artifacts in
regions with weak cross-attention correspondences. Our methods transfer style
characteristics while preserving semantic boundaries and structural coherence,
unlike approaches that apply global transformations or generate content without
constraints. Experiments using GTA5 as source and Cityscapes/ACDC as target
domains show that our approach produces higher quality images with lower FID
scores and better content preservation. Our work demonstrates that class-aware
diffusion-based style transfer effectively bridges the synthetic-to-real domain
gap even with minimal target domain data, advancing robust perception systems
for challenging real-world applications. The source code is available at:
https://github.com/echigot/cactif.",2025-05-22,"Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Thomas Oberlin",http://arxiv.org/pdf/2505.16360v1,cs.LG
Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning,"In this paper, we introduce a versatile scheme for optimizing the arrival
rates of quasi-reversible queueing systems. We first propose an alternative
definition of quasi-reversibility that encompasses reversibility and highlights
the importance of the definition of customer classes. In a second time, we
introduce balanced arrival control policies, which generalize the notion of
balanced arrival rates introduced in the context of Whittle networks, to the
much broader class of quasi-reversible queueing systems. We prove that
supplementing a quasi-reversible queueing system with a balanced
arrival-control policy preserves the quasi-reversibility, and we specify the
form of the stationary measures. We revisit two canonical examples of
quasi-reversible queueing systems, Whittle networks and order-independent
queues. Lastly, we focus on the problem of admission control and leverage our
results in the frameworks of optimization and reinforcement learning.",2025-05-22,"Céline Comte, Pascal Moyal",http://arxiv.org/pdf/2505.16353v1,cs.LG
Graph Attention Network for Optimal User Association in Wireless Networks,"With increased 5G deployments, network densification is higher than ever to
support the exponentially high throughput requirements. However, this has meant
a significant increase in energy consumption, leading to higher operational
expenditure (OpEx) for network operators creating an acute need for
improvements in network energy savings (NES). A key determinant of operational
efficacy in cellular networks is the user association (UA) policy, as it
affects critical aspects like spectral efficiency, load balancing etc. and
therefore impacts the overall energy consumption of the network directly.
Furthermore, with cellular network topologies lending themselves well to
graphical abstractions, use of graphs in network optimization has gained
significant prominence. In this work, we propose and analyze a graphical
abstraction based optimization for UA in cellular networks to improve NES by
determining when energy saving features like cell switch off can be activated.
A comparison with legacy approaches establishes the superiority of the proposed
approach.",2025-05-22,"Javad Mirzaei, Jeebak Mitra, Gwenael Poitau",http://arxiv.org/pdf/2505.16347v1,cs.LG
A Square Peg in a Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning,"This paper studies the long-tailed semi-supervised learning (LTSSL) with
distribution mismatch, where the class distribution of the labeled training
data follows a long-tailed distribution and mismatches with that of the
unlabeled training data. Most existing methods introduce auxiliary classifiers
(experts) to model various unlabeled data distributions and produce
pseudo-labels, but the expertises of various experts are not fully utilized. We
observe that different experts are good at predicting different intervals of
samples, e.g., long-tailed expert is skilled in samples located in the head
interval and uniform expert excels in samples located in the medium interval.
Therefore, we propose a dynamic expert assignment module that can estimate the
class membership (i.e., head, medium, or tail class) of samples, and
dynamically assigns suitable expert to each sample based on the estimated
membership to produce high-quality pseudo-label in the training phase and
produce prediction in the testing phase. We also theoretically reveal that
integrating different experts' strengths will lead to a smaller generalization
error bound. Moreover, we find that the deeper features are more biased toward
the head class but with more discriminative ability, while the shallower
features are less biased but also with less discriminative ability. We,
therefore, propose a multi-depth feature fusion module to utilize different
depth features to mitigate the model bias. Our method demonstrates its
effectiveness through comprehensive experiments on the CIFAR-10-LT, STL-10-LT,
and SVHN-LT datasets across various settings. The code is available at
https://github.com/yaxinhou/Meta-Expert.",2025-05-22,"Yaxin Hou, Yuheng Jia",http://arxiv.org/pdf/2505.16341v1,cs.LG
Improving Chemical Understanding of LLMs via SMILES Parsing,"Large language models (LLMs) are increasingly recognized as powerful tools
for scientific discovery, particularly in molecular science. A fundamental
requirement for these models is the ability to accurately understand molecular
structures, commonly encoded in the SMILES representation. However, current
LLMs struggle to interpret SMILES, even failing to carry out basic tasks such
as counting molecular rings. To address this limitation, we introduce CLEANMOL,
a novel framework that formulates SMILES parsing into a suite of clean and
deterministic tasks explicitly designed to promote graph-level molecular
comprehension. These tasks span from subgraph matching to global graph
matching, providing structured supervision aligned with molecular structural
properties. We construct a molecular pretraining dataset with adaptive
difficulty scoring and pre-train open-source LLMs on these tasks. Our results
show that CLEANMOL not only enhances structural comprehension but also achieves
the best or competes with the baseline on the Mol-Instructions benchmark.",2025-05-22,"Yunhui Jang, Jaehyung Kim, Sungsoo Ahn",http://arxiv.org/pdf/2505.16340v1,cs.LG
Understanding Differential Transformer Unchains Pretrained Self-Attentions,"Differential Transformer has recently gained significant attention for its
impressive empirical performance, often attributed to its ability to perform
noise canceled attention. However, precisely how differential attention
achieves its empirical benefits remains poorly understood. Moreover,
Differential Transformer architecture demands large-scale training from
scratch, hindering utilization of open pretrained weights. In this work, we
conduct an in-depth investigation of Differential Transformer, uncovering three
key factors behind its success: (1) enhanced expressivity via negative
attention, (2) reduced redundancy among attention heads, and (3) improved
learning dynamics. Based on these findings, we propose DEX, a novel method to
efficiently integrate the advantages of differential attention into pretrained
language models. By reusing the softmax attention scores and adding a
lightweight differential operation on the output value matrix, DEX effectively
incorporates the key advantages of differential attention while remaining
lightweight in both training and inference. Evaluations confirm that DEX
substantially improves the pretrained LLMs across diverse benchmarks, achieving
significant performance gains with minimal adaptation data (< 0.01\%).",2025-05-22,"Chaerin Kong, Jiho Jang, Nojun Kwak",http://arxiv.org/pdf/2505.16333v1,cs.LG
Better Rates for Private Linear Regression in the Proportional Regime via Aggressive Clipping,"Differentially private (DP) linear regression has received significant
attention in the recent theoretical literature, with several works aimed at
obtaining improved error rates. A common approach is to set the clipping
constant much larger than the expected norm of the per-sample gradients. While
simplifying the analysis, this is however in sharp contrast with what empirical
evidence suggests to optimize performance. Our work bridges this gap between
theory and practice: we provide sharper rates for DP stochastic gradient
descent (DP-SGD) by crucially operating in a regime where clipping happens
frequently. Specifically, we consider the setting where the data is
multivariate Gaussian, the number of training samples $n$ is proportional to
the input dimension $d$, and the algorithm guarantees constant-order zero
concentrated DP. Our method relies on establishing a deterministic equivalent
for the trajectory of DP-SGD in terms of a family of ordinary differential
equations (ODEs). As a consequence, the risk of DP-SGD is bounded between two
ODEs, with upper and lower bounds matching for isotropic data. By studying
these ODEs when $n / d$ is large enough, we demonstrate the optimality of
aggressive clipping, and we uncover the benefits of decaying learning rate and
private noise scheduling.",2025-05-22,"Simone Bombari, Inbar Seroussi, Marco Mondelli",http://arxiv.org/pdf/2505.16329v1,cs.LG
ChemMLLM: Chemical Multimodal Large Language Model,"Multimodal large language models (MLLMs) have made impressive progress in
many applications in recent years. However, chemical MLLMs that can handle
cross-modal understanding and generation remain underexplored. To fill this
gap, in this paper, we propose ChemMLLM, a unified chemical multimodal large
language model for molecule understanding and generation. Also, we design five
multimodal tasks across text, molecular SMILES strings, and image, and curate
the datasets. We benchmark ChemMLLM against a range of general leading MLLMs
and Chemical LLMs on these tasks. Experimental results show that ChemMLLM
achieves superior performance across all evaluated tasks. For example, in
molecule image optimization task, ChemMLLM outperforms the best baseline
(GPT-4o) by 118.9\% (4.27 vs 1.95 property improvement). The code is publicly
available at https://github.com/bbsbz/ChemMLLM.git.",2025-05-22,"Qian Tan, Dongzhan Zhou, Peng Xia, Wanhao Liu, Wanli Ouyang, Lei Bai, Yuqiang Li, Tianfan Fu",http://arxiv.org/pdf/2505.16326v1,cs.LG
AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners,"Self-Taught Reasoners (STaR), synonymously known as Rejection sampling
Fine-Tuning (RFT), is an integral part of the training pipeline of
self-improving reasoning Language Models (LMs). The self-improving mechanism
often employs random observation (data) sampling. However, this results in
trained observation imbalance; inefficiently over-training on solved examples
while under-training on challenging ones. In response, we introduce Adaptive
STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two
adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting
balanced training across observations, and (2) Adaptive Sampling for
Curriculum: dynamically adjusting data difficulty to match the model's evolving
strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all
instances (6/6) and reduces training FLOPs by an average of 58.6% against an
extensive list of baselines. These improvements in performance and efficiency
generalize to different pre-trained LMs and larger models, paving the way for
more efficient and effective self-improving LMs.",2025-05-22,"Woosung Koh, Wonbeen Oh, Jaein Jang, MinHyung Lee, Hyeongjin Kim, Ah Yeon Kim, Joonkee Kim, Junghyun Lee, Taehyeon Kim, Se-Young Yun",http://arxiv.org/pdf/2505.16322v1,cs.LG
Learning novel representations of variable sources from multi-modal $\textit{Gaia}$ data via autoencoders,"Gaia Data Release 3 (DR3) published for the first time epoch photometry,
BP/RP (XP) low-resolution mean spectra, and supervised classification results
for millions of variable sources. This extensive dataset offers a unique
opportunity to study their variability by combining multiple Gaia data
products. In preparation for DR4, we propose and evaluate a machine learning
methodology capable of ingesting multiple Gaia data products to achieve an
unsupervised classification of stellar and quasar variability. A dataset of 4
million Gaia DR3 sources is used to train three variational autoencoders (VAE),
which are artificial neural networks (ANNs) designed for data compression and
generation. One VAE is trained on Gaia XP low-resolution spectra, another on a
novel approach based on the distribution of magnitude differences in the Gaia G
band, and the third on folded Gaia G band light curves. Each Gaia source is
compressed into 15 numbers, representing the coordinates in a 15-dimensional
latent space generated by combining the outputs of these three models. The
learned latent representation produced by the ANN effectively distinguishes
between the main variability classes present in Gaia DR3, as demonstrated
through both supervised and unsupervised classification analysis of the latent
space. The results highlight a strong synergy between light curves and
low-resolution spectral data, emphasising the benefits of combining the
different Gaia data products. A two-dimensional projection of the latent
variables reveals numerous overdensities, most of which strongly correlate with
astrophysical properties, showing the potential of this latent space for
astrophysical discovery. We show that the properties of our novel latent
representation make it highly valuable for variability analysis tasks,
including classification, clustering and outlier detection.",2025-05-22,"P. Huijse, J. De Ridder, L. Eyer, L. Rimoldini, B. Holl, N. Chornay, J. Roquette, K. Nienartowicz, G. Jevardat de Fombelle, D. J. Fritzewski, A. Kemp, V. Vanlaer, M. Vanrespaille, H. Wang, M. I. Carnerero, C. M. Raiteri, G. Marton, M. Madarász, G. Clementini, P. Gavras, C. Aerts",http://arxiv.org/pdf/2505.16320v1,cs.LG
FreshRetailNet-50K: A Stockout-Annotated Censored Demand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail,"Accurate demand estimation is critical for the retail business in guiding the
inventory and pricing policies of perishable products. However, it faces
fundamental challenges from censored sales data during stockouts, where
unobserved demand creates systemic policy biases. Existing datasets lack the
temporal resolution and annotations needed to address this censoring effect. To
fill this gap, we present FreshRetailNet-50K, the first large-scale benchmark
for censored demand estimation. It comprises 50,000 store-product time series
of detailed hourly sales data from 898 stores in 18 major cities, encompassing
863 perishable SKUs meticulously annotated for stockout events. The hourly
stock status records unique to this dataset, combined with rich contextual
covariates, including promotional discounts, precipitation, and temporal
features, enable innovative research beyond existing solutions. We demonstrate
one such use case of two-stage demand modeling: first, we reconstruct the
latent demand during stockouts using precise hourly annotations. We then
leverage the recovered demand to train robust demand forecasting models in the
second stage. Experimental results show that this approach achieves a 2.73%
improvement in prediction accuracy while reducing the systematic demand
underestimation from 7.37% to near-zero bias. With unprecedented temporal
granularity and comprehensive real-world information, FreshRetailNet-50K opens
new research directions in demand imputation, perishable inventory
optimization, and causal retail analytics. The unique annotation quality and
scale of the dataset address long-standing limitations in retail AI, providing
immediate solutions and a platform for future methodological innovation. The
data (https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K) and code
(https://github.com/Dingdong-Inc/frn-50k-baseline}) are openly released.",2025-05-22,"Yangyang Wang, Jiawei Gu, Li Long, Xin Li, Li Shen, Zhouyu Fu, Xiangjun Zhou, Xu Jiang",http://arxiv.org/pdf/2505.16319v2,cs.LG
Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings,"Deep neural networks for image classification remain vulnerable to
adversarial examples -- small, imperceptible perturbations that induce
misclassifications. In black-box settings, where only the final prediction is
accessible, crafting targeted attacks that aim to misclassify into a specific
target class is particularly challenging due to narrow decision regions.
Current state-of-the-art methods often exploit the geometric properties of the
decision boundary separating a source image and a target image rather than
incorporating information from the images themselves. In contrast, we propose
Targeted Edge-informed Attack (TEA), a novel attack that utilizes edge
information from the target image to carefully perturb it, thereby producing an
adversarial image that is closer to the source image while still achieving the
desired target classification. Our approach consistently outperforms current
state-of-the-art methods across different models in low query settings (nearly
70\% fewer queries are used), a scenario especially relevant in real-world
applications with limited queries and black-box access. Furthermore, by
efficiently generating a suitable adversarial example, TEA provides an improved
target initialization for established geometry-based attacks.",2025-05-22,"Arjhun Swaminathan, Mete Akgün",http://arxiv.org/pdf/2505.16313v1,cs.LG
MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification with Spatial-Temporal Hypergraph Enhanced Meta-Learning,"Accurate classification of sleep stages based on bio-signals is fundamental
for automatic sleep stage annotation. Traditionally, this task relies on
experienced clinicians to manually annotate data, a process that is both
time-consuming and labor-intensive. In recent years, deep learning methods have
shown promise in automating this task. However, three major challenges remain:
(1) deep learning models typically require large-scale labeled datasets, making
them less effective in real-world settings where annotated data is limited; (2)
significant inter-individual variability in bio-signals often results in
inconsistent model performance when applied to new subjects, limiting
generalization; and (3) existing approaches often overlook the high-order
relationships among bio-signals, failing to simultaneously capture signal
heterogeneity and spatial-temporal dependencies. To address these issues, we
propose MetaSTH-Sleep, a few-shot sleep stage classification framework based on
spatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid
adaptation to new subjects using only a few labeled samples, while the
hypergraph structure effectively models complex spatial interconnections and
temporal dynamics simultaneously in EEG signals. Experimental results
demonstrate that MetaSTH-Sleep achieves substantial performance improvements
across diverse subjects, offering valuable insights to support clinicians in
sleep stage annotation.",2025-05-22,"Jingyu Li, Tiehua Zhang, Jinze Wang, Yi Zhang, Yuhuan Li, Yifan Zhao, Zhishu Shen, Jiannan Liu",http://arxiv.org/pdf/2505.17142v1,cs.LG
Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions,"Recent advances in generative artificial intelligence (GenAI) models have
enabled the generation of personalized content that adapts to up-to-date user
context. While personalized decision systems are often modeled using bandit
formulations, the integration of GenAI introduces new structure into otherwise
classical sequential learning problems. In GenAI-powered interventions, the
agent selects a query, but the environment experiences a stochastic response
drawn from the generative model. Standard bandit methods do not explicitly
account for this structure, where actions influence rewards only through
stochastic, observed treatments. We introduce generator-mediated
bandit-Thompson sampling (GAMBITTS), a bandit approach designed for this
action/treatment split, using mobile health interventions with large language
model-generated text as a motivating case study. GAMBITTS explicitly models
both the treatment and reward generation processes, using information in the
delivered treatment to accelerate policy learning relative to standard methods.
We establish regret bounds for GAMBITTS by decomposing sources of uncertainty
in treatment and reward, identifying conditions where it achieves stronger
guarantees than standard bandit approaches. In simulation studies, GAMBITTS
consistently outperforms conventional algorithms by leveraging observed
treatments to more accurately estimate expected rewards.",2025-05-22,"Marc Brooks, Gabriel Durham, Kihyuk Hong, Ambuj Tewari",http://arxiv.org/pdf/2505.16311v1,cs.LG
CAIFormer: A Causal Informed Transformer for Multivariate Time Series Forecasting,"Most existing multivariate time series forecasting methods adopt an
all-to-all paradigm that feeds all variable histories into a unified model to
predict their future values without distinguishing their individual roles.
However, this undifferentiated paradigm makes it difficult to identify
variable-specific causal influences and often entangles causally relevant
information with spurious correlations. To address this limitation, we propose
an all-to-one forecasting paradigm that predicts each target variable
separately. Specifically, we first construct a Structural Causal Model from
observational data and then, for each target variable, we partition the
historical sequence into four sub-segments according to the inferred causal
structure: endogenous, direct causal, collider causal, and spurious
correlation. The prediction relies solely on the first three causally relevant
sub-segments, while the spurious correlation sub-segment is excluded.
Furthermore, we propose Causal Informed Transformer (CAIFormer), a novel
forecasting model comprising three components: Endogenous Sub-segment
Prediction Block, Direct Causal Sub-segment Prediction Block, and Collider
Causal Sub-segment Prediction Block, which process the endogenous, direct
causal, and collider causal sub-segments, respectively. Their outputs are then
combined to produce the final prediction. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness of the CAIFormer.",2025-05-22,"Xingyu Zhang, Wenwen Qiang, Siyu Zhao, Huijie Guo, Jiangmeng Li, Chuxiong Sun, Changwen Zheng",http://arxiv.org/pdf/2505.16308v1,cs.LG
PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models,"Prompt optimization offers a practical and broadly applicable alternative to
fine-tuning for improving large language model (LLM) performance. However,
existing methods often rely on costly output generation, self-critiquing
abilities, or human-annotated preferences, which limit their scalability,
especially for smaller or non-instruction-tuned models. We introduce PMPO
(Probabilistic Metric Prompt Optimization), a unified framework that refines
prompts using token-level cross-entropy loss as a direct, lightweight
evaluation signal. PMPO identifies low-quality prompt segments by masking and
measuring their impact on loss, then rewrites and selects improved variants by
minimizing loss over positive and negative examples. Unlike prior methods, it
requires no output sampling or human evaluation during optimization, relying
only on forward passes and log-likelihoods. PMPO supports both supervised and
preference-based tasks through a closely aligned loss-based evaluation
strategy. Experiments show that PMPO consistently outperforms prior methods
across model sizes and tasks: it achieves the highest average accuracy on BBH,
performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates
by over 19 points. These results highlight PMPO's effectiveness, efficiency,
and broad applicability.",2025-05-22,"Chenzhuo Zhao, Ziqian Liu, Xingda Wang, Junting Lu, Chaoyi Ruan",http://arxiv.org/pdf/2505.16307v1,cs.LG
Large-Scale Bayesian Tensor Reconstruction: An Approximate Message Passing Solution,"Tensor CANDECOMP/PARAFAC decomposition (CPD) is a fundamental model for
tensor reconstruction. Although the Bayesian framework allows for principled
uncertainty quantification and automatic hyperparameter learning, existing
methods do not scale well for large tensors because of high-dimensional matrix
inversions. To this end, we introduce CP-GAMP, a scalable Bayesian CPD
algorithm. This algorithm leverages generalized approximate message passing
(GAMP) to avoid matrix inversions and incorporates an expectation-maximization
routine to jointly infer the tensor rank and noise power. Through multiple
experiments, for synthetic 100x100x100 rank 20 tensors with only 20% elements
observed, the proposed algorithm reduces runtime by 82.7% compared to the
state-of-the-art variational Bayesian CPD method, while maintaining comparable
reconstruction accuracy.",2025-05-22,"Bingyang Cheng, Zhongtao Chen, Yichen Jin, Hao Zhang, Chen Zhang, Edmud Y. Lam, Yik-Chung Wu",http://arxiv.org/pdf/2505.16305v1,cs.LG
Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space,"Molecular dynamics (MD) is a powerful tool for exploring the behavior of
atomistic systems, but its reliance on sequential numerical integration limits
simulation efficiency. We present MDtrajNet-1, a foundational AI model that
directly generates MD trajectories across chemical space, bypassing force
calculations and integration. This approach accelerates simulations by up to
two orders of magnitude compared to traditional MD, even those enhanced by
machine-learning interatomic potentials. MDtrajNet-1 combines equivariant
neural networks with a Transformer-based architecture to achieve strong
accuracy and transferability in predicting long-time trajectories for both
known and unseen systems. Remarkably, the errors of the trajectories generated
by MDtrajNet-1 for various molecular systems are close to those of the
conventional ab initio MD. The model's flexible design supports diverse
application scenarios, including different statistical ensembles, boundary
conditions, and interaction types. By overcoming the intrinsic speed barrier of
conventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable
atomistic simulations.",2025-05-22,"Fuchun Ge, Pavlo O. Dral",http://arxiv.org/pdf/2505.16301v1,cs.LG
Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs,"As the knowledge of large language models (LLMs) becomes outdated over time,
there is a growing need for efficient methods to update them, especially when
injecting proprietary information. Our study reveals that
comprehension-intensive fine-tuning tasks (e.g., question answering and blanks)
achieve substantially higher knowledge retention rates (48%) compared to
mapping-oriented tasks like translation (17%) or text-to-JSON conversion (20%),
despite exposure to identical factual content. We demonstrate that this pattern
persists across model architectures and follows scaling laws, with larger
models showing improved retention across all task types. However, all models
exhibit significant performance drops when applying injected knowledge in
broader contexts, suggesting limited semantic integration. These findings show
the importance of task selection in updating LLM knowledge, showing that
effective knowledge injection relies not just on data exposure but on the depth
of cognitive engagement during fine-tuning.",2025-05-22,"Essa Jan, Moiz Ali, Muhammad Saram Hassan, Fareed Zaffar, Yasir Zaki",http://arxiv.org/pdf/2505.17140v1,cs.LG
Fairness under Competition,"Algorithmic fairness has emerged as a central issue in ML, and it has become
standard practice to adjust ML algorithms so that they will satisfy fairness
requirements such as Equal Opportunity. In this paper we consider the effects
of adopting such fair classifiers on the overall level of ecosystem fairness.
Specifically, we introduce the study of fairness with competing firms, and
demonstrate the failure of fair classifiers in yielding fair ecosystems. Our
results quantify the loss of fairness in systems, under a variety of
conditions, based on classifiers' correlation and the level of their data
overlap. We show that even if competing classifiers are individually fair, the
ecosystem's outcome may be unfair; and that adjusting biased algorithms to
improve their individual fairness may lead to an overall decline in ecosystem
fairness. In addition to these theoretical results, we also provide supporting
experimental evidence. Together, our model and results provide a novel and
essential call for action.",2025-05-22,"Ronen Gradwohl, Eilam Shapira, Moshe Tennenholtz",http://arxiv.org/pdf/2505.16291v1,cs.LG
Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank Collapse,"Attention mechanisms lie at the heart of modern large language models (LLMs).
Straightforward algorithms for forward and backward (gradient) computation take
quadratic time, and a line of work initiated by [Alman and Song NeurIPS 2023]
and [Alman and Song NeurIPS 2024] has shown that quadratic time is necessary
unless the model weights are small, in which case almost linear time algorithms
are possible. In this paper, we show that large weights are necessary to avoid
a strong preclusion to representational strength we call layer collapse, which
means that the entire network can be approximated well by a network with only a
single layer. Thus, the quadratic running time of attention is unavoidable for
expressive transformers.
  The notion of layer collapse that we introduce is a variant on the notion of
rank collapse from the work of [Dong, Cordonnier, and Loukas ICML 2021]. They
showed that in Self Attention Networks with small weights and with skip
connections, rank collapse must occur. This is typically interpreted as
justifying the necessity of skip connections in expressive networks. However,
our result shows that even with skip connections, if the weights are small,
then layer collapse still occurs. Thus, only large weights, and not skip
connections, can prevent these representational weaknesses.",2025-05-22,"Josh Alman, Zhao Song",http://arxiv.org/pdf/2505.16284v1,cs.LG
RAP: Runtime-Adaptive Pruning for LLM Inference,"Large language models (LLMs) excel at language understanding and generation,
but their enormous computational and memory requirements hinder deployment.
Compression offers a potential solution to mitigate these constraints. However,
most existing methods rely on fixed heuristics and thus fail to adapt to
runtime memory variations or heterogeneous KV-cache demands arising from
diverse user requests. To address these limitations, we propose RAP, an elastic
pruning framework driven by reinforcement learning (RL) that dynamically
adjusts compression strategies in a runtime-aware manner. Specifically, RAP
dynamically tracks the evolving ratio between model parameters and KV-cache
across practical execution. Recognizing that FFNs house most parameters,
whereas parameter -light attention layers dominate KV-cache formation, the RL
agent retains only those components that maximize utility within the current
memory budget, conditioned on instantaneous workload and device state.
Extensive experiments results demonstrate that RAP outperforms state-of-the-art
baselines, marking the first time to jointly consider model weights and
KV-cache on the fly.",2025-05-22,"Huanrong Liu, Chunlin Tian, Xuyang Wei, Jiaheng Dai, Qin Liu, Tianqi Wei, Qingbiao Li, Li Li",http://arxiv.org/pdf/2505.17138v2,cs.LG
BAGELS: Benchmarking the Automated Generation and Extraction of Limitations from Scholarly Text,"In scientific research, limitations refer to the shortcomings, constraints,
or weaknesses within a study. Transparent reporting of such limitations can
enhance the quality and reproducibility of research and improve public trust in
science. However, authors often a) underreport them in the paper text and b)
use hedging strategies to satisfy editorial requirements at the cost of
readers' clarity and confidence. This underreporting behavior, along with an
explosion in the number of publications, has created a pressing need to
automatically extract or generate such limitations from scholarly papers. In
this direction, we present a complete architecture for the computational
analysis of research limitations. Specifically, we create a dataset of
limitations in ACL, NeurIPS, and PeerJ papers by extracting them from papers'
text and integrating them with external reviews; we propose methods to
automatically generate them using a novel Retrieval Augmented Generation (RAG)
technique; we create a fine-grained evaluation framework for generated
limitations; and we provide a meta-evaluation for the proposed evaluation
techniques.",2025-05-22,"Ibrahim Al Azher, Miftahul Jannat Mokarrama, Zhishuai Guo, Sagnik Ray Choudhury, Hamed Alhoori",http://arxiv.org/pdf/2505.18207v1,cs.LG
Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning,"Large language models are typically adapted to downstream tasks through
supervised fine-tuning on domain-specific data. While standard fine-tuning
focuses on minimizing generation loss to optimize model parameters, we take a
deeper step by retaining and leveraging the model's own learning signals,
analogous to how human learners reflect on past mistakes to improve future
performance. We first introduce the concept of Mistake Log to systematically
track the model's learning behavior and recurring errors throughout
fine-tuning. Treating the original transformer-based model as the Pilot, we
correspondingly design a Copilot model to refine the Pilot's inference
performance via logits rectification. We name the overall Pilot-Copilot
framework the Transformer Copilot, which introduces (i) a novel Copilot model
design, (ii) a joint training paradigm where the Copilot continuously learns
from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference
paradigm where the Copilot rectifies the Pilot's logits for enhanced
generation. We provide both theoretical and empirical analyses on our new
learning framework. Experiments on 12 benchmarks spanning commonsense,
arithmetic, and recommendation tasks demonstrate that Transformer Copilot
consistently improves performance by up to 34.5%, while introducing marginal
computational overhead to Pilot models and exhibiting strong scalability and
transferability.",2025-05-22,"Jiaru Zou, Yikun Ban, Zihao Li, Yunzhe Qi, Ruizhong Qiu, Ling Yang, Jingrui He",http://arxiv.org/pdf/2505.16270v1,cs.LG
Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models,"Reinforcement learning from human feedback (RLHF) has become a powerful
post-training paradigm for aligning large language models with human
preferences. A core challenge in RLHF is constructing accurate reward signals,
where the conventional Bradley-Terry reward models (BT RMs) often suffer from
sensitivity to data size and coverage, as well as vulnerability to reward
hacking. Generative reward models (GenRMs) offer a more robust alternative by
generating chain-of-thought (CoT) rationales followed by a final reward.
However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting
their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks.
Moreover, their pairwise preference outputs are incompatible with standard RLHF
algorithms that require pointwise reward signals. In this work, we introduce
Think-RM, a training framework that enables long-horizon reasoning in GenRMs by
modeling an internal thinking process. Rather than producing structured,
externally provided rationales, Think-RM generates flexible, self-guided
reasoning traces that support advanced capabilities such as self-reflection,
hypothetical reasoning, and divergent reasoning. To elicit these reasoning
abilities, we first warm-up the models by supervised fine-tuning (SFT) over
long CoT data. We then further improve the model's long-horizon abilities by
rule-based reinforcement learning (RL). In addition, we propose a novel
pairwise RLHF pipeline that directly optimizes policies using pairwise
preference rewards, eliminating the need for pointwise reward conversion and
enabling more effective use of Think-RM outputs. Experiments show that Think-RM
achieves state-of-the-art results on RM-Bench, outperforming both BT RM and
vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline,
it demonstrates superior end-policy performance compared to traditional
approaches.",2025-05-22,"Ilgee Hong, Changlong Yu, Liang Qiu, Weixiang Yan, Zhenghao Xu, Haoming Jiang, Qingru Zhang, Qin Lu, Xin Liu, Chao Zhang, Tuo Zhao",http://arxiv.org/pdf/2505.16265v1,cs.LG
"All You Need is ""Leet"": Evading Hate-speech Detection AI","Social media and online forums are increasingly becoming popular.
Unfortunately, these platforms are being used for spreading hate speech. In
this paper, we design black-box techniques to protect users from hate-speech on
online platforms by generating perturbations that can fool state of the art
deep learning based hate speech detection models thereby decreasing their
efficiency. We also ensure a minimal change in the original meaning of
hate-speech. Our best perturbation attack is successfully able to evade
hate-speech detection for 86.8 % of hateful text.",2025-05-22,"Sampanna Yashwant Kahu, Naman Ahuja",http://arxiv.org/pdf/2505.16263v1,cs.LG
Small-to-Large Generalization: Data Influences Models Consistently Across Scale,"Choice of training data distribution greatly influences model behavior. Yet,
in large-scale settings, precisely characterizing how changes in training data
affects predictions is often difficult due to model training costs. Current
practice is to instead extrapolate from scaled down, inexpensive-to-train proxy
models. However, changes in data do not influence smaller and larger models
identically. Therefore, understanding how choice of data affects large-scale
models raises the question: how does training data distribution influence model
behavior across compute scale? We find that small- and large-scale language
model predictions (generally) do highly correlate across choice of training
data. Equipped with these findings, we characterize how proxy scale affects
effectiveness in two downstream proxy model applications: data attribution and
dataset selection.",2025-05-22,"Alaa Khaddaj, Logan Engstrom, Aleksander Madry",http://arxiv.org/pdf/2505.16260v1,cs.LG
Higher-Order Asymptotics of Test-Time Adaptation for Batch Normalization Statistics,"This study develops a higher-order asymptotic framework for test-time
adaptation (TTA) of Batch Normalization (BN) statistics under distribution
shift by integrating classical Edgeworth expansion and saddlepoint
approximation techniques with a novel one-step M-estimation perspective. By
analyzing the statistical discrepancy between training and test distributions,
we derive an Edgeworth expansion for the normalized difference in BN means and
obtain an optimal weighting parameter that minimizes the mean-squared error of
the adapted statistic. Reinterpreting BN TTA as a one-step M-estimator allows
us to derive higher-order local asymptotic normality results, which incorporate
skewness and other higher moments into the estimator's behavior. Moreover, we
quantify the trade-offs among bias, variance, and skewness in the adaptation
process and establish a corresponding generalization bound on the model risk.
The refined saddlepoint approximations further deliver uniformly accurate
density and tail probability estimates for the BN TTA statistic. These
theoretical insights provide a comprehensive understanding of how higher-order
corrections and robust one-step updating can enhance the reliability and
performance of BN layers in adapting to changing data distributions.",2025-05-22,Masanari Kimura,http://arxiv.org/pdf/2505.16257v1,cs.LG
Graph-Smoothed Bayesian Black-Box Shift Estimator and Its Information Geometry,"Label shift adaptation aims to recover target class priors when the labelled
source distribution $P$ and the unlabelled target distribution $Q$ share $P(X
\mid Y) = Q(X \mid Y)$ but $P(Y) \neq Q(Y)$. Classical black-box shift
estimators invert an empirical confusion matrix of a frozen classifier,
producing a brittle point estimate that ignores sampling noise and similarity
among classes. We present Graph-Smoothed Bayesian BBSE (GS-B$^3$SE), a fully
probabilistic alternative that places Laplacian-Gaussian priors on both target
log-priors and confusion-matrix columns, tying them together on a
label-similarity graph. The resulting posterior is tractable with HMC or a fast
block Newton-CG scheme. We prove identifiability, $N^{-1/2}$ contraction,
variance bounds that shrink with the graph's algebraic connectivity, and
robustness to Laplacian misspecification. We also reinterpret GS-B$^3$SE
through information geometry, showing that it generalizes existing shift
estimators.",2025-05-22,Masanari Kimura,http://arxiv.org/pdf/2505.16251v1,cs.LG
Graph Neural Network-Based Collaborative Perception for Adaptive Scheduling in Distributed Systems,"This paper addresses the limitations of multi-node perception and delayed
scheduling response in distributed systems by proposing a GNN-based multi-node
collaborative perception mechanism. The system is modeled as a graph structure.
Message-passing and state-update modules are introduced. A multi-layer graph
neural network is constructed to enable efficient information aggregation and
dynamic state inference among nodes. In addition, a perception representation
method is designed by fusing local states with global features. This improves
each node's ability to perceive the overall system status. The proposed method
is evaluated within a customized experimental framework. A dataset featuring
heterogeneous task loads and dynamic communication topologies is used.
Performance is measured in terms of task completion rate, average latency, load
balancing, and transmission efficiency. Experimental results show that the
proposed method outperforms mainstream algorithms under various conditions,
including limited bandwidth and dynamic structural changes. It demonstrates
superior perception capabilities and cooperative scheduling performance. The
model achieves rapid convergence and efficient responses to complex system
states.",2025-05-22,"Wenxuan Zhu, Qiyuan Wu, Tengda Tang, Renzi Meng, Sheng Chai, Xuehui Quan",http://arxiv.org/pdf/2505.16248v1,cs.LG
Generalized Power Priors for Improved Bayesian Inference with Historical Data,"The power prior is a class of informative priors designed to incorporate
historical data alongside current data in a Bayesian framework. It includes a
power parameter that controls the influence of historical data, providing
flexibility and adaptability. A key property of the power prior is that the
resulting posterior minimizes a linear combination of KL divergences between
two pseudo-posterior distributions: one ignoring historical data and the other
fully incorporating it. We extend this framework by identifying the posterior
distribution as the minimizer of a linear combination of Amari's
$\alpha$-divergence, a generalization of KL divergence. We show that this
generalization can lead to improved performance by allowing for the data to
adapt to appropriate choices of the $\alpha$ parameter. Theoretical properties
of this generalized power posterior are established, including behavior as a
generalized geodesic on the Riemannian manifold of probability distributions,
offering novel insights into its geometric interpretation.",2025-05-22,"Masanari Kimura, Howard Bondell",http://arxiv.org/pdf/2505.16244v1,cs.LG
Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies,"When applying offline reinforcement learning (RL) in healthcare scenarios,
the out-of-distribution (OOD) issues pose significant risks, as inappropriate
generalization beyond clinical expertise can result in potentially harmful
recommendations. While existing methods like conservative Q-learning (CQL)
attempt to address the OOD issue, their effectiveness is limited by only
constraining action selection by suppressing uncertain actions. This
action-only regularization imitates clinician actions that prioritize
short-term rewards, but it fails to regulate downstream state trajectories,
thereby limiting the discovery of improved long-term treatment strategies. To
safely improve policy beyond clinician recommendations while ensuring that
state-action trajectories remain in-distribution, we propose \textit{Offline
Guarded Safe Reinforcement Learning} ($\mathsf{OGSRL}$), a theoretically
grounded model-based offline RL framework. $\mathsf{OGSRL}$ introduces a novel
dual constraint mechanism for improving policy with reliability and safety.
First, the OOD guardian is established to specify clinically validated regions
for safe policy exploration. By constraining optimization within these regions,
it enables the reliable exploration of treatment strategies that outperform
clinician behavior by leveraging the full patient state history, without
drifting into unsupported state-action trajectories. Second, we introduce a
safety cost constraint that encodes medical knowledge about physiological
safety boundaries, providing domain-specific safeguards even in areas where
training data might contain potentially unsafe interventions. Notably, we
provide theoretical guarantees on safety and near-optimality: policies that
satisfy these constraints remain in safe and reliable regions and achieve
performance close to the best possible policy supported by the data.",2025-05-22,"Runze Yan, Xun Shen, Akifumi Wachi, Sebastien Gros, Anni Zhao, Xiao Hu",http://arxiv.org/pdf/2505.16242v1,cs.LG
"Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning","Personalizing jargon detection and explanation is essential for making
technical documents accessible to readers with diverse disciplinary
backgrounds. However, tailoring models to individual users typically requires
substantial annotation efforts and computational resources due to user-specific
finetuning. To address this, we present a systematic study of personalized
jargon detection, focusing on methods that are both efficient and scalable for
real-world deployment. We explore two personalization strategies: (1)
lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,
and (2) personalized prompting, which tailors model behavior at inference time
without retaining. To reflect realistic constraints, we also investigate hybrid
approaches that combine limited annotated data with unsupervised user
background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in
F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,
our method achieves comparable performance using only 10% of the annotated
training data, demonstrating its practicality for resource-constrained
settings. Our study offers the first work to systematically explore efficient,
low-resource personalization of jargon detection using open-source language
models, offering a practical path toward scalable, user-adaptive NLP system.",2025-05-22,"Bohao Wu, Qingyun Wang, Yue Guo",http://arxiv.org/pdf/2505.16227v1,cs.LG
Realistic Evaluation of TabPFN v2 in Open Environments,"Tabular data, owing to its ubiquitous presence in real-world domains, has
garnered significant attention in machine learning research. While tree-based
models have long dominated tabular machine learning tasks, the recently
proposed deep learning model TabPFN v2 has emerged, demonstrating unparalleled
performance and scalability potential. Although extensive research has been
conducted on TabPFN v2 to further improve performance, the majority of this
research remains confined to closed environments, neglecting the challenges
that frequently arise in open environments. This raises the question: Can
TabPFN v2 maintain good performance in open environments? To this end, we
conduct the first comprehensive evaluation of TabPFN v2's adaptability in open
environments. We construct a unified evaluation framework covering various
real-world challenges and assess the robustness of TabPFN v2 under open
environments scenarios using this framework. Empirical results demonstrate that
TabPFN v2 shows significant limitations in open environments but is suitable
for small-scale, covariate-shifted, and class-balanced tasks. Tree-based models
remain the optimal choice for general tabular tasks in open environments. To
facilitate future research on open environments challenges, we advocate for
open environments tabular benchmarks, multi-metric evaluation, and universal
modules to strengthen model robustness. We publicly release our evaluation
framework at https://anonymous.4open.science/r/tabpfn-ood-4E65.",2025-05-22,"Zi-Jian Cheng, Zi-Yi Jia, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo",http://arxiv.org/pdf/2505.16226v1,cs.LG
MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network,"In this paper, we propose MADCluster, a novel model-agnostic anomaly
detection framework utilizing self-supervised clustering. MADCluster is
applicable to various deep learning architectures and addresses the
'hypersphere collapse' problem inherent in existing deep learning-based anomaly
detection methods. The core idea is to cluster normal pattern data into a
'single cluster' while simultaneously learning the cluster center and mapping
data close to this center. Also, to improve expressiveness and enable effective
single clustering, we propose a new 'One-directed Adaptive loss'. The
optimization of this loss is mathematically proven. MADCluster consists of
three main components: Base Embedder capturing high-dimensional temporal
dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous
center updates. Its model-agnostic characteristics are achieved by applying
various architectures to the Base Embedder. Experiments on four time series
benchmark datasets demonstrate that applying MADCluster improves the overall
performance of comparative models. In conclusion, the compatibility of
MADCluster shows potential for enhancing model performance across various
architectures.",2025-05-22,"Sangyong Lee, Subo Hwang, Dohoon Kim",http://arxiv.org/pdf/2505.16223v1,cs.LG
Reward-Aware Proto-Representations in Reinforcement Learning,"In recent years, the successor representation (SR) has attracted increasing
attention in reinforcement learning (RL), and it has been used to address some
of its key challenges, such as exploration, credit assignment, and
generalization. The SR can be seen as representing the underlying credit
assignment structure of the environment by implicitly encoding its induced
transition dynamics. However, the SR is reward-agnostic. In this paper, we
discuss a similar representation that also takes into account the reward
dynamics of the problem. We study the default representation (DR), a recently
proposed representation with limited theoretical (and empirical) analysis.
Here, we lay some of the theoretical foundation underlying the DR in the
tabular case by (1) deriving dynamic programming and (2) temporal-difference
methods to learn the DR, (3) characterizing the basis for the vector space of
the DR, and (4) formally extending the DR to the function approximation case
through default features. Empirically, we analyze the benefits of the DR in
many of the settings in which the SR has been applied, including (1) reward
shaping, (2) option discovery, (3) exploration, and (4) transfer learning. Our
results show that, compared to the SR, the DR gives rise to qualitatively
different, reward-aware behaviour and quantitatively better performance in
several settings.",2025-05-22,"Hon Tik Tse, Siddarth Chandrasekar, Marlos C. Machado",http://arxiv.org/pdf/2505.16217v1,cs.LG
A Scalable Hierarchical Intrusion Detection System for Internet of Vehicles,"Due to its nature of dynamic, mobility, and wireless data transfer, the
Internet of Vehicles (IoV) is prone to various cyber threats, ranging from
spoofing and Distributed Denial of Services (DDoS) attacks to malware. To
safeguard the IoV ecosystem from intrusions, malicious activities, policy
violations, intrusion detection systems (IDS) play a critical role by
continuously monitoring and analyzing network traffic to identify and mitigate
potential threats in real-time. However, most existing research has focused on
developing centralized, machine learning-based IDS systems for IoV without
accounting for its inherently distributed nature. Due to intensive computing
requirements, these centralized systems often rely on the cloud to detect cyber
threats, increasing delay of system response. On the other hand, edge nodes
typically lack the necessary resources to train and deploy complex machine
learning algorithms. To address this issue, this paper proposes an effective
hierarchical classification framework tailored for IoV networks. Hierarchical
classification allows classifiers to be trained and tested at different levels,
enabling edge nodes to detect specific types of attacks independently. With
this approach, edge nodes can conduct targeted attack detection while
leveraging cloud nodes for comprehensive threat analysis and support. Given the
resource constraints of edge nodes, we have employed the Boruta feature
selection method to reduce data dimensionality, optimizing processing
efficiency. To evaluate our proposed framework, we utilize the latest IoV
security dataset CIC-IoV2024, achieving promising results that demonstrate the
feasibility and effectiveness of our models in securing IoV networks.",2025-05-22,"Md Ashraf Uddin, Nam H. Chu, Reza Rafeh, Mutaz Barika",http://arxiv.org/pdf/2505.16215v1,cs.LG
NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics,"Large Language Models (LLMs) have demonstrated remarkable proficiency across
a wide range of tasks. However, LLMs often require larger batch sizes to
enhance throughput or longer context lengths to meet task demands, which
significantly increases the memory resource consumption of the Key-Value (KV)
cache during inference, becoming a major bottleneck in LLM deployment. To
address this issue, quantization is a common and straightforward approach.
Currently, quantization methods for activations are limited to 8-bit, and
quantization to even lower bits can lead to substantial accuracy drops. To
further save space by quantizing the KV cache to even lower bits, we analyzed
the element distribution of the KV cache and designed the NQKV algorithm. Since
the elements within each block of the KV cache follow a normal distribution,
NQKV employs per-block quantile quantization to achieve
information-theoretically optimal quantization error. Without significantly
compromising model output quality, NQKV enables the OPT model to perform
inference with an 2x larger batch size or a 4x longer context length, and it
improves throughput by 9.3x compared to when the KV cache is not used.",2025-05-22,"Zhihang Cai, Xingjun Zhang, Zhendong Tan, Zheng Wei",http://arxiv.org/pdf/2505.16210v1,cs.LG
Using Echo-State Networks to Reproduce Rare Events in Chaotic Systems,"We apply the Echo-State Networks to predict the time series and statistical
properties of the competitive Lotka-Volterra model in the chaotic regime. In
particular, we demonstrate that Echo-State Networks successfully learn the
chaotic attractor of the competitive Lotka-Volterra model and reproduce
histograms of dependent variables, including tails and rare events. We use the
Generalized Extreme Value distribution to quantify the tail behavior.",2025-05-22,"Anton Erofeev, Balasubramanya T. Nadiga, Ilya Timofeyev",http://arxiv.org/pdf/2505.16208v1,cs.LG
"Directional Convergence, Benign Overfitting of Gradient Descent in leaky ReLU two-layer Neural Networks","In this paper, we prove directional convergence of network parameters of
fixed width leaky ReLU two-layer neural networks optimized by gradient descent
with exponential loss, which was previously only known for gradient flow. By a
careful analysis of the convergent direction, we establish sufficient
conditions of benign overfitting and discover a new phase transition in the
test error bound. All of these results hold beyond the nearly orthogonal data
setting which was studied in prior works. As an application, we demonstrate
that benign overfitting occurs with high probability in sub-Gaussian mixture
models.",2025-05-22,Ichiro Hashimoto,http://arxiv.org/pdf/2505.16204v1,cs.LG
SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet,"Foley synthesis aims to synthesize high-quality audio that is both
semantically and temporally aligned with video frames. Given its broad
application in creative industries, the task has gained increasing attention in
the research community. To avoid the non-trivial task of training audio
generative models from scratch, adapting pretrained audio generative models for
video-synchronized foley synthesis presents an attractive direction.
ControlNet, a method for adding fine-grained controls to pretrained generative
models, has been applied to foley synthesis, but its use has been limited to
handcrafted human-readable temporal conditions. In contrast, from-scratch
models achieved success by leveraging high-dimensional deep features extracted
using pretrained video encoders. We have observed a performance gap between
ControlNet-based and from-scratch foley models. To narrow this gap, we propose
SpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward
video-synchronized foley synthesis via ControlNet. To unlock the potential of a
single ControlNet branch, we resolve the discrepancy between the temporal video
features and the time-frequency nature of the pretrained SpecMaskGIT via a
frequency-aware temporal feature aligner, eliminating the need for complicated
conditioning mechanisms widely used in prior arts. Evaluations on a common
foley synthesis benchmark demonstrate that SpecMaskFoley could even outperform
strong from-scratch baselines, substantially advancing the development of
ControlNet-based foley synthesis models. Demo page:
https://zzaudio.github.io/SpecMaskFoley_Demo/",2025-05-22,"Zhi Zhong, Akira Takahashi, Shuyang Cui, Keisuke Toyama, Shusuke Takahashi, Yuki Mitsufuji",http://arxiv.org/pdf/2505.16195v1,cs.LG
Enhancing Federated Survival Analysis through Peer-Driven Client Reputation in Healthcare,"Federated Learning (FL) holds great promise for digital health by enabling
collaborative model training without compromising patient data privacy.
However, heterogeneity across institutions, lack of sustained reputation, and
unreliable contributions remain major challenges. In this paper, we propose a
robust, peer-driven reputation mechanism for federated healthcare that employs
a hybrid communication model to integrate decentralized peer feedback with
clustering-based noise handling to enhance model aggregation. Crucially, our
approach decouples the federated aggregation and reputation mechanisms by
applying differential privacy to client-side model updates before sharing them
for peer evaluation. This ensures sensitive information remains protected
during reputation computation, while unaltered updates are sent to the server
for global model training. Using the Cox Proportional Hazards model for
survival analysis across multiple federated nodes, our framework addresses both
data heterogeneity and reputation deficit by dynamically adjusting trust scores
based on local performance improvements measured via the concordance index.
Experimental evaluations on both synthetic datasets and the SEER dataset
demonstrate that our method consistently achieves high and stable C-index
values, effectively down-weighing noisy client updates and outperforming FL
methods that lack a reputation system.",2025-05-22,"Navid Seidi, Satyaki Roy, Sajal Das",http://arxiv.org/pdf/2505.16190v1,cs.LG
Learning Probabilities of Causation from Finite Population Data,"Probabilities of causation play a crucial role in modern decision-making.
This paper addresses the challenge of predicting probabilities of causation for
subpopulations with \textbf{insufficient} data using machine learning models.
Tian and Pearl first defined and derived tight bounds for three fundamental
probabilities of causation: the probability of necessity and sufficiency (PNS),
the probability of sufficiency (PS), and the probability of necessity (PN).
However, estimating these probabilities requires both experimental and
observational distributions specific to each subpopulation, which are often
unavailable or impractical to obtain with limited population-level data.
Therefore, for most subgroups, the amount of data they have is not enough to
guarantee the accuracy of their probabilities. Hence, to estimate these
probabilities for subpopulations with \textbf{insufficient} data, we propose
using machine learning models that draw insights from subpopulations with
sufficient data. Our evaluation of multiple machine learning models indicates
that, given the population-level data and an appropriate choice of machine
learning model and activation function, PNS can be effectively predicted.
Through simulation studies on multiple Structured Causal Models (SCMs), we show
that our multilayer perceptron (MLP) model with the Mish activation function
achieves a mean absolute error (MAE) of approximately $0.02$ in predicting PNS
for $32,768$ subpopulations across most SCMs using data from only $2,000$
subpopulations with known PNS values.",2025-05-22,"Shuai Wang, Song Jiang, Yizhou Sun, Judea Pearl, Ang Li",http://arxiv.org/pdf/2505.17133v1,cs.LG
Why Can Accurate Models Be Learned from Inaccurate Annotations?,"Learning from inaccurate annotations has gained significant attention due to
the high cost of precise labeling. However, despite the presence of erroneous
labels, models trained on noisy data often retain the ability to make accurate
predictions. This intriguing phenomenon raises a fundamental yet largely
unexplored question: why models can still extract correct label information
from inaccurate annotations remains unexplored. In this paper, we conduct a
comprehensive investigation into this issue. By analyzing weight matrices from
both empirical and theoretical perspectives, we find that label inaccuracy
primarily accumulates noise in lower singular components and subtly perturbs
the principal subspace. Within a certain range, the principal subspaces of
weights trained on inaccurate labels remain largely aligned with those learned
from clean labels, preserving essential task-relevant information. We formally
prove that the angles of principal subspaces exhibit minimal deviation under
moderate label inaccuracy, explaining why models can still generalize
effectively. Building on these insights, we propose LIP, a lightweight plug-in
designed to help classifiers retain principal subspace information while
mitigating noise induced by label inaccuracy. Extensive experiments on tasks
with various inaccuracy conditions demonstrate that LIP consistently enhances
the performance of existing algorithms. We hope our findings can offer valuable
theoretical and practical insights to understand of model robustness under
inaccurate supervision.",2025-05-22,"Chongjie Si, Yidan Cui, Fuchao Yang, Xiaokang Yang, Wei Shen",http://arxiv.org/pdf/2505.16159v1,cs.LG
Integral Imprecise Probability Metrics,"Quantifying differences between probability distributions is fundamental to
statistics and machine learning, primarily for comparing statistical
uncertainty. In contrast, epistemic uncertainty (EU) -- due to incomplete
knowledge -- requires richer representations than those offered by classical
probability. Imprecise probability (IP) theory offers such models, capturing
ambiguity and partial belief. This has driven growing interest in imprecise
probabilistic machine learning (IPML), where inference and decision-making rely
on broader uncertainty models -- highlighting the need for metrics beyond
classical probability. This work introduces the Integral Imprecise Probability
Metric (IIPM) framework, a Choquet integral-based generalisation of classical
Integral Probability Metric (IPM) to the setting of capacities -- a broad class
of IP models encompassing many existing ones, including lower probabilities,
probability intervals, belief functions, and more. Theoretically, we establish
conditions under which IIPM serves as a valid metric and metrises a form of
weak convergence of capacities. Practically, IIPM not only enables comparison
across different IP models but also supports the quantification of epistemic
uncertainty within a single IP model. In particular, by comparing an IP model
with its conjugate, IIPM gives rise to a new class of EU measures -- Maximum
Mean Imprecision -- which satisfy key axiomatic properties proposed in the
Uncertainty Quantification literature. We validate MMI through selective
classification experiments, demonstrating strong empirical performance against
established EU measures, and outperforming them when classical methods struggle
to scale to a large number of classes. Our work advances both theory and
practice in IPML, offering a principled framework for comparing and quantifying
epistemic uncertainty under imprecision.",2025-05-22,"Siu Lun Chau, Michele Caprio, Krikamol Muandet",http://arxiv.org/pdf/2505.16156v1,cs.LG
NAN: A Training-Free Solution to Coefficient Estimation in Model Merging,"Model merging offers a training-free alternative to multi-task learning by
combining independently fine-tuned models into a unified one without access to
raw data. However, existing approaches often rely on heuristics to determine
the merging coefficients, limiting their scalability and generality. In this
work, we revisit model merging through the lens of least-squares optimization
and show that the optimal merging weights should scale with the amount of
task-specific information encoded in each model. Based on this insight, we
propose NAN, a simple yet effective method that estimates model merging
coefficients via the inverse of parameter norm. NAN is training-free,
plug-and-play, and applicable to a wide range of merging strategies. Extensive
experiments on show that NAN consistently improves performance of baseline
methods.",2025-05-22,"Chongjie Si, Kangtao Lv, Jingjing Jiang, Yadao Wang, Yongwei Wang, Xiaokang Yang, Wenbo Su, Bo Zheng, Wei Shen",http://arxiv.org/pdf/2505.16148v1,cs.LG
Losing is for Cherishing: Data Valuation Based on Machine Unlearning and Shapley Value,"The proliferation of large models has intensified the need for efficient data
valuation methods to quantify the contribution of individual data providers.
Traditional approaches, such as game-theory-based Shapley value and
influence-function-based techniques, face prohibitive computational costs or
require access to full data and model training details, making them hardly
achieve partial data valuation. To address this, we propose Unlearning Shapley,
a novel framework that leverages machine unlearning to estimate data values
efficiently. By unlearning target data from a pretrained model and measuring
performance shifts on a reachable test set, our method computes Shapley values
via Monte Carlo sampling, avoiding retraining and eliminating dependence on
full data. Crucially, Unlearning Shapley supports both full and partial data
valuation, making it scalable for large models (e.g., LLMs) and practical for
data markets. Experiments on benchmark datasets and large-scale text corpora
demonstrate that our approach matches the accuracy of state-of-the-art methods
while reducing computational overhead by orders of magnitude. Further analysis
confirms a strong correlation between estimated values and the true impact of
data subsets, validating its reliability in real-world scenarios. This work
bridges the gap between data valuation theory and practical deployment,
offering a scalable, privacy-compliant solution for modern AI ecosystems.",2025-05-22,"Le Ma, Shirao Yang, Zihao Wang, Yinggui Wang, Lei Wang, Tao Wei, Kejun Zhang",http://arxiv.org/pdf/2505.16147v1,cs.LG
Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation,"Large vision-language models (LVLMs) have achieved remarkable performance on
multimodal tasks such as visual question answering (VQA) and image captioning.
However, they still suffer from hallucinations, generating text inconsistent
with visual input, posing significant risks in real-world applications.
Existing approaches to address this issue focus on incorporating external
knowledge bases, alignment training, or decoding strategies, all of which
require substantial computational cost and time. Recent works try to explore
more efficient alternatives by adjusting LVLMs' internal representations.
Although promising, these methods may cause hallucinations to be insufficiently
suppressed or lead to excessive interventions that negatively affect normal
semantics. In this work, we leverage sparse autoencoders (SAEs) to identify
semantic directions closely associated with either hallucinations or actuality,
realizing more precise and direct hallucination-related representations. Our
analysis demonstrates that interventions along the faithful direction we
identified can mitigate hallucinations, while those along the hallucinatory
direction can exacerbate them. Building on these insights, we propose Steering
LVLMs via SAE Latent Directions (SSL), a training-free method based on
SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive
experiments demonstrate that SSL significantly outperforms existing decoding
approaches in mitigating hallucinations, while maintaining transferability
across different model architectures with negligible additional time overhead.",2025-05-22,"Zhenglin Hua, Jinghan He, Zijun Yao, Tianxu Han, Haiyun Guo, Yuheng Jia, Junfeng Fang",http://arxiv.org/pdf/2505.16146v1,cs.LG
Exponential Convergence of CAVI for Bayesian PCA,"Probabilistic principal component analysis (PCA) and its Bayesian variant
(BPCA) are widely used for dimension reduction in machine learning and
statistics. The main advantage of probabilistic PCA over the traditional
formulation is allowing uncertainty quantification. The parameters of BPCA are
typically learned using mean-field variational inference, and in particular,
the coordinate ascent variational inference (CAVI) algorithm. So far, the
convergence speed of CAVI for BPCA has not been characterized. In our paper, we
fill this gap in the literature. Firstly, we prove a precise exponential
convergence result in the case where the model uses a single principal
component (PC). Interestingly, this result is established through a connection
with the classical $\textit{power iteration algorithm}$ and it indicates that
traditional PCA is retrieved as points estimates of the BPCA parameters.
Secondly, we leverage recent tools to prove exponential convergence of CAVI for
the model with any number of PCs, thus leading to a more general result, but
one that is of a slightly different flavor. To prove the latter result, we
additionally needed to introduce a novel lower bound for the symmetric
Kullback--Leibler divergence between two multivariate normal distributions,
which, we believe, is of independent interest in information theory.",2025-05-22,"Arghya Datta, Philippe Gagnon, Florian Maire",http://arxiv.org/pdf/2505.16145v1,cs.LG
Multimodal Online Federated Learning with Modality Missing in Internet of Things,"The Internet of Things (IoT) ecosystem generates vast amounts of multimodal
data from heterogeneous sources such as sensors, cameras, and microphones. As
edge intelligence continues to evolve, IoT devices have progressed from simple
data collection units to nodes capable of executing complex computational
tasks. This evolution necessitates the adoption of distributed learning
strategies to effectively handle multimodal data in an IoT environment.
Furthermore, the real-time nature of data collection and limited local storage
on edge devices in IoT call for an online learning paradigm. To address these
challenges, we introduce the concept of Multimodal Online Federated Learning
(MMO-FL), a novel framework designed for dynamic and decentralized multimodal
learning in IoT environments. Building on this framework, we further account
for the inherent instability of edge devices, which frequently results in
missing modalities during the learning process. We conduct a comprehensive
theoretical analysis under both complete and missing modality scenarios,
providing insights into the performance degradation caused by missing
modalities. To mitigate the impact of modality missing, we propose the
Prototypical Modality Mitigation (PMM) algorithm, which leverages prototype
learning to effectively compensate for missing modalities. Experimental results
on two multimodal datasets further demonstrate the superior performance of PMM
compared to benchmarks.",2025-05-22,"Heqiang Wang, Xiang Liu, Xiaoxiong Zhong, Lixing Chen, Fangming Liu, Weizhe Zhang",http://arxiv.org/pdf/2505.16138v1,cs.LG
Interpretable Machine Learning for Macro Alpha: A News Sentiment Case Study,"This study introduces an interpretable machine learning (ML) framework to
extract macroeconomic alpha from global news sentiment. We process the Global
Database of Events, Language, and Tone (GDELT) Project's worldwide news feed
using FinBERT -- a Bidirectional Encoder Representations from Transformers
(BERT) based model pretrained on finance-specific language -- to construct
daily sentiment indices incorporating mean tone, dispersion, and event impact.
These indices drive an XGBoost classifier, benchmarked against logistic
regression, to predict next-day returns for EUR/USD, USD/JPY, and 10-year U.S.
Treasury futures (ZN). Rigorous out-of-sample (OOS) backtesting (5-fold
expanding-window cross-validation, OOS period: c. 2017-April 2025) demonstrates
exceptional, cost-adjusted performance for the XGBoost strategy: Sharpe ratios
achieve 5.87 (EUR/USD), 4.65 (USD/JPY), and 4.65 (Treasuries), with respective
compound annual growth rates (CAGRs) exceeding 50% in Foreign Exchange (FX) and
22% in bonds. Shapley Additive Explanations (SHAP) affirm that sentiment
dispersion and article impact are key predictive features. Our findings
establish that integrating domain-specific Natural Language Processing (NLP)
with interpretable ML offers a potent and explainable source of macro alpha.",2025-05-22,Yuke Zhang,http://arxiv.org/pdf/2505.16136v1,cs.LG
Sudoku-Bench: Evaluating creative reasoning with Sudoku variants,"Existing reasoning benchmarks for large language models (LLMs) frequently
fail to capture authentic creativity, often rewarding memorization of
previously observed patterns. We address this shortcoming with Sudoku-Bench, a
curated benchmark of challenging and unconventional Sudoku variants
specifically selected to evaluate creative, multi-step logical reasoning.
Sudoku variants form an unusually effective domain for reasoning research: each
puzzle introduces unique or subtly interacting constraints, making memorization
infeasible and requiring solvers to identify novel logical breakthroughs
(``break-ins''). Despite their diversity, Sudoku variants maintain a common and
compact structure, enabling clear and consistent evaluation. Sudoku-Bench
includes a carefully chosen puzzle set, a standardized text-based puzzle
representation, and flexible tools compatible with thousands of publicly
available puzzles -- making it easy to extend into a general research
environment. Baseline experiments show that state-of-the-art LLMs solve fewer
than 15\% of puzzles unaided, highlighting significant opportunities to advance
long-horizon, strategic reasoning capabilities.",2025-05-22,"Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, Llion Jones",http://arxiv.org/pdf/2505.16135v1,cs.LG
Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models,"Large language models exhibit positional bias -- systematic neglect of
information at specific context positions -- yet its interplay with linguistic
diversity remains poorly understood. We present a cross-linguistic study across
five typologically distinct languages (English, Russian, German, Hindi,
Vietnamese), examining how positional bias interacts with model uncertainty,
syntax, and prompting. Key findings: (1) Positional bias is model-driven, with
language-specific variations -- Qwen2.5-7B favors late positions, challenging
assumptions of early-token bias; (2) Explicit positional guidance (e.g.,
correct context is at position X) reduces accuracy across languages,
undermining prompt-engineering practices; (3) Aligning context with positional
bias increases entropy, yet minimal entropy does not predict accuracy. (4) We
further uncover that LLMs differently impose dominant word order in
free-word-order languages like Hindi.",2025-05-22,"Menschikov Mikhail, Alexander Kharitonov, Maiia Kotyga, Vadim Porvatov, Anna Zhukovskaya, David Kagramanyan, Egor Shvetsov, Evgeny Burnaev",http://arxiv.org/pdf/2505.16134v1,cs.LG
Machine Learning the 6d Supergravity Landscape,"In this paper, we apply both supervised and unsupervised machine learning
algorithms to the study of the string landscape and swampland in 6-dimensions.
Our data are the (almost) anomaly-free 6-dimensional $\mathcal{N} = (1,0)$
supergravity models, characterised by the Gram matrix of anomaly coefficients.
Our work demonstrates the ability of machine learning algorithms to efficiently
learn highly complex features of the landscape and swampland. Employing an
autoencoder for unsupervised learning, we provide an auto-classification of
these models by compressing the Gram matrix data to 2-dimensions. Through
compression, similar models cluster together, and we identify prominent
features of these clusters. The autoencoder also identifies outlier models
which are difficult to reconstruct. One of these outliers proves to be
incredibly difficult to combine with other models such that the
$\text{tr}R^{4}$ anomaly vanishes, making its presence in the landscape
extremely rare. Further, we utilise supervised learning to build two
classifiers predicting (1) model consistency under probe string insertion
(precision: 0.78, predicting consistency for 214,837 models with reasonable
certainty) and (2) inconsistency under anomaly inflow (precision: 0.91,
predicting inconsistency for 1,909,359 models). Notably, projecting these
predictions onto the autoencoder's 2-dimensional latent layer shows consistent
models clustering together, further indicating that the autoencoder has learnt
interesting and complex features of the set of models and potentially offers a
novel approach to mapping the landscape and swampland of 6-dimensional
supergravity theories.",2025-05-22,"Nathan Brady, David Tennyson, Thomas Vandermeulen",http://arxiv.org/pdf/2505.16131v1,cs.LG
Scalable Graph Generative Modeling via Substructure Sequences,"Graph neural networks (GNNs) has been predominantly driven by
message-passing, where node representations are iteratively updated via local
neighborhood aggregation. Despite their success, message-passing suffers from
fundamental limitations -- including constrained expressiveness,
over-smoothing, over-squashing, and limited capacity to model long-range
dependencies. These issues hinder scalability: increasing data size or model
size often fails to yield improved performance, limiting the viability of GNNs
as backbones for graph foundation models. In this work, we explore pathways
beyond message-passing and introduce Generative Graph Pattern Machine
(G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM
represents graph instances (nodes, edges, or entire graphs) as sequences of
substructures, and employs generative pre-training over the sequences to learn
generalizable, transferable representations. Empirically, G$^2$PM demonstrates
strong scalability: on the ogbn-arxiv benchmark, it continues to improve with
model sizes up to 60M parameters, outperforming prior generative approaches
that plateau at significantly smaller scales (e.g., 3M). In addition, we
systematically analyze the model design space, highlighting key architectural
choices that contribute to its scalability and generalization. Across diverse
tasks -- including node classification, graph classification, and transfer
learning -- G$^2$PM consistently outperforms strong baselines, establishing a
compelling foundation for scalable graph learning. The code and dataset are
available at https://github.com/Zehong-Wang/G2PM.",2025-05-22,"Zehong Wang, Zheyuan Zhang, Tianyi Ma, Chuxu Zhang, Yanfang Ye",http://arxiv.org/pdf/2505.16130v1,cs.LG
Robust Invariant Representation Learning by Distribution Extrapolation,"Invariant risk minimization (IRM) aims to enable out-of-distribution (OOD)
generalization in deep learning by learning invariant representations. As IRM
poses an inherently challenging bi-level optimization problem, most existing
approaches -- including IRMv1 -- adopt penalty-based single-level
approximations. However, empirical studies consistently show that these methods
often fail to outperform well-tuned empirical risk minimization (ERM),
highlighting the need for more robust IRM implementations. This work
theoretically identifies a key limitation common to many IRM variants: their
penalty terms are highly sensitive to limited environment diversity and
over-parameterization, resulting in performance degradation. To address this
issue, a novel extrapolation-based framework is proposed that enhances
environmental diversity by augmenting the IRM penalty through synthetic
distributional shifts. Extensive experiments -- ranging from synthetic setups
to realistic, over-parameterized scenarios -- demonstrate that the proposed
method consistently outperforms state-of-the-art IRM variants, validating its
effectiveness and robustness.",2025-05-22,"Kotaro Yoshida, Konstantinos Slavakis",http://arxiv.org/pdf/2505.16126v2,cs.LG
Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning,"Large Language Models (LLMs) have achieved remarkable success in complex
reasoning tasks, but their inference remains computationally inefficient. We
observe a common failure mode in many prevalent LLMs, overthinking, where
models generate verbose and tangential reasoning traces even for simple
queries. Recent works have tried to mitigate this by enforcing fixed token
budgets, however, this can lead to underthinking, especially on harder
problems. Through empirical analysis, we identify that this inefficiency often
stems from unclear problem-solving strategies. To formalize this, we develop a
theoretical model, BBAM (Bayesian Budget Allocation Model), which models
reasoning as a sequence of sub-questions with varying uncertainty, and
introduce the $E^3$ metric to capture the trade-off between correctness and
computation efficiency. Building on theoretical results from BBAM, we propose
Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex
queries into sub-questions and allocates token budgets based on estimated
complexity using adaptive scheduling. Plan-and-Budget improves reasoning
efficiency across a range of tasks and models, achieving up to +70% accuracy
gains, -39% token reduction, and +187.5% improvement in $E^3$. Notably, it
elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger
model (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close
performance gaps without retraining. Our code is available at
anonymous.4open.science/r/P-and-B-6513/.",2025-05-22,"Junhong Lin, Xinyue Zeng, Jie Zhu, Song Wang, Julian Shun, Jun Wu, Dawei Zhou",http://arxiv.org/pdf/2505.16122v1,cs.LG
A Generic Framework for Conformal Fairness,"Conformal Prediction (CP) is a popular method for uncertainty quantification
with machine learning models. While conformal prediction provides probabilistic
guarantees regarding the coverage of the true label, these guarantees are
agnostic to the presence of sensitive attributes within the dataset. In this
work, we formalize \textit{Conformal Fairness}, a notion of fairness using
conformal predictors, and provide a theoretically well-founded algorithm and
associated framework to control for the gaps in coverage between different
sensitive groups. Our framework leverages the exchangeability assumption
(implicit to CP) rather than the typical IID assumption, allowing us to apply
the notion of Conformal Fairness to data types and tasks that are not IID, such
as graph data. Experiments were conducted on graph and tabular datasets to
demonstrate that the algorithm can control fairness-related gaps in addition to
coverage aligned with theoretical expectations.",2025-05-22,"Aditya T. Vadlamani, Anutam Srinivasan, Pranav Maneriker, Ali Payani, Srinivasan Parthasarathy",http://arxiv.org/pdf/2505.16115v1,cs.LG
Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools,"Modern Large Language Models (LLMs) often require external tools, such as
machine learning classifiers or knowledge retrieval systems, to provide
accurate answers in domains where their pre-trained knowledge is insufficient.
This integration of LLMs with external tools expands their utility but also
introduces a critical challenge: determining the trustworthiness of responses
generated by the combined system. In high-stakes applications, such as medical
decision-making, it is essential to assess the uncertainty of both the LLM's
generated text and the tool's output to ensure the reliability of the final
response. However, existing uncertainty quantification methods do not account
for the tool-calling scenario, where both the LLM and external tool contribute
to the overall system's uncertainty. In this work, we present a novel framework
for modeling tool-calling LLMs that quantifies uncertainty by jointly
considering the predictive uncertainty of the LLM and the external tool. We
extend previous methods for uncertainty quantification over token sequences to
this setting and propose efficient approximations that make uncertainty
computation practical for real-world applications. We evaluate our framework on
two new synthetic QA datasets, derived from well-known machine learning
datasets, which require tool-calling for accurate answers. Additionally, we
apply our method to retrieval-augmented generation (RAG) systems and conduct a
proof-of-concept experiment demonstrating the effectiveness of our uncertainty
metrics in scenarios where external information retrieval is needed. Our
results show that the framework is effective in enhancing trust in LLM-based
systems, especially in cases where the LLM's internal knowledge is insufficient
and external tools are required.",2025-05-22,"Panagiotis Lymperopoulos, Vasanth Sarathy",http://arxiv.org/pdf/2505.16113v1,cs.LG
Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models,"With the increasing size of Large Vision-Language Models (LVLMs), network
pruning techniques aimed at compressing models for deployment in
resource-constrained environments have garnered significant attention. However,
we observe that pruning often leads to a degradation in safety performance. To
address this issue, we present a novel and lightweight approach, termed
Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the
contribution of each attention head to safety, identifying the most critical
ones, and then selectively restoring neurons directly within these attention
heads that play a pivotal role in maintaining safety. This process
hierarchically realigns the safety of pruned LVLMs, progressing from the
attention head level to the neuron level. We validate HSR across various models
and pruning strategies, consistently achieving notable improvements in safety
performance. To our knowledge, this is the first work explicitly focused on
restoring safety in LVLMs post-pruning.",2025-05-22,"Yue Li, Xin Yi, Dongsheng Shi, Gerard de Melo, Xiaoling Wang, Linlin Wang",http://arxiv.org/pdf/2505.16104v1,cs.LG
Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI,"Keylogger detection involves monitoring for unusual system behaviors such as
delays between typing and character display, analyzing network traffic patterns
for data exfiltration. In this study, we provide a comprehensive analysis for
keylogger detection with traditional machine learning models - SVC, Random
Forest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes
and advanced ensemble methods including Stacking, Blending and Voting.
Moreover, feature selection approaches such as Information gain, Lasso L1 and
Fisher Score are thoroughly assessed to improve predictive performance and
lower computational complexity. The Keylogger Detection dataset from publicly
available Kaggle website is used in this project. In addition to accuracy-based
classification, this study implements the approach for model interpretation
using Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to
deliver finer explanations for how much each feature contributes in assisting
or hindering the detection process. To evaluate the models result, we have used
AUC score, sensitivity, Specificity, Accuracy and F1 score. The best
performance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99,
100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is
near-perfect classification with Fisher Score.",2025-05-22,Monirul Islam Mahmud,http://arxiv.org/pdf/2505.16103v1,cs.LG
Reinforcement Learning for Stock Transactions,"Much research has been done to analyze the stock market. After all, if one
can determine a pattern in the chaotic frenzy of transactions, then they could
make a hefty profit from capitalizing on these insights. As such, the goal of
our project was to apply reinforcement learning (RL) to determine the best time
to buy a stock within a given time frame. With only a few adjustments, our
model can be extended to identify the best time to sell a stock as well. In
order to use the format of free, real-world data to train the model, we define
our own Markov Decision Process (MDP) problem. These two papers [5] [6] helped
us in formulating the state space and the reward system of our MDP problem. We
train a series of agents using Q-Learning, Q-Learning with linear function
approximation, and deep Q-Learning. In addition, we try to predict the stock
prices using machine learning regression and classification models. We then
compare our agents to see if they converge on a policy, and if so, which one
learned the best policy to maximize profit on the stock market.",2025-05-22,"Ziyi Zhou, Nicholas Stern, Julien Laasri",http://arxiv.org/pdf/2505.16099v2,cs.LG
Dimension-adapted Momentum Outscales SGD,"We investigate scaling laws for stochastic momentum algorithms with small
batch on the power law random features model, parameterized by data complexity,
target complexity, and model size. When trained with a stochastic momentum
algorithm, our analysis reveals four distinct loss curve shapes determined by
varying data-target complexities. While traditional stochastic gradient descent
with momentum (SGD-M) yields identical scaling law exponents to SGD,
dimension-adapted Nesterov acceleration (DANA) improves these exponents by
scaling momentum hyperparameters based on model size and data complexity. This
outscaling phenomenon, which also improves compute-optimal scaling behavior, is
achieved by DANA across a broad range of data and target complexities, while
traditional methods fall short. Extensive experiments on high-dimensional
synthetic quadratics validate our theoretical predictions and large-scale text
experiments with LSTMs show DANA's improved loss exponents over SGD hold in a
practical setting.",2025-05-22,"Damien Ferbach, Katie Everett, Gauthier Gidel, Elliot Paquette, Courtney Paquette",http://arxiv.org/pdf/2505.16098v1,cs.LG
A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization,"Large language models (LLMs) are introducing a paradigm shift in molecular
discovery by enabling text-guided interaction with chemical spaces through
natural language, symbolic notations, with emerging extensions to incorporate
multi-modal inputs. To advance the new field of LLM for molecular discovery,
this survey provides an up-to-date and forward-looking review of the emerging
use of LLMs for two central tasks: molecule generation and molecule
optimization. Based on our proposed taxonomy for both problems, we analyze
representative techniques in each category, highlighting how LLM capabilities
are leveraged across different learning settings. In addition, we include the
commonly used datasets and evaluation protocols. We conclude by discussing key
challenges and future directions, positioning this survey as a resource for
researchers working at the intersection of LLMs and molecular science. A
continuously updated reading list is available at
https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.",2025-05-22,"Ziqing Wang, Kexin Zhang, Zihan Zhao, Yibo Wen, Abhishek Pandey, Han Liu, Kaize Ding",http://arxiv.org/pdf/2505.16094v1,cs.LG
FR-Mamba: Time-Series Physical Field Reconstruction Based on State Space Model,"Physical field reconstruction (PFR) aims to predict the state distribution of
physical quantities (e.g., velocity, pressure, and temperature) based on
limited sensor measurements. It plays a critical role in domains such as fluid
dynamics and thermodynamics. However, existing deep learning methods often fail
to capture long-range temporal dependencies, resulting in suboptimal
performance on time-evolving physical systems. To address this, we propose
FR-Mamba, a novel spatiotemporal flow field reconstruction framework based on
state space modeling. Specifically, we design a hybrid neural network
architecture that combines Fourier Neural Operator (FNO) and State Space Model
(SSM) to capture both global spatial features and long-range temporal
dependencies. We adopt Mamba, a recently proposed efficient SSM architecture,
to model long-range temporal dependencies with linear time complexity. In
parallel, the FNO is employed to capture non-local spatial features by
leveraging frequency-domain transformations. The spatiotemporal representations
extracted by these two components are then fused to reconstruct the full-field
distribution of the physical system. Extensive experiments demonstrate that our
approach significantly outperforms existing PFR methods in flow field
reconstruction tasks, achieving high-accuracy performance on long sequences.",2025-05-21,"Jiahuan Long, Wenzhe Zhang, Ning Wang, Tingsong Jiang, Wen Yao",http://arxiv.org/pdf/2505.16083v1,cs.LG
Oh SnapMMD! Forecasting Stochastic Dynamics Beyond the Schrödinger Bridge's End,"Scientists often want to make predictions beyond the observed time horizon of
""snapshot"" data following latent stochastic dynamics. For example, in time
course single-cell mRNA profiling, scientists have access to cellular
transcriptional state measurements (snapshots) from different biological
replicates at different time points, but they cannot access the trajectory of
any one cell because measurement destroys the cell. Researchers want to
forecast (e.g.) differentiation outcomes from early state measurements of stem
cells. Recent Schr\""odinger-bridge (SB) methods are natural for interpolating
between snapshots. But past SB papers have not addressed forecasting -- likely
since existing methods either (1) reduce to following pre-set reference
dynamics (chosen before seeing data) or (2) require the user to choose a fixed,
state-independent volatility since they minimize a Kullback-Leibler divergence.
Either case can lead to poor forecasting quality. In the present work, we
propose a new framework, SnapMMD, that learns dynamics by directly fitting the
joint distribution of both state measurements and observation time with a
maximum mean discrepancy (MMD) loss. Unlike past work, our method allows us to
infer unknown and state-dependent volatilities from the observed data. We show
in a variety of real and synthetic experiments that our method delivers
accurate forecasts. Moreover, our approach allows us to learn in the presence
of incomplete state measurements and yields an $R^2$-style statistic that
diagnoses fit. We also find that our method's performance at interpolation (and
general velocity-field reconstruction) is at least as good as (and often better
than) state-of-the-art in almost all of our experiments.",2025-05-21,"Renato Berlinghieri, Yunyi Shen, Jialong Jiang, Tamara Broderick",http://arxiv.org/pdf/2505.16082v1,cs.LG
Ensembling Sparse Autoencoders,"Sparse autoencoders (SAEs) are used to decompose neural network activations
into human-interpretable features. Typically, features learned by a single SAE
are used for downstream applications. However, it has recently been shown that
SAEs trained with different initial weights can learn different features,
demonstrating that a single SAE captures only a limited subset of features that
can be extracted from the activation space. Motivated by this limitation, we
propose to ensemble multiple SAEs through naive bagging and boosting.
Specifically, SAEs trained with different weight initializations are ensembled
in naive bagging, whereas SAEs sequentially trained to minimize the residual
error are ensembled in boosting. We evaluate our ensemble approaches with three
settings of language models and SAE architectures. Our empirical results
demonstrate that ensembling SAEs can improve the reconstruction of language
model activations, diversity of features, and SAE stability. Furthermore,
ensembling SAEs performs better than applying a single SAE on downstream tasks
such as concept detection and spurious correlation removal, showing improved
practical utility.",2025-05-21,"Soham Gadgil, Chris Lin, Su-In Lee",http://arxiv.org/pdf/2505.16077v1,cs.LG
Bidirectional Variational Autoencoders,"We present the new bidirectional variational autoencoder (BVAE) network
architecture. The BVAE uses a single neural network both to encode and decode
instead of an encoder-decoder network pair. The network encodes in the forward
direction and decodes in the backward direction through the same synaptic web.
Simulations compared BVAEs and ordinary VAEs on the four image tasks of image
reconstruction, classification, interpolation, and generation. The image
datasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and
CelebA-64 face images. The bidirectional structure of BVAEs cut the parameter
count by almost 50% and still slightly outperformed the unidirectional VAEs.",2025-05-21,"Bart Kosko, Olaoluwa Adigun",http://arxiv.org/pdf/2505.16074v1,cs.LG
Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts,"Multimodal Large Language Models (MLLMs) perform well on tasks such as visual
question answering, but it remains unclear whether their reasoning relies more
on memorized world knowledge or on the visual information present in the input
image. To investigate this, we introduce Visual CounterFact, a new dataset of
visually-realistic counterfactuals that put world knowledge priors (e.g, red
strawberry) into direct conflict with visual input (e.g, blue strawberry).
Using Visual CounterFact, we show that model predictions initially reflect
memorized priors, but shift toward visual evidence in mid-to-late layers. This
dynamic reveals a competition between the two modalities, with visual input
ultimately overriding priors during evaluation. To control this behavior, we
propose Pixels Versus Priors (PvP) steering vectors, a mechanism for
controlling model outputs toward either world knowledge or visual input through
activation-level interventions. On average, PvP successfully shifts 92.5% of
color and 74.6% of size predictions from priors to counterfactuals. Together,
these findings offer new tools for interpreting and controlling factual
behavior in multimodal models.",2025-05-21,"Michal Golovanevsky, William Rudman, Michael Lepori, Amir Bar, Ritambhara Singh, Carsten Eickhoff",http://arxiv.org/pdf/2505.17127v1,cs.LG
Conformal Language Model Reasoning with Coherent Factuality,"Language models are increasingly being used in important decision pipelines,
so ensuring the correctness of their outputs is crucial. Recent work has
proposed evaluating the ""factuality"" of claims decomposed from a language model
generation and applying conformal prediction techniques to filter out those
claims that are not factual. This can be effective for tasks such as
information retrieval, where constituent claims may be evaluated in isolation
for factuality, but is not appropriate for reasoning tasks, as steps of a
logical argument can be evaluated for correctness only within the context of
the claims that precede them. To capture this, we define ""coherent factuality""
and develop a conformal-prediction-based method to guarantee coherent
factuality for language model outputs. Our approach applies split conformal
prediction to subgraphs within a ""deducibility"" graph"" that represents the
steps of a reasoning problem. We evaluate our method on mathematical reasoning
problems from the MATH and FELM datasets and find that our algorithm
consistently produces correct and substantiated orderings of claims, achieving
coherent factuality across target coverage levels. Moreover, we achieve 90%
factuality on our stricter definition while retaining 80% or more of the
original claims, highlighting the utility of our deducibility-graph-guided
approach.",2025-05-21,"Maxon Rubin-Toles, Maya Gambhir, Keshav Ramji, Aaron Roth, Surbhi Goel",http://arxiv.org/pdf/2505.17126v1,cs.LG
Merge to Mix: Mixing Datasets via Model Merging,"Mixing datasets for fine-tuning large models (LMs) has become critical for
maximizing performance on downstream tasks. However, composing effective
dataset mixtures typically relies on heuristics and trial-and-error, often
requiring multiple fine-tuning runs to achieve the desired outcome. We propose
a novel method, $\textit{Merge to Mix}$, that accelerates composing dataset
mixtures through model merging. Model merging is a recent technique that
combines the abilities of multiple individually fine-tuned LMs into a single LM
by using a few simple arithmetic operations. Our key insight is that merging
models individually fine-tuned on each dataset in a mixture can effectively
serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix
leverages this insight to accelerate selecting dataset mixtures without
requiring full fine-tuning on each candidate mixture. Our experiments
demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset
selection for fine-tuning LMs.",2025-05-21,"Zhixu Silvia Tao, Kasper Vinken, Hao-Wei Yeh, Avi Cooper, Xavier Boix",http://arxiv.org/pdf/2505.16066v1,cs.LG
Few-Shot Test-Time Optimization Without Retraining for Semiconductor Recipe Generation and Beyond,"We introduce Model Feedback Learning (MFL), a novel test-time optimization
framework for optimizing inputs to pre-trained AI models or deployed hardware
systems without requiring any retraining of the models or modifications to the
hardware. In contrast to existing methods that rely on adjusting model
parameters, MFL leverages a lightweight reverse model to iteratively search for
optimal inputs, enabling efficient adaptation to new objectives under
deployment constraints. This framework is particularly advantageous in
real-world settings, such as semiconductor manufacturing recipe generation,
where modifying deployed systems is often infeasible or cost-prohibitive. We
validate MFL on semiconductor plasma etching tasks, where it achieves target
recipe generation in just five iterations, significantly outperforming both
Bayesian optimization and human experts. Beyond semiconductor applications, MFL
also demonstrates strong performance in chemical processes (e.g., chemical
vapor deposition) and electronic systems (e.g., wire bonding), highlighting its
broad applicability. Additionally, MFL incorporates stability-aware
optimization, enhancing robustness to process variations and surpassing
conventional supervised learning and random search methods in high-dimensional
control settings. By enabling few-shot adaptation, MFL provides a scalable and
efficient paradigm for deploying intelligent control in real-world
environments.",2025-05-21,"Shangding Gu, Donghao Ying, Ming Jin, Yu Joe Lu, Jun Wang, Javad Lavaei, Costas Spanos",http://arxiv.org/pdf/2505.16060v1,cs.LG
Mesh-free sparse identification of nonlinear dynamics,"Identifying the governing equations of a dynamical system is one of the most
important tasks for scientific modeling. However, this procedure often requires
high-quality spatio-temporal data uniformly sampled on structured grids. In
this paper, we propose mesh-free SINDy, a novel algorithm which leverages the
power of neural network approximation as well as auto-differentiation to
identify governing equations from arbitrary sensor placements and non-uniform
temporal data sampling. We show that mesh-free SINDy is robust to high noise
levels and limited data while remaining computationally efficient. In our
implementation, the training procedure is straight-forward and nearly free of
hyperparameter tuning, making mesh-free SINDy widely applicable to many
scientific and engineering problems. In the experiments, we demonstrate its
effectiveness on a series of PDEs including the Burgers' equation, the heat
equation, the Korteweg-De Vries equation and the 2D advection-diffusion
equation. We conduct detailed numerical experiments on all datasets, varying
the noise levels and number of samples, and we also compare our approach to
previous state-of-the-art methods. It is noteworthy that, even in high-noise
and low-data scenarios, mesh-free SINDy demonstrates robust PDE discovery,
achieving successful identification with up to 75% noise for the Burgers'
equation using 5,000 samples and with as few as 100 samples and 1% noise. All
of this is achieved within a training time of under one minute.",2025-05-21,"Mars Liyao Gao, J. Nathan Kutz, Bernat Font",http://arxiv.org/pdf/2505.16058v1,cs.LG
Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models,"Mixture-of-Experts (MoE) enables efficient scaling of large language models
(LLMs) with sparsely activated experts during inference. To effectively deploy
large MoE models on memory-constrained devices, many systems introduce *expert
offloading* that caches a subset of experts in fast memory, leaving others on
slow memory to run on CPU or load on demand. While some research has exploited
the locality of expert activations, where consecutive tokens activate similar
experts, the degree of this **local routing consistency** varies across models
and remains understudied. In this paper, we propose two metrics to measure
local routing consistency of MoE models: (1) **Segment Routing Best Performance
(SRP)**, which evaluates how well a fixed group of experts can cover the needs
of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which
measures the optimal segment-level cache hit rate under a given cache size
limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found
that models that apply MoE on every layer and do not use shared experts exhibit
the highest local routing consistency. We further showed that
domain-specialized experts contribute more to routing consistency than
vocabulary-specialized ones, and that most models can balance between cache
effectiveness and efficiency with cache sizes approximately 2x the active
experts. These findings pave the way for memory-efficient MoE design and
deployment without compromising inference speed. We publish the code for
replicating experiments at https://github.com/ljcleo/moe-lrc .",2025-05-21,"Jingcong Liang, Siyuan Wang, Miren Tian, Yitong Li, Duyu Tang, Zhongyu Wei",http://arxiv.org/pdf/2505.16056v1,cs.LG
Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs,"Boolean Satisfiability (SAT) solvers are foundational to computer science,
yet their performance typically hinges on hand-crafted heuristics. This work
introduces Reinforcement Learning from Algorithm Feedback (RLAF) as a paradigm
for learning to guide SAT solver branching heuristics with Graph Neural
Networks (GNNs). Central to our approach is a novel and generic mechanism for
injecting inferred variable weights and polarities into the branching
heuristics of existing SAT solvers. In a single forward pass, a GNN assigns
these parameters to all variables. Casting this one-shot guidance as a
reinforcement learning problem lets us train the GNN with off-the-shelf
policy-gradient methods, such as GRPO, directly using the solver's
computational cost as the sole reward signal. Extensive evaluations demonstrate
that RLAF-trained policies significantly reduce the mean solve times of
different base solvers across diverse SAT problem distributions, achieving more
than a 2x speedup in some cases, while generalizing effectively to larger and
harder problems after training. Notably, these policies consistently outperform
expert-supervised approaches based on learning handcrafted weighting
heuristics, offering a promising path towards data-driven heuristic design in
combinatorial optimization.",2025-05-21,"Jan Tönshoff, Martin Grohe",http://arxiv.org/pdf/2505.16053v1,cs.LG
PO-Flow: Flow-based Generative Models for Sampling Potential Outcomes and Counterfactuals,"We propose PO-Flow, a novel continuous normalizing flow (CNF) framework for
causal inference that jointly models potential outcomes and counterfactuals.
Trained via flow matching, PO-Flow provides a unified framework for
individualized potential outcome prediction, counterfactual predictions, and
uncertainty-aware density learning. Among generative models, it is the first to
enable density learning of potential outcomes without requiring explicit
distributional assumptions (e.g., Gaussian mixtures), while also supporting
counterfactual prediction conditioned on factual outcomes in general
observational datasets. On benchmarks such as ACIC, IHDP, and IBM, it
consistently outperforms prior methods across a range of causal inference
tasks. Beyond that, PO-Flow succeeds in high-dimensional settings, including
counterfactual image generation, demonstrating its broad applicability.",2025-05-21,"Dongze Wu, David I. Inouye, Yao Xie",http://arxiv.org/pdf/2505.16051v1,cs.LG
Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation,"Studies on schizophrenia assessments using deep learning typically treat it
as a classification task to detect the presence or absence of the disorder,
oversimplifying the condition and reducing its clinical applicability. This
traditional approach overlooks the complexity of schizophrenia, limiting its
practical value in healthcare settings. This study shifts the focus to
individual symptom severity estimation using a multimodal approach that
integrates speech, video, and text inputs. We develop unimodal models for each
modality and a multimodal framework to improve accuracy and robustness. By
capturing a more detailed symptom profile, this approach can help in enhancing
diagnostic precision and support personalized treatment, offering a scalable
and objective tool for mental health assessment.",2025-05-21,"Gowtham Premananth, Philip Resnik, Sonia Bansal, Deanna L. Kelly, Carol Espy-Wilson",http://arxiv.org/pdf/2505.16044v1,cs.LG
Physics-based machine learning for mantle convection simulations,"Mantle convection simulations are an essential tool for understanding how
rocky planets evolve. However, the poorly known input parameters to these
simulations, the non-linear dependence of transport properties on pressure and
temperature, and the long integration times in excess of several billion years
all pose a computational challenge for numerical solvers. We propose a
physics-based machine learning approach that predicts creeping flow velocities
as a function of temperature while conserving mass, thereby bypassing the
numerical solution of the Stokes problem. A finite-volume solver then uses the
predicted velocities to advect and diffuse the temperature field to the next
time-step, enabling autoregressive rollout at inference. For training, our
model requires temperature-velocity snapshots from a handful of simulations
(94). We consider mantle convection in a two-dimensional rectangular box with
basal and internal heating, pressure- and temperature-dependent viscosity.
Overall, our model is up to 89 times faster than the numerical solver. We also
show the importance of different components in our convolutional neural network
architecture such as mass conservation, learned paddings on the boundaries, and
loss scaling for the overall rollout performance. Finally, we test our approach
on unseen scenarios to demonstrate some of its strengths and weaknesses.",2025-05-21,"Siddhant Agarwal, Ali Can Bekar, Christian Hüttig, David S. Greenberg, Nicola Tosi",http://arxiv.org/pdf/2505.16041v1,cs.LG
Causal LLM Routing: End-to-End Regret Minimization from Observational Data,"LLM routing aims to select the most appropriate model for each query,
balancing competing performance metrics such as accuracy and cost across a pool
of language models. Prior approaches typically adopt a decoupled strategy,
where the metrics are first predicted and the model is then selected based on
these estimates. This setup is prone to compounding errors and often relies on
full-feedback data, where each query is evaluated by all candidate models,
which is costly to obtain and maintain in practice. In contrast, we learn from
observational data, which records only the outcome of the model actually
deployed. We propose a causal end-to-end framework that learns routing policies
by minimizing decision-making regret from observational data. To enable
efficient optimization, we introduce two theoretically grounded surrogate
objectives: a classification-based upper bound, and a softmax-weighted regret
approximation shown to recover the optimal policy at convergence. We further
extend our framework to handle heterogeneous cost preferences via an
interval-conditioned architecture. Experiments on public benchmarks show that
our method outperforms existing baselines, achieving state-of-the-art
performance across different embedding models.",2025-05-21,"Asterios Tsiourvas, Wei Sun, Georgia Perakis",http://arxiv.org/pdf/2505.16037v1,cs.LG
"Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces","We introduce Equivariant Neural Eikonal Solvers, a novel framework that
integrates Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers. Our
approach employs a single neural field where a unified shared backbone is
conditioned on signal-specific latent variables - represented as point clouds
in a Lie group - to model diverse Eikonal solutions. The ENF integration
ensures equivariant mapping from these latent representations to the solution
field, delivering three key benefits: enhanced representation efficiency
through weight-sharing, robust geometric grounding, and solution steerability.
This steerability allows transformations applied to the latent point cloud to
induce predictable, geometrically meaningful modifications in the resulting
Eikonal solution. By coupling these steerable representations with
Physics-Informed Neural Networks (PINNs), our framework accurately models
Eikonal travel-time solutions while generalizing to arbitrary Riemannian
manifolds with regular group actions. This includes homogeneous spaces such as
Euclidean, position-orientation, spherical, and hyperbolic manifolds. We
validate our approach through applications in seismic travel-time modeling of
2D and 3D benchmark datasets. Experimental results demonstrate superior
performance, scalability, adaptability, and user controllability compared to
existing Neural Operator-based Eikonal solver methods.",2025-05-21,"Alejandro García-Castellanos, David R. Wessels, Nicky J. van den Berg, Remco Duits, Daniël M. Pelt, Erik J. Bekkers",http://arxiv.org/pdf/2505.16035v1,cs.LG
Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging,"Diffusion trajectory distillation methods aim to accelerate sampling in
diffusion models, which produce high-quality outputs but suffer from slow
sampling speeds. These methods train a student model to approximate the
multi-step denoising process of a pretrained teacher model in a single step,
enabling one-shot generation. However, theoretical insights into the trade-off
between different distillation strategies and generative quality remain
limited, complicating their optimization and selection. In this work, we take a
first step toward addressing this gap. Specifically, we reinterpret trajectory
distillation as an operator merging problem in the linear regime, where each
step of the teacher model is represented as a linear operator acting on noisy
data. These operators admit a clear geometric interpretation as projections and
rescalings corresponding to the noise schedule. During merging, signal
shrinkage occurs as a convex combination of operators, arising from both
discretization and limited optimization time of the student model. We propose a
dynamic programming algorithm to compute the optimal merging strategy that
maximally preserves signal fidelity. Additionally, we demonstrate the existence
of a sharp phase transition in the optimal strategy, governed by data
covariance structures. Our findings enhance the theoretical understanding of
diffusion trajectory distillation and offer practical insights for improving
distillation strategies.",2025-05-21,"Weiguo Gao, Ming Li",http://arxiv.org/pdf/2505.16024v1,cs.LG
NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning,"Recent advances such as DeepSeek R1-Zero highlight the effectiveness of
incentive training, a reinforcement learning paradigm that computes rewards
solely based on the final answer part of a language model's output, thereby
encouraging the generation of intermediate reasoning steps. However, these
methods fundamentally rely on external verifiers, which limits their
applicability to domains like mathematics and coding where such verifiers are
readily available. Although reward models can serve as verifiers, they require
high-quality annotated data and are costly to train. In this work, we propose
NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning
framework that requires only standard supervised fine-tuning data with no need
for an external verifier. NOVER enables incentive training across a wide range
of text-to-text tasks and outperforms the model of the same size distilled from
large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the
flexibility of NOVER enables new possibilities for optimizing large language
models, such as inverse incentive training.",2025-05-21,"Wei Liu, Siya Qi, Xinyu Wang, Chen Qian, Yali Du, Yulan He",http://arxiv.org/pdf/2505.16022v1,cs.LG
GradPCA: Leveraging NTK Alignment for Reliable Out-of-Distribution Detection,"We introduce GradPCA, an Out-of-Distribution (OOD) detection method that
exploits the low-rank structure of neural network gradients induced by Neural
Tangent Kernel (NTK) alignment. GradPCA applies Principal Component Analysis
(PCA) to gradient class-means, achieving more consistent performance than
existing methods across standard image classification benchmarks. We provide a
theoretical perspective on spectral OOD detection in neural networks to support
GradPCA, highlighting feature-space properties that enable effective detection
and naturally emerge from NTK alignment. Our analysis further reveals that
feature quality -- particularly the use of pretrained versus non-pretrained
representations -- plays a crucial role in determining which detectors will
succeed. Extensive experiments validate the strong performance of GradPCA, and
our theoretical framework offers guidance for designing more principled
spectral OOD detectors.",2025-05-21,"Mariia Seleznova, Hung-Hsu Chou, Claudio Mayrink Verdun, Gitta Kutyniok",http://arxiv.org/pdf/2505.16017v1,cs.LG
Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations,"Sparse autoencoders (SAEs) are commonly used to interpret the internal
activations of large language models (LLMs) by mapping them to
human-interpretable concept representations. While existing evaluations of SAEs
focus on metrics such as the reconstruction-sparsity tradeoff, human
(auto-)interpretability, and feature disentanglement, they overlook a critical
aspect: the robustness of concept representations to input perturbations. We
argue that robustness must be a fundamental consideration for concept
representations, reflecting the fidelity of concept labeling. To this end, we
formulate robustness quantification as input-space optimization problems and
develop a comprehensive evaluation framework featuring realistic scenarios in
which adversarial perturbations are crafted to manipulate SAE representations.
Empirically, we find that tiny adversarial input perturbations can effectively
manipulate concept-based interpretations in most scenarios without notably
affecting the outputs of the base LLMs themselves. Overall, our results suggest
that SAE concept representations are fragile and may be ill-suited for
applications in model monitoring and oversight.",2025-05-21,"Aaron J. Li, Suraj Srinivas, Usha Bhalla, Himabindu Lakkaraju",http://arxiv.org/pdf/2505.16004v1,cs.LG
Towards Identifiability of Interventional Stochastic Differential Equations,"We study identifiability of stochastic differential equation (SDE) models
under multiple interventions. Our results give the first provable bounds for
unique recovery of SDE parameters given samples from their stationary
distributions. We give tight bounds on the number of necessary interventions
for linear SDEs, and upper bounds for nonlinear SDEs in the small noise regime.
We experimentally validate the recovery of true parameters in synthetic data,
and motivated by our theoretical results, demonstrate the advantage of
parameterizations with learnable activation functions.",2025-05-21,"Aaron Zweig, Zaikang Lin, Elham Azizi, David Knowles",http://arxiv.org/pdf/2505.15987v1,cs.LG
"Diffusion Probabilistic Generative Models for Accelerated, in-NICU Permanent Magnet Neonatal MRI","Purpose: Magnetic Resonance Imaging (MRI) enables non-invasive assessment of
brain abnormalities during early life development. Permanent magnet scanners
operating in the neonatal intensive care unit (NICU) facilitate MRI of sick
infants, but have long scan times due to lower signal-to-noise ratios (SNR) and
limited receive coils. This work accelerates in-NICU MRI with diffusion
probabilistic generative models by developing a training pipeline accounting
for these challenges.
  Methods: We establish a novel training dataset of clinical, 1 Tesla neonatal
MR images in collaboration with Aspect Imaging and Sha'are Zedek Medical
Center. We propose a pipeline to handle the low quantity and SNR of our
real-world dataset (1) modifying existing network architectures to support
varying resolutions; (2) training a single model on all data with learned class
embedding vectors; (3) applying self-supervised denoising before training; and
(4) reconstructing by averaging posterior samples. Retrospective under-sampling
experiments, accounting for signal decay, evaluated each item of our proposed
methodology. A clinical reader study with practicing pediatric
neuroradiologists evaluated our proposed images reconstructed from 1.5x
under-sampled data.
  Results: Combining all data, denoising pre-training, and averaging posterior
samples yields quantitative improvements in reconstruction. The generative
model decouples the learned prior from the measurement model and functions at
two acceleration rates without re-training. The reader study suggests that
proposed images reconstructed from approximately 1.5x under-sampled data are
adequate for clinical use.
  Conclusion: Diffusion probabilistic generative models applied with the
proposed pipeline to handle challenging real-world datasets could reduce scan
time of in-NICU neonatal MRI.",2025-05-21,"Yamin Arefeen, Brett Levac, Bhairav Patel, Chang Ho, Jonathan I. Tamir",http://arxiv.org/pdf/2505.15984v1,cs.LG
"Real-Time Stress Monitoring, Detection, and Management in College Students: A Wearable Technology and Machine-Learning Approach","College students are increasingly affected by stress, anxiety, and
depression, yet face barriers to traditional mental health care. This study
evaluated the efficacy of a mobile health (mHealth) intervention, Mental Health
Evaluation and Lookout Program (mHELP), which integrates a smartwatch sensor
and machine learning (ML) algorithms for real-time stress detection and
self-management. In a 12-week randomized controlled trial (n = 117),
participants were assigned to a treatment group using mHELP's full suite of
interventions or a control group using the app solely for real-time stress
logging and weekly psychological assessments. The primary outcome, ""Moments of
Stress"" (MS), was assessed via physiological and self-reported indicators and
analyzed using Generalized Linear Mixed Models (GLMM) approaches. Similarly,
secondary outcomes of psychological assessments, including the Generalized
Anxiety Disorder-7 (GAD-7) for anxiety, the Patient Health Questionnaire
(PHQ-8) for depression, and the Perceived Stress Scale (PSS), were also
analyzed via GLMM. The finding of the objective measure, MS, indicates a
substantial decrease in MS among the treatment group compared to the control
group, while no notable between-group differences were observed in subjective
scores of anxiety (GAD-7), depression (PHQ-8), or stress (PSS). However, the
treatment group exhibited a clinically meaningful decline in GAD-7 and PSS
scores. These findings underscore the potential of wearable-enabled mHealth
tools to reduce acute stress in college populations and highlight the need for
extended interventions and tailored features to address chronic symptoms like
depression.",2025-05-21,"Alan Ta, Nilsu Salgin, Mustafa Demir, Kala Philips Randal, Ranjana K. Mehta, Anthony McDonald, Carly McCord, Farzan Sasangohar",http://arxiv.org/pdf/2505.15974v1,cs.LG
Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders,"The ImageNet hierarchy provides a structured taxonomy of object categories,
offering a valuable lens through which to analyze the representations learned
by deep vision models. In this work, we conduct a comprehensive analysis of how
vision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders
(SAEs) to probe their internal representations. SAEs have been widely used as
an explanation tool for large language models (LLMs), where they enable the
discovery of semantically meaningful features. Here, we extend their use to
vision models to investigate whether learned representations align with the
ontological structure defined by the ImageNet taxonomy. Our results show that
SAEs uncover hierarchical relationships in model activations, revealing an
implicit encoding of taxonomic structure. We analyze the consistency of these
representations across different layers of the popular vision foundation model
DINOv2 and provide insights into how deep vision models internalize
hierarchical category information by increasing information in the class token
through each layer. Our study establishes a framework for systematic
hierarchical analysis of vision model representations and highlights the
potential of SAEs as a tool for probing semantic structure in deep networks.",2025-05-21,"Matthew Lyle Olson, Musashi Hinck, Neale Ratzlaff, Changbai Li, Phillip Howard, Vasudev Lal, Shao-Yen Tseng",http://arxiv.org/pdf/2505.15970v1,cs.LG
Pre-training Large Memory Language Models with Internal and External Knowledge,"Neural language models are black-boxes -- both linguistic patterns and
factual knowledge are distributed across billions of opaque parameters. This
entangled encoding makes it difficult to reliably inspect, verify, or update
specific facts. We propose a new class of language models, Large Memory
Language Models (LMLM) with a pre-training recipe that stores factual knowledge
in both internal weights and an external database. Our approach strategically
masks externally retrieved factual values from the training loss, thereby
teaching the model to perform targeted lookups rather than relying on
memorization in model weights. Our experiments demonstrate that LMLMs achieve
competitive performance compared to significantly larger, knowledge-dense LLMs
on standard benchmarks, while offering the advantages of explicit, editable,
and verifiable knowledge bases. This work represents a fundamental shift in how
language models interact with and manage factual knowledge.",2025-05-21,"Linxi Zhao, Sofian Zalouk, Christian K. Belardi, Justin Lovelace, Jin Peng Zhou, Kilian Q. Weinberger, Yoav Artzi, Jennifer J. Sun",http://arxiv.org/pdf/2505.15962v1,cs.LG
Data-driven Verification of Procedural Programs with Integer Arrays,"We address the problem of verifying automatically procedural programs
manipulating parametric-size arrays of integers, encoded as a constrained Horn
clauses solving problem. We propose a new algorithmic method for synthesizing
loop invariants and procedure pre/post-conditions represented as universally
quantified first-order formulas constraining the array elements and program
variables. We adopt a data-driven approach that extends the decision tree
Horn-ICE framework to handle arrays. We provide a powerful learning technique
based on reducing a complex classification problem of vectors of integer arrays
to a simpler classification problem of vectors of integers. The obtained
classifier is generalized to get universally quantified invariants and
procedure pre/post-conditions. We have implemented our method and shown its
efficiency and competitiveness w.r.t. state-of-the-art tools on a significant
benchmark.",2025-05-21,"Ahmed Bouajjani, Wael-Amine Boutglay, Peter Habermehl",http://arxiv.org/pdf/2505.15958v1,cs.LG
MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding,"Decoding visual experiences from fMRI offers a powerful avenue to understand
human perception and develop advanced brain-computer interfaces. However,
current progress often prioritizes maximizing reconstruction fidelity while
overlooking interpretability, an essential aspect for deriving neuroscientific
insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework
designed for high-fidelity, adaptable, and interpretable visual reconstruction.
MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture
where distinct experts process fMRI signals from functionally related voxel
groups, mimicking specialized brain networks. The experts are first trained to
encode fMRI into the frozen CLIP space. A finetuned diffusion model then
synthesizes images, guided by expert outputs through a novel dual-stage routing
mechanism that dynamically weighs expert contributions across the diffusion
process. MoRE-Brain offers three main advancements: First, it introduces a
novel Mixture-of-Experts architecture grounded in brain network principles for
neuro-decoding. Second, it achieves efficient cross-subject generalization by
sharing core expert networks while adapting only subject-specific routers.
Third, it provides enhanced mechanistic insight, as the explicit routing
reveals precisely how different modeled brain regions shape the semantic and
spatial attributes of the reconstructed image. Extensive experiments validate
MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further
demonstrating its effective utilization of fMRI signals, distinguishing genuine
neural decoding from over-reliance on generative priors. Consequently,
MoRE-Brain marks a substantial advance towards more generalizable and
interpretable fMRI-based visual decoding. Code will be publicly available soon:
https://github.com/yuxiangwei0808/MoRE-Brain.",2025-05-21,"Yuxiang Wei, Yanteng Zhang, Xi Xiao, Tianyang Wang, Xiao Wang, Vince D. Calhoun",http://arxiv.org/pdf/2505.15946v2,cs.LG
AllMetrics: A Unified Python Library for Standardized Metric Evaluation and Robust Data Validation in Machine Learning,"Machine learning (ML) models rely heavily on consistent and accurate
performance metrics to evaluate and compare their effectiveness. However,
existing libraries often suffer from fragmentation, inconsistent
implementations, and insufficient data validation protocols, leading to
unreliable results. Existing libraries have often been developed independently
and without adherence to a unified standard, particularly concerning the
specific tasks they aim to support. As a result, each library tends to adopt
its conventions for metric computation, input/output formatting, error
handling, and data validation protocols. This lack of standardization leads to
both implementation differences (ID) and reporting differences (RD), making it
difficult to compare results across frameworks or ensure reliable evaluations.
To address these issues, we introduce AllMetrics, an open-source unified Python
library designed to standardize metric evaluation across diverse ML tasks,
including regression, classification, clustering, segmentation, and
image-to-image translation. The library implements class-specific reporting for
multi-class tasks through configurable parameters to cover all use cases, while
incorporating task-specific parameters to resolve metric computation
discrepancies across implementations. Various datasets from domains like
healthcare, finance, and real estate were applied to our library and compared
with Python, Matlab, and R components to identify which yield similar results.
AllMetrics combines a modular Application Programming Interface (API) with
robust input validation mechanisms to ensure reproducibility and reliability in
model evaluation. This paper presents the design principles, architectural
components, and empirical analyses demonstrating the ability to mitigate
evaluation errors and to enhance the trustworthiness of ML workflows.",2025-05-21,"Morteza Alizadeh, Mehrdad Oveisi, Sonya Falahati, Ghazal Mousavi, Mohsen Alambardar Meybodi, Somayeh Sadat Mehrnia, Ilker Hacihaliloglu, Arman Rahmim, Mohammad R. Salmanpour",http://arxiv.org/pdf/2505.15931v1,cs.LG
CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision,"Learning complex functions that involve multi-step reasoning poses a
significant challenge for standard supervised learning from input-output
examples. Chain-of-thought (CoT) supervision, which provides intermediate
reasoning steps together with the final output, has emerged as a powerful
empirical technique, underpinning much of the recent progress in the reasoning
capabilities of large language models. This paper develops a statistical theory
of learning under CoT supervision. A key characteristic of the CoT setting, in
contrast to standard supervision, is the mismatch between the training
objective (CoT risk) and the test objective (end-to-end risk). A central part
of our analysis, distinguished from prior work, is explicitly linking those two
types of risk to achieve sharper sample complexity bounds. This is achieved via
the *CoT information measure* $\mathcal{I}_{\mathcal{D},
h_\star}^{\mathrm{CoT}}(\epsilon; \calH)$, which quantifies the additional
discriminative power gained from observing the reasoning process. The main
theoretical results demonstrate how CoT supervision can yield significantly
faster learning rates compared to standard E2E supervision. Specifically, it is
shown that the sample complexity required to achieve a target E2E error
$\epsilon$ scales as $d/\mathcal{I}_{\mathcal{D},
h_\star}^{\mathrm{CoT}}(\epsilon; \calH)$, where $d$ is a measure of hypothesis
class complexity, which can be much faster than standard $d/\epsilon$ rates.
Information-theoretic lower bounds in terms of the CoT information are also
obtained. Together, these results suggest that CoT information is a fundamental
measure of statistical complexity for learning under chain-of-thought
supervision.",2025-05-21,"Awni Altabaa, Omar Montasser, John Lafferty",http://arxiv.org/pdf/2505.15927v1,cs.LG
Is (Selective) Round-To-Nearest Quantization All You Need?,"Quantization became a necessary tool for serving ever-increasing Large
Language Models (LLMs). RTN (Round-to-Nearest) is perhaps the simplest
quantization technique that has been around well before LLMs surged to the
forefront of machine learning (ML) research. Yet, it has been largely dismissed
by recent and more advanced quantization methods that claim superiority over
RTN in nearly every aspect of performance. This work aims to dispel this
established point of view, showing that RTN is not only much cheaper to apply,
but also its token generation throughput can be better than and accuracy can be
similar to more advanced alternatives. In particular, we discuss our
implementation of RTN based on the recent Marlin kernels and demonstrate how
the accuracy of RTN can be gradually improved by selectively increasing the
data precision format of certain model layers and modules. Based on our
results, we argue that RTN presents a viable and practical choice for
quantizing LLMs.",2025-05-21,Alex Kogan,http://arxiv.org/pdf/2505.15909v1,cs.LG
Last Layer Empirical Bayes,"The task of quantifying the inherent uncertainty associated with neural
network predictions is a key challenge in artificial intelligence. Bayesian
neural networks (BNNs) and deep ensembles are among the most prominent
approaches to tackle this task. Both approaches produce predictions by
computing an expectation of neural network outputs over some distribution on
the corresponding weights; this distribution is given by the posterior in the
case of BNNs, and by a mixture of point masses for ensembles. Inspired by
recent work showing that the distribution used by ensembles can be understood
as a posterior corresponding to a learned data-dependent prior, we propose last
layer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a
normalizing flow, which is then trained to maximize the evidence lower bound;
to retain tractability we use the flow only on the last layer. We show why LLEB
is well motivated, and how it interpolates between standard BNNs and ensembles
in terms of the strength of the prior that they use. LLEB performs on par with
existing approaches, highlighting that empirical Bayes is a promising direction
for future research in uncertainty quantification.",2025-05-21,"Valentin Villecroze, Yixin Wang, Gabriel Loaiza-Ganem",http://arxiv.org/pdf/2505.15888v1,cs.LG
Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex,"Understanding functional representations within higher visual cortex is a
fundamental question in computational neuroscience. While artificial neural
networks pretrained on large-scale datasets exhibit striking representational
alignment with human neural responses, learning image-computable models of
visual cortex relies on individual-level, large-scale fMRI datasets. The
necessity for expensive, time-intensive, and often impractical data acquisition
limits the generalizability of encoders to new subjects and stimuli. BraInCoRL
uses in-context learning to predict voxelwise neural responses from few-shot
examples without any additional finetuning for novel subjects and stimuli. We
leverage a transformer architecture that can flexibly condition on a variable
number of in-context image stimuli, learning an inductive bias over multiple
subjects. During training, we explicitly optimize the model for in-context
learning. By jointly conditioning on image features and voxel activations, our
model learns to directly generate better performing voxelwise models of higher
visual cortex. We demonstrate that BraInCoRL consistently outperforms existing
voxelwise encoder designs in a low-data regime when evaluated on entirely novel
images, while also exhibiting strong test-time scaling behavior. The model also
generalizes to an entirely new visual fMRI dataset, which uses different
subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates
better interpretability of neural signals in higher visual cortex by attending
to semantically relevant stimuli. Finally, we show that our framework enables
interpretable mappings from natural language queries to voxel selectivity.",2025-05-21,"Muquan Yu, Mu Nan, Hossein Adeli, Jacob S. Prince, John A. Pyles, Leila Wehbe, Margaret M. Henderson, Michael J. Tarr, Andrew F. Luo",http://arxiv.org/pdf/2505.15813v1,cs.LG
On the creation of narrow AI: hierarchy and nonlocality of neural network skills,"We study the problem of creating strong, yet narrow, AI systems. While recent
AI progress has been driven by the training of large general-purpose foundation
models, the creation of smaller models specialized for narrow domains could be
valuable for both efficiency and safety. In this work, we explore two
challenges involved in creating such systems, having to do with basic
properties of how neural networks learn and structure their representations.
The first challenge regards when it is possible to train narrow models from
scratch. Through experiments on a synthetic task, we find that it is sometimes
necessary to train networks on a wide distribution of data to learn certain
narrow skills within that distribution. This effect arises when skills depend
on each other hierarchically, and training on a broad distribution introduces a
curriculum which substantially accelerates learning. The second challenge
regards how to transfer particular skills from large general models into small
specialized models. We find that model skills are often not perfectly localized
to a particular set of prunable components. However, we find that methods based
on pruning can still outperform distillation. We investigate the use of a
regularization objective to align desired skills with prunable components while
unlearning unnecessary skills.",2025-05-21,"Eric J. Michaud, Asher Parker-Sartori, Max Tegmark",http://arxiv.org/pdf/2505.15811v1,cs.LG
Neural Conditional Transport Maps,"We present a neural framework for learning conditional optimal transport (OT)
maps between probability distributions. Our approach introduces a conditioning
mechanism capable of processing both categorical and continuous conditioning
variables simultaneously. At the core of our method lies a hypernetwork that
generates transport layer parameters based on these inputs, creating adaptive
mappings that outperform simpler conditioning methods. Comprehensive ablation
studies demonstrate the superior performance of our method over baseline
configurations. Furthermore, we showcase an application to global sensitivity
analysis, offering high performance in computing OT-based sensitivity indices.
This work advances the state-of-the-art in conditional optimal transport,
enabling broader application of optimal transport principles to complex,
high-dimensional domains such as generative modeling and black-box model
explainability.",2025-05-21,"Carlos Rodriguez-Pardo, Leonardo Chiani, Emanuele Borgonovo, Massimo Tavoni",http://arxiv.org/pdf/2505.15808v1,cs.LG
The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation,"Large language models are able to exploit in-context learning to access
external knowledge beyond their training data through retrieval-augmentation.
While promising, its inner workings remain unclear. In this work, we shed light
on the mechanism of in-context retrieval augmentation for question answering by
viewing a prompt as a composition of informational components. We propose an
attribution-based method to identify specialized attention heads, revealing
in-context heads that comprehend instructions and retrieve relevant contextual
information, and parametric heads that store entities' relational knowledge. To
better understand their roles, we extract function vectors and modify their
attention weights to show how they can influence the answer generation process.
Finally, we leverage the gained insights to trace the sources of knowledge used
during inference, paving the way towards more safe and transparent language
models.",2025-05-21,"Patrick Kahardipraja, Reduan Achtibat, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin",http://arxiv.org/pdf/2505.15807v1,cs.LG
Adaptive Estimation and Learning under Temporal Distribution Shift,"In this paper, we study the problem of estimation and learning under temporal
distribution shift. Consider an observation sequence of length $n$, which is a
noisy realization of a time-varying groundtruth sequence. Our focus is to
develop methods to estimate the groundtruth at the final time-step while
providing sharp point-wise estimation error rates. We show that, without prior
knowledge on the level of temporal shift, a wavelet soft-thresholding estimator
provides an optimal estimation error bound for the groundtruth. Our proposed
estimation method generalizes existing researches Mazzetto and Upfal (2023) by
establishing a connection between the sequence's non-stationarity level and the
sparsity in the wavelet-transformed domain. Our theoretical findings are
validated by numerical experiments. Additionally, we applied the estimator to
derive sparsity-aware excess risk bounds for binary classification under
distribution shift and to develop computationally efficient training
objectives. As a final contribution, we draw parallels between our results and
the classical signal processing problem of total-variation denoising (Mammen
and van de Geer,1997; Tibshirani, 2014), uncovering novel optimal algorithms
for such task.",2025-05-21,"Dheeraj Baby, Yifei Tang, Hieu Duy Nguyen, Yu-Xiang Wang, Rohit Pyati",http://arxiv.org/pdf/2505.15803v1,cs.LG
"A Deep Learning Framework for Two-Dimensional, Multi-Frequency Propagation Factor Estimation","Accurately estimating the refractive environment over multiple frequencies
within the marine atmospheric boundary layer is crucial for the effective
deployment of radar technologies. Traditional parabolic equation simulations,
while effective, can be computationally expensive and time-intensive, limiting
their practical application. This communication explores a novel approach using
deep neural networks to estimate the pattern propagation factor, a critical
parameter for characterizing environmental impacts on signal propagation.
Image-to-image translation generators designed to ingest modified refractivity
data and generate predictions of pattern propagation factors over the same
domain were developed. Findings demonstrate that deep neural networks can be
trained to analyze multiple frequencies and reasonably predict the pattern
propagation factor, offering an alternative to traditional methods.",2025-05-21,"Sarah E. Wessinger, Leslie N. Smith, Jacob Gull, Jonathan Gehman, Zachary Beever, Andrew J. Kammerer",http://arxiv.org/pdf/2505.15802v1,cs.LG
Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning,"Certifying the IID generalisation ability of deep networks is the first of
many requirements for trusting AI in high-stakes applications from medicine to
security. However, when instantiating generalisation bounds for deep networks
it remains challenging to obtain non-vacuous guarantees, especially when
applying contemporary large models on the small scale data prevalent in such
high-stakes fields. In this paper, we draw a novel connection between a family
of learning methods based on model fusion and generalisation certificates, and
surprisingly show that with minor adjustment several existing learning
strategies already provide non-trivial generalisation guarantees. Essentially,
by focusing on data-driven learning of downstream tasks by fusion rather than
fine-tuning, the certified generalisation gap becomes tiny and independent of
the base network size, facilitating its certification. Our results show for the
first time non-trivial generalisation guarantees for learning with as low as
100 examples, while using vision models such as VIT-B and language models such
as mistral-7B. This observation is significant as it has immediate implications
for facilitating the certification of existing systems as trustworthy, and
opens up new directions for research at the intersection of practice and
theory.",2025-05-21,"Taehoon Kim, Henry Gouk, Minyoung Kim, Timothy Hospedales",http://arxiv.org/pdf/2505.15798v1,cs.LG
HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving,"Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can
enhance autonomous driving (AD) performance in complex scenarios. However,
current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to
hallucinations. Evaluations show that state-of-the-art LLM indicates a
non-hallucination rate of only approximately 57.95% when assessed on essential
driving-related tasks. Thus, in these methods, hallucinations from the LLM can
directly jeopardize the performance of driving policies. This paper argues that
maintaining relative independence between the LLM and the RL is vital for
solving the hallucinations problem. Consequently, this paper is devoted to
propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic
hints for state augmentation and policy optimization to assist RL agent in
motion planning, while the RL agent counteracts potential erroneous semantic
indications through policy learning to achieve excellent driving performance.
Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual
Reinforcement Learning Motion Planner) architecture, which is designed that
includes Augmented Semantic Representation Module to extend state space.
Contextual Stability Anchor Module enhances the reliability of multi-critic
weight hints by utilizing information from the knowledge base. Semantic Cache
Module is employed to seamlessly integrate LLM low-frequency guidance with RL
high-frequency control. Extensive experiments in CARLA validate HCRMP's strong
overall driving performance. HCRMP achieves a task success rate of up to 80.3%
under diverse driving conditions with different traffic densities. Under
safety-critical driving conditions, HCRMP significantly reduces the collision
rate by 11.4%, which effectively improves the driving performance in complex
scenarios.",2025-05-21,"Zhiwen Chen, Bo Leng, Zhuoren Li, Hanming Deng, Guizhe Jin, Ran Yu, Huanxi Wen",http://arxiv.org/pdf/2505.15793v2,cs.LG
Long-Form Information Alignment Evaluation Beyond Atomic Facts,"Information alignment evaluators are vital for various NLG evaluation tasks
and trustworthy LLM deployment, reducing hallucinations and enhancing user
trust. Current fine-grained methods, like FactScore, verify facts individually
but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this
work, we introduce MontageLie, a challenging benchmark that constructs
deceptive narratives by ""montaging"" truthful statements without introducing
explicit hallucinations. We demonstrate that both coarse-grained LLM-based
evaluators and current fine-grained frameworks are susceptible to this attack,
with AUC-ROC scores falling below 65%. To enable more robust fine-grained
evaluation, we propose DoveScore, a novel framework that jointly verifies
factual accuracy and event-order consistency. By modeling inter-fact
relationships, DoveScore outperforms existing fine-grained methods by over 8%,
providing a more robust solution for long-form text alignment evaluation. Our
code and datasets are available at https://github.com/dannalily/DoveScore.",2025-05-21,"Danna Zheng, Mirella Lapata, Jeff Z. Pan",http://arxiv.org/pdf/2505.15792v1,cs.LG
VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL,"Diffusion models have emerged as powerful generative tools across various
domains, yet tailoring pre-trained models to exhibit specific desirable
properties remains challenging. While reinforcement learning (RL) offers a
promising solution,current methods struggle to simultaneously achieve stable,
efficient fine-tuning and support non-differentiable rewards. Furthermore,
their reliance on sparse rewards provides inadequate supervision during
intermediate steps, often resulting in suboptimal generation quality. To
address these limitations, dense and differentiable signals are required
throughout the diffusion process. Hence, we propose VAlue-based Reinforced
Diffusion (VARD): a novel approach that first learns a value function
predicting expection of rewards from intermediate states, and subsequently uses
this value function with KL regularization to provide dense supervision
throughout the generation process. Our method maintains proximity to the
pretrained model while enabling effective and stable training via
backpropagation. Experimental results demonstrate that our approach facilitates
better trajectory guidance, improves training efficiency and extends the
applicability of RL to diffusion models optimized for complex,
non-differentiable reward functions.",2025-05-21,"Fengyuan Dai, Zifeng Zhuang, Yufei Huang, Siteng Huang, Bangyan Liao, Donglin Wang, Fajie Yuan",http://arxiv.org/pdf/2505.15791v1,cs.LG
Fair Supervised Learning Through Constraints on Smooth Nonconvex Unfairness-Measure Surrogates,"A new strategy for fair supervised machine learning is proposed. The main
advantages of the proposed strategy as compared to others in the literature are
as follows. (a) We introduce a new smooth nonconvex surrogate to approximate
the Heaviside functions involved in discontinuous unfairness measures. The
surrogate is based on smoothing methods from the optimization literature, and
is new for the fair supervised learning literature. The surrogate is a tight
approximation which ensures the trained prediction models are fair, as opposed
to other (e.g., convex) surrogates that can fail to lead to a fair prediction
model in practice. (b) Rather than rely on regularizers (that lead to
optimization problems that are difficult to solve) and corresponding
regularization parameters (that can be expensive to tune), we propose a
strategy that employs hard constraints so that specific tolerances for
unfairness can be enforced without the complications associated with the use of
regularization. (c)~Our proposed strategy readily allows for constraints on
multiple (potentially conflicting) unfairness measures at the same time.
Multiple measures can be considered with a regularization approach, but at the
cost of having even more difficult optimization problems to solve and further
expense for tuning. By contrast, through hard constraints, our strategy leads
to optimization models that can be solved tractably with minimal tuning.",2025-05-21,"Zahra Khatti, Daniel P. Robinson, Frank E. Curtis",http://arxiv.org/pdf/2505.15788v1,cs.LG
Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval,"While an image is worth more than a thousand words, only a few provide
crucial information for a given task and thus should be focused on. In light of
this, ideal text-to-image (T2I) retrievers should prioritize specific visual
attributes relevant to queries. To evaluate current retrievers on handling
attribute-focused queries, we build COCO-Facet, a COCO-based benchmark with
9,112 queries about diverse attributes of interest. We find that CLIP-like
retrievers, which are widely adopted due to their efficiency and zero-shot
ability, have poor and imbalanced performance, possibly because their image
embeddings focus on global semantics and subjects while leaving out other
details. Notably, we reveal that even recent Multimodal Large Language Model
(MLLM)-based, stronger retrievers with a larger output dimension struggle with
this limitation. Hence, we hypothesize that retrieving with general image
embeddings is suboptimal for performing such queries. As a solution, we propose
to use promptable image embeddings enabled by these multimodal retrievers,
which boost performance by highlighting required attributes. Our pipeline for
deriving such embeddings generalizes across query types, image pools, and base
retriever architectures. To enhance real-world applicability, we offer two
acceleration strategies: Pre-processing promptable embeddings and using linear
approximations. We show that the former yields a 15% improvement in Recall@5
when prompts are predefined, while the latter achieves an 8% improvement when
prompts are only available during inference.",2025-05-21,"Siting Li, Xiang Gao, Simon Shaolei Du",http://arxiv.org/pdf/2505.15877v1,cs.LG
Large Language Models as Computable Approximations to Solomonoff Induction,"The rapid advancement of large language models (LLMs) calls for a rigorous
theoretical framework to explain their empirical success. While significant
progress has been made in understanding LLM behaviors, existing theoretical
frameworks remain fragmented in explaining emergent phenomena through a unified
mathematical lens. We establish the first formal connection between LLM
architectures and Algorithmic Information Theory (AIT) by proving two
fundamental results: (1) the training process computationally approximates
Solomonoff prior through loss minimization interpreted as program length
optimization, and (2) next-token prediction implements approximate Solomonoff
induction. We leverage AIT to provide a unified theoretical explanation for
in-context learning, few-shot learning, and scaling laws. Furthermore, our
theoretical insights lead to a principled method for few-shot example selection
that prioritizes samples where models exhibit lower predictive confidence. We
demonstrate through experiments on diverse text classification benchmarks that
this strategy yields significant performance improvements, particularly for
smaller model architectures, when compared to selecting high-confidence
examples. Our framework bridges the gap between theoretical foundations and
practical LLM behaviors, providing both explanatory power and actionable
insights for future model development.",2025-05-21,"Jun Wan, Lingrui Mei",http://arxiv.org/pdf/2505.15784v1,cs.LG
Solving General-Utility Markov Decision Processes in the Single-Trial Regime with Online Planning,"In this work, we contribute the first approach to solve infinite-horizon
discounted general-utility Markov decision processes (GUMDPs) in the
single-trial regime, i.e., when the agent's performance is evaluated based on a
single trajectory. First, we provide some fundamental results regarding policy
optimization in the single-trial regime, investigating which class of policies
suffices for optimality, casting our problem as a particular MDP that is
equivalent to our original problem, as well as studying the computational
hardness of policy optimization in the single-trial regime. Second, we show how
we can leverage online planning techniques, in particular a Monte-Carlo tree
search algorithm, to solve GUMDPs in the single-trial regime. Third, we provide
experimental results showcasing the superior performance of our approach in
comparison to relevant baselines.",2025-05-21,"Pedro P. Santos, Alberto Sardinha, Francisco S. Melo",http://arxiv.org/pdf/2505.15782v1,cs.LG
Projection-Based Correction for Enhancing Deep Inverse Networks,"Deep learning-based models have demonstrated remarkable success in solving
illposed inverse problems; however, many fail to strictly adhere to the
physical constraints imposed by the measurement process. In this work, we
introduce a projection-based correction method to enhance the inference of deep
inverse networks by ensuring consistency with the forward model. Specifically,
given an initial estimate from a learned reconstruction network, we apply a
projection step that constrains the solution to lie within the valid solution
space of the inverse problem. We theoretically demonstrate that if the recovery
model is a well-trained deep inverse network, the solution can be decomposed
into range-space and null-space components, where the projection-based
correction reduces to an identity transformation. Extensive simulations and
experiments validate the proposed method, demonstrating improved reconstruction
accuracy across diverse inverse problems and deep network architectures.",2025-05-21,Jorge Bacca,http://arxiv.org/pdf/2505.15777v1,cs.LG
Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention,"Large Language Models (LLMs) encounter significant challenges in
long-sequence inference due to computational inefficiency and redundant
processing, driving interest in context compression techniques. Existing
methods often rely on token importance to perform hard local compression or
encode context into latent representations for soft global compression.
However, the uneven distribution of textual content relevance and the diversity
of demands for user instructions mean these approaches frequently lead to the
loss of potentially valuable information. To address this, we propose
$\textbf{Hy}$brid $\textbf{Co}$ntext $\textbf{Co}$mpression (HyCo$_2$) for
LLMs, which integrates both global and local perspectives to guide context
compression while retaining both the essential semantics and critical details
for task completion. Specifically, we employ a hybrid adapter to refine global
semantics with the global view, based on the observation that different
adapters excel at different tasks. Then we incorporate a classification layer
that assigns a retention probability to each context token based on the local
view, determining whether it should be retained or discarded. To foster a
balanced integration of global and local compression, we introduce auxiliary
paraphrasing and completion pretraining before instruction tuning. This
promotes a synergistic integration that emphasizes instruction-relevant
information while preserving essential local details, ultimately balancing
local and global information retention in context compression. Experiments show
that our HyCo$_2$ method significantly enhances long-text reasoning while
reducing token usage. It improves the performance of various LLM series by an
average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover,
HyCo$_2$ matches the performance of uncompressed methods while reducing token
consumption by 88.8\%.",2025-05-21,"Huanxuan Liao, Wen Hu, Yao Xu, Shizhu He, Jun Zhao, Kang Liu",http://arxiv.org/pdf/2505.15774v1,cs.LG
Transfer of Structural Knowledge from Synthetic Languages,"This work explores transfer learning from several synthetic languages to
English. We investigate the structure of the embeddings in the fine-tuned
models, the information they contain, and the capabilities of the fine-tuned
models on simple linguistic tasks. We also introduce a new synthetic language
that leads to better transfer to English than the languages used in previous
research. Finally, we introduce Tiny-Cloze Benchmark - a new synthetic
benchmark for natural language understanding that is more informative for less
powerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models in
several domains demonstrating that fine-tuning on a new synthetic language
allows for better performance on a variety of tasks.",2025-05-21,"Mikhail Budnikov, Ivan Yamshchikov",http://arxiv.org/pdf/2505.15769v1,cs.LG
Improving planning and MBRL with temporally-extended actions,"Continuous time systems are often modeled using discrete time dynamics but
this requires a small simulation step to maintain accuracy. In turn, this
requires a large planning horizon which leads to computationally demanding
planning problems and reduced performance. Previous work in model free
reinforcement learning has partially addressed this issue using action repeats
where a policy is learned to determine a discrete action duration. Instead we
propose to control the continuous decision timescale directly by using
temporally-extended actions and letting the planner treat the duration of the
action as an additional optimization variable along with the standard action
variables. This additional structure has multiple advantages. It speeds up
simulation time of trajectories and, importantly, it allows for deep horizon
search in terms of primitive actions while using a shallow search depth in the
planner. In addition, in the model based reinforcement learning (MBRL) setting,
it reduces compounding errors from model learning and improves training time
for models. We show that this idea is effective and that the range for action
durations can be automatically selected using a multi-armed bandit formulation
and integrated into the MBRL framework. An extensive experimental evaluation
both in planning and in MBRL, shows that our approach yields faster planning,
better solutions, and that it enables solutions to problems that are not solved
in the standard formulation.",2025-05-21,"Palash Chatterjee, Roni Khardon",http://arxiv.org/pdf/2505.15754v1,cs.LG
Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval,"Large Language Models (LLMs) are known to be vulnerable to jailbreaking
attacks, wherein adversaries exploit carefully engineered prompts to induce
harmful or unethical responses. Such threats have raised critical concerns
about the safety and reliability of LLMs in real-world deployment. While
existing defense mechanisms partially mitigate such risks, subsequent
advancements in adversarial techniques have enabled novel jailbreaking methods
to circumvent these protections, exposing the limitations of static defense
frameworks. In this work, we explore defending against evolving jailbreaking
threats through the lens of context retrieval. First, we conduct a preliminary
study demonstrating that even a minimal set of safety-aligned examples against
a particular jailbreak can significantly enhance robustness against this attack
pattern. Building on this insight, we further leverage the retrieval-augmented
generation (RAG) techniques and propose Safety Context Retrieval (SCR), a
scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our
comprehensive experiments demonstrate how SCR achieves superior defensive
performance against both established and emerging jailbreaking tactics,
contributing a new paradigm to LLM safety. Our code will be available upon
publication.",2025-05-21,"Taiye Chen, Zeming Wei, Ang Li, Yisen Wang",http://arxiv.org/pdf/2505.15753v1,cs.LG
Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs,"We propose a novel framework for integrating fragmented multi-modal data in
Alzheimer's disease (AD) research using large language models (LLMs) and
knowledge graphs. While traditional multimodal analysis requires matched
patient IDs across datasets, our approach demonstrates population-level
integration of MRI, gene expression, biomarkers, EEG, and clinical indicators
from independent cohorts. Statistical analysis identified significant features
in each modality, which were connected as nodes in a knowledge graph. LLMs then
analyzed the graph to extract potential correlations and generate hypotheses in
natural language. This approach revealed several novel relationships, including
a potential pathway linking metabolic risk factors to tau protein abnormalities
via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between
frontal EEG channels and specific gene expression profiles (r=0.42-0.58,
p<0.01). Cross-validation with independent datasets confirmed the robustness of
major findings, with consistent effect sizes across cohorts (variance <15%).
The reproducibility of these findings was further supported by expert review
(Cohen's k=0.82) and computational validation. Our framework enables cross
modal integration at a conceptual level without requiring patient ID matching,
offering new possibilities for understanding AD pathology through fragmented
data reuse and generating testable hypotheses for future research.",2025-05-21,"Kanan Kiguchi, Yunhao Tu, Katsuhiro Ajito, Fady Alnajjar, Kazuyuki Murase",http://arxiv.org/pdf/2505.15747v2,cs.LG
Higher-order Structure Boosts Link Prediction on Temporal Graphs,"Temporal Graph Neural Networks (TGNNs) have gained growing attention for
modeling and predicting structures in temporal graphs. However, existing TGNNs
primarily focus on pairwise interactions while overlooking higher-order
structures that are integral to link formation and evolution in real-world
temporal graphs. Meanwhile, these models often suffer from efficiency
bottlenecks, further limiting their expressive power. To tackle these
challenges, we propose a Higher-order structure Temporal Graph Neural Network,
which incorporates hypergraph representations into temporal graph learning. In
particular, we develop an algorithm to identify the underlying higher-order
structures, enhancing the model's ability to capture the group interactions.
Furthermore, by aggregating multiple edge features into hyperedge
representations, HTGN effectively reduces memory cost during training. We
theoretically demonstrate the enhanced expressiveness of our approach and
validate its effectiveness and efficiency through extensive experiments on
various real-world temporal graphs. Experimental results show that HTGN
achieves superior performance on dynamic link prediction while reducing memory
costs by up to 50\% compared to existing methods.",2025-05-21,"Jingzhe Liu, Zhigang Hua, Yan Xie, Bingheng Li, Harry Shomer, Yu Song, Kaveh Hassani, Jiliang Tang",http://arxiv.org/pdf/2505.15746v1,cs.LG
Neuro-Argumentative Learning with Case-Based Reasoning,"We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual
AA-CBR), a data-driven, neurosymbolic classification model in which the outcome
is determined by an argumentation debate structure that is learned
simultaneously with neural-based feature extractors. Each argument in the
debate is an observed case from the training data, favouring their labelling.
Cases attack or support those with opposing or agreeing labellings, with the
strength of each argument and relationship learned through gradient-based
methods. This argumentation debate structure provides human-aligned reasoning,
improving model interpretability compared to traditional neural networks (NNs).
Unlike the existing purely symbolic variant, Abstract Argumentation for
Case-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class
classification, automatic learning of feature and data point importance,
assigning uncertainty values to outcomes, using all available data points, and
does not require binary features. We show that Gradual AA-CBR performs
comparably to NNs whilst significantly outperforming existing AA-CBR
formulations.",2025-05-21,"Adam Gould, Francesca Toni",http://arxiv.org/pdf/2505.15742v1,cs.LG
Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses,"Large language models (LLMs) are rapidly deployed in real-world applications
ranging from chatbots to agentic systems. Alignment is one of the main
approaches used to defend against attacks such as prompt injection and
jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even
against Greedy Coordinate Gradient (GCG), a white-box attack that generates
adversarial suffixes to induce attacker-desired outputs. However, this search
space over discrete tokens is extremely large, making the task of finding
successful attacks difficult. GCG has, for instance, been shown to converge to
local minima, making it sensitive to initialization choices. In this paper, we
assess the future-proof robustness of these defenses using a more informed
threat model: attackers who have access to some information about the alignment
process. Specifically, we propose an informed white-box attack leveraging the
intermediate model checkpoints to initialize GCG, with each checkpoint acting
as a stepping stone for the next one. We show this approach to be highly
effective across state-of-the-art (SOTA) defenses and models. We further show
our informed initialization to outperform other initialization methods and show
a gradient-informed checkpoint selection strategy to greatly improve attack
performance and efficiency. Importantly, we also show our method to
successfully find universal adversarial suffixes -- single suffixes effective
across diverse inputs. Our results show that, contrary to previous beliefs,
effective adversarial suffixes do exist against SOTA alignment-based defenses,
that these can be found by existing attack methods when adversaries exploit
alignment knowledge, and that even universal suffixes exist. Taken together,
our results highlight the brittleness of current alignment-based methods and
the need to consider stronger threat models when testing the safety of LLMs.",2025-05-21,"Xiaoxue Yang, Bozhidar Stevanoski, Matthieu Meeus, Yves-Alexandre de Montjoye",http://arxiv.org/pdf/2505.15738v1,cs.LG
"DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning","Large language models (LLMs) have improved significantly in their reasoning
through extensive training on massive datasets. However, relying solely on
additional data for improvement is becoming increasingly impractical,
highlighting the need for models to autonomously enhance their reasoning
without external supervision. In this paper, we propose Debate, Train, Evolve
(DTE), a novel ground truth-free training framework that uses multi-agent
debate traces to evolve a single language model. We also introduce a new
prompting strategy Reflect-Critique-Refine, to improve debate quality by
explicitly instructing agents to critique and refine their reasoning. Extensive
evaluations on five reasoning benchmarks with six open-weight models show that
our DTE framework achieve substantial improvements, with an average accuracy
gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe
strong cross-domain generalization, with an average accuracy gain of 5.8% on
all other benchmarks, suggesting that our method captures general reasoning
capabilities.",2025-05-21,"Gaurav Srivastava, Zhenyu Bi, Meng Lu, Xuan Wang",http://arxiv.org/pdf/2505.15734v1,cs.LG
Decouple and Orthogonalize: A Data-Free Framework for LoRA Merging,"With more open-source models available for diverse tasks, model merging has
gained attention by combining models into one, reducing training, storage, and
inference costs. Current research mainly focuses on model merging for full
fine-tuning, overlooking the popular LoRA. However, our empirical analysis
reveals that: a) existing merging methods designed for full fine-tuning perform
poorly on LoRA; b) LoRA modules show much larger parameter magnitude variance
than full fine-tuned weights; c) greater parameter magnitude variance
correlates with worse merging performance. Considering that large magnitude
variances cause deviations in the distribution of the merged parameters,
resulting in information loss and performance degradation, we propose a
Decoupled and Orthogonal merging approach(DO-Merging). By separating parameters
into magnitude and direction components and merging them independently, we
reduce the impact of magnitude differences on the directional alignment of the
merged models, thereby preserving task information. Furthermore, we introduce a
data-free, layer-wise gradient descent method with orthogonal constraints to
mitigate interference during the merging of direction components. We provide
theoretical guarantees for both the decoupling and orthogonal components. And
we validate through extensive experiments across vision, language, and
multi-modal domains that our proposed DO-Merging can achieve significantly
higher performance than existing merging methods at a minimal cost. Notably,
each component can be flexibly integrated with existing methods, offering near
free-lunch improvements across tasks.",2025-05-21,"Shenghe Zheng, Hongzhi Wang, Chenyu Huang, Xiaohui Wang, Tao Chen, Jiayuan Fan, Shuyue Hu, Peng Ye",http://arxiv.org/pdf/2505.15875v1,cs.LG
Are machine learning interpretations reliable? A stability study on global interpretations,"As machine learning systems are increasingly used in high-stakes domains,
there is a growing emphasis placed on making them interpretable to improve
trust in these systems. In response, a range of interpretable machine learning
(IML) methods have been developed to generate human-understandable insights
into otherwise black box models. With these methods, a fundamental question
arises: Are these interpretations reliable? Unlike with prediction accuracy or
other evaluation metrics for supervised models, the proximity to the true
interpretation is difficult to define. Instead, we ask a closely related
question that we argue is a prerequisite for reliability: Are these
interpretations stable? We define stability as findings that are consistent or
reliable under small random perturbations to the data or algorithms. In this
study, we conduct the first systematic, large-scale empirical stability study
on popular machine learning global interpretations for both supervised and
unsupervised tasks on tabular data. Our findings reveal that popular
interpretation methods are frequently unstable, notably less stable than the
predictions themselves, and that there is no association between the accuracy
of machine learning predictions and the stability of their associated
interpretations. Moreover, we show that no single method consistently provides
the most stable interpretations across a range of benchmark datasets. Overall,
these results suggest that interpretability alone does not warrant trust, and
underscores the need for rigorous evaluation of interpretation stability in
future work. To support these principles, we have developed and released an
open source IML dashboard and Python package to enable researchers to assess
the stability and reliability of their own data-driven interpretations and
discoveries.",2025-05-21,"Luqin Gan, Tarek M. Zikry, Genevera I. Allen",http://arxiv.org/pdf/2505.15728v1,cs.LG
Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models,"Recent research leverages large language models (LLMs) for early mental
health detection, such as depression, often optimized with machine-generated
data. However, their detection may be subject to unknown weaknesses. Meanwhile,
quality control has not been applied to these generated corpora besides limited
human verifications. Our goal is to systematically evaluate LLM reasoning and
reveal potential weaknesses. To this end, we first provide a systematic
evaluation of the reasoning over machine-generated detection and
interpretation. Then we use the models' reasoning abilities to explore
mitigation strategies for enhanced performance. Specifically, we do the
following: A. Design an LLM instruction strategy that allows for systematic
analysis of the detection by breaking down the task into several subtasks. B.
Design contrastive few-shot and chain-of-thought prompts by selecting typical
positive and negative examples of detection reasoning. C. Perform human
annotation for the subtasks identified in the first step and evaluate the
performance. D. Identify human-preferred detection with desired logical
reasoning from the few-shot generation and use them to explore different
optimization strategies. We conducted extensive comparisons on the DepTweet
dataset across the following subtasks: 1. identifying whether the speaker is
describing their own depression; 2. accurately detecting the presence of PHQ-9
symptoms, and 3. finally, detecting depression. Human verification of
statistical outliers shows that LLMs demonstrate greater accuracy in analyzing
and detecting explicit language of depression as opposed to implicit
expressions of depression. Two optimization methods are used for performance
enhancement and reduction of the statistic bias: supervised fine-tuning (SFT)
and direct preference optimization (DPO). Notably, the DPO approach achieves
significant performance improvement.",2025-05-21,"Zongru Shao, Xin Wang, Zhanyang Liu, Chenhan Wang, K. P. Subbalakshmi",http://arxiv.org/pdf/2505.17119v1,cs.LG
Privacy-Preserving Conformal Prediction Under Local Differential Privacy,"Conformal prediction (CP) provides sets of candidate classes with a
guaranteed probability of containing the true class. However, it typically
relies on a calibration set with clean labels. We address privacy-sensitive
scenarios where the aggregator is untrusted and can only access a perturbed
version of the true labels. We propose two complementary approaches under local
differential privacy (LDP). In the first approach, users do not access the
model but instead provide their input features and a perturbed label using a
k-ary randomized response. In the second approach, which enforces stricter
privacy constraints, users add noise to their conformity score by binary search
response. This method requires access to the classification model but preserves
both data and label privacy. Both approaches compute the conformal threshold
directly from noisy data without accessing the true labels. We prove
finite-sample coverage guarantees and demonstrate robust coverage even under
severe randomization. This approach unifies strong local privacy with
predictive uncertainty control, making it well-suited for sensitive
applications such as medical imaging or large language model queries,
regardless of whether users can (or are willing to) compute their own scores.",2025-05-21,"Coby Penso, Bar Mahpud, Jacob Goldberger, Or Sheffet",http://arxiv.org/pdf/2505.15721v1,cs.LG
Advancing LLM Safe Alignment with Safety Representation Ranking,"The rapid advancement of large language models (LLMs) has demonstrated
milestone success in a variety of tasks, yet their potential for generating
harmful content has raised significant safety concerns. Existing safety
evaluation approaches typically operate directly on textual responses,
overlooking the rich information embedded in the model's internal
representations. In this paper, we propose Safety Representation Ranking (SRR),
a listwise ranking framework that selects safe responses using hidden states
from the LLM itself. SRR encodes both instructions and candidate completions
using intermediate transformer representations and ranks candidates via a
lightweight similarity-based scorer. Our approach directly leverages internal
model states and supervision at the list level to capture subtle safety
signals. Experiments across multiple benchmarks show that SRR significantly
improves robustness to adversarial prompts. Our code will be available upon
publication.",2025-05-21,"Tianqi Du, Zeming Wei, Quan Chen, Chenheng Zhang, Yisen Wang",http://arxiv.org/pdf/2505.15710v1,cs.LG
HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases,"Large Language Models (LLMs) have demonstrated their potential in hardware
design tasks, such as Hardware Description Language (HDL) generation and
debugging. Yet, their performance in real-world, repository-level HDL projects
with thousands or even tens of thousands of code lines is hindered. To this
end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval
Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph
representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow
Graphs (DFGs) to capture both code graph view and hardware graph view.
HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the
limited recall issues inherent in similarity-based semantic retrieval by
incorporating structural information, but also enhances its extensibility to
various real-world tasks by a task-specific retrieval finetuning. Additionally,
to address the lack of comprehensive HDL search benchmarks, we introduce
HDLSearch, a multi-granularity evaluation dataset derived from real-world
repository-level projects. Experimental results demonstrate that HDLxGraph
significantly improves average search accuracy, debugging efficiency and
completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based
RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are
available at https://github.com/Nick-Zheng-Q/HDLxGraph.",2025-05-21,"Pingqing Zheng, Jiayin Qin, Fuqi Zhang, Shang Wu, Yu Cao, Caiwen Ding, Yang, Zhao",http://arxiv.org/pdf/2505.15701v1,cs.LG
MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation,"The [CLS] token in BERT is commonly used as a fixed-length representation for
classification tasks, yet prior work has shown that both other tokens and
intermediate layers encode valuable contextual information. In this work, we
propose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS]
representation by aggregating information across layers and tokens.
Specifically, we explore three modifications: (i) max-pooling the [CLS] token
across multiple layers, (ii) enabling the [CLS] token to attend over the entire
final layer using an additional multi-head attention (MHA) layer, and (iii)
combining max-pooling across the full sequence with MHA. Our approach enhances
BERT's classification accuracy (especially on low-resource tasks) without
requiring pre-training or significantly increasing model size. Experiments on
the GLUE benchmark show that MaxPoolBERT consistently achieves a better
performance on the standard BERT-base model.",2025-05-21,"Maike Behrendt, Stefan Sylvius Wagner, Stefan Harmeling",http://arxiv.org/pdf/2505.15696v1,cs.LG
A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO,"In this paper, we theoretically investigate the effects of noisy labels in
offline alignment, with a focus on the interplay between privacy and robustness
against adversarial corruption. Specifically, under linear modeling
assumptions, we present a unified analysis covering both reinforcement learning
from human feedback (RLHF) and direct preference optimization (DPO) under
different privacy-corruption scenarios, such as Local differential
privacy-then-Corruption (LTC), where human preference labels are privatized
before being corrupted by an adversary, and Corruption-then-Local differential
privacy (CTL), where labels are corrupted before privacy protection. Our
analysis leverages a reduction framework that reduces the offline alignment
problem under linear modeling assumptions to parameter estimation in logistic
regression. This framework allows us to establish an interesting separation
result between LTC and CTL, demonstrating that LTC presents a greater challenge
than CTL in offline alignment, even under linear models. As important
by-products, our findings also advance the state-of-the-art theoretical results
in offline alignment under privacy-only or corruption-only scenarios.",2025-05-21,"Xingyu Zhou, Yulian Wu, Francesco Orabona",http://arxiv.org/pdf/2505.15694v1,cs.LG
Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities,"Reinforcement learning (RL) has emerged as an effective method for training
reasoning models. However, existing RL approaches typically bias the model's
output distribution toward reward-maximizing paths without introducing external
knowledge. This limits their exploration capacity and results in a narrower
reasoning capability boundary compared to base models. To address this
limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel
framework that augments RL by incorporating external high-level guidance
(""thought patterns""). By adaptively integrating structured thoughts during
training, TAPO effectively balances model-internal exploration and external
guidance exploitation. Extensive experiments show that our approach
significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva
Math. Notably, these high-level thought patterns, abstracted from only 500
prior samples, generalize effectively across various tasks and models. This
highlights TAPO's potential for broader applications across multiple tasks and
domains. Our further analysis reveals that introducing external guidance
produces powerful reasoning models with superior explainability of inference
behavior and enhanced output readability.",2025-05-21,"Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Pengpeng Shao, Huazhe Xu, Jianhua Tao",http://arxiv.org/pdf/2505.15692v2,cs.LG
A packing lemma for VCN${}_k$-dimension and learning high-dimensional data,"Recently, the authors introduced the theory of high-arity PAC learning, which
is well-suited for learning graphs, hypergraphs and relational structures. In
the same initial work, the authors proved a high-arity analogue of the
Fundamental Theorem of Statistical Learning that almost completely
characterizes all notions of high-arity PAC learning in terms of a
combinatorial dimension, called the Vapnik--Chervonenkis--Natarajan (VCN${}_k$)
$k$-dimension, leaving as an open problem only the characterization of
non-partite, non-agnostic high-arity PAC learnability.
  In this work, we complete this characterization by proving that non-partite
non-agnostic high-arity PAC learnability implies a high-arity version of the
Haussler packing property, which in turn implies finiteness of
VCN${}_k$-dimension. This is done by obtaining direct proofs that classic PAC
learnability implies classic Haussler packing property, which in turn implies
finite Natarajan dimension and noticing that these direct proofs nicely lift to
high-arity.",2025-05-21,"Leonardo N. Coregliano, Maryanthe Malliaris",http://arxiv.org/pdf/2505.15688v1,cs.LG
Graph Conditional Flow Matching for Relational Data Generation,"Data synthesis is gaining momentum as a privacy-enhancing technology. While
single-table tabular data generation has seen considerable progress, current
methods for multi-table data often lack the flexibility and expressiveness
needed to capture complex relational structures. In particular, they struggle
with long-range dependencies and complex foreign-key relationships, such as
tables with multiple parent tables or multiple types of links between the same
pair of tables. We propose a generative model for relational data that
generates the content of a relational dataset given the graph formed by the
foreign-key relationships. We do this by learning a deep generative model of
the content of the whole relational database by flow matching, where the neural
network trained to denoise records leverages a graph neural network to obtain
information from connected records. Our method is flexible, as it can support
relational datasets with complex structures, and expressive, as the generation
of each record can be influenced by any other record within the same connected
component. We evaluate our method on several benchmark datasets and show that
it achieves state-of-the-art performance in terms of synthetic data fidelity.",2025-05-21,"Davide Scassola, Sebastiano Saccani, Luca Bortolussi",http://arxiv.org/pdf/2505.15668v1,cs.LG
Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms,"Gradient-based learning imposes (deep) neural networks to be differentiable
at all steps. This includes model-based architectures constructed by unrolling
iterations of an iterative algorithm onto layers of a neural network, known as
algorithm unrolling. However, greedy sparse recovery algorithms depend on the
non-differentiable argsort operator, which hinders their integration into
neural networks. In this paper, we address this challenge in Orthogonal
Matching Pursuit (OMP) and Iterative Hard Thresholding (IHT), two popular
representative algorithms in this class. We propose permutation-based variants
of these algorithms and approximate permutation matrices using ""soft""
permutation matrices derived from softsort, a continuous relaxation of argsort.
We demonstrate -- both theoretically and numerically -- that Soft-OMP and
Soft-IHT, as differentiable counterparts of OMP and IHT and fully compatible
with neural network training, effectively approximate these algorithms with a
controllable degree of accuracy. This leads to the development of OMP- and
IHT-Net, fully trainable network architectures based on Soft-OMP and Soft-IHT,
respectively. Finally, by choosing weights as ""structure-aware"" trainable
parameters, we connect our approach to structured sparse recovery and
demonstrate its ability to extract latent sparsity patterns from data.",2025-05-21,"Sina Mohammad-Taheri, Matthew J. Colbrook, Simone Brugiapaglia",http://arxiv.org/pdf/2505.15661v1,cs.LG
FLARE: Robot Learning with Implicit World Modeling,"We introduce $\textbf{F}$uture $\textbf{LA}$tent $\textbf{RE}$presentation
Alignment ($\textbf{FLARE}$), a novel framework that integrates predictive
latent world modeling into robot policy learning. By aligning features from a
diffusion transformer with latent embeddings of future observations,
$\textbf{FLARE}$ enables a diffusion transformer policy to anticipate latent
representations of future observations, allowing it to reason about long-term
consequences while generating actions. Remarkably lightweight, $\textbf{FLARE}$
requires only minimal architectural modifications -- adding a few tokens to
standard vision-language-action (VLA) models -- yet delivers substantial
performance gains. Across two challenging multitask simulation imitation
learning benchmarks spanning single-arm and humanoid tabletop manipulation,
$\textbf{FLARE}$ achieves state-of-the-art performance, outperforming prior
policy learning baselines by up to 26%. Moreover, $\textbf{FLARE}$ unlocks the
ability to co-train with human egocentric video demonstrations without action
labels, significantly boosting policy generalization to a novel object with
unseen geometry with as few as a single robot demonstration. Our results
establish $\textbf{FLARE}$ as a general and scalable approach for combining
implicit world modeling with high-frequency robotic control.",2025-05-21,"Ruijie Zheng, Jing Wang, Scott Reed, Johan Bjorck, Yu Fang, Fengyuan Hu, Joel Jang, Kaushil Kundalia, Zongyu Lin, Loic Magne, Avnish Narayan, You Liang Tan, Guanzhi Wang, Qi Wang, Jiannan Xiang, Yinzhen Xu, Seonghyeon Ye, Jan Kautz, Furong Huang, Yuke Zhu, Linxi Fan",http://arxiv.org/pdf/2505.15659v1,cs.LG
LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought,"Sample-wise learning curves plot performance versus training set size. They
are useful for studying scaling laws and speeding up hyperparameter tuning and
model selection. Learning curves are often assumed to be well-behaved: monotone
(i.e. improving with more data) and convex. By constructing the Learning Curves
Database 1.1 (LCDB 1.1), a large-scale database with high-resolution learning
curves, we show that learning curves are less often well-behaved than
previously thought. Using statistically rigorous methods, we observe
significant ill-behavior in approximately 14% of the learning curves, almost
twice as much as in previous estimates. We also identify which learners are to
blame and show that specific learners are more ill-behaved than others.
Additionally, we demonstrate that different feature scalings rarely resolve
ill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such
as learning curve fitting and model selection, and find it poses significant
challenges, underscoring the relevance and potential of LCDB 1.1 as a
challenging benchmark for future research.",2025-05-21,"Cheng Yan, Felix Mohr, Tom Viering",http://arxiv.org/pdf/2505.15657v1,cs.LG
Learning Small Decision Trees with Few Outliers: A Parameterized Perspective,"Decision trees are a fundamental tool in machine learning for representing,
classifying, and generalizing data. It is desirable to construct ``small''
decision trees, by minimizing either the \textit{size} ($s$) or the
\textit{depth} $(d)$ of the \textit{decision tree} (\textsc{DT}). Recently, the
parameterized complexity of \textsc{Decision Tree Learning} has attracted a lot
of attention. We consider a generalization of \textsc{Decision Tree Learning}
where given a \textit{classification instance} $E$ and an integer $t$, the task
is to find a ``small'' \textsc{DT} that disagrees with $E$ in at most $t$
examples. We consider two problems: \textsc{DTSO} and \textsc{DTDO}, where the
goal is to construct a \textsc{DT} minimizing $s$ and $d$, respectively. We
first establish that both \textsc{DTSO} and \textsc{DTDO} are W[1]-hard when
parameterized by $s+\delta_{max}$ and $d+\delta_{max}$, respectively, where
$\delta_{max}$ is the maximum number of features in which two differently
labeled examples can differ. We complement this result by showing that these
problems become \textsc{FPT} if we include the parameter $t$. We also consider
the kernelization complexity of these problems and establish several positive
and negative results for both \textsc{DTSO} and \textsc{DTDO}.",2025-05-21,"Harmender Gahlawat, Meirav Zehavi",http://arxiv.org/pdf/2505.15648v1,cs.LG
Second-Order Convergence in Private Stochastic Non-Convex Optimization,"We investigate the problem of finding second-order stationary points (SOSP)
in differentially private (DP) stochastic non-convex optimization. Existing
methods suffer from two key limitations: (i) inaccurate convergence error rate
due to overlooking gradient variance in the saddle point escape analysis, and
(ii) dependence on auxiliary private model selection procedures for identifying
DP-SOSP, which can significantly impair utility, particularly in distributed
settings. To address these issues, we propose a generic perturbed stochastic
gradient descent (PSGD) framework built upon Gaussian noise injection and
general gradient oracles. A core innovation of our framework is using model
drift distance to determine whether PSGD escapes saddle points, ensuring
convergence to approximate local minima without relying on second-order
information or additional DP-SOSP identification. By leveraging the adaptive
DP-SPIDER estimator as a specific gradient oracle, we develop a new DP
algorithm that rectifies the convergence error rates reported in prior work. We
further extend this algorithm to distributed learning with arbitrarily
heterogeneous data, providing the first formal guarantees for finding DP-SOSP
in such settings. Our analysis also highlights the detrimental impacts of
private selection procedures in distributed learning under high-dimensional
models, underscoring the practical benefits of our design. Numerical
experiments on real-world datasets validate the efficacy of our approach.",2025-05-21,"Youming Tao, Zuyuan Zhang, Dongxiao Yu, Xiuzhen Cheng, Falko Dressler, Di Wang",http://arxiv.org/pdf/2505.15647v1,cs.LG
Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima,"We study the problem of best-arm identification in stochastic multi-armed
bandits under the fixed-confidence setting, with a particular focus on
instances that admit multiple optimal arms. While the Track-and-Stop algorithm
of Garivier and Kaufmann (2016) is widely conjectured to be instance-optimal,
its performance in the presence of multiple optima has remained insufficiently
understood. In this work, we revisit the Track-and-Stop strategy and propose a
modified stopping rule that ensures instance-optimality even when the set of
optimal arms is not a singleton. Our analysis introduces a new
information-theoretic lower bound that explicitly accounts for multiple optimal
arms, and we demonstrate that our stopping rule tightly matches this bound.",2025-05-21,Lan V. Truong,http://arxiv.org/pdf/2505.15643v1,cs.LG
A Simple Approximation Algorithm for Optimal Decision Tree,"Optimal decision tree (\odt) is a fundamental problem arising in applications
such as active learning, entity identification, and medical diagnosis. An
instance of \odt is given by $m$ hypotheses, out of which an unknown ``true''
hypothesis is drawn according to some probability distribution. An algorithm
needs to identify the true hypothesis by making queries: each query incurs a
cost and has a known response for each hypothesis. The goal is to minimize the
expected query cost to identify the true hypothesis. We consider the most
general setting with arbitrary costs, probabilities and responses. \odt is
NP-hard to approximate better than $\ln m$ and there are $O(\ln m)$
approximation algorithms known for it. However, these algorithms and/or their
analyses are quite complex. Moreover, the leading constant factors are large.
We provide a simple algorithm and analysis for \odt, proving an approximation
ratio of $8 \ln m$.",2025-05-21,"Zhengjia Zhuo, Viswanath Nagarajan",http://arxiv.org/pdf/2505.15641v1,cs.LG
Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes,"We revisit the classical problem of Bayesian ensembles and address the
challenge of learning optimal combinations of Bayesian models in an online,
continual learning setting. To this end, we reinterpret existing approaches
such as Bayesian model averaging (BMA) and Bayesian stacking through a novel
empirical Bayes lens, shedding new light on the limitations and pathologies of
BMA. Further motivated by insights from online optimization, we propose Online
Bayesian Stacking (OBS), a method that optimizes the log-score over predictive
distributions to adaptively combine Bayesian models. A key contribution of our
work is establishing a novel connection between OBS and portfolio selection,
bridging Bayesian ensemble learning with a rich, well-studied theoretical
framework that offers efficient algorithms and extensive regret analysis. We
further clarify the relationship between OBS and online BMA, showing that they
optimize related but distinct cost functions. Through theoretical analysis and
empirical evaluation, we identify scenarios where OBS outperforms online BMA
and provide principled guidance on when practitioners should prefer one
approach over the other.",2025-05-21,"Daniel Waxman, Fernando Llorente, Petar M. Djurić",http://arxiv.org/pdf/2505.15638v1,cs.LG
Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search,"Nearest neighbor search is central in machine learning, information
retrieval, and databases. For high-dimensional datasets, graph-based methods
such as HNSW, DiskANN, and NSG have become popular thanks to their empirical
accuracy and efficiency. These methods construct a directed graph over the
dataset and perform beam search on the graph to find nodes close to a given
query. While significant work has focused on practical refinements and
theoretical understanding of graph-based methods, many questions remain. We
propose a new distance-based termination condition for beam search to replace
the commonly used condition based on beam width. We prove that, as long as the
search graph is navigable, our resulting Adaptive Beam Search method is
guaranteed to approximately solve the nearest-neighbor problem, establishing a
connection between navigability and the performance of graph-based search. We
also provide extensive experiments on our new termination condition for both
navigable graphs and approximately navigable graphs used in practice, such as
HNSW and Vamana graphs. We find that Adaptive Beam Search outperforms standard
beam search over a range of recall values, data sets, graph constructions, and
target number of nearest neighbors. It thus provides a simple and practical way
to improve the performance of popular methods.",2025-05-21,"Yousef Al-Jazzazi, Haya Diwan, Jinrui Gou, Cameron Musco, Christopher Musco, Torsten Suel",http://arxiv.org/pdf/2505.15636v1,cs.LG
Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models,"Large Language Models (LLMs) demonstrate the ability to solve reasoning and
mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT
length, as seen in models such as DeepSeek-R1, significantly enhances this
reasoning for complex problems, but requires costly and high-quality long CoT
data and fine-tuning. This work, inspired by the deep thinking paradigm of
DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of
an LLM without external datasets. Our method first employs Sparse Autoencoders
(SAEs) to extract interpretable features from vanilla CoT. These features are
then used to steer the LLM's internal states during generation. Recognizing
that many LLMs do not have corresponding pre-trained SAEs, we further introduce
a novel SAE-free steering algorithm, which directly computes steering
directions from the residual activations of an LLM, obviating the need for an
explicit SAE. Experimental results demonstrate that both our SAE-based and
subsequent SAE-free steering algorithms significantly enhance the reasoning
capabilities of LLMs.",2025-05-21,"Zihao Li, Xu Wang, Yuzhe Yang, Ziyu Yao, Haoyi Xiong, Mengnan Du",http://arxiv.org/pdf/2505.15634v2,cs.LG
Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions,"Large language models that use retrieval augmented generation have the
potential to unlock valuable knowledge for researchers, policymakers, and the
public by making long and technical climate-related documents more accessible.
While this approach can help alleviate factual hallucinations by relying on
retrieved passages as additional context, its effectiveness depends on whether
the model's output remains faithful to these passages. To address this, we
explore the automatic assessment of faithfulness of different models in this
setting. We then focus on ClimateGPT, a large language model specialised in
climate science, to examine which factors in its instruction fine-tuning impact
the model's faithfulness. By excluding unfaithful subsets of the model's
training data, we develop ClimateGPT Faithful+, which achieves an improvement
in faithfulness from 30% to 57% in supported atomic claims according to our
automatic metric.",2025-05-21,"David Thulke, Jakob Kemmler, Christian Dugast, Hermann Ney",http://arxiv.org/pdf/2505.15633v1,cs.LG
Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks,"Neural Architecture Search (NAS) accelerates progress in deep learning
through systematic refinement of model architectures. The downside is
increasingly large energy consumption during the search process.
Surrogate-based benchmarking mitigates the cost of full training by querying a
pre-trained surrogate to obtain an estimate for the quality of the model.
Specifically, energy-aware benchmarking aims to make it possible for NAS to
favourably trade off model energy consumption against accuracy. Towards this
end, we propose three design principles for such energy-aware benchmarks: (i)
reliable power measurements, (ii) a wide range of GPU usage, and (iii) holistic
cost reporting. We analyse EA-HAS-Bench based on these principles and find that
the choice of GPU measurement API has a large impact on the quality of results.
Using the Nvidia System Management Interface (SMI) on top of its underlying
library influences the sampling rate during the initial data collection,
returning faulty low-power estimations. This results in poor correlation with
accurate measurements obtained from an external power meter. With this study,
we bring to attention several key considerations when performing energy-aware
surrogate-based benchmarking and derive first guidelines that can help design
novel benchmarks. We show a narrow usage range of the four GPUs attached to our
device, ranging from 146 W to 305 W in a single-GPU setting, and narrowing down
even further when using all four GPUs. To improve holistic energy reporting, we
propose calibration experiments over assumptions made in popular tools, such as
Code Carbon, thus achieving reductions in the maximum inaccuracy from 10.3 % to
8.9 % without and to 6.6 % with prior estimation of the expected load on the
device.",2025-05-21,"Nick Kocher, Christian Wassermann, Leona Hennig, Jonas Seng, Holger Hoos, Kristian Kersting, Marius Lindauer, Matthias Müller",http://arxiv.org/pdf/2505.15631v1,cs.LG
Aligning Explanations with Human Communication,"Machine learning explainability aims to make the decision-making process of
black-box models more transparent by finding the most important input features
for a given prediction task. Recent works have proposed composing explanations
from semantic concepts (e.g., colors, patterns, shapes) that are inherently
interpretable to the user of a model. However, these methods generally ignore
the communicative context of explanation-the ability of the user to understand
the prediction of the model from the explanation. For example, while a medical
doctor might understand an explanation in terms of clinical markers, a patient
may need a more accessible explanation to make sense of the same diagnosis. In
this paper, we address this gap with listener-adaptive explanations. We propose
an iterative procedure grounded in principles of pragmatic reasoning and the
rational speech act to generate explanations that maximize communicative
utility. Our procedure only needs access to pairwise preferences between
candidate explanations, relevant in real-world scenarios where a listener model
may not be available. We evaluate our method in image classification tasks,
demonstrating improved alignment between explanations and listener preferences
across three datasets. Furthermore, we perform a user study that demonstrates
our explanations increase communicative utility.",2025-05-21,"Jacopo Teneggi, Zhenzhen Wang, Paul H. Yi, Tianmin Shu, Jeremias Sulam",http://arxiv.org/pdf/2505.15626v1,cs.LG
Mechanistic Insights into Grokking from the Embedding Layer,"Grokking, a delayed generalization in neural networks after perfect training
performance, has been observed in Transformers and MLPs, but the components
driving it remain underexplored. We show that embeddings are central to
grokking: introducing them into MLPs induces delayed generalization in modular
arithmetic tasks, whereas MLPs without embeddings can generalize immediately.
Our analysis identifies two key mechanisms: (1) Embedding update dynamics,
where rare tokens stagnate due to sparse gradient updates and weight decay, and
(2) Bilinear coupling, where the interaction between embeddings and downstream
weights introduces saddle points and increases sensitivity to initialization.
To confirm these mechanisms, we investigate frequency-aware sampling, which
balances token updates by minimizing gradient variance, and embedding-specific
learning rates, derived from the asymmetric curvature of the bilinear loss
landscape. We prove that an adaptive learning rate ratio,
\(\frac{\eta_E}{\eta_W} \propto \frac{\sigma_{\max}(E)}{\sigma_{\max}(W)} \cdot
\frac{f_W}{f_E}\), mitigates bilinear coupling effects, accelerating
convergence. Our methods not only improve grokking dynamics but also extend to
broader challenges in Transformer optimization, where bilinear interactions
hinder efficient training.",2025-05-21,"H. V. AlquBoj, Hilal AlQuabeh, Velibor Bojkovic, Munachiso Nwadike, Kentaro Inui",http://arxiv.org/pdf/2505.15624v1,cs.LG
Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning,"Large language models (LLMs) demonstrate considerable potential in various
natural language tasks but face significant challenges in mathematical
reasoning, particularly in executing precise, multi-step logic. However,
current evaluation frameworks judge their performance solely based on accuracy,
which only accounts for the final answer. This study explores these pitfalls by
employing a novel evaluation framework. We propose an evaluation metric called
the MAPLE score, which holistically quantifies reasoning misalignment by
integrating error rates, redundancy, and validity.",2025-05-21,"Tiasa Singha Roy, Aditeya Baral, Ayush Rajesh Jhaveri, Yusuf Baig",http://arxiv.org/pdf/2505.15623v1,cs.LG
Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI,"The rise of IoT has increased the need for on-edge machine learning, with
TinyML emerging as a promising solution for resource-constrained devices such
as MCU. However, evaluating their performance remains challenging due to
diverse architectures and application scenarios. Current solutions have many
non-negligible limitations. This work introduces an alternative benchmarking
methodology that integrates energy and latency measurements while
distinguishing three execution phases pre-inference, inference, and
post-inference. Additionally, the setup ensures that the device operates
without being powered by an external measurement unit, while automated testing
can be leveraged to enhance statistical significance. To evaluate our setup, we
tested the STM32N6 MCU, which includes a NPU for executing neural networks. Two
configurations were considered: high-performance and Low-power. The variation
of the EDP was analyzed separately for each phase, providing insights into the
impact of hardware configurations on energy efficiency. Each model was tested
1000 times to ensure statistically relevant results. Our findings demonstrate
that reducing the core voltage and clock frequency improve the efficiency of
pre- and post-processing without significantly affecting network execution
performance. This approach can also be used for cross-platform comparisons to
determine the most efficient inference platform and to quantify how pre- and
post-processing overhead varies across different hardware implementations.",2025-05-21,"Pietro Bartoli, Christian Veronesi, Andrea Giudici, David Siorpaes, Diana Trojaniello, Franco Zappa",http://arxiv.org/pdf/2505.15622v1,cs.LG
Learn to Reason Efficiently with Adaptive Length-based Reward Shaping,"Large Reasoning Models (LRMs) have shown remarkable capabilities in solving
complex problems through reinforcement learning (RL), particularly by
generating long reasoning traces. However, these extended outputs often exhibit
substantial redundancy, which limits the efficiency of LRMs. In this paper, we
investigate RL-based approaches to promote reasoning efficiency. Specifically,
we first present a unified framework that formulates various efficient
reasoning methods through the lens of length-based reward shaping. Building on
this perspective, we propose a novel Length-bAsed StEp Reward shaping method
(LASER), which employs a step function as the reward, controlled by a target
length. LASER surpasses previous methods, achieving a superior Pareto-optimal
balance between performance and efficiency. Next, we further extend LASER based
on two key intuitions: (1) The reasoning behavior of the model evolves during
training, necessitating reward specifications that are also adaptive and
dynamic; (2) Rather than uniformly encouraging shorter or longer chains of
thought (CoT), we posit that length-based reward shaping should be
difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.
This approach is expected to facilitate a combination of fast and slow
thinking, leading to a better overall tradeoff. The resulting method is termed
LASER-D (Dynamic and Difficulty-aware). Experiments on
DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and
DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both
reasoning performance and response length efficiency. For instance, LASER-D and
its variant achieve a +6.1 improvement on AIME2024 while reducing token usage
by 63%. Further analysis reveals our RL-based compression produces more concise
reasoning patterns with less redundant ""self-reflections"". Resources are at
https://github.com/hkust-nlp/Laser.",2025-05-21,"Wei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, Junxian He",http://arxiv.org/pdf/2505.15612v1,cs.LG
Deep Learning for Continuous-time Stochastic Control with Jumps,"In this paper, we introduce a model-based deep-learning approach to solve
finite-horizon continuous-time stochastic control problems with jumps. We
iteratively train two neural networks: one to represent the optimal policy and
the other to approximate the value function. Leveraging a continuous-time
version of the dynamic programming principle, we derive two different training
objectives based on the Hamilton-Jacobi-Bellman equation, ensuring that the
networks capture the underlying stochastic dynamics. Empirical evaluations on
different problems illustrate the accuracy and scalability of our approach,
demonstrating its effectiveness in solving complex, high-dimensional stochastic
control tasks.",2025-05-21,"Patrick Cheridito, Jean-Loup Dupret, Donatien Hainaut",http://arxiv.org/pdf/2505.15602v1,cs.LG
Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off,"While foundation models demonstrate impressive performance across various
tasks, they remain vulnerable to adversarial inputs. Current research explores
various approaches to enhance model robustness, with Diffusion Denoised
Smoothing emerging as a particularly promising technique. This method employs a
pretrained diffusion model to preprocess inputs before model inference. Yet,
its effectiveness remains largely unexplored beyond classification. We aim to
address this gap by analyzing three datasets with four distinct downstream
tasks under three different adversarial attack algorithms. Our findings reveal
that while foundation models maintain resilience against conventional
transformations, applying high-noise diffusion denoising to clean images
without any distortions significantly degrades performance by as high as 57%.
Low-noise diffusion settings preserve performance but fail to provide adequate
protection across all attack types. Moreover, we introduce a novel attack
strategy specifically targeting the diffusion process itself, capable of
circumventing defenses in the low-noise regime. Our results suggest that the
trade-off between adversarial robustness and performance remains a challenge to
be addressed.",2025-05-21,"Yury Belousov, Brian Pulfer, Vitaliy Kinakh, Slava Voloshynovskiy",http://arxiv.org/pdf/2505.15594v1,cs.LG
World Models as Reference Trajectories for Rapid Motor Adaptation,"Deploying learned control policies in real-world environments poses a
fundamental challenge. When system dynamics change unexpectedly, performance
degrades until models are retrained on new data. We introduce Reflexive World
Models (RWM), a dual control framework that uses world model predictions as
implicit reference trajectories for rapid adaptation. Our method separates the
control problem into long-term reward maximization through reinforcement
learning and robust motor execution through rapid latent control. This dual
architecture achieves significantly faster adaptation with low online
computational cost compared to model-based RL baselines, while maintaining
near-optimal performance. The approach combines the benefits of flexible policy
learning through reinforcement learning with rapid error correction
capabilities, providing a principled approach to maintaining performance in
high-dimensional continuous control tasks under varying dynamics.",2025-05-21,"Carlos Stein Brito, Daniel McNamee",http://arxiv.org/pdf/2505.15589v1,cs.LG
MIRB: Mathematical Information Retrieval Benchmark,"Mathematical Information Retrieval (MIR) is the task of retrieving
information from mathematical documents and plays a key role in various
applications, including theorem search in mathematical libraries, answer
retrieval on math forums, and premise selection in automated theorem proving.
However, a unified benchmark for evaluating these diverse retrieval tasks has
been lacking. In this paper, we introduce MIRB (Mathematical Information
Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB
includes four tasks: semantic statement retrieval, question-answer retrieval,
premise retrieval, and formula retrieval, spanning a total of 12 datasets. We
evaluate 13 retrieval models on this benchmark and analyze the challenges
inherent to MIR. We hope that MIRB provides a comprehensive framework for
evaluating MIR systems and helps advance the development of more effective
retrieval models tailored to the mathematical domain.",2025-05-21,"Haocheng Ju, Bin Dong",http://arxiv.org/pdf/2505.15585v1,cs.LG
"RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language","Multimodal question answering (QA) often requires identifying which video,
audio, or sensor tokens are relevant to the question. Yet modality
disagreements are common: off-camera speech, background noise, or motion
outside the field of view often mislead fusion models that weight all streams
equally. We present RAVEN, a unified QA architecture whose core is QuART, a
query-conditioned cross-modal gating module that assigns scalar relevance
scores to each token across modalities, enabling the model to amplify
informative signals and suppress distractors before fusion. RAVEN is trained
through a three-stage pipeline comprising unimodal pretraining, query-aligned
fusion, and disagreement-oriented fine-tuning -- each stage targeting a
distinct challenge in multi-modal reasoning: representation quality,
cross-modal relevance, and robustness to modality mismatch. To support training
and evaluation, we release AVS-QA, a dataset of 300K synchronized
Audio--Video-Sensor streams paired with automatically generated question-answer
pairs. Experimental results on seven multi-modal QA benchmarks -- including
egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and
8.0\% gains in accuracy compared to state-of-the-art multi-modal large language
models, respectively. Incorporating sensor data provides an additional 16.4\%
boost, and the model remains robust under modality corruption, outperforming
SOTA baselines by 50.23\%. Our code and dataset are available at
https://github.com/BASHLab/RAVEN.",2025-05-21,"Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam",http://arxiv.org/pdf/2505.17114v1,cs.LG
Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions,"Personalized federated learning has emerged as a popular approach to training
on devices holding statistically heterogeneous data, known as clients. However,
most existing approaches require a client to have labeled data for training or
finetuning in order to obtain their own personalized model. In this paper we
address this by proposing FLowDUP, a novel method that is able to generate a
personalized model using only a forward pass with unlabeled data. The generated
model parameters reside in a low-dimensional subspace, enabling efficient
communication and computation. FLowDUP's learning objective is theoretically
motivated by our new transductive multi-task PAC-Bayesian generalization bound,
that provides performance guarantees for unlabeled clients. The objective is
structured in such a way that it allows both clients with labeled data and
clients with only unlabeled data to contribute to the training process. To
supplement our theoretical results we carry out a thorough experimental
evaluation of FLowDUP, demonstrating strong empirical performance on a range of
datasets with differing sorts of statistically heterogeneous clients. Through
numerous ablation studies, we test the efficacy of the individual components of
the method.",2025-05-21,"Hossein Zakerinia, Jonathan Scott, Christoph H. Lampert",http://arxiv.org/pdf/2505.15579v1,cs.LG
Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models,"Vision-Language Models (VLMs) are essential for multimodal tasks, especially
compositional reasoning (CR) tasks, which require distinguishing fine-grained
semantic differences between visual and textual embeddings. However, existing
methods primarily fine-tune the model by generating text-based hard negative
samples, neglecting the importance of image-based negative samples, which
results in insufficient training of the visual encoder and ultimately impacts
the overall performance of the model. Moreover, negative samples are typically
treated uniformly, without considering their difficulty levels, and the
alignment of positive samples is insufficient, which leads to challenges in
aligning difficult sample pairs. To address these issues, we propose Adaptive
Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard
negatives into the visual domain to generate semantically disturbed image-based
negatives for training the model, thereby enhancing its overall performance.
AHNPL also introduces a contrastive learning approach using a multimodal hard
negative loss to improve the model's discrimination of hard negatives within
each modality and a dynamic margin loss that adjusts the contrastive margin
according to sample difficulty to enhance the distinction of challenging sample
pairs. Experiments on three public datasets demonstrate that our method
effectively boosts VLMs' performance on complex CR tasks. The source code is
available at https://github.com/nynu-BDAI/AHNPL.",2025-05-21,"Xin Huang, Ruibin Li, Tong Jia, Wei Zheng, Ya Wang",http://arxiv.org/pdf/2505.15576v1,cs.LG
Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback,"The data-to-equation (Data2Eqn) task aims to discover interpretable
mathematical equations that map observed values to labels, offering physical
insights and broad applicability across academic and industrial domains.
Genetic programming and traditional deep learning-based approaches suffer from
search inefficiency and poor generalization on small task-specific datasets.
Foundation models showed promise in this area, but existing approaches suffer
from: 1) They are pretrained on general-purpose data distributions, making them
less effective for domain-specific tasks; and 2) their training objectives
focus on token-level alignment, overlooking mathematical semantics, which can
lead to inaccurate equations. To address these issues, we aim to enhance the
domain adaptability of foundation models for Data2Eqn tasks. In this work, we
propose a reinforcement learning-based finetuning framework that directly
optimizes the generation policy of a pretrained model through reward signals
derived from downstream numerical fitness. Our method allows the model to adapt
to specific and complex data distributions and generate mathematically
meaningful equations. Extensive experiments demonstrate that our approach
improves both the accuracy and robustness of equation generation under complex
distributions.",2025-05-21,"Wangyang Ying, Haoyue Bai, Nanxu Gong, Xinyuan Wang, Sixun Dong, Haifeng Chen, Yanjie Fu",http://arxiv.org/pdf/2505.15572v1,cs.LG
Refining Neural Activation Patterns for Layer-Level Concept Discovery in Neural Network-Based Receivers,"Concept discovery in neural networks often targets individual neurons or
human-interpretable features, overlooking distributed layer-wide patterns. We
study the Neural Activation Pattern (NAP) methodology, which clusters
full-layer activation distributions to identify such layer-level concepts.
Applied to visual object recognition and radio receiver models, we propose
improved normalization, distribution estimation, distance metrics, and varied
cluster selection. In the radio receiver model, distinct concepts did not
emerge; instead, a continuous activation manifold shaped by Signal-to-Noise
Ratio (SNR) was observed -- highlighting SNR as a key learned factor,
consistent with classical receiver behavior and supporting physical
plausibility. Our enhancements to NAP improved in-distribution vs.
out-of-distribution separation, suggesting better generalization and indirectly
validating clustering quality. These results underscore the importance of
clustering design and activation manifolds in interpreting and troubleshooting
neural network behavior.",2025-05-21,"Marko Tuononen, Duy Vu, Dani Korpi, Vesa Starck, Ville Hautamäki",http://arxiv.org/pdf/2505.15570v1,cs.LG
Impact of Data Sparsity on Machine Learning for Fault Detection in Power System Protection,"Germany's transition to a renewable energy-based power system is reshaping
grid operations, requiring advanced monitoring and control to manage
decentralized generation. Machine learning (ML) has emerged as a powerful tool
for power system protection, particularly for fault detection (FD) and fault
line identification (FLI) in transmission grids. However, ML model reliability
depends on data quality and availability. Data sparsity resulting from sensor
failures, communication disruptions, or reduced sampling rates poses a
challenge to ML-based FD and FLI. Yet, its impact has not been systematically
validated prior to this work. In response, we propose a framework to assess the
impact of data sparsity on ML-based FD and FLI performance. We simulate
realistic data sparsity scenarios, evaluate their impact, derive quantitative
insights, and demonstrate the effectiveness of this evaluation strategy by
applying it to an existing ML-based framework. Results show the ML model
remains robust for FD, maintaining an F1-score of 0.999 $\pm$ 0.000 even after
a 50x data reduction. In contrast, FLI is more sensitive, with performance
decreasing by 55.61% for missing voltage measurements and 9.73% due to
communication failures at critical network points. These findings offer
actionable insights for optimizing ML models for real-world grid protection.
This enables more efficient FD and supports targeted improvements in FLI.",2025-05-21,"Julian Oelhaf, Georg Kordowich, Changhun Kim, Paula Andrea Perez-Toro, Andreas Maier, Johann Jager, Siming Bayer",http://arxiv.org/pdf/2505.15560v1,cs.LG
Robo-DM: Data Management For Large Robot Datasets,"Recent results suggest that very large datasets of teleoperated robot
demonstrations can be used to train transformer-based models that have the
potential to generalize to new scenes, robots, and tasks. However, curating,
distributing, and loading large datasets of robot trajectories, which typically
consist of video, textual, and numerical modalities - including streams from
multiple cameras - remains challenging. We propose Robo-DM, an efficient
open-source cloud-based data management toolkit for collecting, sharing, and
learning with robot data. With Robo-DM, robot datasets are stored in a
self-contained format with Extensible Binary Meta Language (EBML). Robo-DM can
significantly reduce the size of robot trajectory data, transfer costs, and
data load time during training. Compared to the RLDS format used in OXE
datasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x
(lossless). Robo-DM also accelerates data retrieval by load-balancing video
decoding with memory-mapped decoding caches. Compared to LeRobot, a framework
that also uses lossy video compression, Robo-DM is up to 50x faster when
decoding sequentially. We physically evaluate a model trained by Robo-DM with
lossy compression, a pick-and-place task, and In-Context Robot Transformer.
Robo-DM uses 75x compression of the original dataset and does not suffer
reduction in downstream task accuracy.",2025-05-21,"Kaiyuan Chen, Letian Fu, David Huang, Yanxiang Zhang, Lawrence Yunliang Chen, Huang Huang, Kush Hari, Ashwin Balakrishna, Ted Xiao, Pannag R Sanketi, John Kubiatowicz, Ken Goldberg",http://arxiv.org/pdf/2505.15558v1,cs.LG
Modular Jump Gaussian Processes,"Gaussian processes (GPs) furnish accurate nonlinear predictions with
well-calibrated uncertainty. However, the typical GP setup has a built-in
stationarity assumption, making it ill-suited for modeling data from processes
with sudden changes, or ""jumps"" in the output variable. The ""jump GP"" (JGP) was
developed for modeling data from such processes, combining local GPs and latent
""level"" variables under a joint inferential framework. But joint modeling can
be fraught with difficulty. We aim to simplify by suggesting a more modular
setup, eschewing joint inference but retaining the main JGP themes: (a)
learning optimal neighborhood sizes that locally respect manifolds of
discontinuity; and (b) a new cluster-based (latent) feature to capture regions
of distinct output levels on both sides of the manifold. We show that each of
(a) and (b) separately leads to dramatic improvements when modeling processes
with jumps. In tandem (but without requiring joint inference) that benefit is
compounded, as illustrated on real and synthetic benchmark examples from the
recent literature.",2025-05-21,"Anna R. Flowers, Christopher T. Franck, Mickaël Binois, Chiwoo Park, Robert B. Gramacy",http://arxiv.org/pdf/2505.15557v1,cs.LG
Short-Range Dependency Effects on Transformer Instability and a Decomposed Attention Solution,"Transformer language models have driven significant progress across various
fields, including natural language processing and computer vision. A central
component of these models is the self-attention (SA) mechanism, which learns
rich vector representations of tokens by modeling their relationships with
others in a sequence. However, despite extensive research, transformers
continue to suffer from training instability -- often manifesting as spikes or
divergence in the training loss during a run.
  In this work, we identify one source of this instability: SA's limited
ability to capture short-range dependencies, especially in tasks like language
modeling, where almost every token heavily relies on its nearby neighbors. This
limitation causes the pre-softmax logits of SA to grow rapidly, destabilizing
training. To address this, we propose decomposing the SA into local
(short-range) and global (long-range) attention heads. This decomposed
attention, referred to as Long Short-attention (LS-attention), mitigates logit
explosion and results in more stable training compared to an equivalent
multi-head self-attention (MHSA). Empirical comparisons with two alternative
training stabilization methods show that LS-attention reduces the validation
perplexity to nearly 2/5 of that achieved by one method and reaches a similar
perplexity as the other method using only 1/20 of the GPU hours. Additionally,
our experiments demonstrate that LS-attention reduces inference latency by up
to 36% compared to a state-of-the-art implementation of equivalent MHSA.",2025-05-21,Suvadeep Hajra,http://arxiv.org/pdf/2505.15548v1,cs.LG
"Oversmoothing, ""Oversquashing"", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning","After a renaissance phase in which researchers revisited the message-passing
paradigm through the lens of deep learning, the graph machine learning
community shifted its attention towards a deeper and practical understanding of
message-passing's benefits and limitations. In this position paper, we notice
how the fast pace of progress around the topics of oversmoothing and
oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came
with the consolidation of commonly accepted beliefs and assumptions that are
not always true nor easy to distinguish from each other. We argue that this has
led to ambiguities around the investigated problems, preventing researchers
from focusing on and addressing precise research questions while causing a good
amount of misunderstandings. Our contribution wants to make such common beliefs
explicit and encourage critical thinking around these topics, supported by
simple but noteworthy counterexamples. The hope is to clarify the distinction
between the different issues and promote separate but intertwined research
directions to address them.",2025-05-21,"Adrian Arnaiz-Rodriguez, Federico Errica",http://arxiv.org/pdf/2505.15547v1,cs.LG
A Temporal Difference Method for Stochastic Continuous Dynamics,"For continuous systems modeled by dynamical equations such as ODEs and SDEs,
Bellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman
(HJB) equation, which provides the theoretical target of reinforcement learning
(RL). Although recent advances in RL successfully leverage this formulation,
the existing methods typically assume the underlying dynamics are known a
priori because they need explicit access to the coefficient functions of
dynamical equations to update the value function following the HJB equation. We
address this inherent limitation of HJB-based RL; we propose a model-free
approach still targeting the HJB equation and propose the corresponding
temporal difference method. We demonstrate its potential advantages over
transition kernel-based formulations, both qualitatively and empirically. The
proposed formulation paves the way toward bridging stochastic optimal control
and model-free reinforcement learning.",2025-05-21,"Haruki Settai, Naoya Takeishi, Takehisa Yairi",http://arxiv.org/pdf/2505.15544v3,cs.LG
Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets,"Vision-Language Models (VLMs) acquire real-world knowledge and general
reasoning ability through Internet-scale image-text corpora. They can augment
robotic systems with scene understanding and task planning, and assist
visuomotor policies that are trained on robot trajectory data. We explore the
reverse paradigm - using rich, real, multi-modal robot trajectory data to
enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual
Question Answering (VQA) dataset generation framework for VLMs. Given a human
tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual
and non-descriptive sensory modalities, such as end-effector pose, gripper
aperture, and force sensing. Based on these modalities, it segments the robot
trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses
scene and interaction understanding to identify 3D properties of the robot,
task goal, and the target object. The properties are used to generate
representative VQA queries - images with textural multiple-choice questions -
based on spatial, goal-conditioned, and interaction reasoning question
templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710
questions covering 463 distinct scenes and 3,396 robotic manipulation tasks
from 176k real robot trajectories. Results suggest that Robo2VLM-1 can
benchmark and improve VLM capabilities in spatial and interaction reasoning.",2025-05-21,"Kaiyuan Chen, Shuangyu Xie, Zehan Ma, Ken Goldberg",http://arxiv.org/pdf/2505.15517v1,cs.LG
Explainable embeddings with Distance Explainer,"While eXplainable AI (XAI) has advanced significantly, few methods address
interpretability in embedded vector spaces where dimensions represent complex
abstractions. We introduce Distance Explainer, a novel method for generating
local, post-hoc explanations of embedded spaces in machine learning models. Our
approach adapts saliency-based techniques from RISE to explain the distance
between two embedded data points by assigning attribution values through
selective masking and distance-ranked mask filtering. We evaluate Distance
Explainer on cross-modal embeddings (image-image and image-caption pairs) using
established XAI metrics including Faithfulness, Sensitivity/Robustness, and
Randomization. Experiments with ImageNet and CLIP models demonstrate that our
method effectively identifies features contributing to similarity or
dissimilarity between embedded data points while maintaining high robustness
and consistency. We also explore how parameter tuning, particularly mask
quantity and selection strategy, affects explanation quality. This work
addresses a critical gap in XAI research and enhances transparency and
trustworthiness in deep learning applications utilizing embedded spaces.",2025-05-21,"Christiaan Meijer, E. G. Patrick Bos",http://arxiv.org/pdf/2505.15516v1,cs.LG
AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization,"Proximal Policy Optimization (PPO) is a widely used reinforcement learning
algorithm that heavily relies on accurate advantage estimates for stable and
efficient training. However, raw advantage signals can exhibit significant
variance, noise, and scale-related issues, impeding optimal learning
performance. To address this challenge, we introduce Advantage Modulation PPO
(AM-PPO), a novel enhancement of PPO that adaptively modulates advantage
estimates using a dynamic, non-linear scaling mechanism. This adaptive
modulation employs an alpha controller that dynamically adjusts the scaling
factor based on evolving statistical properties of the advantage signals, such
as their norm, variance, and a predefined target saturation level. By
incorporating a tanh-based gating function driven by these adaptively scaled
advantages, AM-PPO reshapes the advantage signals to stabilize gradient updates
and improve the conditioning of the policy gradient landscape. Crucially, this
modulation also influences value function training by providing consistent and
adaptively conditioned learning targets. Empirical evaluations across standard
continuous control benchmarks demonstrate that AM-PPO achieves superior reward
trajectories, exhibits sustained learning progression, and significantly
reduces the clipping required by adaptive optimizers. These findings underscore
the potential of advantage modulation as a broadly applicable technique for
enhancing reinforcement learning optimization.",2025-05-21,Soham Sane,http://arxiv.org/pdf/2505.15514v1,cs.LG
NOMAD Projection,"The rapid adoption of generative AI has driven an explosion in the size of
datasets consumed and produced by AI models. Traditional methods for
unstructured data visualization, such as t-SNE and UMAP, have not kept up with
the pace of dataset scaling. This presents a significant challenge for AI
explainability, which relies on methods such as t-SNE and UMAP for exploratory
data analysis. In this paper, we introduce Negative Or Mean Affinity
Discrimination (NOMAD) Projection, the first method for unstructured data
visualization via nonlinear dimensionality reduction that can run on multiple
GPUs at train time. We provide theory that situates NOMAD Projection as an
approximate upper bound on the InfoNC-t-SNE loss, and empirical results that
demonstrate NOMAD Projection's superior performance and speed profile compared
to existing state-of-the-art methods. We demonstrate the scalability of NOMAD
Projection by computing the first complete data map of Multilingual Wikipedia.",2025-05-21,"Brandon Duderstadt, Zach Nussbaum, Laurens van der Maaten",http://arxiv.org/pdf/2505.15511v1,cs.LG
Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning,"We introduce a new algebraic structure for multi-dimensional compositional
embeddings, built on directional non-commutative monoidal operators. The core
contribution of this work is this novel framework, which exhibits appealing
theoretical properties (associativity along each dimension and an interchange
law ensuring global consistency) while remaining compatible with modern machine
learning architectures. Our construction defines a distinct composition
operator circ_i for each axis i, ensuring associative combination along each
axis without imposing global commutativity. Importantly, all axis-specific
operators commute with one another, enforcing a global interchange law that
enables consistent crossaxis compositions. This is, to our knowledge, the first
approach that provides a common foundation that generalizes classical
sequence-modeling paradigms (e.g., structured state-space models (SSMs) and
transformer self-attention) to a unified multi-dimensional framework. For
example, specific one-dimensional instances of our framework can recover the
familiar affine transformation algebra, vanilla self-attention, and the
SSM-style recurrence. The higher-dimensional generalizations naturally support
recursive, structure-aware operations in embedding spaces. We outline several
potential applications unlocked by this structure-including structured
positional encodings in Transformers, directional image embeddings, and
symbolic modeling of sequences or grids-indicating that it could inform future
deep learning model designs. We formally establish the algebraic properties of
our framework and discuss efficient implementations. Finally, as our focus is
theoretical, we include no experiments here and defer empirical validation to
future work, which we plan to undertake.",2025-05-21,Mahesh Godavarti,http://arxiv.org/pdf/2505.15507v1,cs.LG
Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts,"Recently, Vision-Language foundation models like CLIP and ALIGN, which are
pre-trained on large-scale data have shown remarkable zero-shot generalization
to diverse datasets with different classes and even domains. In this work, we
take a step further and analyze whether these models can be adapted to target
datasets having very different distributions and classes compared to what these
models have been trained on, using only a few labeled examples from the target
dataset. In such scenarios, finetuning large pretrained models is challenging
due to problems of overfitting as well as loss of generalization, and has not
been well explored in prior literature. Since, the pre-training data of such
models are unavailable, it is difficult to comprehend the performance on
various downstream datasets. First, we try to answer the question: Given a
target dataset with a few labelled examples, can we estimate whether further
fine-tuning can enhance the performance compared to zero-shot evaluation? by
analyzing the common vision-language embedding space. Based on the analysis, we
propose a novel prompt-tuning method, PromptMargin for adapting such
large-scale VLMs directly on the few target samples. PromptMargin effectively
tunes the text as well as visual prompts for this task, and has two main
modules: 1) Firstly, we use a selective augmentation strategy to complement the
few training samples in each task; 2) Additionally, to ensure robust training
in the presence of unfamiliar class names, we increase the inter-class margin
for improved class discrimination using a novel Multimodal Margin Regularizer.
Extensive experiments and analysis across fifteen target benchmark datasets,
with varying degrees of distribution shifts from natural images, shows the
effectiveness of the proposed framework over the existing state-of-the-art
approaches applied to this setting. github.com/debarshigit/PromptMargin.",2025-05-21,"Debarshi Brahma, Anuska Roy, Soma Biswas",http://arxiv.org/pdf/2505.15506v1,cs.LG
Coloring Between the Lines: Personalization in the Null Space of Planning Constraints,"Generalist robots must personalize in-the-wild to meet the diverse needs and
preferences of long-term users. How can we enable flexible personalization
without sacrificing safety or competency? This paper proposes Coloring Between
the Lines (CBTL), a method for personalization that exploits the null space of
constraint satisfaction problems (CSPs) used in robot planning. CBTL begins
with a CSP generator that ensures safe and competent behavior, then
incrementally personalizes behavior by learning parameterized constraints from
online interaction. By quantifying uncertainty and leveraging the
compositionality of planning constraints, CBTL achieves sample-efficient
adaptation without environment resets. We evaluate CBTL in (1) three diverse
simulation environments; (2) a web-based user study; and (3) a real-robot
assisted feeding system, finding that CBTL consistently achieves more effective
personalization with fewer interactions than baselines. Our results demonstrate
that CBTL provides a unified and practical approach for continual, flexible,
active, and safe robot personalization. Website:
https://emprise.cs.cornell.edu/cbtl/",2025-05-21,"Tom Silver, Rajat Kumar Jenamani, Ziang Liu, Ben Dodson, Tapomayukh Bhattacharjee",http://arxiv.org/pdf/2505.15503v1,cs.LG
Certified Neural Approximations of Nonlinear Dynamics,"Neural networks hold great potential to act as approximate models of
nonlinear dynamical systems, with the resulting neural approximations enabling
verification and control of such systems. However, in safety-critical contexts,
the use of neural approximations requires formal bounds on their closeness to
the underlying system. To address this fundamental challenge, we propose a
novel, adaptive, and parallelizable verification method based on certified
first-order models. Our approach provides formal error bounds on the neural
approximations of dynamical systems, allowing them to be safely employed as
surrogates by interpreting the error bound as bounded disturbances acting on
the approximated dynamics. We demonstrate the effectiveness and scalability of
our method on a range of established benchmarks from the literature, showing
that it outperforms the state-of-the-art. Furthermore, we highlight the
flexibility of our framework by applying it to two novel scenarios not
previously explored in this context: neural network compression and an
autoencoder-based deep learning architecture for learning Koopman operators,
both yielding compelling results.",2025-05-21,"Frederik Baymler Mathiesen, Nikolaus Vertovec, Francesco Fabiano, Luca Laurenti, Alessandro Abate",http://arxiv.org/pdf/2505.15497v1,cs.LG
Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes,"We present new fast-rate generalization bounds for multi-task and
meta-learning in the unbalanced setting, i.e. when the tasks have training sets
of different sizes, as is typically the case in real-world scenarios.
Previously, only standard-rate bounds were known for this situation, while
fast-rate bounds were limited to the setting where all training sets are of
equal size. Our new bounds are numerically computable as well as interpretable,
and we demonstrate their flexibility in handling a number of cases where they
give stronger guarantees than previous bounds. Besides the bounds themselves,
we also make conceptual contributions: we demonstrate that the unbalanced
multi-task setting has different statistical properties than the balanced
situation, specifically that proofs from the balanced situation do not carry
over to the unbalanced setting. Additionally, we shed light on the fact that
the unbalanced situation allows two meaningful definitions of multi-task risk,
depending on whether if all tasks should be considered equally important or if
sample-rich tasks should receive more weight than sample-poor ones.",2025-05-21,"Hossein Zakerinia, Christoph H. Lampert",http://arxiv.org/pdf/2505.15496v1,cs.LG
Machine Learning Derived Blood Input for Dynamic PET Images of Rat Heart,"Dynamic FDG PET imaging study of n = 52 rats including 26 control
Wistar-Kyoto (WKY) rats and 26 experimental spontaneously hypertensive rats
(SHR) were performed using a Siemens microPET and Albira trimodal scanner
longitudinally at 1, 2, 3, 5, 9, 12 and 18 months of age. A 15-parameter dual
output model correcting for spill over contamination and partial volume effects
with peak fitting cost functions was developed for simultaneous estimation of
model corrected blood input function (MCIF) and kinetic rate constants for
dynamic FDG PET images of rat heart in vivo. Major drawbacks of this model are
its dependence on manual annotations for the Image Derived Input Function
(IDIF) and manual determination of crucial model parameters to compute MCIF. To
overcome these limitations, we performed semi-automated segmentation and then
formulated a Long-Short-Term Memory (LSTM) cell network to train and predict
MCIF in test data using a concatenation of IDIFs and myocardial inputs and
compared them with reference-modeled MCIF. Thresholding along 2D plane slices
with two thresholds, with T1 representing high-intensity myocardium, and T2
representing lower-intensity rings, was used to segment the area of the LV
blood pool. The resultant IDIF and myocardial TACs were used to compute the
corresponding reference (model) MCIF for all data sets. The segmented IDIF and
the myocardium formed the input for the LSTM network. A k-fold cross validation
structure with a 33:8:11 split and 5 folds was utilized to create the model and
evaluate the performance of the LSTM network for all datasets. To overcome the
sparseness of data as time steps increase, midpoint interpolation was utilized
to increase the density of datapoints beyond time = 10 minutes. The model
utilizing midpoint interpolation was able to achieve a 56.4% improvement over
previous Mean Squared Error (MSE).",2025-05-21,"Shubhrangshu Debsarkar, Bijoy Kundu",http://arxiv.org/pdf/2505.15488v1,cs.LG
AI-based Decision Support System for Heritage Aircraft Corrosion Prevention,"The paper presents a decision support system for the long-term preservation
of aeronautical heritage exhibited/stored in sheltered sites. The aeronautical
heritage is characterized by diverse materials of which this heritage is
constituted. Heritage aircraft are made of ancient aluminum alloys, (ply)wood,
and particularly fabrics. The decision support system (DSS) designed, starting
from a conceptual model, is knowledge-based on degradation/corrosion mechanisms
of prevailing materials of aeronautical heritage. In the case of historical
aircraft wooden parts, this knowledge base is filled in by the damage function
models developed within former European projects. Model-based corrosion
prediction is implemented within the new DSS for ancient aluminum alloys. The
novelty of this DSS consists of supporting multi-material heritage protection
and tailoring to peculiarities of aircraft exhibition/storage hangars and the
needs of aviation museums. The novel DSS is tested on WWII aircraft heritage
exhibited in the Aviation Museum Kbely, Military History Institute Prague,
Czech Republic.",2025-05-21,"Michal Kuchař, Jaromír Fišer, Cyril Oswald, Tomáš Vyhlídal",http://arxiv.org/pdf/2505.15462v1,cs.LG
Reinforcement Twinning for Hybrid Control of Flapping-Wing Drones,"Controlling the flight of flapping-wing drones requires versatile controllers
that handle their time-varying, nonlinear, and underactuated dynamics from
incomplete and noisy sensor data. Model-based methods struggle with accurate
modeling, while model-free approaches falter in efficiently navigating very
high-dimensional and nonlinear control objective landscapes. This article
presents a novel hybrid model-free/model-based approach to flight control based
on the recently proposed reinforcement twinning algorithm. The model-based (MB)
approach relies on an adjoint formulation using an adaptive digital twin,
continuously identified from live trajectories, while the model-free (MF)
approach relies on reinforcement learning. The two agents collaborate through
transfer learning, imitation learning, and experience sharing using the real
environment, the digital twin and a referee. The latter selects the best agent
to interact with the real environment based on performance within the digital
twin and a real-to-virtual environment consistency ratio. The algorithm is
evaluated for controlling the longitudinal dynamics of a flapping-wing drone,
with the environment simulated as a nonlinear, time-varying dynamical system
under the influence of quasi-steady aerodynamic forces. The hybrid control
learning approach is tested with three types of initialization of the adaptive
model: (1) offline identification using previously available data, (2) random
initialization with full online identification, and (3) offline pre-training
with an estimation bias, followed by online adaptation. In all three scenarios,
the proposed hybrid learning approach demonstrates superior performance
compared to purely model-free and model-based methods.",2025-05-21,"Romain Poletti, Lorenzo Schena, Lilla Koloszar, Joris Degroote, Miguel Alfonso Mendez",http://arxiv.org/pdf/2505.18201v1,cs.LG
Stronger ViTs With Octic Equivariance,"Recent efforts at scaling computer vision models have established Vision
Transformers (ViTs) as the leading architecture. ViTs incorporate weight
sharing over image patches as an important inductive bias. In this work, we
show that ViTs benefit from incorporating equivariance under the octic group,
i.e., reflections and 90-degree rotations, as a further inductive bias. We
develop new architectures, octic ViTs, that use octic-equivariant layers and
put them to the test on both supervised and self-supervised learning. Through
extensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show
that octic ViTs yield more computationally efficient networks while also
improving performance. In particular, we achieve approximately 40% reduction in
FLOPs for ViT-H while simultaneously improving both classification and
segmentation results.",2025-05-21,"David Nordström, Johan Edstedt, Fredrik Kahl, Georg Bökman",http://arxiv.org/pdf/2505.15441v2,cs.LG
CrossRF: A Domain-Invariant Deep Learning Approach for RF Fingerprinting,"Radio Frequency (RF) fingerprinting offers a promising approach for drone
identification and security, although it suffers from significant performance
degradation when operating on different transmission channels. This paper
presents CrossRF, a domain-invariant deep learning approach that addresses the
problem of cross-channel RF fingerprinting for Unmanned Aerial Vehicle (UAV)
identification. Our approach aims to minimize the domain gap between different
RF channels by using adversarial learning to train a more robust model that
maintains consistent identification performance despite channel variations. We
validate our approach using the UAVSig dataset, comprising real-world
over-the-air RF signals from identical drone models operating across several
frequency channels, ensuring that the findings correspond to real-world
scenarios. The experimental results show CrossRF's efficiency, achieving up to
99.03% accuracy when adapting from Channel 3 to Channel 4, compared to only
26.39% using conventional methods. The model maintains robust performance in
more difficult multi-channel scenarios (87.57% accuracy adapting from Channels
1,3 to 2,4) and achieves 89.45% accuracy with 0.9 precision for controller
classification. These results confirm CrossRF's ability to significantly reduce
performance degradation due to cross-channel variations while maintaining high
identification accuracy with minimal training data requirements, making it
particularly suitable for practical drone security applications.",2025-05-21,"Fahrettin Emin Tiras, Hayriye Serra Altinoluk",http://arxiv.org/pdf/2505.18200v1,cs.LG
Adaptive Temperature Scaling with Conformal Prediction,"Conformal prediction enables the construction of high-coverage prediction
sets for any pre-trained model, guaranteeing that the true label lies within
the set with a specified probability. However, these sets do not provide
probability estimates for individual labels, limiting their practical use. In
this paper, we propose, to the best of our knowledge, the first method for
assigning calibrated probabilities to elements of a conformal prediction set.
Our approach frames this as an adaptive calibration problem, selecting an
input-specific temperature parameter to match the desired coverage level.
Experiments on several challenging image classification datasets demonstrate
that our method maintains coverage guarantees while significantly reducing
expected calibration error.",2025-05-21,"Nikita Kotelevskii, Mohsen Guizani, Eric Moulines, Maxim Panov",http://arxiv.org/pdf/2505.15437v1,cs.LG
Set-LLM: A Permutation-Invariant LLM,"While large language models (LLMs) demonstrate impressive capabilities across
numerous applications, their robustness remains a critical concern. This paper
is motivated by a specific vulnerability: the order sensitivity of LLMs. This
vulnerability manifests itself as the order bias observed when LLMs decide
between possible options (for example, a preference for the first option) and
the tendency of LLMs to provide different answers when options are reordered.
The use cases for this scenario extend beyond the classical case of
multiple-choice question answering to the use of LLMs as automated evaluators
in AI pipelines, comparing output generated by different models. We introduce
Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the
processing of mixed set-text inputs with permutation invariance guarantees. The
adaptations involve a new attention mask and new positional encodings
specifically designed for sets. We provide a theoretical proof of invariance
and demonstrate through experiments that Set-LLM can be trained effectively,
achieving comparable or improved performance and maintaining the runtime of the
original model, while eliminating order sensitivity.",2025-05-21,"Beni Egressy, Jan Stühmer",http://arxiv.org/pdf/2505.15433v1,cs.LG
Uncertainty Quantification in SVM prediction,"This paper explores Uncertainty Quantification (UQ) in SVM predictions,
particularly for regression and forecasting tasks. Unlike the Neural Network,
the SVM solutions are typically more stable, sparse, optimal and interpretable.
However, there are only few literature which addresses the UQ in SVM
prediction. At first, we provide a comprehensive summary of existing Prediction
Interval (PI) estimation and probabilistic forecasting methods developed in the
SVM framework and evaluate them against the key properties expected from an
ideal PI model. We find that none of the existing SVM PI models achieves a
sparse solution. To introduce sparsity in SVM model, we propose the Sparse
Support Vector Quantile Regression (SSVQR) model, which constructs PIs and
probabilistic forecasts by solving a pair of linear programs. Further, we
develop a feature selection algorithm for PI estimation using SSVQR that
effectively eliminates a significant number of features while improving PI
quality in case of high-dimensional dataset. Finally we extend the SVM models
in Conformal Regression setting for obtaining more stable prediction set with
finite test set guarantees. Extensive experiments on artificial, real-world
benchmark datasets compare the different characteristics of both existing and
proposed SVM-based PI estimation methods and also highlight the advantages of
the feature selection in PI estimation. Furthermore, we compare both, the
existing and proposed SVM-based PI estimation models, with modern deep learning
models for probabilistic forecasting tasks on benchmark datasets. Furthermore,
SVM models show comparable or superior performance to modern complex deep
learning models for probabilistic forecasting task in our experiments.",2025-05-21,Pritam Anand,http://arxiv.org/pdf/2505.15429v1,cs.LG
SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding,"Capturing nonlinear relationships without sacrificing interpretability
remains a persistent challenge in regression modeling. We introduce SplitWise,
a novel framework that enhances stepwise regression. It adaptively transforms
numeric predictors into threshold-based binary features using shallow decision
trees, but only when such transformations improve model fit, as assessed by the
Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).
This approach preserves the transparency of linear models while flexibly
capturing nonlinear effects. Implemented as a user-friendly R package,
SplitWise is evaluated on both synthetic and real-world datasets. The results
show that it consistently produces more parsimonious and generalizable models
than traditional stepwise and penalized regression techniques.",2025-05-21,"Marcell T. Kurbucz, Nikolaos Tzivanakis, Nilufer Sari Aslam, Adam M. Sykulski",http://arxiv.org/pdf/2505.15423v1,cs.LG
Guided Policy Optimization under Partial Observability,"Reinforcement Learning (RL) in partially observable environments poses
significant challenges due to the complexity of learning under uncertainty.
While additional information, such as that available in simulations, can
enhance training, effectively leveraging it remains an open problem. To address
this, we introduce Guided Policy Optimization (GPO), a framework that co-trains
a guider and a learner. The guider takes advantage of privileged information
while ensuring alignment with the learner's policy that is primarily trained
via imitation learning. We theoretically demonstrate that this learning scheme
achieves optimality comparable to direct RL, thereby overcoming key limitations
inherent in existing approaches. Empirical evaluations show strong performance
of GPO across various tasks, including continuous control with partial
observability and noise, and memory-based challenges, significantly
outperforming existing methods.",2025-05-21,"Yueheng Li, Guangming Xie, Zongqing Lu",http://arxiv.org/pdf/2505.15418v1,cs.LG
Robust Multimodal Learning via Entropy-Gated Contrastive Fusion,"Real-world multimodal systems routinely face missing-input scenarios, and in
reality, robots lose audio in a factory or a clinical record omits lab tests at
inference time. Standard fusion layers either preserve robustness or
calibration but never both. We introduce Adaptive Entropy-Gated Contrastive
Fusion (AECF), a single light-weight layer that (i) adapts its entropy
coefficient per instance, (ii) enforces monotone calibration across all
modality subsets, and (iii) drives a curriculum mask directly from
training-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP
by +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%
run-time. All back-bones remain frozen, making AECF an easy drop-in layer for
robust, calibrated multimodal inference.",2025-05-21,"Leon Chlon, Maggie Chlon, MarcAntonio M. Awada",http://arxiv.org/pdf/2505.15417v1,cs.LG
SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval,"Despite the dominance of convolutional and transformer-based architectures in
image-to-image retrieval, these models are prone to biases arising from
low-level visual features, such as color. Recognizing the lack of semantic
understanding as a key limitation, we propose a novel scene graph-based
retrieval framework that emphasizes semantic content over superficial image
characteristics. Prior approaches to scene graph retrieval predominantly rely
on supervised Graph Neural Networks (GNNs), which require ground truth graph
pairs driven from image captions. However, the inconsistency of caption-based
supervision stemming from variable text encodings undermine retrieval
reliability. To address these, we present SCENIR, a Graph Autoencoder-based
unsupervised retrieval framework, which eliminates the dependence on labeled
training data. Our model demonstrates superior performance across metrics and
runtime efficiency, outperforming existing vision-based, multimodal, and
supervised GNN approaches. We further advocate for Graph Edit Distance (GED) as
a deterministic and robust ground truth measure for scene graph similarity,
replacing the inconsistent caption-based alternatives for the first time in
image-to-image retrieval evaluation. Finally, we validate the generalizability
of our method by applying it to unannotated datasets via automated scene graph
generation, while substantially contributing in advancing state-of-the-art in
counterfactual image retrieval.",2025-05-21,"Nikolaos Chaidos, Angeliki Dimitriou, Maria Lymperaiou, Giorgos Stamou",http://arxiv.org/pdf/2505.15867v1,cs.LG
Multi-omic Causal Discovery using Genotypes and Gene Expression,"Causal discovery in multi-omic datasets is crucial for understanding the
bigger picture of gene regulatory mechanisms, but remains challenging due to
high dimensionality, differentiation of direct from indirect relationships, and
hidden confounders. We introduce GENESIS (GEne Network inference from
Expression SIgnals and SNPs), a constraint-based algorithm that leverages the
natural causal precedence of genotypes to infer ancestral relationships in
transcriptomic data. Unlike traditional causal discovery methods that start
with a fully connected graph, GENESIS initialises an empty ancestrality matrix
and iteratively populates it with direct, indirect or non-causal relationships
using a series of provably sound marginal and conditional independence tests.
By integrating genotypes as fixed causal anchors, GENESIS provides a principled
``head start'' to classical causal discovery algorithms, restricting the search
space to biologically plausible edges. We test GENESIS on synthetic and
real-world genomic datasets. This framework offers a powerful avenue for
uncovering causal pathways in complex traits, with promising applications to
functional genomics, drug discovery, and precision medicine.",2025-05-21,"Stephen Asiedu, David Watson",http://arxiv.org/pdf/2505.15866v1,cs.LG
Efficient Differentiable Approximation of Generalized Low-rank Regularization,"Low-rank regularization (LRR) has been widely applied in various machine
learning tasks, but the associated optimization is challenging. Directly
optimizing the rank function under constraints is NP-hard in general. To
overcome this difficulty, various relaxations of the rank function were
studied. However, optimization of these relaxed LRRs typically depends on
singular value decomposition, which is a time-consuming and nondifferentiable
operator that cannot be optimized with gradient-based techniques. To address
these challenges, in this paper we propose an efficient differentiable
approximation of the generalized LRR. The considered LRR form subsumes many
popular choices like the nuclear norm, the Schatten-$p$ norm, and various
nonconvex relaxations. Our method enables LRR terms to be appended to loss
functions in a plug-and-play fashion, and the GPU-friendly operations enable
efficient and convenient implementation. Furthermore, convergence analysis is
presented, which rigorously shows that both the bias and the variance of our
rank estimator rapidly reduce with increased sample size and iteration steps.
In the experimental study, the proposed method is applied to various tasks,
which demonstrates its versatility and efficiency. Code is available at
https://github.com/naiqili/EDLRR.",2025-05-21,"Naiqi Li, Yuqiu Xie, Peiyuan Liu, Tao Dai, Yong Jiang, Shu-Tao Xia",http://arxiv.org/pdf/2505.15407v1,cs.LG
HOPSE: Scalable Higher-Order Positional and Structural Encoder for Combinatorial Representations,"While Graph Neural Networks (GNNs) have proven highly effective at modeling
relational data, pairwise connections cannot fully capture multi-way
relationships naturally present in complex real-world systems. In response to
this, Topological Deep Learning (TDL) leverages more general combinatorial
representations -- such as simplicial or cellular complexes -- to accommodate
higher-order interactions. Existing TDL methods often extend GNNs through
Higher-Order Message Passing (HOMP), but face critical \emph{scalability
challenges} due to \textit{(i)} a combinatorial explosion of message-passing
routes, and \textit{(ii)} significant complexity overhead from the propagation
mechanism. To overcome these limitations, we propose HOPSE (Higher-Order
Positional and Structural Encoder) -- a \emph{message passing-free} framework
that uses Hasse graph decompositions to derive efficient and expressive
encodings over \emph{arbitrary higher-order domains}. Notably, HOPSE scales
linearly with dataset size while preserving expressive power and permutation
equivariance. Experiments on molecular, expressivity and topological benchmarks
show that HOPSE matches or surpasses state-of-the-art performance while
achieving up to 7 $times$ speedups over HOMP-based models, opening a new path
for scalable TDL.",2025-05-21,"Martin Carrasco, Guillermo Bernardez, Marco Montagna, Nina Miolane, Lev Telyatnikov",http://arxiv.org/pdf/2505.15405v1,cs.LG
InTreeger: An End-to-End Framework for Integer-Only Decision Tree Inference,"Integer quantization has emerged as a critical technique to facilitate
deployment on resource-constrained devices. Although they do reduce the
complexity of the learning models, their inference performance is often prone
to quantization-induced errors. To this end, we introduce InTreeger: an
end-to-end framework that takes a training dataset as input, and outputs an
architecture-agnostic integer-only C implementation of tree-based machine
learning model, without loss of precision. This framework enables anyone, even
those without prior experience in machine learning, to generate a highly
optimized integer-only classification model that can run on any hardware simply
by providing an input dataset and target variable. We evaluated our generated
implementations across three different architectures (ARM, x86, and RISC-V),
resulting in significant improvements in inference latency. In addition, we
show the energy efficiency compared to typical decision tree implementations
that rely on floating-point arithmetic. The results underscore the advantages
of integer-only inference, making it particularly suitable for energy- and
area-constrained devices such as embedded systems and edge computing platforms,
while also enabling the execution of decision trees on existing ultra-low power
devices.",2025-05-21,"Duncan Bart, Bruno Endres Forlin, Ana-Lucia Varbanescu, Marco Ottavi, Kuan-Hsun Chen",http://arxiv.org/pdf/2505.15391v1,cs.LG
Inter-Subject Variance Transfer Learning for EMG Pattern Classification Based on Bayesian Inference,"In electromyogram (EMG)-based motion recognition, a subject-specific
classifier is typically trained with sufficient labeled data. However, this
process demands extensive data collection over extended periods, burdening the
subject. To address this, utilizing information from pre-training on multiple
subjects for the training of the target subject could be beneficial. This paper
proposes an inter-subject variance transfer learning method based on a Bayesian
approach. This method is founded on the simple hypothesis that while the means
of EMG features vary greatly across subjects, their variances may exhibit
similar patterns. Our approach transfers variance information, acquired through
pre-training on multiple source subjects, to a target subject within a Bayesian
updating framework, thereby allowing accurate classification using limited
target calibration data. A coefficient was also introduced to adjust the amount
of information transferred for efficient transfer learning. Experimental
evaluations using two EMG datasets demonstrated the effectiveness of our
variance transfer strategy and its superiority compared to existing methods.",2025-05-21,"Seitaro Yoneda, Akira Furui",http://arxiv.org/pdf/2505.15381v1,cs.LG
