{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS GraphRAG Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a demo of using IRIS Vector Search capabilities for a graphrag application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is used to get all the requirements. The jupyter image should already have these downloaded, but running this cell just to be safe is advised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /opt/anaconda3/lib/python3.12/site-packages (0.3.21)\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: arxiv in /opt/anaconda3/lib/python3.12/site-packages (2.2.0)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: langchainhub in /opt/anaconda3/lib/python3.12/site-packages (0.1.21)\n",
      "Requirement already satisfied: pymilvus in /opt/anaconda3/lib/python3.12/site-packages (2.5.6)\n",
      "Collecting pymilvus\n",
      "  Using cached pymilvus-2.5.10-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (0.3.23)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: langgraph in /opt/anaconda3/lib/python3.12/site-packages (0.3.27)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.4.7-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tavily-python in /opt/anaconda3/lib/python3.12/site-packages (0.5.4)\n",
      "Collecting tavily-python\n",
      "  Downloading tavily_python-0.7.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (4.0.2)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langchain-milvus in /opt/anaconda3/lib/python3.12/site-packages (0.1.9)\n",
      "Collecting langchain-milvus\n",
      "  Using cached langchain_milvus-0.1.10-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: langchain-ollama in /opt/anaconda3/lib/python3.12/site-packages (0.3.1)\n",
      "Collecting langchain-ollama\n",
      "  Using cached langchain_ollama-0.3.3-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-huggingface in /opt/anaconda3/lib/python3.12/site-packages (0.1.2)\n",
      "Collecting langchain-huggingface\n",
      "  Using cached langchain_huggingface-0.2.0-py3-none-any.whl.metadata (941 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.13.3)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: langchain-experimental in /opt/anaconda3/lib/python3.12/site-packages (0.3.4)\n",
      "Requirement already satisfied: neo4j in /opt/anaconda3/lib/python3.12/site-packages (5.28.1)\n",
      "Requirement already satisfied: json-repair in /opt/anaconda3/lib/python3.12/site-packages (0.40.0)\n",
      "Collecting json-repair\n",
      "  Downloading json_repair-0.46.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.12/site-packages (0.3.12)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.3.18-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: iris in /opt/anaconda3/lib/python3.12/site-packages (1.0.7)\n",
      "Requirement already satisfied: flask in /opt/anaconda3/lib/python3.12/site-packages (3.1.0)\n",
      "Collecting flask\n",
      "  Downloading flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: setuptools==69.0.3 in /opt/anaconda3/lib/python3.12/site-packages (69.0.3)\n",
      "Collecting langchain-core<1.0.0,>=0.3.59 (from langchain_community)\n",
      "  Using cached langchain_core-0.3.62-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (2.6.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (0.1.136)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchainhub) (23.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchainhub) (2.32.0.20241016)\n",
      "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (1.67.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (6.30.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (1.1.0)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (5.10.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (2.2.2)\n",
      "Requirement already satisfied: milvus-lite>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (2.4.11)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.9.2)\n",
      "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: langgraph-sdk>=0.1.42 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph) (0.1.51)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from tavily-python) (0.27.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.50.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.11.0)\n",
      "Collecting ollama<1.0.0,>=0.4.8 (from langchain-ollama)\n",
      "  Using cached ollama-0.4.9-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-huggingface) (0.21.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.12/site-packages (from neo4j) (2024.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (1.72.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from flask) (1.9.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from flask) (2.1.3)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: sgmllib3k in /opt/anaconda3/lib/python3.12/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\n",
      "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
      "  Downloading ormsgpack-1.10.0-cp312-cp312-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (43 kB)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.7)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (2.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Using cached langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "Using cached pymilvus-2.5.10-py3-none-any.whl (227 kB)\n",
      "Using cached langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "Downloading langgraph-0.4.7-py3-none-any.whl (154 kB)\n",
      "Downloading tavily_python-0.7.3-py3-none-any.whl (15 kB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached langchain_milvus-0.1.10-py3-none-any.whl (29 kB)\n",
      "Using cached langchain_ollama-0.3.3-py3-none-any.whl (21 kB)\n",
      "Using cached langchain_huggingface-0.2.0-py3-none-any.whl (27 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading json_repair-0.46.0-py3-none-any.whl (22 kB)\n",
      "Using cached langchain_openai-0.3.18-py3-none-any.whl (63 kB)\n",
      "Downloading flask-3.1.1-py3-none-any.whl (103 kB)\n",
      "Using cached langchain_core-0.3.62-py3-none-any.whl (438 kB)\n",
      "Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n",
      "Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
      "Using cached ollama-0.4.9-py3-none-any.whl (13 kB)\n",
      "Downloading ormsgpack-1.10.0-cp312-cp312-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (376 kB)\n",
      "Installing collected packages: ormsgpack, json-repair, beautifulsoup4, flask, tavily-python, pymilvus, ollama, langchain-core, sentence-transformers, langgraph-checkpoint, langchain-openai, langchain-ollama, langchain-milvus, langgraph-prebuilt, langchain-huggingface, langchain, langgraph, langchain_community\n",
      "  Attempting uninstall: json-repair\n",
      "    Found existing installation: json_repair 0.40.0\n",
      "    Uninstalling json_repair-0.40.0:\n",
      "      Successfully uninstalled json_repair-0.40.0\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.13.3\n",
      "    Uninstalling beautifulsoup4-4.13.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.13.3\n",
      "  Attempting uninstall: flask\n",
      "    Found existing installation: Flask 3.1.0\n",
      "    Uninstalling Flask-3.1.0:\n",
      "      Successfully uninstalled Flask-3.1.0\n",
      "  Attempting uninstall: tavily-python\n",
      "    Found existing installation: tavily-python 0.5.4\n",
      "    Uninstalling tavily-python-0.5.4:\n",
      "      Successfully uninstalled tavily-python-0.5.4\n",
      "  Attempting uninstall: pymilvus\n",
      "    Found existing installation: pymilvus 2.5.6\n",
      "    Uninstalling pymilvus-2.5.6:\n",
      "      Successfully uninstalled pymilvus-2.5.6\n",
      "  Attempting uninstall: ollama\n",
      "    Found existing installation: ollama 0.4.7\n",
      "    Uninstalling ollama-0.4.7:\n",
      "      Successfully uninstalled ollama-0.4.7\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.51\n",
      "    Uninstalling langchain-core-0.3.51:\n",
      "      Successfully uninstalled langchain-core-0.3.51\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 4.0.2\n",
      "    Uninstalling sentence-transformers-4.0.2:\n",
      "      Successfully uninstalled sentence-transformers-4.0.2\n",
      "  Attempting uninstall: langgraph-checkpoint\n",
      "    Found existing installation: langgraph-checkpoint 2.0.10\n",
      "    Uninstalling langgraph-checkpoint-2.0.10:\n",
      "      Successfully uninstalled langgraph-checkpoint-2.0.10\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 0.3.12\n",
      "    Uninstalling langchain-openai-0.3.12:\n",
      "      Successfully uninstalled langchain-openai-0.3.12\n",
      "  Attempting uninstall: langchain-ollama\n",
      "    Found existing installation: langchain-ollama 0.3.1\n",
      "    Uninstalling langchain-ollama-0.3.1:\n",
      "      Successfully uninstalled langchain-ollama-0.3.1\n",
      "  Attempting uninstall: langchain-milvus\n",
      "    Found existing installation: langchain-milvus 0.1.9\n",
      "    Uninstalling langchain-milvus-0.1.9:\n",
      "      Successfully uninstalled langchain-milvus-0.1.9\n",
      "  Attempting uninstall: langgraph-prebuilt\n",
      "    Found existing installation: langgraph-prebuilt 0.1.2\n",
      "    Uninstalling langgraph-prebuilt-0.1.2:\n",
      "      Successfully uninstalled langgraph-prebuilt-0.1.2\n",
      "  Attempting uninstall: langchain-huggingface\n",
      "    Found existing installation: langchain-huggingface 0.1.2\n",
      "    Uninstalling langchain-huggingface-0.1.2:\n",
      "      Successfully uninstalled langchain-huggingface-0.1.2\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.23\n",
      "    Uninstalling langchain-0.3.23:\n",
      "      Successfully uninstalled langchain-0.3.23\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 0.3.27\n",
      "    Uninstalling langgraph-0.3.27:\n",
      "      Successfully uninstalled langgraph-0.3.27\n",
      "  Attempting uninstall: langchain_community\n",
      "    Found existing installation: langchain-community 0.3.21\n",
      "    Uninstalling langchain-community-0.3.21:\n",
      "      Successfully uninstalled langchain-community-0.3.21\n",
      "Successfully installed beautifulsoup4-4.13.4 flask-3.1.1 json-repair-0.46.0 langchain-0.3.25 langchain-core-0.3.62 langchain-huggingface-0.2.0 langchain-milvus-0.1.10 langchain-ollama-0.3.3 langchain-openai-0.3.18 langchain_community-0.3.24 langgraph-0.4.7 langgraph-checkpoint-2.0.26 langgraph-prebuilt-0.2.2 ollama-0.4.9 ormsgpack-1.10.0 pymilvus-2.5.10 sentence-transformers-4.1.0 tavily-python-0.7.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U langchain_community arxiv tiktoken langchainhub pymilvus langchain langgraph tavily-python sentence-transformers langchain-milvus langchain-ollama langchain-huggingface beautifulsoup4 langchain-experimental neo4j json-repair langchain-openai langchain-ollama iris flask setuptools==69.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just some basic setup for the langchain application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "max_papers=30\n",
    "\n",
    "data_path=\"/home/jovyan/workspace/data/\"\n",
    "\n",
    "#load_dotenv()\n",
    "\n",
    "# Set langchain variables\n",
    "set_debug(False)\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should set your OPENAI KEY to be used for the llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"openai_key\"\n",
    "\n",
    "gpt4omini = \"gpt-4o-mini\"\n",
    "\n",
    "model = gpt4omini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our project we are using immunology and clinical trials papers from arxiv x. This data has already been converted into graphs and exported as CSV files. Below our example you will find the code we used to load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell connects to our IRIS container, which has the Vector Search Code. If you are running this locally, please enter the information for you own IRIS server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "<COMMUNICATION LINK ERROR> Failed to connect to server; Details: <COMMUNICATION LINK ERROR> Failed to connect to server; Details: Error code: -1 Error message: <SSL Error> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m password \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSYS\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# connect\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m connection \u001b[38;5;241m=\u001b[39m iris\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{:}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{:}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hostname, port, namespace), username, password)\n\u001b[1;32m     16\u001b[0m irispy \u001b[38;5;241m=\u001b[39m iris\u001b[38;5;241m.\u001b[39mcreateIRIS(connection)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/iris/_init_elsdk.py:78\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Return a new open connection to an IRIS instance.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03miris.connect(hostname,port,namespace,username,password,timeout,sharedmemory,logfile,sslconfig,autoCommit,isolationLevel,featureOptions)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    A new client connection to an IRIS server\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m createConnection(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/iris/_init_elsdk.py:83\u001b[0m, in \u001b[0;36mcreateConnection\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''This method is an alias to the connect() method.'''\u001b[39;00m\n\u001b[1;32m     82\u001b[0m connection \u001b[38;5;241m=\u001b[39m iris\u001b[38;5;241m.\u001b[39mIRISConnection()\n\u001b[0;32m---> 83\u001b[0m connection\u001b[38;5;241m.\u001b[39m_connect(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[0;31mRuntimeError\u001b[0m: <COMMUNICATION LINK ERROR> Failed to connect to server; Details: <COMMUNICATION LINK ERROR> Failed to connect to server; Details: Error code: -1 Error message: <SSL Error> "
     ]
    }
   ],
   "source": [
    "# load iris module\n",
    "import iris\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "# change these variables to reflect your connection\n",
    "hostname = \"iris\"\n",
    "port = 1972\n",
    "namespace = \"IRISAPP\"\n",
    "username = \"SuperUser\"\n",
    "password = \"SYS\"\n",
    "\n",
    "# connect\n",
    "connection = iris.connect(\"{:}:{:}/{:}\".format(hostname, port, namespace), username, password)\n",
    "irispy = iris.createIRIS(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/iris/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(iris.__file__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is used to load the data into IRIS, and create embeddings for the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result Set idx: 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docsfile = '/home/irisowner/dev/CSV/papers300.csv'\n",
    "relationsfile = '/home/irisowner/dev/CSV/relations300.csv'\n",
    "entitiesfile = '/home/irisowner/dev/CSV/entities300.csv'\n",
    "entitiesembeddingsfile = '/home/irisowner/dev/CSV/entities_embeddings300.csv'\n",
    "papersembeddingsfile = '/home/irisowner/dev/CSV/papers_embeddings300.csv'\n",
    "\n",
    "\n",
    "irispy.classMethodValue(\"GraphKB.Documents\", \"LoadData\", docsfile)\n",
    "irispy.classMethodValue(\"GraphKB.Entity\", \"LoadData\", entitiesfile)\n",
    "irispy.classMethodValue(\"GraphKB.Relations\", \"LoadData\", relationsfile)\n",
    "irispy.classMethodValue(\"GraphKB.DocumentsEmbeddings\", \"LoadData\", papersembeddingsfile)\n",
    "irispy.classMethodValue(\"GraphKB.EntityEmbeddings\", \"LoadData\", entitiesembeddingsfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here are the functions which will perform the GraphRAG. Note that these are relatively simple implementations, and can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import ast\n",
    "\n",
    "import os, sys\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "def extract_query_entities(query):\n",
    "\n",
    "  prompt_text = '''Based on the following example, extract entities from the user provided queries.\n",
    "                Below are a number of example queries and their extracted entities. Provide only the entities.\n",
    "                'How many wars was George Washington involved in' -> ['War', 'George Washington'].\\n\n",
    "                'What are the relationships between the employees' -> ['relationships','employees].\\n\n",
    "\n",
    "                For the following query, extract entities as in the above example.\\n query: {content}'''\n",
    "\n",
    "  llm = ChatOpenAI(temperature=0, model_name=model)\n",
    "  prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "  chain = prompt | llm | StrOutputParser()\n",
    "  response = chain.invoke({\"content\": query})\n",
    "  return ast.literal_eval(response)\n",
    "\n",
    "def global_query(query, items=50,vector_search=10, batch_size = 10):\n",
    "    with HiddenPrints():\n",
    "        docs = irispy.classMethodValue(\"GraphKB.Query\",\"Search\",query,items/2,items/2)\n",
    "        docs = docs.split('\\n\\r\\n')\n",
    "    \n",
    "    answers = []\n",
    "    \n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i+batch_size]\n",
    "        response = llm_answer_for_batch(batch, query)\n",
    "        answers.append(response)\n",
    "\n",
    "    return llm_answer_summarize(query, answers)\n",
    "\n",
    "def ask_query(query, items = 10, method='local'):\n",
    "    with HiddenPrints():\n",
    "        docs = [irispy.classMethodValue(\"GraphKB.Query\",\"Search\",query,items/2,items/2)]\n",
    "        \n",
    "    response = llm_answer_for_batch(docs, query, False)\n",
    "    return response\n",
    "\n",
    "\n",
    "def llm_answer_summarize(query, answers):\n",
    "    llm = ChatOpenAI(temperature=0, model_name=model)\n",
    "    prompt_text = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following answers to a query derived from analyzing batches of documents. Please compile these answers into one overall answer. If you don't know the answer, just say that you don't know. \n",
    "    Question: {question}  \n",
    "    Previous Answers: {answers}\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": query, 'answers':answers})\n",
    "    return response\n",
    "    \n",
    "    \n",
    "    \n",
    "def llm_answer_for_batch(batch, query, cutoff=True):\n",
    "    llm = ChatOpenAI(temperature=0, model_name=model)\n",
    "    prompt_text = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context from a graph database to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    \"\"\" + ((\"Use three sentences maximum and keep the answer concise:\") if cutoff else \" \") + \"\"\"\n",
    "    Question: {question}  \n",
    "    Graph Context: {graph_context}\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": query, 'graph_context':batch})\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here we show an example of running the GraphRAG algorithm on our loaded data. First, we use the ask_query method, which will retrieve the inputted number of relevant documents and perform RAG with them. The most relevant documents identified using Vector Search, as well as traversing the created graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most significant papers in immunology summarized from the provided abstracts are:\n",
      "\n",
      "1. **Longitudinal Evaluation of T and B Cell Immunity to SARS-CoV-2**: This paper presents a detailed longitudinal study of immune responses in a single individual over a year following vaccination. It emphasizes the insights gained from personal biological sample analysis, contributing to understanding vaccine-elicited immunity.\n",
      "\n",
      "2. **Vaccine Development and Immune Response Markers**: This research discusses the challenges in developing effective vaccines for high-burden diseases like HIV. It proposes a nonparametric methodology for estimating the impact of immune response markers on infection probabilities, enhancing the evaluation of immune responses in clinical trials.\n",
      "\n",
      "3. **Diagnostic Algorithm for HIV Treatment Monitoring**: This paper critiques existing WHO guidelines for monitoring HIV treatment effectiveness and proposes a new diagnostic algorithm that optimizes the use of viral load testing based on clinical and immunological markers, aiming to reduce misdiagnosis rates.\n",
      "\n",
      "4. **Natural Language Processing in Clinical Trials**: This study introduces CT-BERT, a framework for extracting relevant information from clinical trial documents using NLP techniques. It demonstrates improved performance in identifying eligibility criteria compared to existing methods.\n",
      "\n",
      "5. **Automated Medical Coding for Clinical Trials**: The paper presents ALIGN, a novel system for automated medical coding that addresses interoperability challenges in clinical trial data. It showcases high accuracy in coding medication and medical history terms, significantly enhancing data integration and reusability in immunology research.\n",
      "\n",
      "These papers collectively highlight advancements in vaccine research, diagnostic methodologies, and the application of technology in clinical trials within the field of immunology.\n"
     ]
    }
   ],
   "source": [
    "print(ask_query(\"what data do you have\", items = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "You may notice that if we decide to search for many items (maybe 100), these may be too large for the LLM context, and thus we won't be able to get a good result.\n",
    "\n",
    "The 'global_query' method thus takes advantage of another GraphRAG concept. This query will batch files together, answer the question for each batch, and then summarize the batch answers into one final answer. This means that we can now ask for as many items as we think may be necessary, and won't run into an context issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next directions in HIV treatment involve several key areas of focus. Firstly, there is an emphasis on optimizing clinical trial designs through advanced data analysis and the use of synthetic clinical trial data to enhance patient outcomes. This includes developing diagnostic algorithms that selectively utilize viral load testing to monitor treatment effectiveness, particularly in resource-limited settings. \n",
      "\n",
      "Additionally, there is ongoing research into personalized treatment regimens that take into account immunologic and virologic parameters, which may improve the efficacy of structured treatment interruptions. Another important direction is the development of effective preventive vaccines, which, despite slow progress, hold significant potential for public health impact. Furthermore, leveraging historical clinical trial data and advanced automated coding systems is expected to enhance data interoperability, thereby accelerating research and drug development. Collectively, these approaches aim to improve the efficiency and effectiveness of HIV treatment strategies.\n"
     ]
    }
   ],
   "source": [
    "print(global_query(\"What are the next directions in HIV treatment\", items = 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ends the example. Feel free to play around with the above cell, asking more questions about the dataset we are using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "Below is the code we used to download the arxiv data, create graphs from the data, and then export the graphs into csv files that can be imported into IRIS. The below code can be modified to make use of other types of data and documents.\n",
    "\n",
    "(Note -> The GraphKB classes are currently hard-coded to accept the format of data we provide. We did not have time to write a more generalized implementation, but this should prove to be relatively simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1:\n",
      "Title: Bayesian Models and Decision Algorithms for Complex Early Phase Clinical Trials\n",
      "Summary: An early phase clinical trial is the first step in evaluating the effects in\n",
      "humans of a potential new anti-disease agent or combination of agents. Usually\n",
      "called \"phase I\" or \"phase I/II\" trials, these experiments typically have the\n",
      "nominal scientific goal of determining an acceptable dose, most often based on\n",
      "adverse event probabilities. This arose from a tradition of phase I trials to\n",
      "evaluate cytotoxic agents for treating cancer, although some methods may be\n",
      "applied in other medical settings, such as treatment of stroke or immunological\n",
      "diseases. Most modern statistical designs for early phase trials include\n",
      "model-based, outcome-adaptive decision rules that choose doses for successive\n",
      "patient cohorts based on data from previous patients in the trial. Such designs\n",
      "have seen limited use in clinical practice, however, due to their complexity,\n",
      "the requirement of intensive, computer-based data monitoring, and the medical\n",
      "community's resistance to change. Still, many actual applications of\n",
      "model-based outcome-adaptive designs have been remarkably successful in terms\n",
      "of both patient benefit and scientific outcome. In this paper I will review\n",
      "several Bayesian early phase trial designs that were tailored to accommodate\n",
      "specific complexities of the treatment regime and patient outcomes in\n",
      "particular clinical settings.\n",
      "URL: http://arxiv.org/abs/1011.6494v1\n",
      "Authors: Peter F. Thall\n",
      "--------------------------------------------------\n",
      "Paper 2:\n",
      "Title: Deep Historical Borrowing Framework to Prospectively and Simultaneously Synthesize Control Information in Confirmatory Clinical Trials with Multiple Endpoints\n",
      "Summary: In current clinical trial development, historical information is receiving\n",
      "more attention as it provides utility beyond sample size calculation.\n",
      "Meta-analytic-predictive (MAP) priors and robust MAP priors have been proposed\n",
      "for prospectively borrowing historical data on a single endpoint. To\n",
      "simultaneously synthesize control information from multiple endpoints in\n",
      "confirmatory clinical trials, we propose to approximate posterior probabilities\n",
      "from a Bayesian hierarchical model and estimate critical values by deep\n",
      "learning to construct pre-specified strategies for hypothesis testing. This\n",
      "feature is important to ensure study integrity by establishing prospective\n",
      "decision functions before the trial conduct. Simulations are performed to show\n",
      "that our method properly controls family-wise error rate (FWER) and preserves\n",
      "power as compared with a typical practice of choosing constant critical values\n",
      "given a subset of null space. Satisfactory performance under prior-data\n",
      "conflict is also demonstrated. We further illustrate our method using a case\n",
      "study in Immunology.\n",
      "URL: http://arxiv.org/abs/2008.12774v2\n",
      "Authors: Tianyu Zhan, Yiwang Zhou, Ziqian Geng, Yihua Gu, Jian Kang, Li Wang, Xiaohong Huang, Elizabeth H. Slate\n",
      "--------------------------------------------------\n",
      "Paper 3:\n",
      "Title: Parametric Resonance May Explain Virologic Failure to HIV Treatment Interruptions\n",
      "Summary: Pilot studies of structured treatment interruptions (STI) in HIV therapy have\n",
      "shown that patients can maintain low viral loads whilst benefiting from reduced\n",
      "treatment toxicity. However, a recent STI clinical trial reported a high degree\n",
      "of virologic failure. Here we present a novel hypothesis that could explain\n",
      "virologic failure to STI and provides new insights of great clinical relevance.\n",
      "We analyze a classic mathematical model of HIV within-host viral dynamics and\n",
      "find that nonlinear parametric resonance occurs when STI are added to the\n",
      "model; resonance is observed as virologic failure. We use the model to simulate\n",
      "clinical trial data and to calculate patient-specific resonant spectra. We gain\n",
      "two important insights. Firstly, within an STI trial, we determine that\n",
      "patients who begin with similar viral loads can be expected to show extremely\n",
      "different virologic responses as a result of resonance. Thus, high\n",
      "heterogeneity of patient response within a STI clinical trial is to be\n",
      "expected. Secondly and more importantly, we determine that virologic failure is\n",
      "not simply due to STI or patient characteristics; rather it is the result of a\n",
      "complex dynamic interaction between STI and patient viral dynamics. Hence, our\n",
      "analyses demonstrate that no universal regimen with periodic interruptions will\n",
      "be effective for all patients. On the basis of our results, we suggest that\n",
      "immunologic and virologic parameters should be used to design patient-specific\n",
      "STI regimens.\n",
      "URL: http://arxiv.org/abs/q-bio/0504031v1\n",
      "Authors: Romulus Breban, Sally Blower\n",
      "--------------------------------------------------\n",
      "Number of papers: 30\n",
      "Number of chunks: 30\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import tarfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "#Uncomment and replace with your own data if desired\n",
    "search_query = \"immunology OR 'clinical trials' OR 'neuroscience'\"\n",
    "max_papers=30\n",
    "max_results = max_papers\n",
    "max_papers=30\n",
    "\n",
    "# Fetch papers from arXiv\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=search_query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for result in client.results(search):\n",
    "    docs.append(\n",
    "        {\"title\": result.title, \"summary\": result.summary, \"url\": result.entry_id, \"authors\": result.authors}\n",
    "    )\n",
    "docs_to_print = docs[:3]\n",
    "\n",
    "\n",
    "# Print the details of each paper in the docs list\n",
    "for i, doc in enumerate(docs_to_print, start=1):\n",
    "    authors_str = \", \".join([str(author) for author in doc['authors']])  # Convert authors to strings\n",
    "    print(f\"Paper {i}:\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Summary: {doc['summary']}\")\n",
    "    print(f\"URL: {doc['url']}\")\n",
    "    print(f\"Authors: {authors_str}\")  # Join the authors as a string\n",
    "    print(\"-\" * 50)  # Divider to separate the papers\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.create_documents(\n",
    "    [doc[\"summary\"]+\" \"+doc[\"title\"]+\"\"+str(doc[\"authors\"]) for doc in docs], metadatas=docs\n",
    ")\n",
    "\n",
    "print(f\"Number of papers: {len(docs)}\")\n",
    "print(f\"Number of chunks: {len(doc_splits)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we save the raw data files as a csv, which will then be loaded into an IRIS table in future steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file into the datapath\n",
    "data_path=\"/home/jovyan/workspace/data/\"\n",
    "filename=data_path+\"docs\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|title|abstract|url|authors\",file=file)\n",
    "    s=\"|,\"\n",
    "    for i,doc in enumerate(docs):\n",
    "        abstract=doc['summary'].replace(\"\\n\",' ')\n",
    "        title=doc['title']\n",
    "        try:\n",
    "            print(f\"{i}|{title}|{abstract}|{doc['url']}\",end=\"\",file=file)\n",
    "        except UnicodeEncodeError:\n",
    "            err=1\n",
    "        a=0\n",
    "        for author in doc[\"authors\"]:\n",
    "            auth=str(author).replace('\\u0107','').replace('\\u0131','').replace('\\u0142','').replace('\\u016b','').replace('\\u010d','')\n",
    "            auth=auth.replace('\\u0111','').replace('\\u015f','')\n",
    "            try:\n",
    "                print(f\"{s[a]}{auth}\",end=\"\",file=file)\n",
    "                a=1\n",
    "            except UnicodeEncodeError:\n",
    "                err=2\n",
    "        print(file=file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph documents: 30\n",
      "Nodes from 1st graph doc:[Node(id='Early Phase Clinical Trial', type='Topic', properties={'summary': 'An early phase clinical trial is the first step in evaluating the effects in humans of a potential new anti-disease agent or combination of agents.', 'title': 'Bayesian Models and Decision Algorithms for Complex Early Phase Clinical Trials'}), Node(id='Peter F. Thall', type='Author', properties={'author': 'Peter F. Thall'})]\n",
      "Relationships from 1st graph doc:[Relationship(source=Node(id='Bayesian Models And Decision Algorithms For Complex Early Phase Clinical Trials', type='Topic', properties={}), target=Node(id='Peter F. Thall', type='Author', properties={}), type='AUTHORED', properties={})]\n"
     ]
    }
   ],
   "source": [
    "# GraphRAG Setup\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "graph_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    "    allowed_nodes=[\"Paper\", \"Author\", \"Topic\"],\n",
    "    node_properties=[\"title\", \"summary\", \"url\", \"author\"],\n",
    "    allowed_relationships=[\"AUTHORED\", \"DISCUSSES\", \"RELATED_TO\"],\n",
    ")\n",
    "\n",
    "graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)\n",
    "\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename=data_path+\"entities\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|entityid|type\",file=file)\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        for node in doc.nodes:\n",
    "            try:\n",
    "                print(f\"{i}|{node.id}|{node.type}\",file=file)\n",
    "            except UnicodeEncodeError:\n",
    "                err=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=data_path+\"relations\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|source|sourcetype|target|targettype|type\",file=file)\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        for rel in doc.relationships:\n",
    "            try:\n",
    "                print(f\"{i}|{rel.source.id}|{rel.source.type}|{rel.target.id}|{rel.target.type}|{rel.type}\",file=file)\n",
    "            except UnicodeEncodeError:\n",
    "                err=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /opt/conda\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda env list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /opt/conda\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda env list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list --explicit > environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (945115591.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [40], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python app.py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <module 'threading' from '/opt/conda/lib/python3.10/threading.py'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1567, in _shutdown\n",
      "    lock.acquire()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/werkzeug/_reloader.py\", line 452, in <lambda>\n",
      "    signal.signal(signal.SIGTERM, lambda *args: sys.exit(0))\n",
      "SystemExit: 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
